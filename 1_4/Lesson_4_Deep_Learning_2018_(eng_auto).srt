1
00:00:00,060 --> 00:00:04,944
okay hi everybody welcome back let's see

2
00:00:05,044 --> 00:00:23,330
you all here it's been another busy week
of deep learning lots of cool things
going on and like last week I would have
to highlight a few really interesting
articles that some of some of you folks
have have written

3
00:00:24,480 --> 00:00:35,910
vitaliy wrote one of the best articles
I've seen for a while I think actually
talking about differential learning
rates and stochastic gradient descent
with restarts be sure to check it out if

4
00:00:39,960 --> 00:00:49,569
you can because what he's done I feel
like he's done a great job of kind of
positioning it a place that you can get
a lot out of it you know regardless of

5
00:00:49,669 --> 00:00:55,079
your background but for those who want
to go further he's also got links to
like the academic papers it came from

6
00:00:55,079 --> 00:01:08,755
and kind of rests of showing examples of
all of all the things he's talking about
and I think it's a it's a particularly
nicely done article so good kind of role
model for technical communication one of

7
00:01:08,855 --> 00:01:32,939
the things I've liked about you know
seeing people post these post these
articles during the week is the
discussion on the forums have also been
like really great there's been a lot of
a lot of people helping out like
explaining things you know which you
know maybe those parts of the post
period where people have said actually
that's not quite how it works and people
have learnt new things that way people
have come up with new ideas as a result

8
00:01:32,939 --> 00:01:49,590
as well these discussions of stochastic
gradient descent with restarts and
cyclic or learning rates just being a
few of them actually Anand
sahar has written another great post
talking about a similar similar topic

9
00:01:49,590 --> 00:02:01,670
and why it works so well and again lots
of great pictures and references to
papers and most importantly perhaps code
showing how it actually works

10
00:02:01,950 --> 00:02:07,600
mark Hoffman covered the same topic at
kind of a nice introductory level I
think really really kind of clear

11
00:02:09,580 --> 00:02:17,950
intuition many Cantor talk specifically
about differential learning rates and
why it's interesting and again providing

12
00:02:18,290 --> 00:02:31,019
some nice context to people not familiar
with transfer learning you're not going
right back to saying like or what is
transfer learning why is that
interesting and given that why good
differential learning rates be helpful

13
00:02:31,019 --> 00:02:59,829
and then one thing I particularly liked
about arjen's
article was that he talked not just
about the technology that we're looking
at but also talked about some of the
implications particularly from a
commercial point of view so thinking
about like based on some of the things
we've learned about so far what are some
of the implications that that has you
know in real life and lots of background
lots of pictures and then discussing
some of the yeah some of the

14
00:02:59,829 --> 00:03:06,904
implications so there's been lots of
great stuff online and thanks to
everybody for all the great work that
you've been doing as we talked about

15
00:03:07,004 --> 00:03:28,350
last week if you're kind of vaguely
wondering about writing something but
you're feeling a bit intimidated about
it because you've never really written a
technical post before just jump in you
know it's it's it's it's a really
welcoming and encouraging group I think
to to work with

16
00:03:30,690 --> 00:03:37,930
so we're going to have a kind of an
interesting lesson today which is we're
going to cover a whole lot of different

17
00:03:38,030 --> 00:03:51,090
applications so we've we've spent quite
a lot of time on computer vision and
today we're going to try if we can to
get through three totally different
areas structured learning so looking at

18
00:03:51,090 --> 00:04:09,180
kind of how you look at so we're going
to start out looking at structured
learning or structured data learning by
which I mean building models on top of
things look more like database tables so
kind of columns of different types of
data there might be financial or

19
00:04:09,180 --> 00:04:21,954
geographical or whatever we're going to
look at using deep learning for language
natural language processing and we're
going to look at using deep learning for
recommendation systems and so we're

20
00:04:22,054 --> 00:04:31,080
going to cover these at a very high
level and the focus will be on
here's how to use the software to do it
more then here is what's going on behind

21
00:04:33,510 --> 00:04:46,675
the scenes and then the next three
lessons we'll be digging into the
details of what's been going on behind
the scenes and also coming back to
looking at a lot of the details of
computer vision that we've kind of

22
00:04:46,775 --> 00:04:58,000
skipped over so far so the focus today
is really on like how do you actually do
these applications and we'll kind of
talk briefly about some of the concepts

23
00:04:58,100 --> 00:05:14,980
involved before we do I did want to talk
about one key new concept which is
dropout and you might have seen dropout
mentioned a bunch of times already and
got there got the impression that this
is something important and indeed it is

24
00:05:15,080 --> 00:05:21,145
so look at dropout I'm going to look at
the dog breeds current cable competition

25
00:05:21,245 --> 00:05:40,770
that's going on and what I've done is
I've gone ahead and I've created a pre
train network as per usual and I've
passed in pre compute equals true and so
that's going to pre compute the
activations that come out of the last
convolutional layer remember an

26
00:05:40,770 --> 00:05:57,960
activation is just a number
it's a number just to remind you an
activation like here is one activation
it's a number and specifically the
activations are calculated based on some
weights also called parameters that make

27
00:05:57,960 --> 00:06:09,460
up kernels or filters and they get
applied to the previous layers
activations but it could well be the
inputs or they could themselves be the
results of other calculations okay so

28
00:06:09,560 --> 00:06:14,210
when we say activation keep remembering
we're talking about a number that's
being calculated
so we've pre compute some activations

29
00:06:17,550 --> 00:06:27,480
and then what we do is we put on top of
that a bunch of additional initially
randomly generated fully connected
layers so we're just going to do some

30
00:06:27,480 --> 00:06:43,009
matrix multiplications on top of these
just like in our Excel worksheet at the
very end
we had this matrix that we just did a
matrix multiplication but so what you

31
00:06:43,009 --> 00:06:58,870
can actually do is if you just type the
name of your loner object you can
actually see what's in it you can see
the layers in it so when I was
previously been skipping over a little
bit about are we add a few layers to the
end these are actually the layers of yet

32
00:06:58,870 --> 00:07:02,900
we're going to do batch norm in the last
lesson so don't worry about that for now

33
00:07:02,900 --> 00:07:18,505
a linear layer simply means a matrix
multiply okay so this is a matrix which
has a 1024 rows and 512 columns and so
in other words it's going to take in
1024 activations and spit out 512

34
00:07:18,605 --> 00:07:26,750
activations then we have a rail unit
which remember is just replace the
negatives with 0 we'll skip over the

35
00:07:26,750 --> 00:07:38,594
batch norm we'll come back drop out then
we have a second linear layer that takes
those 512 activations from the previous
linear layer and puts them through a new
matrix multiply 5 12 by 120 it spits out

36
00:07:38,694 --> 00:07:48,829
a new 120 activations and then finally
put that through soft mats and for those
of you that don't remember softmax we
looked at that last year last week it's

37
00:07:48,929 --> 00:08:02,180
this idea that we basically just take
the the activation let's say the dog go
e to the power of that and then divide
that into the sum of e to the power of

38
00:08:02,180 --> 00:08:09,850
all the intermissions so that was the
thing that adds up to one all of them
add up to one and each one individually

39
00:08:09,950 --> 00:08:16,595
is between 0 and 1 ok so that's that's
what we added on top and that's the
thing when we have pre computed calls
true that's the thing we trained so I

40
00:08:16,695 --> 00:08:24,404
wanted to talk about what this dropout
is and what this key is because it's a
really important thing that we get to

41
00:08:24,504 --> 00:08:53,085
choose so a dropout layer with P equals
0.5 literally does this we go over to
our spreadsheet and let's pick any layer
with some activations and let's say ok
I'm going to apply dropout with a P of
0.5 to con true what that means is I go
through and with a 50% chance I pick a
cell right pick an activation so I kept
like
half of them randomly and I delete them

42
00:08:53,185 --> 00:09:03,970
okay that's that's what dropout is right
so it's so the P equals 0.5 means what's
the probability of deleting that cell

43
00:09:03,970 --> 00:09:14,924
all right so when I delete those cells
if you have a log like look at the
output it doesn't actually change by
very much at all just a little bit

44
00:09:15,024 --> 00:09:25,100
particularly because remember it's
getting through a Mac spalling layer
right so it's only going to change it at
all if it was actually the maximum in
that group of four and furthermore it's

45
00:09:25,100 --> 00:09:31,360
just one piece of you know if it's going
into a convolution rather than into a
max Paul is just one piece of that that

46
00:09:31,460 --> 00:09:53,060
filter so interestingly the idea of like
randomly throwing away half of the
activations in a layer has a really
interesting result and one important
thing to mention is each mini batch we
throw away a different random half of

47
00:09:53,060 --> 00:10:08,279
activations earlier and so what it means
is it forces it to not over fit right in
other words if there's some particular
activation that's really learnt just
that exact that exact dog or that exact

48
00:10:08,379 --> 00:10:18,405
cat right then when that gets dropped
out the whole thing now isn't going to
work as well it's not going to recognize
that image right so it has to in order

49
00:10:18,505 --> 00:10:31,485
for this to work it has to try and find
a representation that that actually
continues to work even as random half of
the activations get thrown away every

50
00:10:31,585 --> 00:10:50,310
time all right so it's a it's it's I
guess about four years old now three or
four years old and it's been absolutely
critical in making modern deep learning
work and the reason why is it really
just about solve the problem of

51
00:10:50,410 --> 00:11:12,079
generalization for us before drop out
came along if you try to train a model
with lots of parameters and you were
overfitting and you already tried all
the
imitation you could and you already had
as much data as you could you there were
some other things you could try but to a
large degree you were kind of stuck okay

52
00:11:12,079 --> 00:11:35,870
and so then Geoffrey Hinton and his
colleagues came up with this this
dropout idea that was loosely inspired
by the way the brain works and also
loosely inspired by Geoffrey Hinton's
experience in bank teller Hugh's
apparently and yeah somehow they came up
with this amazing idea of like hey let's
let's try throwing things away at random

53
00:11:35,870 --> 00:11:57,539
and so as you could imagine if your P
was like point O one then you're
throwing away 1% of your activations for
that layer at random it's not gonna
randomly change things up very much at
all so it's not really going to protect
you from overfitting much at all on the

54
00:11:57,639 --> 00:12:17,805
other hand if your pain was 0.99 then
that would be like going through the
whole thing and throwing away nearly
everything right and that would be very
hard for it to overfit so that would be
great for generalization but it's also
going to kill your accuracy so this is

55
00:12:17,905 --> 00:12:32,509
kind of play off between high p-values
generalized well but will decrease your
training accuracy and low p-values will
generalize less well that will give you
a less good training accuracy so for

56
00:12:32,509 --> 00:12:43,589
those of you that have been wondering
why is it that particularly early in
training my validation losses better
than my training losses but which seems
otherwise really surprising hopefully

57
00:12:43,689 --> 00:12:59,870
some of you have been wondering why that
is because on a data set that it never
gets to see you wouldn't expect the
losses to ever be that's better
and the reason why is because when we
look at the validation set we turn off
dropout right so in other words when

58
00:12:59,870 --> 00:13:10,529
you're doing inference when you're
trying to say is this or cat or is this
a dog we certainly don't want to be
including random drop out there right we
want to be using the best model we can

59
00:13:10,629 --> 00:13:26,520
okay so that's why early in training in
particular
actually see that our validation
accuracy and loss tends to be better if
we're using dropout okay so yes you know

60
00:13:26,520 --> 00:13:30,460
you have to do anything to accommodate
for the fact that you are throwing away

61
00:13:30,460 --> 00:13:41,375
some that's a great question so we don't
that PI torch does so PI torch behind
the scenes does two things if you say P

62
00:13:41,475 --> 00:13:47,600
equals point five it throws away half of
the activations but it also doubles all
the activations that are already there

63
00:13:47,700 --> 00:13:56,885
so when average the kind of the average
activation doesn't change which is

64
00:13:56,985 --> 00:14:06,520
pretty pretty neat trick so yeah you
don't have to worry about it basically
it's it's done for you so if we say so

65
00:14:06,520 --> 00:14:18,370
you can pass in peas this is the this is
the p value for all of the added layers
to say with first AI what dropout do you
want on each of the layers in these

66
00:14:18,370 --> 00:14:27,670
these added layers it won't change the
dropout in the pre trained network like
the hope is that that's already been
pretty trained with some appropriate
level of dropout we don't change it put

67
00:14:29,740 --> 00:14:33,220
on these layers that we add you can say
how much and so you can see here as a

68
00:14:33,220 --> 00:14:40,839
T's equals 0.5 so my first dropout has
0.5 my second dropout has 0.5 I remember

69
00:14:40,839 --> 00:14:46,390
coming to the input of this was the
output of the last convolutional layer
of pre-trained network and we go over it

70
00:14:48,820 --> 00:14:55,804
and we actually throw away half of that
before you can start go through our
linear layer throw away the negatives

71
00:14:55,904 --> 00:15:05,230
throw away half the result of that go
through another linear layer and then
pass it to our softness for minor

72
00:15:05,230 --> 00:15:11,950
numerical precision region reasons it
turns out to be better to take the log
of the softmax then softmax directly and

73
00:15:12,520 --> 00:15:22,480
that's why you'll have noticed that when
you actually get predictions out of our
models you always have to go npx both
the predictions but again the details as

74
00:15:22,480 --> 00:15:36,380
to why aren't important so if we want to
try removing dropout we could go peas
equals zero all right and you'll see
where else before we started with the
point seven six accuracy in the first
epoch now you could have point eight

75
00:15:36,380 --> 00:15:45,649
accuracy in the first debug alright so
by not doing drop out our first teapot
worked better not surprisingly because
we're not throwing anything away but by

76
00:15:45,649 --> 00:15:52,350
the third epoch here we had eighty four
point eight and here we have eighty four
point one so it started out better and

77
00:15:52,450 --> 00:16:02,489
ended up worse so even after three
epochs you can already see where master
for overfitting right we've got point
three loss on the train and point five

78
00:16:02,589 --> 00:16:15,975
loss on the validation yep and so if you
look now you can see in the resulting
model there's no drop out at all so if
the P is zero we don't even add it to

79
00:16:16,075 --> 00:16:29,450
the model another thing to mention is
you might have noticed that what we've
been doing is we've been adding two
linear layers right in our additional
layers you don't have to do that by the

80
00:16:29,450 --> 00:16:41,720
way there's actually a parameter called
extra fully connected layers that you
can basically pass a list of how long do
you want or how big do you want each of
the additional fully connected layers to

81
00:16:41,720 --> 00:16:51,170
be and so by default well you need to
have at least one right because you need
something that takes the output of the
convolutional layer which in this case

82
00:16:51,170 --> 00:16:56,660
is of size thousand twenty-four and
turns it into the number of classes you

83
00:16:56,660 --> 00:17:05,024
have cats versus dogs would be two dog
breeds would be 120 planet satellite
seventeen whatever that's you always

84
00:17:05,124 --> 00:17:14,445
need one linear layer at least and you
can't pick how big that is that's
defined by your problem but you can
choose what the other size is or if it

85
00:17:14,545 --> 00:17:25,880
happens at all so if we were to pass in
an empty list and now we're saying don't
add any additional mini layers just the
one that we have to have right so here
we've got P is equals zero

86
00:17:25,880 --> 00:17:35,080
extra fully connected layers is empty
this is like the minimum possible kind
of top model we can put on top and again

87
00:17:36,800 --> 00:17:53,059
like if we do that
you can see above we actually end up
with in this case a reasonably good
result because we're not training it for
very long and this particular
pre-trained Network is really well
suited to this particular problem

88
00:17:53,059 --> 00:18:01,640
yesterday so Jeremy what kind of piece
should we were using by default so the

89
00:18:01,640 --> 00:18:20,710
one that's there by default for the
first layer is 0.25 and for the second
layer is 0.5 that seems to work pretty
well for most things right so like it's
it's it you don't necessarily need to
change it at all

90
00:18:20,710 --> 00:18:38,080
basically if you find it's overfitting
just start bumping it up so try first of
all setting it to 0.5 that'll set them
both to 0.5 if it still overfitting a
lot try 0.7 like you can you can narrow
down and like it's not that many numbers

91
00:18:38,080 --> 00:18:47,320
change right and if you're under fitting
then you can try and making it lower
it's unlikely you would need to make it
much lower because like even in these

92
00:18:47,420 --> 00:19:06,089
dogs versus cats situations you know we
don't see they have to make it lower so
it's more likely to be increasing at
about 0.6 0.7 but you can fiddle around
I find these the ones that are there by
defaults in work pretty well most of the

93
00:19:06,189 --> 00:19:21,689
time so one place I actually did
increase this was in the dog breeds one
I did set it them both to 0.5 when I
used a bigger model so like ResNet 34
has less parameters so it doesn't over

94
00:19:21,789 --> 00:19:32,984
fit as much but then when I started
bumping pumping it up to like a resonate
50 which has a lot more parameters and
noticed it started overfitting so then I
also increased my drop out so as you use

95
00:19:33,084 --> 00:19:41,350
like bigger models you'll often need to
add more drama can you pass it over
there please you know

96
00:19:42,000 --> 00:19:49,169
if we set B to 0.5 roughly what
percentage is it 50%

97
00:19:49,169 --> 00:20:01,174
we say RP pasta
is there a particular way in which you
can determine if the data is being all
fitted yeah you can see that the like

98
00:20:01,474 --> 00:20:11,889
here you can see that the training error
is a loss is much lower than the
validation list you can't tell if it's

99
00:20:11,889 --> 00:20:23,914
like to over fitted like zero
overfitting is not generally optimal
like the only way to find that out is
remember the only thing you're trying to
do is to get this number low right the

100
00:20:24,014 --> 00:20:38,919
validation loss number low so in the end
you kind of have to play around with a
few different things and see which thing
ends up getting the validation loss low
but you kind of get a feel overtime for
your particular problem what does
overfitting what does too much River

101
00:20:38,919 --> 00:20:51,376
fitting look like great so so that's
dropout and we're going to be using that
a lot and remember it's there by default
service here another question

102
00:20:51,476 --> 00:21:07,809
oh so I have two questions so one is so
when it says the dropout rate is 0.5 it
does it like you know I delete each cell
with a probability of 0.5 or does it

103
00:21:07,809 --> 00:21:12,639
just pick 50% randomly I mean I know
both effectively is the 4-month yeah

104
00:21:12,639 --> 00:21:19,960
okay okay a second question is why does
the average activation matter well it

105
00:21:19,960 --> 00:21:40,139
matters because the remember if you look
the Excel spreadsheet that the result of
this cell for example is equal to these
nine multiplied by each of these nine

106
00:21:40,139 --> 00:21:52,624
right and add it up so if we deleted
half of these then that would also cause
this number to half which would cause
like everything else after that to
change and so if you change what it

107
00:21:52,724 --> 00:22:03,970
means you know like you then you're
changing something that used to say like
Oh fluffy ears are fluffy if this is
greater than point six now it's only
fluffy if it's greater than point three
like we're changing the meaning of

108
00:22:04,070 --> 00:22:09,825
everything so you
here is to delete things without

109
00:22:09,925 --> 00:22:19,400
changing where are you using a linear
activation for one of the earlier
activations why are we using when you

110
00:22:19,400 --> 00:22:42,260
yeah why that particular activation
because that's what this set of layers
is so we've with the the pre-trained
network is or is the convolutional net
work and that's pre computed so we don't
see it so what that spits out is it's a
vector so the only choice we have is to
use linear layers at this point okay can

111
00:22:42,260 --> 00:22:47,690
we have different level of dropout by
layer yes absolutely how to do that

112
00:22:47,690 --> 00:23:00,797
great so so you can absolutely have
different dropout by layer and that's
why this is actually called peas so you
could pass in an array here so if I went
0 comma 0.2 for example and then extra

113
00:23:00,897 --> 00:23:13,650
fully connecting it I might add 512
right then that's going to be 0 drop out
before the first of them and point to
drop out before the second of them yes

114
00:23:13,750 --> 00:23:28,100
requests and I must admit I don't have a
great intuition even after doing this
for a few years for like when should
earlier or later layers have different
amounts of dropping out it's still
something I kind of play with and I

115
00:23:30,260 --> 00:23:35,690
can't quite find rules of thumb so if
some of you come up with some good rules
of thumb I'd love to hear about them I

116
00:23:35,790 --> 00:23:52,450
think if in doubt you can use the same
drop out and every fully connected layer
the other thing you can try is often
people only put drop out on the very
last linear layer so there'd be the two
things to try

117
00:23:53,670 --> 00:24:04,565
so Jeremy why do you monitor the log
loss the loss instead of the accuracy
going up well because the loss is the

118
00:24:04,665 --> 00:24:13,920
only thing that we can see for both the
validation set in the training set so
it's nice to be able to compare them

119
00:24:13,920 --> 00:24:31,310
also as we learn about later the loss is
the thing that we're actually optimizing
so it's it's kind of a little more it's
a little easier to monitor that and
understand what that means

120
00:24:31,410 --> 00:24:44,225
can you pass it over there so with the
drop out we're kind of adding some
random noise every iteration right you
know so that means that we don't do as
much learning yeah that's right so it

121
00:24:44,325 --> 00:25:01,415
doesn't seem to impact the learning rate
enough thrive ever noticed it I I would
say you're probably right in theory it
might but not enough that it's ever

122
00:25:01,515 --> 00:25:13,600
affected me okay so let's talk about
this structured data problem and so to
remind you we were looking at kegels

123
00:25:13,600 --> 00:25:26,060
rossmann competition which is a German
chain of supermarkets I believe and you
can find this in lesson three Russman

124
00:25:26,160 --> 00:25:36,880
and the main data set is the one where
we were looking to say at a particular
store how much did they sell okay and

125
00:25:36,880 --> 00:25:53,890
there's a few big key piece of
information one is what was the date
another was were they open
did they have a promotion on was it a
holiday in that state and was it a
holiday as for school a state holiday
there wasn't a school holiday yeah and

126
00:25:53,890 --> 00:26:04,750
then we had some more information about
stores like what for this store what
kind of stuff did they tend to sell what
kind of store are they how far away the
competition and so forth so with the

127
00:26:04,750 --> 00:26:11,590
data
set like this there's really two main
kinds of column there's columns that we
think of as categorical they have a

128
00:26:11,590 --> 00:26:19,715
number of levels
so the assortment column is categorical
and it has levels such as a B and C

129
00:26:19,815 --> 00:26:27,910
where else something like competition
distance we will call continuous it has
a number attached to it where

130
00:26:27,910 --> 00:26:32,380
differences or ratios even if that
number have some kind of meaning and so

131
00:26:32,380 --> 00:26:37,720
we need to deal with these two things
quite differently okay so anybody who's

132
00:26:37,720 --> 00:26:49,230
done any machine learning of any kind
will be familiar with using continuous
columns if you've done any linear
regression for example you can just like
modify them by parameters for instance

133
00:26:49,230 --> 00:26:54,850
categorical columns we're going to have
to think about a little bit more we're

134
00:26:54,850 --> 00:27:06,944
not going to go through the data
cleaning we're going to assume that
that's a feature Engineering we're going
to assume all that's been done and so
basically at the end of that we have a

135
00:27:07,044 --> 00:27:29,045
list of columns and the in this case I
didn't do any of the thinking around the
feature engineering or dedicating myself
this is all directly from the
third-place winners of this competition
and so they came up with all of these
different columns that they found useful

136
00:27:29,145 --> 00:27:46,660
and so you'll notice the list here is a
list of the things that we're going to
treat as categorical variables numbers
like year a month and day although we
could treat them as continuous like they

137
00:27:46,660 --> 00:27:52,179
the different you know differences
between 2000 and 2003 is meaningful we

138
00:27:52,179 --> 00:28:14,570
don't have to right and you'll see
shortly how how categorical variables
are treated but basically if we decide
to make something a categorical variable
what we're telling our neural net down
the track is that for every different
level of say year you know 2000 2001
2002 you can treat it totally

139
00:28:14,670 --> 00:28:23,109
differently where else if we say it's
continuous its
have to come up with some kind of like
function some kind of smooth ish

140
00:28:23,209 --> 00:28:36,415
function right and so often even for
things like a year that actually are
continuous but they don't actually have
many distinct levels it often works
better to treat it as categorical so

141
00:28:36,515 --> 00:28:47,529
another good example day of week right
so like day of week between naught & 6
it's a number and it means something
motifs between 3 & 5 is two days and has

142
00:28:47,629 --> 00:28:56,609
meaning but if you think about like how
word sales in a strawberry buy a day of
week it could well be that like you know

143
00:28:56,609 --> 00:29:05,700
Saturdays and Sundays are over here and
Fridays are over here and Wednesdays are
over here like each day is going to
behave kind of qualitatively differently

144
00:29:05,700 --> 00:29:12,909
right so by saying this is the
categorical variable as you'll see we're
going to let the neural-net do that

145
00:29:13,009 --> 00:29:24,399
right so this thing where we get where
we say which are continuous in which a
categorical to some extent this is the
modeling decision you get to make now if

146
00:29:24,499 --> 00:29:37,619
something is coded in your data is like
a B and C or you know Jeremy and you
knit or whatever you actually you're
going to have to call that categorical
right there's no way to treat that

147
00:29:37,619 --> 00:29:51,415
directly as a continuous variable on the
other hand if it starts out as a
continuous variable like age or day of
week you get to decide whether you want
to treat it as continuous or categorical

148
00:29:51,515 --> 00:29:59,199
okay so summarize if it's categorical
and data it's going to have to be
categorical in the model if it's
continuous in the data you get to pick
whether to make it continuous or

149
00:29:59,299 --> 00:30:15,594
categorical in the model so in this case
again what I just did whatever the
third-place winners of this competition
did these are the ones that they decided
to use as categorical these were the
ones they decided to use as continuous

150
00:30:15,694 --> 00:30:29,545
and you can see that basically the
continuous ones are all of the ones
which are actual floating-point numbers
like competition distance actually has a
decimal place to it right and
temperature actually has a decimal place

151
00:30:29,645 --> 00:30:46,300
to it so these would be very hard to
make
categorical because they have many many
levels right like if it's like five
digits of floating-point then
potentially there will be as many levels
as there are as there are roads and by

152
00:30:46,300 --> 00:30:54,520
the way the word we use to say how many
levels are in a category we use the word
cardinality right so if you see me say
cardinality example the cardinality of

153
00:30:54,520 --> 00:31:00,390
the day of week variable is 7 because
there are 7 different days of the week

154
00:31:02,010 --> 00:31:09,165
do you have a heuristic for one to have
been continuous variables or do you ever
in variables I don't ever been

155
00:31:09,265 --> 00:31:13,033
continuous variables so yeah so one

156
00:31:13,133 --> 00:31:23,140
thing we could do with like max
temperature is group it into nought to
10 10 to 20 20 to 30 and then call that
categorical interestingly a paper just

157
00:31:23,140 --> 00:31:36,160
came out last week in which a group of
researchers found that sometimes bidding
can be helpful but it literally came out
in the last week and until that time I
haven't seen anything in deep learning
saying that so I haven't I haven't

158
00:31:36,260 --> 00:31:44,507
looked at it myself until this week I
would have said it's a bad idea now I
have to think differently I guess maybe

159
00:31:44,607 --> 00:32:00,880
it is sometimes so if you're using year
as a category what happens when you run
the model of a year it's never seen so
your training will get there yeah the

160
00:32:00,880 --> 00:32:16,760
short answer is it will be treated as an
unknown category and so pandas which is
the underlying data frame thinking we're
using with categories as a special
category called unknown and if it stays
a category it hasn't seen before it gets

161
00:32:16,860 --> 00:32:24,840
treated as unknown so for AB deep
learning model unknown will just be
another category

162
00:32:25,130 --> 00:32:35,980
if our data set training the data set
doesn't have a category and test has
unknown how will it did you know just
paper this unknown category it's still

163
00:32:36,080 --> 00:32:46,679
predict it will predict something right
like it will just have the value 0 barn
scenes and if there's been any unknowns

164
00:32:46,679 --> 00:32:55,164
of any kind in the training set then it
off learnt a way to predict unknown if
it hasn't it's going to have some random

165
00:32:55,264 --> 00:33:05,529
vector and so that's a interesting
detail around training that we probably
want to talk about in this part of the
course but we can certainly talk about
on the forum

166
00:33:05,529 --> 00:33:21,764
okay so we've got our categorical and
continuous variable lists defined in
this case there was eight hundred
thousand rows so eight hundred thousand
dates basically by Storz and so you can

167
00:33:21,864 --> 00:33:36,835
now take all of these columns look
through each one and replace it in the
data frame where the version where you
say take it and change its type to
category okay and so that just that just

168
00:33:36,935 --> 00:33:48,379
a panda's things so I'm not going to
teach you
pandas there's plenty of books so
particularly with McKinney's books book
on python for data analysis is great but

169
00:33:48,379 --> 00:33:53,564
hopefully it's intuitive as to what's
going on even if you haven't seen the
specific syntax before so we're going to

170
00:33:53,664 --> 00:34:04,149
turn that column into a categorical
column and then for the continuous
variables we're going to make them all
32-bit floating-point and for the reason

171
00:34:04,249 --> 00:34:10,569
for that is that pipe torch expects
everything to be 32-bit floating-point

172
00:34:10,669 --> 00:34:30,584
okay so like some of these include like
1 0 things like I can't see them
straight away but anyway so much yeah
like was there a promo was was a holiday
and so that'll become the floating point
values 1 and 0 for instance ok so I try

173
00:34:30,684 --> 00:34:44,339
to do as much of my work as possible
on small data sets for when I'm working
with images that generally means
resizing the images to like 64 by 64 or

174
00:34:44,439 --> 00:34:52,460
128 by 128 we can't do that with
structured data so instead I tend to
take a sample so I randomly pick a few

175
00:34:52,460 --> 00:35:07,609
rows so I start running with a sample
and I can use exactly the same thing
that we've seen before for getting a
validation set we can use the same way
to get some random random row numbers to
use in a random sample okay so this is

176
00:35:07,609 --> 00:35:21,300
just a bunch of random numbers
and then okay so that's going to be a
size 150,000 rather than 800 40,000 and

177
00:35:21,300 --> 00:35:32,910
so my data that before I go any further
it basically looks like this you can see
I've got some boolean x' here I've got
some integers here of various different
scales here's my year 2014 and I've got

178
00:35:37,020 --> 00:35:52,480
some letters here so even though I said
please call that a pandas category
pandas still displays that in the
notebook as strings right it's just
stored in internally differently so then

179
00:35:52,580 --> 00:36:05,070
the first day our library has a special
little function called processed data
frame and process data frame takes a
data frame and you tell it what's my
dependent variable right and it does a

180
00:36:05,070 --> 00:36:22,589
few different things the first thing is
it's pulled out that dependent variable
and puts it into a separate variable
okay and deletes it from the original
data frame so DF now does not have the
sales column in where else Y just
contains a sales column something else

181
00:36:22,589 --> 00:36:36,865
that it does is it does scaling so
neural nets really like to have the
input data to all be somewhere around
zero with a standard deviation of
somewhere around one all right so we can

182
00:36:36,965 --> 00:36:43,650
always take our data and subtract the
mean and divide by the standard
deviation to make that happen so that's

183
00:36:43,650 --> 00:36:51,900
what do see a littles true that's and it
actually returns a special object which
keeps track of what mean and standard
deviation did it use for that

184
00:36:51,900 --> 00:36:57,780
normalizing so you can then do the same
thing to the test set later it also

185
00:36:57,780 --> 00:37:08,669
handles missing values so missing values
and categorical variables just become
the ID 0 and then all the other
categories become 1 2 3 4 5 4 that

186
00:37:08,769 --> 00:37:21,952
categorical variable for continuous
variables
it replaces the missing value with the
median and creates a new column that's a
boolean and just says is this missing or

187
00:37:22,052 --> 00:37:29,369
not and I'm gonna skip over this pretty
quickly because we talked about this in
detail
the machine learning course okay so if

188
00:37:29,369 --> 00:37:36,140
you've got any questions about this part
that would be a good place to go it's
nothing deep learning specific there so

189
00:37:36,900 --> 00:37:42,330
you can see afterwards year 2014 for
example has become year two okay because

190
00:37:42,330 --> 00:37:47,220
these categorical variables have all
been replaced with with contiguous

191
00:37:47,220 --> 00:37:59,560
integers starting at zero and the reason
for that is later on we're going to be
putting them into a matrix right and so
we wouldn't want the matrix to be 2014
rows long when it could just be two rows

192
00:37:59,660 --> 00:38:11,950
one there so that's the basic idea there
and you'll see that the AC for example
has been replaced in the same way with
one and three okay so we now have a data

193
00:38:12,050 --> 00:38:20,220
frame which does not contain the
dependent variable and where everything
is a number okay and so that's that

194
00:38:20,220 --> 00:38:35,050
that's where we need to get to to do
deep learning and all of the stage about
that as I said we talked about in detail
in the machine learning course nothing
deep learning specific about any of it
this is exactly what we throw into our
random forests as well so another thing

195
00:38:35,150 --> 00:38:41,200
we talk about a lot in the machine
learning core of course is validation

196
00:38:41,300 --> 00:38:54,510
sets in this case we need to predict the
next two weeks of sales right it's not
like pick a random set of sales but we
have to pick the next two weeks of sales
that was what the cattle competition

197
00:38:54,510 --> 00:39:05,755
folks told us to do and therefore I'm
going to create a validation set which
is the last two weeks of my training set
right to try and make it as similar to
the test set as possible and we just

198
00:39:05,855 --> 00:39:18,279
posted actually Rachel wrote this thing
last week about creating validation sets
so if you go too fast at AI you can
check it out we'll put that in the

199
00:39:18,379 --> 00:39:34,460
lesson wiki as well but it's basically a
summary of a recent machine learning
lesson that we did the videos are
available for that as well and this is
kind of a written a written summary of
it okay

200
00:39:35,579 --> 00:39:42,125
so yeah so Rachel and I spend a lot of
time thinking about kind of you know how
do you need to think about validation
sets and training sets and test sets and

201
00:39:42,225 --> 00:39:50,710
so forth and that's all there but again
nothing deep learning specific so let's
get straight to the deep learning action

202
00:39:50,710 --> 00:40:05,479
okay so in this particular competition
as always with any competition or any
kind of machine learning project you
really need to make sure you have a
strong understanding of your metric how

203
00:40:05,579 --> 00:40:17,770
are you going to be judged here and in
this case you know Carol makes it easy
they tell us how we're going to be
judged and so we're going to be judged
on the roots mean squared percentage
error right so we're gonna say like oh

204
00:40:17,770 --> 00:40:25,245
you predicted three it was actually
three point three so you were can sent
out and then we're gonna average all

205
00:40:25,345 --> 00:40:47,989
those percents right and remember I
warned you that you are gonna need to
make sure you know logarithms really
well right and so in this case from you
know we're basically being saying your
prediction divided by the actual the
mean of that right is the thing that we

206
00:40:48,089 --> 00:41:01,930
care about and so we don't have a metric
in play torch called root mean squared
percent error we could actually easily
create it by the way if you look at the
source code you'll see like it's you

207
00:41:01,930 --> 00:41:22,380
know a line of code but easiest deal
would be to realize that that if you
have that right then you could replace a
with like log of a dash and be with like
log of B dash and then you can replace
that whole thing with a subtraction

208
00:41:22,380 --> 00:41:41,990
that's just the rule of loaves right and
so if you don't know that rule then
don't make sure you go look it up
because it's super helpful but it means
in this case all we need to do is to
take the log of our data which I
actually did earlier in this notebook
and when you take the log of the data

209
00:41:42,090 --> 00:42:04,510
getting the root mean squared error will
actually get you there
means great percent error for free okay
but then when we want to like print out
our it means percent error we actually
have to go e ^ it again right and then
we can actually return the percent
difference so that's all that's going on

210
00:42:04,510 --> 00:42:11,020
here it's again not really deep learning
specific at all so here we finally get
to the deep learning alright so as per

211
00:42:14,530 --> 00:42:19,162
usual like you'll see everything we look
at today looks exactly the same as
everything we've looked at so far which

212
00:42:19,262 --> 00:42:28,185
is first we create a model data object
something that has a validation set
training set and optional test set built

213
00:42:28,285 --> 00:42:41,110
into it from that we will get a learner
we will then optionally called learner
dot LR find real then called learner dot
fetch it'll be all the same parameters
and everything that you've seen many

214
00:42:41,110 --> 00:42:55,030
times before okay so the difference
though is obviously we're not going to
go image classify a data dot from CSV or
dot from paths we need to get some
different kind of model data and so for

215
00:42:55,030 --> 00:43:08,765
stuff that is in rows and columns we use
columnar model data but this will return
an object with basically the same API
that you're familiar with and rather
than from paths or from CSV this is from

216
00:43:08,865 --> 00:43:24,590
data frame okay so this gets passed a
few things the path here is just used
for it to know where should it store
like model files or stuff like that
right this is just basically saying
where do you want to store anything that

217
00:43:24,690 --> 00:43:29,075
you saved later this is the list of the
indexes of the rows that we want to put
in the validation set we created earlier

218
00:43:29,175 --> 00:43:51,290
here's our data frame okay and then look
here's this is where we did the log
right so I took the the Y that came out
of property F our dependent variable I
logged it and I call that yl all right
so we tell it when we create our model
data we need to tell it that's our

219
00:43:51,390 --> 00:44:03,380
dependent variable okay so so far we've
got most of the stuff from the
validation set which is what's our
independent variables
how dependent variables and then we have

220
00:44:03,380 --> 00:44:19,769
to tell it which things do we want
traded as categorical right because
remember by this time everything's a
number
right so it could do the whole things
it's continuous it would just be totally

221
00:44:19,769 --> 00:44:30,469
meaningless right so we need to tell it
which things do we want to treat as
categories and so here we just pass in
that list of names that we used before

222
00:44:30,469 --> 00:44:39,469
okay and then a bunch of the parameters
are the same as the ones you're used to
for example you can set the batch size

223
00:44:39,569 --> 00:45:04,349
yeah so after we do that we've got a
little standard model data object but
there's a trained DL attribute there's a
Val DL attribute a trained es attribute
of LDS attribute it's got a length it's
got all the stuff exactly like it did in
all of our image based data objects okay

224
00:45:04,349 --> 00:45:18,709
so now we need to create the the model
or create the learner and so to skip
ahead a little bit we're basically going
to pass in something that looks pretty
familiar we're going to be passing thing
from our model from our model data
create a learner that is suitable for it

225
00:45:21,769 --> 00:45:37,614
and will basically be passing in a few
other bits of information which will
include how much dropout to use at the
very start how many how many activations
to have in each layer
how much dropout to use at the later

226
00:45:37,714 --> 00:45:53,519
layers but then there's a couple of
extra things that we need to learn about
and specifically it's this thing called
embeddings so this is really the key new
concept we have to learn about all right

227
00:45:53,519 --> 00:46:05,549
so all we're doing basically is we're
going to take our let's forget about
categorical variables for a moment and
just think about the continuous

228
00:46:05,549 --> 00:46:29,740
variables for our continuous variables
all we're going to do is we're going to
grab them all
okay so for our continuous variables
we're basically going to say like okay
here's a big list of all of our
continuous variables like the minimum
temperature and the maximum temperature
and the distance to the nearest
competitor and so forth right and so

229
00:46:32,589 --> 00:46:36,700
here's just a bunch of floating-point
numbers and so basically what the neuron

230
00:46:36,700 --> 00:46:54,130
that's going to do is going to take that
that 1d array or or vector or to be very
DL like rank one tensor or means the
same thing
okay so we're going to take our egg one
tensor and let's put it through a matrix

231
00:46:54,130 --> 00:47:12,940
multiplication so let's say this has got
like I don't know 20 continuous
variables and then we can put it through
a matrix which must have 20 rows that's
how matrix multiplication works and then
we can decide how many columns we want
right so maybe we decided 100 right and

232
00:47:12,940 --> 00:47:28,410
so that matrix model captions going to
spit out a new length 100 rank 1 tensor
okay
that's that's what that's what a linear
that's what a matrix product does and
that's the definition of a linear layer

233
00:47:28,410 --> 00:47:39,670
indeed what okay and so then the next
thing we do is we can put that through a
rail you right which means we throw away
the negatives okay and now we can put

234
00:47:39,670 --> 00:47:59,349
that through another matrix product okay
so this is going to have to have a
hundred rows by definition and we can
have as many columns as we like and so
let's say maybe this was the last layer
so the next thing we're trying to do is
to predict sales so there's just one
value we're trying to predict for sales

235
00:47:59,349 --> 00:48:13,444
so we could put it through a matrix
product that just had one column and
that's going to spit out a single number
all right so that's like that's kind of
like a one layer neural net if you like

236
00:48:13,544 --> 00:48:25,070
now in practice you know we wouldn't
make it one layer so we would actually
have leg
you know maybe we'd have 50 here and so

237
00:48:25,170 --> 00:48:42,420
then that gives us a 50 long vector and
then maybe we then put that into our
final 50 by one and that's if it's out a
single number and one reason I would

238
00:48:42,420 --> 00:49:06,119
have to change that there was to point
out you know rally you would never put
rally you in the last layer
I could never want to throw away the
negatives because that the softmax let's
go back to the softness the soft max
needs negatives in it because it's the
negatives that are the things that allow
it to create low probabilities that's
minor detail but it's useful to remember

239
00:49:13,390 --> 00:49:22,400
so basically a simple view of a fully
connected euro net is something that

240
00:49:22,400 --> 00:49:43,579
takes in as an input a rank one tensor
it's bits it's through a linear layer an
activation layer another linear layer
softmax and that's the output okay and

241
00:49:43,579 --> 00:50:02,839
so we could obviously decide to add more
linear layers we could decide maybe to
add dropout all right so these are some
of the decisions that we need we get to
make right but we there's not that much
we can do right there's not much really
crazy architecture stuff to do so when

242
00:50:02,839 --> 00:50:12,619
we come back to image models later in
the course we're going to learn about
all the weird things that go on and like
resonates and inception networks and but

243
00:50:12,619 --> 00:50:27,170
in these fully connected networks
they're really pretty simple they're
just in dispersed
linear layers that is matrix products
and activation functions like value and
a soft mix at the edge and if it's not

244
00:50:27,170 --> 00:50:42,434
classification which actually ours is
not classification in this case we're
trying to predict sales there isn't even
a soft mix right we don't want it to be
between 0 and 1 ok so we can just throw
away the last activation altogether if

245
00:50:42,534 --> 00:50:51,440
we have time we can talk about a slight
trick we can do there but for now we can
think of it that way so that was all

246
00:50:51,440 --> 00:50:56,420
assuming that everything was continuous
right but what about categorical right

247
00:50:56,420 --> 00:51:06,019
so we've got like day of week right and
we're going to treat it as categorical
practice like Saturday Sunday Monday

248
00:51:11,259 --> 00:51:24,252
that should be 6
ready okay how do we feed that in
because I want to find a way of getting
that in so that we still end up with a
wreck one tends to refloat and so the

249
00:51:24,352 --> 00:51:45,327
trick is this we create a new little
matrix of with seven rows and as many
columns as we choose right so let's pick
four all right so here's our seven rows
and four columns right and basically

250
00:51:45,427 --> 00:51:57,060
what we do is let's add our categorical
variables to the end so let's say the
first row was Sunday right then what we

251
00:51:57,160 --> 00:52:12,239
do is we do a lookup into this matrix we
say oh here's sunday we do and look up
into here and we grab this row and so
this matrix we basically fill with
floating-point numbers so we're going to

252
00:52:12,339 --> 00:52:29,055
end up grabbing little subset of for
floating-point numbers at Sunday's
particular for floating-point numbers
and so that way we convert Sunday into a
rank 1 tensor of for floating-point

253
00:52:29,155 --> 00:52:40,990
numbers and initially those four numbers
are random all right and in fact this
whole thing we initially start out
random okay but then we're going to put

254
00:52:41,090 --> 00:52:57,945
that through our neural net right so we
basically then take those four numbers
and we remove sunday instead we add our
four numbers on here right so we've
turned our categorical thing into a
floating-point vector and so now we can

255
00:52:58,045 --> 00:53:28,730
just put that throughout neural net just
like before and at the very end we found
out the loss and then we can figure out
which direction is down and do gradient
descent in that direction and eventually
that will find its way back to this
little list of four numbers and it'll
say okay those random numbers weren't
very good this one needs to go up a bit
that one is to go up a bit that one is
to go down a bit that one is to go up a
bit and so will actually update our
original those four numbers in that

256
00:53:28,730 --> 00:53:42,645
match
and we'll do this again and again and
again and so this this matrix will stop
looking random and it will start looking
more and more like like the exact four
numbers that happen to work best for
Sunday the exact four numbers that

257
00:53:42,745 --> 00:53:52,300
happen to work best for Friday and so
forth
and so in other words this matrix is
just another bunch of weights in our

258
00:53:52,400 --> 00:54:01,480
neural net all right and so matrices of
this type are called embedding matrices

259
00:54:03,640 --> 00:54:20,490
so an embedding matrix is something
where we start out with an integer
between zero and the maximum number of
levels of that category we literally
index into a matrix to find a particular

260
00:54:20,590 --> 00:54:37,070
row so if it was the level was one we
take the first row we grab that road and
we append it to all of our continuous
variables and so we now have a new
vector of continuous variables and when

261
00:54:37,070 --> 00:54:50,910
we can do the same thing so let's say
zip code right so we could like have an
embedding matrix let's say there are
5,000 zip codes it would be 5,000 rows
long as wide as we decide maybe it's 50

262
00:54:51,010 --> 00:54:56,097
wide and so we'd say ok here's 9 4 0 0 3
that zip code is index number 4 you know

263
00:54:56,197 --> 00:55:04,905
matrix ordered out and we'd find the
fourth row regret those 50 numbers and
append those on to our big vector and

264
00:55:05,005 --> 00:55:10,635
then everything after that is just the
same we just put it through our linear

265
00:55:10,735 --> 00:55:15,080
layer a linear layer whatever what are
those 4 numbers represent that's a great

266
00:55:15,180 --> 00:55:34,965
question and we'll learn more about that
when we look at collaborative filtering
but now they represent no more or no
less than any other parameter in a
neural net you know they're just they're
just parameters that we're learning that
happen to end up giving us a good loss

267
00:55:35,065 --> 00:55:47,270
we will discover later that these
particular parameters often however are
human interpretive all and quote can
quite interesting but that's a side
effect of them it's not fundamental

268
00:55:47,270 --> 00:55:52,099
they're just for random numbers for now
that we're that we're learning or sets

269
00:55:52,099 --> 00:56:03,059
of four random numbers to have a good
heuristic for at the dimensionality of
embedding matrix so why four here I sure

270
00:56:03,140 --> 00:56:18,164
do so what I first of all did was I made
a little list of every categorical
variable and its cardinality okay so

271
00:56:18,264 --> 00:56:29,089
they're they allow so there's a hundred
and there's a thousand plus different
stores
apparently in Rothman's Network
there are eight days of the week that's

272
00:56:29,089 --> 00:56:42,074
because there are seven days of the week
plus one left over for unknown even if
there were no missing values in the
original data I always still set aside
one just in case there's a missing or an
unknown or something different in the

273
00:56:42,174 --> 00:56:47,835
test set again for years but there's
actually three plus room for an unknown
and so forth right so what I do my rule

274
00:56:47,935 --> 00:57:04,180
of thumb is this take the cardinality
with the variable divide it by two but
don't make it bigger than 50 okay so

275
00:57:04,180 --> 00:57:12,780
these are my embedding matrices so my
store matrix so there has to have a
thousand one hundred and sixteen rows

276
00:57:12,880 --> 00:57:20,900
cuz I need to look up right to find his
store number three and then it's been a
return back a rank one tensor of length

277
00:57:20,900 --> 00:57:29,190
fifty day of week it's going to look up
into which one of the eight and
returning the thing of length four so

278
00:57:31,220 --> 00:57:36,279
what you typically build on embedding
metrics for each categorical feature yes

279
00:57:36,279 --> 00:57:56,310
yeah so that's what I've done here so
I've said for see in categorical
variables see how many categories there
are and then for each of those things
create one of these and then this is

280
00:57:56,410 --> 00:58:03,320
called embedding sizes and then you may
have noticed that that's actually the
first thing that we pass to get learner

281
00:58:03,320 --> 00:58:11,690
and so that tells it for every
categorical variable that's the
embedding matrix to use for that
variable that is behind you listen

282
00:58:11,690 --> 00:58:22,530
yes traffic aggression so besides our
random initialization and there are
other ways to actually initialize
embedding yes or no there's two ways one

283
00:58:22,630 --> 00:58:28,920
is random
the other is pre-trained and we'll

284
00:58:29,020 --> 00:58:36,094
probably talk about pre-trained more
later in the course but the basic idea
though is if somebody else at Rossmann
had already trained a neural net just

285
00:58:36,194 --> 00:58:45,829
like you you would use a pre trained net
from imagenet to look at pictures of
cats and dogs if somebody else is
pre-trained a network to predict cheese

286
00:58:45,829 --> 00:58:52,790
sales in ruspin you may as well start
with their embedding matrix of stores to
predict liquor sales in Rossmann and

287
00:58:53,480 --> 00:59:06,920
this is what happens for example at
Pinterest and Institute they both use
this technique instacart uses it for
routing their shoppers Pinterest uses it
for deciding what to display on a web

288
00:59:06,920 --> 00:59:21,819
page when you go there and they have
embedding matrices of products in
instigates case of stores that get
shared in the organization so people
don't have to train you once

289
00:59:23,260 --> 00:59:36,500
so for the embedding sighs why wouldn't
you just use like open hot scheme and
just well what is the advantage of doing
this they're supposed to just do it well

290
00:59:36,500 --> 00:59:50,359
good question so so we could easily as
you point out have instead of passing in
these four numbers record instead of
passed in seven numbers all zeroes but
one of them is one and that also is a

291
00:59:52,309 --> 01:00:09,509
list of floats and that would totally
work and that's how generally speaking
categorical variables have been used in
statistics for many years it's called
dummy variables the problem is that in

292
01:00:09,609 --> 01:00:24,140
that case the concept of sundae could
only ever be associated with a single
floating-point number right and so it
basically gets this kind of linear
behavior it says like sunday is more or

293
01:00:24,140 --> 01:00:38,430
less of a single thing yeah worth
noticing directions it's saying like now
sunday is a concept in four dimensional
space right and so what we tend to find
happen is that these embedding vectors
tend to get these kind of rich semantic

294
01:00:38,530 --> 01:00:57,349
concepts so for example if it turns out
that weekends kind of have a different
behavior you'll tend to see that
Saturday and Sunday will have like some
particular number higher or more likely

295
01:00:57,349 --> 01:01:28,584
it turns out that certain days of the
week are associated with higher sales of
certain kinds of goods that you kind of
can't go without I don't know like gas
or milk see where else there might be
other products like like wine for
example like wine that tend to be
associated with like the days before
weekends or holidays right so there

296
01:01:28,684 --> 01:01:41,185
might be kind of a column which is like
to what extent is this day of the week
kind of associated with people going out
you know so basically yeah by by having

297
01:01:41,285 --> 01:01:52,860
this higher dimensionality dektor rather
than just a single number it gives the
deep Learning Network a chance to learn
these rich representations and so this

298
01:01:52,960 --> 01:02:04,005
idea of an embedding is actually what's
called a distributed representation it's
kind of the fun most fundamental concept
of neural networks this is the idea that

299
01:02:04,105 --> 01:02:12,980
a concept in a neural network has a kind
of a a high dimensional representation
and often it can be hard to interpret

300
01:02:12,980 --> 01:02:18,255
because the idea is like each of these
numbers in this vector doesn't even have
to have just one meaning you know it

301
01:02:18,355 --> 01:02:26,595
could mean one thing if this is low and
that one's high and something else if
that one's high and that one's low
because it's going through this kind of
rich nonlinear function right and so

302
01:02:26,695 --> 01:02:41,540
it's this it's this rich representation
that allows it to learn such such such
interesting relationships I'm kind of oh

303
01:02:41,540 --> 01:03:07,280
another question sure I'll speak louder
so are there he's in a meeting so I get
the the fundamental of be like the word
vector were to Vic vector algebra even
run on this thing are the embedding
suited suitable for certain types of
variables like or are these only
suitable for there are different
categories that that the embeddings are

304
01:03:07,280 --> 01:03:20,385
suitable for an embedding is suitable
for any categorical variable okay so so
the only thing it it can't really work
well at all four would be something that
is too high cardinality so I'm like in

305
01:03:20,485 --> 01:03:33,140
other words we had like whatever it was
six hundred thousand rows if you had a
variable with six hundred thousand
levels that's just not a useful
categorical variable you could packetize

306
01:03:33,140 --> 01:03:47,860
it I guess but yeah in general like you
can see here that the the third place
getters in this competition really
decided that everything that was not too
high cardinality they put them all as
categorical very

307
01:03:47,860 --> 01:03:56,170
and I think that's a good rule of thumb
you know if you can make a categorical
variable you may as well because that
way it can learn this rich distributed

308
01:03:56,170 --> 01:04:05,470
representation where else if you leave
it as continuous you know the most it
can do is to kind of try and find a know
a single functional form that fits it

309
01:04:05,470 --> 01:04:24,340
well after question so you were saying
that you are kind of increasing the
dimension but actually in most cases we
will use a one holding column which has
even a bigger dimension that so in a way
you are also reducing but in the most

310
01:04:24,340 --> 01:04:39,250
reach I think that's very good yeah it
like yes you know you can figure this
one hot encoding which actually is high
dimensional but it's not meaningfully
high dimensional because everything set
one is easy right I'm saying that also

311
01:04:39,250 --> 01:04:46,175
because even this will reduce the amount
of memory and things like this that you
have to write you're absolutely right

312
01:04:46,275 --> 01:04:52,720
and and so we may as well go ahead and
actually destroyed like what's going on
with the matrix algebra behind the

313
01:04:52,720 --> 01:04:58,880
scenes see this if this doesn't quite
make sense you can kind of skip over it
but for some people I know this really

314
01:04:58,980 --> 01:05:15,160
helps if we started out with something
saying this is Sunday right we could
represent this as a one hot encoded
vector right and so Sunday you know
maybe was position here so that would be
a 1 and then the rest of zeros okay and

315
01:05:19,240 --> 01:05:30,540
then we've got our embedding matrix
right with eight rows and in this case
four columns

316
01:05:32,440 --> 01:06:08,849
one way to think of this actually is a
matrix product right so I said you could
think of this as like looking up the
number one you know and finding like its
index in the array but if you think
about it that's actually identical to
doing a matrix product between a one-pot
encoded vector and the embedding matrix
like you're going to go zero times this
row one times this row zero times this
row and so it's like a one hot embedding
matrix product is identical to during

317
01:06:08,949 --> 01:06:23,450
the lookup and so some people in the bad
old days actually implemented embedding
matrices by doing a one hot encoding and
then a matrix product and in fact a lot
of like machine learning methods still

318
01:06:23,450 --> 01:06:34,490
kind of do that but as you know that was
kind of alluding to it's that's terribly
inefficient so all of the modern
libraries implement this as taking take
an integer and do a lookup into an array

319
01:06:37,069 --> 01:06:49,700
but the nice thing about realizing that
is actually a matrix product
mathematically is it makes it more
obvious how the gradients are going to
flow so when we do stochastic gradient
descent it's we can think of it as just
another linear layer okay does it say

320
01:06:52,819 --> 01:06:59,510
that's like somewhat minor detail but
hopefully for some of you it helps could

321
01:06:59,510 --> 01:07:06,234
you touch on using dates and times this
category course and how that affects
seasonality yeah absolutely that's a

322
01:07:06,334 --> 01:07:21,165
great question did I cover dates it all
remember no okay so I cover dates in a
lot of detail in the machine learning
course but it's worth briefly mentioning

323
01:07:21,265 --> 01:07:33,170
here there's a fast AI function called
add date part which takes a data frame
and column in that column name needs to

324
01:07:33,170 --> 01:07:46,170
be a date it removes unless you squat
drop equals false it optionally removes
the column from the data frame and
replaces it with lots of column
representing all of the useful

325
01:07:46,270 --> 01:07:59,570
information about that date like day of
week day of month month of year year is
at the start of the quarter is at the
end of the quarter basically everything
that pandas gives us and so that way we

326
01:07:59,570 --> 01:08:14,260
end up when we look at our list of
features where you can see them here
right yeah month week data etc so these
all get created for us by a date pad so

327
01:08:14,360 --> 01:08:34,500
we end up with you know this eight long
embedding matrix so I guess eight rows
by four column embedding matrix for day
of week and conceptually that allows us
or allows our model to create some
pretty interesting time series models

328
01:08:34,600 --> 01:08:53,700
all right like it can if there's
something that has a seven day period
cycle that kind of goes up on Mondays
and down on Wednesdays but only for
dairy and only in Berlin it can totally
do that but it has all the information
it needs to do that so this turns out to

329
01:08:53,800 --> 01:09:01,010
be a really fantastic way to deal with
time series so I'm really glad you asked
the question you just need to make sure

330
01:09:01,010 --> 01:09:10,515
that that the the cycle indicator in
your time series exists as a column so
if you didn't have a column there called

331
01:09:10,615 --> 01:09:28,020
day of week it would be very very
difficult for the neural network to
somehow learn to do like a divide mod
seven and then somehow look that up in
an omitting matrix like it not
impossible but really hard it would use
lots of computation wouldn't do it very
well

332
01:09:28,120 --> 01:09:36,400
so an example of the kind of thing that
you need to think about might be
holidays for example you know or if you

333
01:09:36,500 --> 01:09:44,720
are doing something in you know sales of
beverages in San Francisco you probably
want a list of like when weather that

334
01:09:44,720 --> 01:09:52,080
when is the ball game on at AT&T Park
because that's going to impact how many
people that are drinking beer in Soma

335
01:09:52,180 --> 01:10:03,820
right so you need to make sure that the
kind of the basic indicators or
or periodicity x' or whatever there and
your data and as long as they are the
neuron it's going to learn to use them

336
01:10:03,820 --> 01:10:12,050
so I'm kind of trying to skip over some
of the non deep learning parts alright

337
01:10:12,050 --> 01:10:17,270
so the key thing here is that we've got
our model data that came from the data

338
01:10:17,270 --> 01:10:37,220
frame we tell it how big to make the
embedding matrices we also have to
tailor of the columns in that data frame
how many of those categorical variables
or how many of them are continuous
variables so the actual parameter is
number of continuous variables so you

339
01:10:37,220 --> 01:10:54,510
can here you can see we just pass in how
many columns are there - how many
categorical variables are there so then
that way the the neural net knows how to
create something that puts the
continuous variables over here and the
categorical variables over there the

340
01:10:54,610 --> 01:11:02,840
embedding matrix has its own drop out
alright so this is the dropout to apply
to the embedding matrix this is the

341
01:11:02,840 --> 01:11:12,690
number of activations in the first
linear player the number of activations
in the second linear layer the dropout
in the first linear player the drop out
for the second linear layer this bit we

342
01:11:12,790 --> 01:11:26,610
won't worry about for now and then
finally is how many outputs do we want
to create okay so this is the output of
the last mini layer and obviously it's
one because we want to predict a single
number which is sales okay so after that

343
01:11:26,710 --> 01:11:35,000
we now have a learner where we can call
LR find and we get the standard looking
shape and we can say what amount do we

344
01:11:35,000 --> 01:11:58,720
want to use and we can then go ahead and
start training using exactly the same
API we've seen before so this is all
identical you can pass in I'm not sure
if you've seen this before custom
metrics what this does is it just says
please print out a number at the end of
every epoch by calling this function

345
01:11:58,720 --> 01:12:13,344
this is a function we defined a little
bit earlier which was the root means
bread percentage error first of all
going either the power of our sales
because our sales were
originally logged so this doesn't change

346
01:12:13,444 --> 01:12:17,609
the training at all it just it's just
something to print out so we trained

347
01:12:17,709 --> 01:12:35,489
that for a while and you know we've got
some benefits that the original people
that built this don't have specifically
we've got things like cyclic all muscle
learning rate stochastic gradient
descent with restarts and so it's
actually interesting to have a look and

348
01:12:37,020 --> 01:12:49,619
compare although our validation set
isn't identical to the test set it's
very similar it's a two-week period that
is at the end of the training data and
so our numbers should be similar and if

349
01:12:52,530 --> 01:13:12,000
we look at what we get point oh nine
seven and compare that to the
leaderboard public leaderboard you can
see we're kind of sort of look in the

350
01:13:12,000 --> 01:13:19,779
top actually that's interesting there is
a big difference between the public and
private leaderboard it would have it
would have been right at the top of the

351
01:13:19,879 --> 01:13:27,060
private leaderboard but only in the top
thirty or forty on the public
leaderboards so not quite sure but you

352
01:13:27,060 --> 01:13:50,280
can see like we're certainly in the top
end of this competition I actually tried
running the third place to get his code
and their final result was over a point
one so I actually think that we're
trippy compared to private leaderboard
but I'm not sure so anyway so you can

353
01:13:50,280 --> 01:14:04,230
see they're basically there's a
technique for dealing with time series
and structured data and you know
interestingly the group that that used
this technique they actually wrote a
paper about it that's linked in this

354
01:14:04,230 --> 01:14:14,336
notebook when you compare it to the
folks that won this competition and came
second they did the other folks did way
more feature engineering like the

355
01:14:14,436 --> 01:14:25,500
winners of this competition were
actually subject matter experts in
logistics sales forecasting and so they
had their own code to create lots and

356
01:14:25,500 --> 01:14:35,190
lots of features and talking to the
folks at Pinterest who built their very
similar model for recommendations for
Pinterest they say the same thing which

357
01:14:35,190 --> 01:14:42,429
is that when they switched from gradient
boosting machines to deep learning they
did like way way way less feature
engineering it was a much much simpler

358
01:14:42,529 --> 01:14:47,149
model and requires much less maintenance

359
01:14:47,929 --> 01:15:00,619
and so this is like one of the big
benefits of using this approach to deep
learning you can get state of the at
results but with a lot less work yes

360
01:15:01,989 --> 01:15:06,514
are you using any time series in any of

361
01:15:06,614 --> 01:15:11,140
these fits indirectly absolutely using

362
01:15:11,140 --> 01:15:21,100
what we just saw we have a day of week
month of year all that stuff our columns
and most of them are being treated as
categories so we're building a

363
01:15:21,100 --> 01:15:28,719
distributed representation of January
we're building a distributed
representation of Sunday we're building
a distributed representation of
Christmas

364
01:15:28,719 --> 01:15:43,592
so we're not using any plastic time
series techniques all we're doing is
true fully connected layers in a neural
net better metrics
that's what exactly exactly yeah so the

365
01:15:43,692 --> 01:16:00,500
embedding matrix is able to deal with
this stuff like day of week periodicity
and so forth in a way richer way than
any standard time series technique I've
ever come across

366
01:16:00,600 --> 01:16:13,820
one last question the matrix in the
earlier models we did CNN did not pass
it during the fig we passed it when the
data was when we got the data so we're

367
01:16:14,380 --> 01:16:24,425
not passing anything to fit just the
learning rate and the number of cycles
in this case we're passing in metrics is
not a printout some extra stuff there is

368
01:16:24,525 --> 01:16:31,475
a difference in the we're calling data
get learner

369
01:16:31,575 --> 01:16:39,680
so with the imaging approach we just go
learner dot trained and pass at the data

370
01:16:39,780 --> 01:17:04,469
but in for these kinds of models in fact
for a lot of the models the model that
we build depends on the data in this
case we actually need to know like what
embedding matrices do we have and stuff
like that so in this case it's actually
the data object that creates the learner
so yeah it is it is a bit upside down to
what we've seen before

371
01:17:04,639 --> 01:17:52,014
yeah so just to summarize or maybe I'm
confused
so in this case what we are doing is
that we have some kind of structured
data did feature engineering we got some
columnar database or something
embedding matrix for the categorical
variables so the continuous we just put
them straight feature engineering yeah
then to map it to deep learning I just
have to figure out which one I can great
question

372
01:17:52,114 --> 01:17:56,169
so yes exactly if you want to use this

373
01:17:57,780 --> 01:18:07,855
on your own data set step one is list
the categorical variable names list the
continuous variable names put it in a

374
01:18:07,955 --> 01:18:16,074
data frame pandas dataframe step two is
to create a list of which row indexes do
you want in your validation set step

375
01:18:16,174 --> 01:18:28,645
three is to call this line of code using
this except like these exact you can

376
01:18:28,745 --> 01:18:37,260
just copy and paste it step four is to
create your list of how big you want
each embedding matrix to be and then

377
01:18:37,260 --> 01:18:45,239
step 5 is to call get loner you can use
these exact parameters to start with and
if it over fits or under fits you can

378
01:18:45,339 --> 01:18:56,269
fiddle with them and then the final step
is to call fit so yeah almost all of
this code will be nearly identical

379
01:18:59,620 --> 01:19:14,000
have a couple of questions one is how is
data element ation can be used in this
case and the second one is why whatever
dropouts doing in here

380
01:19:14,000 --> 01:19:32,680
okay so data augmentation I have no idea
I mean that's a really interesting
question
I think it's gotta be domain-specific
I've never seen any paper or anybody in
industry doing data augmentation with
structured data and deep blow so I don't
think it can be done I just haven't seen
it done

381
01:19:32,680 --> 01:19:54,470
what is dropout doing exactly the same
as before so at each point we have the
output of each of these linear layers is
just a rank one tensor and so dropout is
going to go ahead and say let's throw

382
01:19:54,470 --> 01:20:12,860
away half of the activations and the
very first dropout imbedding drop out
literally goes through the embedding
matrix and says let's throw away half
the activations that's it okay let's

383
01:20:12,860 --> 01:20:21,490
take a break and let's come back at five

384
01:20:24,590 --> 01:20:49,370
past eight okay thanks everybody so now
we're gonna move into something equally
exciting actually before I do I just
been sure that I had a good question
during the break which was what's the
downside
like like almost no one's using this why

385
01:20:49,370 --> 01:21:10,100
not and and basically I think the answer
is like as we discussed before no one in
academia almost is working on this
because it's not something that people
really publish on and as a result there
haven't been really great examples where
people could look at and say oh here's a
technique that works well so let's have

386
01:21:10,100 --> 01:21:26,802
our company implement it but perhaps
equally importantly until now with this
fast AI library there hasn't been any
way to do it conveniently if you wanted
to implement one of these models you had
to write all the custom code yourself

387
01:21:26,902 --> 01:21:42,025
where else now as we discussed it's you
know sick
it's basically a six step process you
know involving about you know not much
more than six lines of code so the

388
01:21:42,125 --> 01:22:00,220
reason I mentioned this is to say like I
think there are a lot of big commercial
and scientific opportunities to use this
to solve problems that previously
haven't been solved very well before so
like I'll be really interested to hear
if some of you try this out you know

389
01:22:00,320 --> 01:22:09,690
maybe on like old cattle competitions
you might find like oh I would have won
this if I'd use this technique that

390
01:22:09,690 --> 01:22:17,640
would be interesting or if you've got
some data set you work with at work
without some kind of model that you've
been doing to the GBM or a random forest

391
01:22:17,640 --> 01:22:39,300
does this help you know the thing I I'm
still somewhat new to this I've been
doing this for basically since the start
of the year was when I started working
on these structured deep learning models
so I haven't had enough opportunity to
know where might it fail it's worked for
nearly everything I've tried it with so

392
01:22:39,300 --> 01:22:54,540
far but yeah I think this class is the
first time that there's going to be like
more than half a dozen people fulfilled
who actually are working on this so I
think you know as a group we're gonna
hopefully learn a lot and build some

393
01:22:54,540 --> 01:23:06,085
interesting things and this would be a
great thing if you're thinking of
writing a post about something or here's
an area that there's a couple of that
there's a poster in staccato about what

394
01:23:06,185 --> 01:23:12,250
they did Pinterest has a an O'Reilly a a
video about what they did that's about

395
01:23:12,350 --> 01:23:21,540
it and there's two academic papers both
about Carroll competition victories one
from Yoshi Joshua Ben geo and his group

396
01:23:21,540 --> 01:23:33,960
they won a taxi destination forecasting
competition and then also the one linked
for this rossmann competition so yeah
there's some background on that

397
01:23:33,960 --> 01:23:46,760
alright so language natural language
processing is the area which is kind of
like the most up-and-coming area

398
01:23:46,860 --> 01:23:59,960
moaning it's kind of like two or three
years behind computer vision in deep
learning it was kind of like the the
second area that deep learning started
getting really popular in and you know

399
01:23:59,960 --> 01:24:10,820
computer vision got to the point where
it was like clear state of the art for
most computer vision things maybe in
like 2014 you know and in some things in

400
01:24:10,820 --> 01:24:19,195
like 2012 in NLP we're still at the
point where for a lot of things deep
learning is now the state of the art but

401
01:24:19,295 --> 01:24:31,170
not quite everything but as you'll see
the state of kind of the software and
some of the concepts is much less mature
than it is for computer vision so in

402
01:24:31,270 --> 01:24:36,230
general none of the stuff we talked
about
after computer vision is going to be as
like settled as the computer vision and

403
01:24:39,620 --> 01:24:51,195
stuff was so NLP one of the interesting
things is in the last few months some of
the good ideas from computer vision have
started to spread into NLP for the first
time and we've seen some really big
advances so a lot of the stuff you'll

404
01:24:51,295 --> 01:25:08,760
see in NLP is is pretty new so I'm going
to start with a particular kind of NLP
problem and one of the things refined in
NLP is like there are particular
problems you can solve and they have
particular names and so there's a

405
01:25:08,860 --> 01:25:13,927
particular kind of problem in NLP called
language modeling and language modeling
has a very specific definition that

406
01:25:14,027 --> 01:25:24,530
means build a model we're given a few
words of a sentence can you predict what
the next word is going to be so if

407
01:25:24,530 --> 01:25:30,900
you're using your mobile phone and
you're typing away and you press space
and then it says like this is what the

408
01:25:31,000 --> 01:25:39,310
next word might be like SwiftKey does
this like really well and SwiftKey
actually uses deep learning for this
that's that's a language model okay so

409
01:25:39,410 --> 01:25:47,322
it has a very specific meaning when we
say language modeling we mean a model
that can predict the next word of a
sentence

410
01:25:47,422 --> 01:25:57,785
so let me give you an example I
downloaded about 18 months worth of
papers from archive so for those of you

411
01:25:57,885 --> 01:26:09,007
that don't know what archive is the most
popular preprint server in this
community and various others and has you
know lots of academic papers and so I

412
01:26:09,107 --> 01:26:32,080
grabbed the abstracts and the topics for
each and so here's an example so the
category of this particular paper what
computer CSMA is computer science and
networking and then the summary let the
abstract of the paper they're seeing the
exploitation of mm-wave bands is one of
the key enabler for 5g mobile bla bla

413
01:26:32,180 --> 01:26:38,560
bla okay so here's like an example piece
of text from my language model so I

414
01:26:38,660 --> 01:26:46,740
trained a language model on this
archived data set that I downloaded and
then I built a simple little test which

415
01:26:46,740 --> 01:26:53,040
basically you would pass it some like
priming text so you'd say like Oh

416
01:26:53,040 --> 01:27:19,540
imagine you started reading a document
that said category is computer science
networking and the summary is algorithms
that and then I said please write an
archive abstract so it said that if it's
networking
algorithms that use the same network as
a single node I'm not able to achieve
the same performance as a traditional
network based routing algorithms in this
paper we propose a novel routing scheme

417
01:27:19,640 --> 01:27:30,970
but okay so it it's learnt by reading
archive papers that somebody who is
playing algorithms that where the word
cat CSM ie came before it is going to

418
01:27:31,070 --> 01:27:38,820
talk like this and remember it started
out not knowing English at all right it
actually started out with an embedding
matrix for every word in English that

419
01:27:41,700 --> 01:27:49,090
was random okay and by reading lots of
archive papers it weren't what kind of
words followed others so then I tried

420
01:27:49,190 --> 01:28:10,225
what if we said cat computer science
computer vision summary algorithms that
use the same data to perform image
specification are increasingly being
used to
proves the performance of image
classification algorithms and this paper
we propose a novel method for image
specification using a deeper
convolutional neural network parentheses
CNN so you can see like it's kind of

421
01:28:10,325 --> 01:28:20,190
like almost the same sentence as back
here but things have just changed into
this world of computer vision rather
than networking so I tried something

422
01:28:22,710 --> 01:28:29,875
else which is like okay category
computer vision and I created the
world's shortest ever abstract that

423
01:28:29,975 --> 01:28:42,180
words and then I said title on and the
title of this is going to be on that
performance object learning for image
classification in OS is end of string so
that's like end of title what if it is

424
01:28:42,180 --> 01:28:47,130
networking summary algorithms title on
the performance of wireless networks as

425
01:28:47,130 --> 01:29:00,085
opposed to towards computer vision
towards a new approach to image
specification networking towards then
you approach to the analysis of wireless
networks so like I find this

426
01:29:00,185 --> 01:29:09,960
mind-blowing right I started out with
some random matrices which had like
literally no no pre-trade anything

427
01:29:09,960 --> 01:29:24,530
I fed at 18 months worth of archived
articles and it learnt not only how to
write English pretty well but also after
you say something's a convolutional
neural network you should then use
parentheses to say what it's called and

428
01:29:24,530 --> 01:29:40,902
furthermore that the kinds of things
people talk could say create algorithms
for in computer vision are performing
image classification and in networking
are achieving the same performance as
traditional network based routing
algorithms so like a language model is

429
01:29:41,002 --> 01:29:55,290
can be like incredibly deep and subtle
right and so we're going to try and
build that but actually not because we
care about this at all we're going to

430
01:29:55,290 --> 01:30:05,235
build it because we're going to try and
create a pre-trained model what we're
actually going to try and do is take
IMDB movie reviews and figure out
whether they're positive or negative so

431
01:30:05,335 --> 01:30:17,045
if you think about it this is a lot like
cats vs. dogs
that's a classification algorithm
but rather than an image we're going to
have the text of a review so I'd really

432
01:30:17,145 --> 01:30:28,160
like to use a pre-trained Network
like I would at least my connect to
start with a network that knows how to
read English right and so my view was

433
01:30:28,260 --> 01:30:33,860
like okay to know how to read English
means you should be able to like predict
the next word of a sentence so what if

434
01:30:33,960 --> 01:30:54,130
we pre train a language model and then
use that pre-trained language model and
then just like in computer vision stick
some new layers on the end and ask it
instead of - predicting the next word in
the sentence instead predict whether
something is positive or negative so

435
01:30:54,130 --> 01:31:01,084
when I started working on this this was
actually a new idea unfortunately in the
last couple of months I've been doing it

436
01:31:01,184 --> 01:31:14,219
you know a few people have actually
couple people started publishing this
and so this has moved from being a
totally new idea to being a you know
somewhat new idea so so this idea of

437
01:31:14,219 --> 01:31:21,410
creating a language model making that
the pre-trained model for a
classification model is what we're going

438
01:31:21,510 --> 01:31:36,305
to learn to do now and so the idea is
we're really kind of trying to leverage
exactly what we learnt in our computer
vision work which is how do we do fine
tuning to create powerful classification
models yes you know so why don't you

439
01:31:36,405 --> 01:31:43,505
think that doing just directly what you
want to do doesn't work better well

440
01:31:43,605 --> 01:31:54,400
because it doesn't just turns out it
doesn't empirically and the reason it
doesn't is a number of things first of

441
01:31:54,400 --> 01:32:11,195
all as we know fine-tuning a pre-trained
network is really powerful right so if
we can get it to learn some related
tasks first then we can use all that
information to try and help it on the

442
01:32:11,295 --> 01:32:24,920
second task the other reason is IMDB
movie reviews you know up to a thousand
words long
they're pretty big and so after reading
a thousand words knowing nothing about

443
01:32:24,920 --> 01:32:37,860
how English is structured or even what
the concept of a word is or punctuation
or whatever at the end of this thousand
integers you know they end up being
integers all you get is a 1 or a 0

444
01:32:37,960 --> 01:32:48,500
positive or negative and so trying to
like learn the entire structure of
English and then how it expresses
positive and negative sentiments from a
single number is just too much to expect

445
01:32:48,500 --> 01:32:56,625
so by building a language model first we
can try to build a neural network that
kind of understands the English of movie

446
01:32:56,725 --> 01:33:03,860
reviews and then we hope that some of
the things that's learnt about are going
to be useful in deciding whether
something's a positive or a negative

447
01:33:05,030 --> 01:33:08,835
nutrition that's a great question you

448
01:33:08,935 --> 01:33:16,920
can pass that thanks is this similar to
the car RNN yeah this is somewhat

449
01:33:17,020 --> 01:33:28,340
similar to our Olympic apathy so the
famous car as in CH AR AR and in try to
predict the next letter given a number

450
01:33:28,340 --> 01:33:42,710
of previous letters language models
generally work at a word level they
don't have to and doing things that a
word level turns out to be can be quite
a bit more powerful and we're going to
focus on word level modeling in this

451
01:33:42,710 --> 01:33:59,990
course to what extent are these
generated words actually copies of what
it found in the in the training data set
or are these completely random things
that it actually learned and how do we
know how to distinguish between those

452
01:33:59,990 --> 01:34:10,140
two yeah I mean these are awkward
questions the the words are definitely
words we've seen before the work because
it's not at a character level so it can
only give us the word it seen before the

453
01:34:10,240 --> 01:34:15,015
sentences there's a number of kind of
rigorous ways of doing it but I think
the easiest is to get a sense of like

454
01:34:15,115 --> 01:34:26,540
well here are two like different
categories where it's kind of created
very similar concepts but mixing them up
in just the right way like it it would

455
01:34:26,640 --> 01:34:45,260
be very hard to to do what we've seen
here just by like speeding back things
at scene before but you could of course
actually go back and check you know
have you seen that sentence before or
like a stream distance - have you seen a
similar sentence before in this case oh

456
01:34:45,260 --> 01:34:53,669
and of course another way to do it is
the length most importantly when we
train the language model as we'll see
we'll have a validation set and so we're
trying to predict the next word of
something that's never seen before

457
01:34:54,569 --> 01:35:00,859
and so if it's good at doing that it
should be good at generating text in

458
01:35:01,849 --> 01:35:10,874
this case the purpose the purpose is not
to generate text that was just a fun
example and so I'm not really gonna
study that too much but you know you're

459
01:35:10,974 --> 01:35:19,269
during the week turtley can like you can
totally build your you know Great
American Novel generator or whatever

460
01:35:19,269 --> 01:35:28,657
there are actually some tricks to to
using language models to generate text
that I'm not using here they're pretty
simple we can talk about them on the

461
01:35:28,757 --> 01:35:40,905
forum if you like but my focus is
actually on classification so I think
that's the thing which is incredibly
powerful like text classification I

462
01:35:41,005 --> 01:35:57,309
don't know you're a hedge fund you want
to like read every article as soon as it
comes out through writers or Twitter or
whatever and immediately identify things
which in the past have caused you know
massive market drops that's a

463
01:35:57,409 --> 01:36:12,320
classification model or you want to
recognize all of the customer service
queries which tend to be associated with
people who who leave your you know who
cancel their contracts in the next

464
01:36:12,320 --> 01:36:25,730
month's that's a classification problem
so like it's a really powerful kind of
thing for data journalism Activision
that activism more promise so forth

465
01:36:25,730 --> 01:36:35,039
right like I'm trying to class documents
into whether they're part of legal
discovery or not part of legal discovery

466
01:36:35,139 --> 01:36:41,564
okay so you get the idea so in terms of
stuff we're importing we're importing a

467
01:36:41,664 --> 01:36:51,230
few new things here one of the bunch of
things we're importing is torch text
torch text is PI torches like

468
01:36:51,230 --> 01:37:03,739
LP library and so fast AI is designed to
work hand in hand with torch text as
you'll see and then there's a few text
specific sub bits of faster fast AI that

469
01:37:03,739 --> 01:37:20,290
we'll be using so we're going to be
working with the IMDB large movie review
data set it's very very well studied in
academia you know lots and lots of
people over the years have studied this
data set

470
01:37:20,290 --> 01:37:29,190
fifty thousand reviews highly polarized
reviews either positive or negative each
one has been classified by sentiment

471
01:37:29,290 --> 01:37:35,090
okay so we're going to try our first of
all however to create a language model
so we're going to ignore the sentiment

472
01:37:35,090 --> 01:37:41,760
entirely all right so just like the dogs
and cats Cree trainer model to do one
thing and then fine tune it to do

473
01:37:41,860 --> 01:37:54,150
something else because this kind of idea
in NLP is is so so so new there's
basically no models you can download for
this so we're going to have to create

474
01:37:54,250 --> 01:38:04,080
their own right so having downloaded the
data you can use the link here we do the
usual stuff of saying the path to
training and validation path and as you

475
01:38:04,180 --> 01:38:14,720
can see it looks pretty pretty
traditional compared to a vision there's
a directory of training there's a
directory of tests we don't actually
have separate test and validation in

476
01:38:14,720 --> 01:38:28,970
this case and just like in envision the
training directory has a bunch of files
in it in this case not representing
images but representing movie reviews so

477
01:38:28,970 --> 01:39:02,410
we could cat one of those files and here
we learn about the classic zombie garand
movie I have to say with a name like
zombie gedan and an atom bomb on the
front cover I was expecting a flat-out
chop-socky fun coup rent it if you want
to get stoned on a Friday night and
laugh with your buddies don't rent it if
you're an uptight weenie or want a
zombie movie or lots of fresh eating I
think I'm going to enjoy zombie getting
so alright so we've learned something
today

478
01:39:02,530 --> 01:39:09,395
all right so we can just use standard
UNIX stuff to see like how many words
are in the data set so the training set

479
01:39:09,495 --> 01:39:18,005
we've got seventeen and a half million
words test set we've got 5.6 million

480
01:39:18,105 --> 01:39:29,920
words so he is these are this is IMDB so
IMDB is random people this is not New
York Times listed review as far as I

481
01:39:29,920 --> 01:39:40,870
know okay so before we can do anything
with text we have to turn it into a list

482
01:39:40,870 --> 01:39:47,315
of tokens token is basically like a word
right so we're going to try and turn
this eventually into a list of numbers

483
01:39:47,415 --> 01:39:51,915
so the first step is to turn it into a
list of words
that's called tokenization in NLP NLP

484
01:39:52,315 --> 01:39:57,605
has a huge lot of jargon that will we'll
learn over time

485
01:39:57,705 --> 01:40:07,985
one thing that's a bit tricky though
when we're doing tokenization is here
I've I've tokenized that review and then
joined it back up with spaces and you'll

486
01:40:08,085 --> 01:40:24,430
see here that wasn't has become two
tokens which makes perfect sense right
was is two things right dot dot dot has
become one token right where else lots
of exclamation marks has become lots of

487
01:40:24,430 --> 01:40:43,380
tokens so like a good tokenizer
will do a good job of recognizing like
pieces of it in your sentence each
separate piece of punctuation will be
separated and each part of a multi-part
word will be separated as appropriate so

488
01:40:43,380 --> 01:40:58,600
Spacey is I think it's an Australian
develop piece of software actually that
does lots of you know P stuff it's got
the best tokenizer I know and so past AI
is designed to work well with the Spacey
tokenizer as is torch text so here's an

489
01:40:58,600 --> 01:41:15,610
example of tokenization alright so what
we do with torch text is we basically
have to start out by creating something
called a field and a field is a
definition of how to pre-process some
text and so here's an example with the
definition of a field

490
01:41:15,610 --> 01:41:23,975
it says I want to lowercase a text and I
want to tokenize it with the function
called Spacey tokenize okay so it hasn't

491
01:41:24,075 --> 01:41:37,300
done anything yet we're just telling you
when we do do something this is what to
do and so that we're going to store that
description of what to do in a thing
called capital text and so this is this

492
01:41:37,300 --> 01:41:44,680
is none of this but this is not fast AI
specific at all this is part of torch
text you can go to the torch text
website read the docs there's not lots

493
01:41:44,680 --> 01:41:51,845
of Doc's yet this is all very very new
so probably the best information you'll
find about it is in this lesson but
there's some more information on this

494
01:41:51,945 --> 01:42:04,447
site all right so what we can now do is
go ahead and create the usual fast AI
model data object okay and so to create
the model data object we have to provide

495
01:42:04,547 --> 01:42:17,050
a few bits of information but you have
to say what's the training set so the
path to the text files the validation
set and the test set in this case just

496
01:42:17,050 --> 01:42:22,745
to keep things simple I don't have a
separate validation and test set so I'm
going to pass in the validation set for
both of those two things right

497
01:42:22,845 --> 01:42:29,240
so now we can create our model data
object as per usual the first thing you
give it as a path the second thing we

498
01:42:29,340 --> 01:42:47,805
give it is the torch text field
definition of how to pre-process that
text the third thing we give it is the
dictionary or the list of all of the
files we have trained validation tests
as per usual we can pass in a batch size

499
01:42:47,905 --> 01:42:57,130
and then we've got a special special
couple of extra things here one is very
commonly used in NLP minimum frequency

500
01:42:57,130 --> 01:43:08,350
what this says is in a moment we're
going to be replacing every one of these
words with an integer which basically
will be a unique index for every word

501
01:43:08,350 --> 01:43:21,929
and this basically says if there are any
words that occur less than 10 times just
call it unknown right don't think of it
as a word but we'll see that indeed more
detail in a moment

502
01:43:21,929 --> 01:43:29,904
we're going to see this in more detail
as well be PTT stands for back prop
through time and this is where we define

503
01:43:30,004 --> 01:43:40,749
how long a sentence will we stick on the
GPU at once so we're going to break them
up in this case we're going to break
them up into sentences of 70 tokens or

504
01:43:40,849 --> 01:43:46,330
less on the whole so we're going to see
all this in a moment

505
01:43:46,430 --> 01:43:57,789
okay so after building our model data
object right what it actually does is
it's going to fill this text field with
an additional attribute called vocab and

506
01:43:57,889 --> 01:44:06,640
this is a really important and LP
concept I'm sorry there's so many NLP
concepts we just have to throw at you
kind of quickly but we'll see them a few

507
01:44:06,740 --> 01:44:19,710
times right the vocab is the vocabulary
and the vocabulary in NLP has a very
specific meaning it is what is the list
of unique words that appear in this text
so every one of them is going to get a

508
01:44:19,710 --> 01:44:35,885
unique in this so let's take a look
right here is text vocab dot I to s this
dancer this is all torch text not faster
hide text upper cap dot int 2 string
Maps the integer 0 to unknown the

509
01:44:35,985 --> 01:44:43,434
integer 1 the padding unit 2 to desert
then in comma dot and of 2 and so forth

510
01:44:43,534 --> 01:44:57,580
right so this is the first 12 elements
of the array of the vocab from the IMDB
movie review and it's been sorted by
frequency and except for the first two

511
01:44:57,680 --> 01:45:04,280
special ones so for example we can then
go backwards
s2i string to int here is the it's in

512
01:45:04,580 --> 01:45:09,640
position 0 1 2 so stream to end the is 2

513
01:45:09,740 --> 01:45:20,790
so the vocab lets us take a word and map
it to an integer or take an integer and
that a tour word right and so that means

514
01:45:20,790 --> 01:45:31,800
that we can then take the first 12
tokens for example of our text and turn
them into twelve inch so for example

515
01:45:31,900 --> 01:45:39,950
here is of the agency 7 2
and here you can see 7/2 right so we're

516
01:45:40,050 --> 01:45:46,955
going to be working in this form did you
have a question deputy plus that back

517
01:45:47,055 --> 01:45:53,030
there you know is it a common tyranny
stemming or limit izing not really no

518
01:45:53,130 --> 01:46:08,420
generally tokenization is is what we
want like with a language model we you
know to keep it as general as possible
we want to know what's coming next and
so like whether its future tense or past
tense or plural or seem to learn like we

519
01:46:08,520 --> 01:46:25,300
don't really know which things are going
to be interesting in which ant so it
seems that it's generally best to kind
of leave it alone as much as possible be
the short answer you know having said

520
01:46:25,300 --> 01:46:34,685
that as I say this is all pretty new so
if there are some particular areas that
some researcher maybe is already
discovered that some other kinds of
pre-processing you're helpful you know I

521
01:46:34,785 --> 01:46:43,110
wouldn't be surprised not to know about
it so we Abdullah we know you don't
natural language is in context important

522
01:46:43,110 --> 01:46:59,840
context is very important so individual
words no no we're not looking worth this
is this look this is I just don't get
some of the big premises of this like
they're there in order yeah so just

523
01:46:59,940 --> 01:47:05,870
because we replaced I with the number 12
these are still in that order

524
01:47:05,970 --> 01:47:16,570
yeah there is a different way of dealing
with natural language called a bag of
words and bag of words you through throw
away the order in the context and in the
machine learning course we'll be
learning about working with bag of words

525
01:47:18,190 --> 01:47:32,075
representation z' but my belief is that
they are no longer useful or in the
verge of becoming no longer useful we're
starting to learn how to use deep
learning to use context properly now but

526
01:47:32,175 --> 01:47:38,135
it's kind of for the first time it's
really like only in the last few months
right so I mentioned that we've got two

527
01:47:38,235 --> 01:47:49,590
numbers batch size and B PTT back crop
through time so this is kind of subtle

528
01:47:52,650 --> 01:48:15,490
so we've got some big long piece of text
okay so we've got some big long piece of
text you know here's our sentence it's a
bunch of words right and actually what
happens in a language model is even
though we have lots of movie reviews
they actually all get concatenated
together into one big block of text

529
01:48:15,490 --> 01:48:25,330
right so it's basically predict the next
word in this huge long thing which is
all of the IMDB movie reviews
concatenated together so this thing is

530
01:48:25,330 --> 01:48:35,644
you know what do we say it was like tens
of millions of words long and so what we
do is we split it up into batches first

531
01:48:35,744 --> 01:48:47,949
right so these like aerial spits into
batches right and so if we said we want
a batch size of 64 we actually break the
whatever it was sixty million words into

532
01:48:50,739 --> 01:49:40,180
64 sections right and then we take each
one of the 64 sections and we move it
like underneath the previous one I
didn't do a great job of that
all right move it underneath so we end
up with a matrix which is
you
sixty-four actually I think we've moved
them across twice so it's actually I
think just transpose it we end up with
the matrix it's like 64 columns wide and

533
01:49:40,280 --> 01:49:51,929
the length let's say the original was 64
million right then the length is like 10
million long right so each of these

534
01:49:51,929 --> 01:49:59,790
represents one sixty-fourth with our
entire IMDB refused it all right and so

535
01:49:59,790 --> 01:50:21,472
that's our starting point so then what
we do is we then grab a little chunk of
this at a time and those chunk lengths
are approximately equal to be PTT which
I think we had equal to 70 so he
basically grab a little 70 long section
and that's the first thing we check into

536
01:50:21,572 --> 01:50:37,619
our GPU that's a batch right so a batch
is always of length of width 64 or batch
size and each bit is a sequence of
length up to 70 so let me show you all

537
01:50:37,619 --> 01:50:59,585
right so here if I go take my train data
loader I know if you folks have tried
playing with this yet but you can take
any data loader wrap it with inner -
turn it into an iterator and then call
next on it to grab a batch of data just
as if you were a neural net you get
exactly what the neuron that gets and

538
01:50:59,685 --> 01:51:08,645
you can see here we get back a 75 by 64
sensor right so it's 64 wide right and I
said it's approximately 70 high and but

539
01:51:08,745 --> 01:51:22,190
not exactly and that's actually kind of
interesting a really neat trick that
torch text does is they randomly change
the backprop through time number every

540
01:51:25,800 --> 01:51:33,330
time so each epoch it's getting slightly
different bits of text this is kind of

541
01:51:33,330 --> 01:51:49,475
like in computer vision we randomly
shuffle the images we can't randomly
shuffle the words right because we
needed to be in the right order so
instead we randomly move their
breakpoints a little bit okay so this is
the equivalent so in other words this

542
01:51:49,575 --> 01:52:08,650
this here is of length 75 right there's
a there's an ellipsis in the middle and
that represents the first 75 words of
the first review right where else this

543
01:52:08,750 --> 01:52:19,049
75 here represents the first 75 words of
this of the second of the 64 segments
let's it have to go in like 10 million
words to find that one right and so

544
01:52:19,149 --> 01:52:23,489
here's the first 75 words of the last of
those 64 segments okay and so then what

545
01:52:27,450 --> 01:53:08,365
we have down here is the next sequence
right so 51 there's 51 6 1 5 there's 6 1
5 25 is 25 right and in this case it
actually is of the same size it's also
75 plus 64
but for minor technical reasons it's
being flattened out into a single vector
but basically it's exactly the same at
this matrix but it's just moved down by
one because we're trying to predict the
next word right so that all happens for

546
01:53:08,465 --> 01:53:21,850
us right if we ask for and this is the
first date I know if you ask for a
language model data object then it's
going to create these batches of batch
size width by B PTT height bits of our

547
01:53:21,950 --> 01:53:40,050
language corpus along with the same
thing shuffled along by one word right
and so we're always going to try and
predict the next word
yes

548
01:53:40,860 --> 01:54:08,975
so why don't you instead of just
arbitrarily choosing 64 why don't you
choose like like 64 is a large number
maybe like stood by sentences and make
it a large number and then padded with
zero or something if you you know so
that you actually have a one full
sentence per line basically wouldn't
that make more sense not really because

549
01:54:09,075 --> 01:54:15,460
remember we're using columns right so
each of our columns is of length about
10 million right so although it's true

550
01:54:15,460 --> 01:54:23,735
that those columns aren't always exactly
finishing on a full stop there's so damn
long we don't care because they're like

551
01:54:23,835 --> 01:54:38,260
10 million won and we're trying to also
line contains multiple cents incentive
column contains more costumes and sorry
yeah it's of length about 10 million and
it contains many many many many many

552
01:54:38,260 --> 01:54:45,690
sentences because remember the first
thing we did was take all thing and
split it into 64 groups

553
01:54:47,159 --> 01:55:03,709
okay great so um I found this you know
pertaining to this question this thing
about like what's in this language model
matrix a little mind-bending for quite a
while so don't worry if it takes a while

554
01:55:03,809 --> 01:55:16,804
and you have to ask a thousand questions
on the forum that's fine right but go
back and listen to what I just said in
this lecture again go back to that bit
where I showed you splitting it up to 64
and moving them around and try it with

555
01:55:16,904 --> 01:55:23,380
some sentences in Excel or something and
see if you can do a better job of
explaining it than I did okay because

556
01:55:23,380 --> 01:55:46,110
this is like how torch text works and
then what fast AI adds on is this idea
of like kind of how to build a a
language model out of it well they'll
actually a lot of that stolen from torch
text as well like there's some times
where torch text starts and fast AI ends
is or vice versa trees a little saddle
they really work closely together okay

557
01:55:46,110 --> 01:55:58,640
so now that we have a model data object
that can feed us batches we can go ahead
and create a model right and so in this

558
01:55:58,740 --> 01:56:05,600
case we're going to create an embedding
matrix and our vocab we can see how big

559
01:56:05,700 --> 01:56:29,285
a vocab was let's have a look back here
so we can see here in the model data
object there are four thousand six
hundred and two kind of pieces that
we're going to go through that's
basically equal to the number of the
total length of everything divided by
batch size times B PTT and this one I
wanted to show you NT I've got the

560
01:56:29,385 --> 01:56:47,119
definition up here number of unique
tokens NT is the number of tokens that's
the size of our vocab so we've got three
thirty four thousand nine hundred and
forty five unique words and notice the
unique words that had to appear at least

561
01:56:47,219 --> 01:56:53,944
ten times okay because otherwise they've
been replaced with the length of the

562
01:56:54,044 --> 01:57:02,650
data set is one
because as far as the language model is
concerned there's only one thing which
is the whole corpus all right and then

563
01:57:05,170 --> 01:57:12,330
that thing has I hear it is twenty point
six million words you know right

564
01:57:12,330 --> 01:57:32,600
so those thirty four thousand hundred
and forty five things are used to create
an embedding matrix of number of rows is
equal to thirty four nine four five
right and so the first one represents

565
01:57:32,700 --> 01:57:38,719
UNK the second one represents pad the
third one was dot the fourth one was
comma with one under sketching was there

566
01:57:38,819 --> 01:57:53,284
and so forth right and so each one of
these gets an embedding vector so this
is literally identical to what we did
before the brick right this is a

567
01:57:53,384 --> 01:58:00,850
categorical variable it's just a very
high cardinality categorical variable
and furthermore it's the only variable

568
01:58:00,850 --> 01:58:19,090
right this is pretty standard in NLP you
have a variable which is a word right
you have a single categorical variable
single column basically and it's it's of
thirty four thousand nine hundred forty
five cardinality categorical variable

569
01:58:19,090 --> 01:58:25,360
and so we're going to create an
embedding matrix for it so M size is the
size of the omitting vector 200 okay

570
01:58:28,210 --> 01:58:35,497
so that's going to be length 200 a lot
bigger than our previous embedding
vectors not surprising because a word

571
01:58:35,597 --> 01:58:45,400
has a lot more nuance to it than the
concept of Sunday right or Russ means
Berlin's door or whatever right so it's

572
01:58:45,400 --> 01:58:53,105
generally an embedding size for a word
will be somewhere between about 50 and
about 600 okay so I've kind of done some

573
01:58:53,205 --> 01:59:04,900
in the middle we then have to say as per
usual how many activations do you want
in your layers so we're going to use 500
and then how many layers do you want in
your neural net we're going to use three

574
01:59:04,900 --> 01:59:20,587
okay this is a minor technical detail it
turns out
that we're going to learn later about
the atom optimizer that basically the
defaults for it don't work very well
with these kinds of models so you just
have to change some of these you know

575
01:59:20,687 --> 01:59:30,740
basically any time you're doing NLP you
should probably include this mine
because it works pretty well so having

576
01:59:30,840 --> 01:59:41,060
done that we can now again take our
model data object and grab a model out
of it and we can pass in a few different
things
what optimization function do we want

577
01:59:41,160 --> 01:59:51,400
how big an embedding do we want
how many hidden activate how many
activations number of Hitler how many
layers and how much drop out of many

578
01:59:51,500 --> 02:00:00,690
different kinds so this language model
we're going to use is a very recent
development called AWD LS TM by Steven
marady who's a NLP research based in San

579
02:00:03,690 --> 02:00:12,715
Francisco and his main contribution
really was to show like how to put drop
out all over the place in in these NLP

580
02:00:12,815 --> 02:00:22,540
models so we're not going to worry now
we'll do this in the last lecture is
worrying about like what all that like
what is the architecture and what are
all these dropouts

581
02:00:22,640 --> 02:00:29,610
for now just know is the same as per
usual if you try to build an NLP model
and draw underfitting
and decrease all of these dropouts if

582
02:00:29,710 --> 02:00:40,100
you're overfitting then increase all of
these dropouts in roughly this ratio
okay that's that's my rule of thumb and

583
02:00:40,200 --> 02:00:53,820
again this is such a recent paper nobody
else is working on this model anyway so
there's not a lot of guidance but I
found this these ratios worked well it's
what Stephens been using as well there's

584
02:00:53,820 --> 02:00:59,905
another kind of way we can avoid
overfitting that we'll talk about in the
last class again for now this one

585
02:01:00,005 --> 02:01:09,000
actually works totally reliably so all
of your NLP models probably want this
particular line of code and then this

586
02:01:09,000 --> 02:01:16,700
point we're going to talk about at the
end last lecture as well you can always
improve this basically what it says is

587
02:01:17,060 --> 02:01:36,870
when you do when you look at your
gradients and you multiply them by the
learning rate and you decide how much to
update you
or weights by this is clip them like
literally like sit like don't let them
be more than 0.3 and this is quite a

588
02:01:36,870 --> 02:01:57,690
cool little trick right because like if
you're learning rates pretty high and
you kind of don't want to get in that
situation we talked about where you're
kind of got this kind of thing where you
go
you know rather than little snippets
that little step instead you go like Oh
too big Oh too big right with gradient

589
02:02:01,320 --> 02:02:12,540
clipping it kind of goes this far and
it's like oh my goodness I'm going too
far I'll stop and that's basically what
gradient flipping does so anyway so

590
02:02:12,540 --> 02:02:17,580
these are a bunch of parameters the
details don't matter too much right now
you can just deal these and then we can

591
02:02:17,680 --> 02:02:23,190
go ahead and call fit with exactly the
same parameters as usual so Jeremy um

592
02:02:26,160 --> 02:02:50,310
there are all this other work embedding
things like like worked vague and glow
so I have two questions about that one
is how are those different from these
and the second question why don't you
initialize them with one of those yeah

593
02:02:50,310 --> 02:03:15,330
so so basically that's a great question
so basically people have pre trained
these embedding matrices before to do
various other tasks they're not called
pre-trained models they're just a pre
trained embedding matrix and you can
download them and as unit says they have
names like word to Veck and love and
they're literally just a matrix there's

594
02:03:15,330 --> 02:03:31,885
no reason we couldn't download them
really it's just like kind of I found
that building a whole pre-trained model
in this way didn't seem to benefit much
if at all from using pre trained word

595
02:03:31,985 --> 02:03:36,190
vectors where else using a whole
pre-trained language model made a much

596
02:03:36,290 --> 02:03:46,120
bigger difference so I can remember what
a big those of you who saw word Tyvek it
made a big splash when it came out
I mean I'm finding this technique of
pre-trained language models seems much

597
02:03:46,220 --> 02:03:54,010
more powerful basically but I think we
can combine both to make them a little

598
02:03:54,110 --> 02:04:01,010
better still what what is the model that
you have used like how can I know that
architecture of the model

599
02:04:01,010 --> 02:04:15,415
so we'll be learning about the model
architecture in the last lesson and for
now it's a recurrent neural network
using something called an LS TN long
short-term memory okay so so if they had

600
02:04:15,515 --> 02:04:22,585
lots of details that we're skipping over
but you know you can do all this without

601
02:04:22,685 --> 02:04:43,230
any of those details we go ahead and fit
the model I found that this language
model took quite a while to fit so I
kind of like ran it for a while
noticed it was still under fitting safer
it was up to ran it a bit more longer
cycle length saved it again it still was
kind of under fitting you know run it
again and kind of finally got to the

602
02:04:45,540 --> 02:04:52,780
point where it's like kind of honestly I
kind of ran out of patience so I just
like saved it at that point and I did

603
02:04:52,880 --> 02:05:05,300
the same kind of tests that we looked at
before so I was like oh it wasn't quite
expecting but I realized it anyway the
best and the most like okay let's see
how that goes the best performance with
one movie were I say okay it looks like
the language models working pretty well

604
02:05:05,400 --> 02:05:18,600
so I've pre-trained a language model and
so now I want to use it fine tune it to
do classification sentiment

605
02:05:18,600 --> 02:05:40,015
classification now obviously if I'm
gonna use a pre trained model I need to
use exactly the same vocab but the the
word the still needs to map for the
number two so that I can look up the
vector that right so that's why I first
of all load back up my my field object
the thing with the vocab in right now in

606
02:05:40,115 --> 02:05:50,650
this case if I ran it straight
afterwards this is unnecessary it's
already in memory but this means I can
come back to this later right in a new

607
02:05:50,750 --> 02:06:08,075
session basically I can then go ahead
and say okay I've never got one more
field right in addition to my field
which represents the reviews I've also
got a field which represents the label
okay and the details are too important

608
02:06:08,175 --> 02:06:20,910
here now this time I need to not treat
the whole thing as one
big piece of text but every review is
separate because each one has a
different sentiment attached to it but

609
02:06:20,910 --> 02:06:29,095
it so happens that torch text already
has a data set that does that for IMDB
so I just used IMDB built into torch

610
02:06:29,195 --> 02:06:43,492
text so basically once we've done all
that we end up with something where we
can like grab for a particular example
or you can grab its label positive and
here's some of the text this is another
great Tom Berenger movie all right so

611
02:06:43,592 --> 02:06:55,830
this is all not nothing faster I
specific here we'll come back to it in
the last lecture but torch text Docs can
help understand what's going on all you

612
02:06:55,830 --> 02:07:06,540
need to know is that once you've used
this special tox torch text thing called
splits to grab a Spitz object you can
passed it straight into faster a text

613
02:07:06,540 --> 02:07:14,635
data from splits and that basically
converts a torch text object into a fast
AI object we can train on so as soon as

614
02:07:14,735 --> 02:07:20,425
you've done that you can just go ahead
and say get model right and that gets us

615
02:07:20,525 --> 02:07:26,590
our learner and then we can load into it
the pre trained model the language model

616
02:07:26,690 --> 02:07:41,665
right and so we can now take that
pre-trained language model and use the
stuff that we're kind of familiar with
right so we can make sure all that you
know all its at the last layers frozen
training a bit unfreeze it train it a

617
02:07:41,765 --> 02:07:52,045
bit and the nice thing is once you've
got a pre trained language model it
actually trains super fast you can see
here it's like a couple of minutes for

618
02:07:52,145 --> 02:08:05,953
epoch and it only took me to get my is
my best one here and he took me like 10
a box so it's like 20 minutes to train
this bit it's really fast and I ended up

619
02:08:06,053 --> 02:08:10,590
with 94.5% so how gone is 94.5% well it

620
02:08:10,590 --> 02:08:23,455
so happens that actually one of Steven
verities colleagues James Bradbury
recently created a paper looking at the
state at like
they tried to create a new state of the

621
02:08:23,555 --> 02:08:36,310
art for a bunch of NLP things and one of
the things that looked at was IMDB and
they actually have here a list of the
current world's best for IMDB and even

622
02:08:36,310 --> 02:08:44,315
with stuff that is highly specialized
for sentiment analysis the best anybody
had previously come up with 94.1 so in

623
02:08:44,415 --> 02:08:55,120
other words this technique getting 94.5
it's literally better than anybody has
created in the world before as far as we

624
02:08:55,120 --> 02:09:08,795
know or as far as James Bradbury knows
so so when I say like there are big
opportunities to use this I mean like
this is a technique that nobody else
currently has access to which you know

625
02:09:08,895 --> 02:09:22,155
you could like you know whatever iBM has
in what CERN or whatever any big company
has you know that they're advertising
unless they have some secret sauce that
they're not publishing which they don't
right because people get you know if

626
02:09:22,255 --> 02:09:29,735
they have a better thing they publish it
then you now have access to a better
text classification method then has ever

627
02:09:29,835 --> 02:09:42,790
existed before so I really hope that you
know you can try this out and see how
you go there may be some things it works
really well on and others that it
doesn't work as well and I don't know I

628
02:09:42,790 --> 02:09:57,230
think this kind of sweet spot here that
we had about 25,000 you know short to
medium size documents if you don't have
at least that much text it may be hard
to train a different language model but

629
02:09:57,330 --> 02:10:05,945
having said that there's a lot more we
do here right and we won't be able to do
it in part 1 of this course but in part
2 that for example we could start like

630
02:10:06,045 --> 02:10:13,030
training language models that look at
like you know lots and lots of medical
journals and then we could like make a
downloadable medical language model that

631
02:10:15,760 --> 02:10:24,200
then anybody could use to like fine tune
on like a prostate cancer subset of
medical literature for instance like

632
02:10:24,300 --> 02:10:33,350
there's so much we could do it's kind of
exciting and then you know to the next
point we could also combine this with
like pre-trained word vectors it's like

633
02:10:33,450 --> 02:10:47,170
even without trying that hard like
you know we even without use like we
could have pre-trained a Wikipedia say
corpus language model and then
fine-tuned it into a IMDB language model

634
02:10:47,170 --> 02:10:54,050
and then fine tune that into an IBM IMDB
sentiment analysis model and we would
have got something better than this so

635
02:10:54,150 --> 02:11:04,112
like this I really think this is the tip
of the iceberg and I was talking there's
a really fantastic researcher called
Sebastian Reuter who is basically the

636
02:11:04,212 --> 02:11:09,989
only NLP researcher I know who's been
really really writing a lot about

637
02:11:10,620 --> 02:11:15,250
pre-training and fine tuning and
transfer learning and NLP and I was

638
02:11:15,250 --> 02:11:23,000
asking him like why isn't this happening
more and his view was it's because there
isn't the software to make it easy you

639
02:11:23,100 --> 02:11:34,595
know so I'm actually going to share this
lecture with with him tomorrow because
you know it feels like there's you know
hopefully gonna be a lot of stuff coming
out now that we're making it really easy

640
02:11:34,695 --> 02:11:50,315
to do this ok we're kind of out of time
so what I'll do is I'll quickly look at
collaborative filtering introduction and
then we'll finish it next time but

641
02:11:50,415 --> 02:11:55,900
collaborative filtering there's very
very little new to learn
we've basically learned everything we're
gonna need so collaborative filtering

642
02:11:59,500 --> 02:12:10,037
will will cover this quite quickly next
week and then we're going to do a really
deep dive into collaborative filtering
next week where we're going to learn
about like we're actually going to from

643
02:12:10,137 --> 02:12:17,779
scratch learn how to do mr. plastic
gradient descent how to create loss
functions how they work exactly

644
02:12:17,879 --> 02:12:30,455
and then we'll grow from there and will
gradually build back up to really deeply
understand what's going on in the
structured models and then what's going
on in confidence and then finally what's
going on in recurrent neural networks

645
02:12:30,555 --> 02:12:36,400
and hopefully we'll be able to build
them all from scratch
ok so this is kind of a gonna be really

646
02:12:36,400 --> 02:12:43,205
important this movie lens data set
because we've got a user to learn a lot
of like really foundational theory and

647
02:12:43,305 --> 02:12:53,389
kind of math behind it so the movie lens
data set this is basically what it looks
like
it contains a bunch of ratings it says
user number one watched movie number 31

648
02:12:53,489 --> 02:13:01,894
and they gave it a rating of two and a
half at this particular time and then

649
02:13:01,994 --> 02:13:12,519
they watched movie 102 nine and they
gave it a rating of three and they
watched reading one one's really one one
seven two and they gave it a rating
before okay and so forth okay so this is

650
02:13:12,519 --> 02:13:24,574
the ratings table this is really the
only one that matters and our goal will
be for some use that we haven't seen
before so for some user movie
combination we haven't seen before we
have to predict if they'll like it right

651
02:13:24,674 --> 02:13:31,809
and so this is how recommendation
systems are built this is how like
Amazon besides what books to recommend

652
02:13:31,809 --> 02:13:34,094
how Netflix decides what movies to
recommend and so forth

653
02:13:34,194 --> 02:13:54,634
to make it more interesting we'll also
actually download a list of movies so
each movie we're actually gonna have the
title and so for that question earlier
about like what's actually going to be
in these embedding matrices how do we
interpret them we're actually going to
be able to look and see how that's
working so basically this is kind of

654
02:13:54,734 --> 02:13:59,162
like what we're creating this is kind of
crosstab of users by movies alright and

655
02:13:59,262 --> 02:14:03,940
so feel free to look ahead during the
week you'll see basically as per usual

656
02:14:06,150 --> 02:14:12,579
collaborative data set from CSP model
data docket learner learn it and we're

657
02:14:12,579 --> 02:14:19,329
done and don't be surprised to hear when
we then take that and we can kick the
benchmarks it seems to be better than
the benchmarks where you look at so

658
02:14:19,329 --> 02:14:25,459
that'll basically be it and then next
week we'll have a deep dive and we'll
see how to actually build this from

659
02:14:25,559 --> 02:14:32,819
scratch alright see you next week
thank you
[Applause]

