1
00:00:00,060 --> 00:00:04,944
Добро пожаловать на четвёртую лекцию.

2
00:00:05,044 --> 00:00:23,330
Эта неделя на форуме была активной. Вы написали несколько полезных статей, как и на прошлой неделе, давайте на них посмотрим.

3
00:00:24,480 --> 00:00:35,910
Vitaly Bushaev написал один из лучших постов за последнее время про скорость обучения и SGDR.

4
00:00:39,960 --> 00:00:49,569
Он проделал отличную работу, объяснив базовые идеи понятным всем языком,

5
00:00:49,669 --> 00:00:55,079
но также добавил ссылки на научные статьи для тех, кто хочет узнать больше,

6
00:00:55,079 --> 00:01:08,755
в посте много примеров и детальных объяснений, я считаю, что такие посты очень полезны в нашей сфере.

7
00:01:08,855 --> 00:01:32,939
Мне очень нравится, что материал перед публикацией обсуждается и улучшается на форуме.

8
00:01:32,939 --> 00:01:49,590
Циклическая скорость обучения — популярная тема, Anand Saha тоже написал про это пост,

9
00:01:49,590 --> 00:02:01,670
опять же — отличная работа, много картинок, ссылок на научные статьи и кода, объясняющего, как всё работает.

10
00:02:01,950 --> 00:02:07,600
Mark Hoffman написал хорошее введение в SGDR.

11
00:02:09,580 --> 00:02:16,450
Manikanta Yadunanda написал про дифференциальные скорости обучения

12
00:02:19,390 --> 00:02:31,019
и их применение в переносе обучения, добавив введение в перенос обучения.

13
00:02:31,019 --> 00:02:59,829
Arjun Rajkumar написал пост о компьютерном зрении, мне нравится, что он упомянул реальные применения этой технологии.

14
00:02:59,829 --> 00:03:06,904
Есть другие отличные посты, спасибо всем вам за такую активность.

15
00:03:07,004 --> 00:03:28,350
Повторюсь — если вы хотите написать что-то своё, но боитесь — не стоит, на форуме люди дружелюбны и готовы вам помочь.

16
00:03:30,690 --> 00:03:37,930
Сегодня будет интересная лекция, много информации на совершенно разные темы.

17
00:03:38,030 --> 00:03:51,090
Мы потратили довольно много времени на компьютерное зрение, а сегодня обсудим сразу три области.

18
00:03:51,090 --> 00:04:09,180
Начнём с анализа структурированных данных, то есть данных, хранящихся в таблицах, как базы данных.

19
00:04:09,180 --> 00:04:21,954
Затем посмотрим на обработку естественного языка и на рекомендательные системы.

20
00:04:22,054 --> 00:04:31,080
Мы рассмотрим эти темы на достаточно высоком уровне, но акцент будет на практической части, а не теоретической.

21
00:04:33,510 --> 00:04:46,675
Теоретическая часть будет в следующих трёх лекциях, там же мы ещё раз обсудим компьютерное зрение.

22
00:04:46,775 --> 00:04:58,000
Сегодня мы сфокусируемся на практическом применении этих идей и введём несколько необходимых новых понятий.

23
00:04:58,100 --> 00:05:14,980
Одно из таких понятий — дропаут, вы наверняка уже слышали про него, это полезная техника.

24
00:05:15,080 --> 00:05:21,145
Я продемонстрирую её на примере соревнования Kaggle про породы собак.

25
00:05:21,245 --> 00:05:40,770
Я создал модель и поставил precompute=True, чтобы получить предвычисленные активации на последнем свёрточном слое.

26
00:05:40,770 --> 00:05:57,960
Напоминаю, что активация — это число, вычисленное с помощью весов, или параметров, в свёрточных фильтрах,

27
00:05:57,960 --> 00:06:09,460
которые применяются к предыдущему слою, содержащему активации либо входные данные.

28
00:06:09,560 --> 00:06:14,210
Помните, что активация — это число. Итак, мы используем предвычисленные активации,

29
00:06:17,550 --> 00:06:27,480
а затем добавляем несколько полносвязных слоёв, заполненных случайными числами,

30
00:06:27,480 --> 00:06:43,009
на которые будут умножаться активации, как в нашей демонстрации в Excel.

31
00:06:43,009 --> 00:06:58,870
Если вы выведете содержимое объекта learn, увидите, какие слои мы добавили в модель.

32
00:06:58,870 --> 00:07:02,900
Нормализацию минибатчей мы обсудим на последней лекции.

33
00:07:02,900 --> 00:07:18,505
Линейный слой — это просто матрица 1024x512, то есть слой принимает 1024 активации и возвращает 512.

34
00:07:18,605 --> 00:07:26,750
Это выпрямитель, он заменяет отрицательные числа на нули.

35
00:07:26,750 --> 00:07:38,594
Второй линейный слой берёт 512 активаций с предыдущего слоя, умножает их на матрицу 512x120 и получает 120 новых активаций.

36
00:07:38,694 --> 00:07:48,829
Результат передаётся в функцию Softmax, которую мы обсуждали на прошлой неделе:

37
00:07:48,929 --> 00:08:02,180
она берёт экспоненту от числа и делит её на сумму экспонент от всех чисел, получает вероятность.

38
00:08:02,180 --> 00:08:09,850
Все вероятности в сумме дают 1 и каждая вероятность лежит между 0 и 1.

39
00:08:09,950 --> 00:08:16,595
Итак, это слои, добавленные поверх предвычисленных свёрточных слоёв, они обучаются при precompute=True.

40
00:08:16,695 --> 00:08:24,404
Давайте обсудим, что такое дропаут и что значит параметр p.

41
00:08:24,504 --> 00:08:53,085
Дропаут с параметром p=0.5 значит, что случайная половина всех активаций слоя уничтожается.

42
00:08:53,185 --> 00:09:03,970
Параметр p — вероятность удаления активации, p=0.5 значит удаление 50% активаций слоя.

43
00:09:03,970 --> 00:09:14,924
При удалении половины активаций следующий слой почти не меняется,

44
00:09:15,024 --> 00:09:25,100
так как этот слой — подвыборка максимумом, что-то меняется, только если удалённая ячейка была максимальной из четырёх.

45
00:09:25,100 --> 00:09:31,360
Если следующий слой — свёрточный, тоже меняется немного, потому что свёрточные фильтры рассматривают области 3x3.

46
00:09:31,460 --> 00:09:53,060
Результат интересный. Важно понимать, что после каждого минибатча выбрасывается случайная половина активаций.

47
00:09:53,060 --> 00:10:08,279
Это помогает избежать переобучения. Если какая-то группа активаций очень хорошо предсказывает определённую кошку или собаку,

48
00:10:08,379 --> 00:10:18,405
то после исчезновения этих активаций модель больше не сможет сказать, что находится на этом изображении,

49
00:10:18,505 --> 00:10:31,485
поэтому ей придётся находить признаки, которые будут работать, даже если половина активаций каждый раз выбрасывается.

50
00:10:31,585 --> 00:10:50,310
Этой технике три или четыре года, и она почти полностью решает проблему переобучения.

51
00:10:50,410 --> 00:11:12,079
До того, как появился дропаут, люди не знали, что делать с переобучением, когда дополнение данных и новые данные перестают помогать.

52
00:11:12,079 --> 00:11:35,870
Джеффри Хинтон и его коллеги изобрели дропаут, вдохновляясь работой человеческого мозга.

53
00:11:35,870 --> 00:11:57,539
Если вероятность p=0.01, это значит, что 1% случайных активаций выбрасывается. Это почти ничего не изменит и не спасёт от переобучения.

54
00:11:57,639 --> 00:12:17,805
Если p=0.99, выбросится 99% активаций, переобучения точно не будет, но доля правильных ответов на обучающей выборке будет низкой.

55
00:12:17,905 --> 00:12:32,509
Высокие p позволят хорошо обобщать, но снизят долю правильных ответов на обучающей выборке, низкие — наоборот.

56
00:12:32,509 --> 00:12:43,589
Дропаут объясняет, почему в начале обучения потери на обучающей выборке были больше потерь на валидационной.

57
00:12:43,689 --> 00:12:59,870
Это может показаться странным, но объяснение простое — на валидационной выборке не используется дропаут.

58
00:12:59,870 --> 00:13:10,529
При предсказаниях валидационной выборки хочется использовать наилучшую из возможных моделей, поэтому активации не выбрасывают.

59
00:13:10,629 --> 00:13:26,520
Поэтому иногда, особенно в начале обучения, доля правильных ответов и функция потерь на обучающей выборке хуже, чем на валидационной.

60
00:13:26,520 --> 00:13:30,460
Янет: Надо ли чем-то заменять выброшенные активации?

61
00:13:30,460 --> 00:13:41,375
При использовании дропаута PyTorch делает две вещи. Положим p=0.5.

62
00:13:41,475 --> 00:13:47,600
PyTorch под капотом, во-первых, выкидывает половину активаций, во-вторых, дублирует оставшиеся,

63
00:13:47,700 --> 00:13:56,885
поэтому в среднем активации не меняются, это удобно.

64
00:13:56,985 --> 00:14:06,520
Вам не нужно об этом думать, это делается автоматически.

65
00:14:06,520 --> 00:14:18,370
Вероятность дропаута для всех новых слоёв передаётся параметром ps метода ConvLearner.pretrained().

66
00:14:18,370 --> 00:14:27,670
Дропаут в предобученных слоях не изменится, потому что эти слои уже были обучены с какой-то вероятностью дропаута,

67
00:14:29,740 --> 00:14:33,220
настраивается дропаут только в новых слоях.

68
00:14:33,220 --> 00:14:40,839
Здесь ps=0.5, поэтому в объекте learn первый и второй дропауты оба имеют вероятность p=0.5.

69
00:14:40,839 --> 00:14:46,390
Напомню, что на вход этих слоёв подаются результаты последнего свёрточного слоя предобученной сети,

70
00:14:48,820 --> 00:14:55,804
половина активаций выбрасывается, оставшееся подаётся в линейный слой, пропускается через выпрямитель,

71
00:14:55,904 --> 00:15:05,230
снова половина активаций выбрасывается, оставшееся подаётся в линейный слой, а потом подаётся в функцию Softmax.

72
00:15:05,230 --> 00:15:11,950
Для небольшого увеличения точности расчётов имеет смысл брать логарифм Softmax, а не просто Softmax,

73
00:15:12,520 --> 00:15:22,480
потому что потом от этого берётся экспонента, но это неважно.

74
00:15:22,480 --> 00:15:36,380
Если убрать дропаут, поставив ps=0, доля правильных ответов в первую эпоху увеличится с 0.76 до 0.8,

75
00:15:36,380 --> 00:15:45,649
что неудивительно, потому что мы перестали выбрасывать важные результаты в начале обучения.

76
00:15:45,649 --> 00:15:52,350
Но на третьей эпохе результаты другие — доля правильных ответов была 84.8%, а стала 84.1%.

77
00:15:52,450 --> 00:16:02,489
Модель сильно переобучилась уже после трёх эпох — потери равны 0.35 на обучающей выборке и 0.55 на валидационной.

78
00:16:02,589 --> 00:16:15,975
Как видите, в модели нет дропаута, если ps=0, эти слои даже не добавляются.

79
00:16:16,075 --> 00:16:29,450
В модели два дополнительных линейных слоя — это не обязательно.

80
00:16:29,450 --> 00:16:41,720
В параметр xtra_fc (extra fully connected layers) передаётся количество дополнительных полносвязных слоёв и их размеры.

81
00:16:41,720 --> 00:16:51,170
Должен быть хотя бы один полносвязный слой, который примет на вход результат работы последнего свёрточного слоя,

82
00:16:51,170 --> 00:16:56,660
здесь размер последнего слоя 1024, и вернёт вектор длиной в количество классов —

83
00:16:56,660 --> 00:17:05,024
2 класса в задаче классификации собак и кошек, 120 пород собак и 17 типов спутниковых снимков.

84
00:17:05,124 --> 00:17:14,445
Размеры единственного линейного слоя нельзя выбрать, они определяются задачей, но можно выбрать размеры дополнительных.

85
00:17:14,545 --> 00:17:25,880
Здесь мы задаём xtra_fc=[], в модели будет только один линейный слой. Вероятность дропаута ps=0.

86
00:17:25,880 --> 00:17:35,080
Это минимальный набор дополнительных слоёв.

87
00:17:36,800 --> 00:17:53,059
Даже так мы получаем неплохие результаты, потому что обучение было недолгим и предобученная модель подходит к датасету.

88
00:17:53,059 --> 00:18:01,640
Янет: Какое значение вероятности дропаута использовать по умолчанию?

89
00:18:01,640 --> 00:18:20,710
По умолчанию fastai выставляет p=0.25 для первого слоя дропаута и p=0.5 для второго, это обычно работает.

90
00:18:20,710 --> 00:18:38,080
Если ваша модель переобучается. поставьте обе вероятности ps=0.5, не помогло — ps=0.7 и так далее.

91
00:18:38,080 --> 00:18:47,320
Если модель недообучается, попробуйте уменьшить вероятность, скорее всего, сильно уменьшать не придётся.

92
00:18:47,420 --> 00:19:06,089
Вероятность дропаута редко приходится уменьшать, обычно приходится увеличивать, и значения 0.6-0.7 для меня оптимальны.

93
00:19:06,189 --> 00:19:21,689
Я увеличил вероятность дропаута ps=0.5 в Jupyter ноутбуке с породами собак, когда перешёл с ResNet34 на ResNet50 —

94
00:19:21,789 --> 00:19:32,984
У ResNet34 меньше параметров и она хуже переобучается, в отличие от ResNet50, которая начала переобучаться.

95
00:19:33,084 --> 00:19:41,350
С большими моделями обычно сложнее.

96
00:19:42,000 --> 00:19:49,169
Вопрос из зала: Параметр p=0.5 значит вероятность в 50%?
Да.

97
00:19:49,169 --> 00:20:01,174
Вопрос из зала: Как понять, что модель переобучается?

98
00:20:01,474 --> 00:20:11,889
Если потери на обучающей выборке меньше потерь на валидационной.

99
00:20:11,889 --> 00:20:23,914
Не стоит концентрироваться на переобучении, просто старайтесь уменьшить потери на валидационной выборке.

100
00:20:24,014 --> 00:20:38,919
Экспериментируйте и научитесь понимать, что работает лучше и когда модель сильно переобучена.

101
00:20:38,919 --> 00:20:51,376
Окей, так устроен дропаут, не забывайте, что он есть по умолчанию. Ещё вопрос?

102
00:20:51,476 --> 00:21:07,809
Вопрос из зала: Вероятность дропаута p=0.5 удаляет каждую активацию с вероятностью 50% или выбирает половину и удаляет её?

103
00:21:07,809 --> 00:21:12,639
Удаляет каждую активацию с вероятностью 50%.

104
00:21:12,639 --> 00:21:19,960
Вопрос из зала: Почему имеет значение среднее активаций?

105
00:21:19,960 --> 00:21:40,139
Вернёмся к демонстрации в Excel. Это число получено перемножением четырёх групп чисел по 9.

106
00:21:40,139 --> 00:21:52,624
Если выкинуть половину чисел на одном слое, числа на другом слое будут примерно вдвое меньше, и так на каждом слое после.

107
00:21:52,724 --> 00:22:03,970
Если раньше ухо считалось пушистым при значении выше 0.6, теперь это значение — 0.3, меняется значение активаций,

108
00:22:04,070 --> 00:22:09,825
а хочется выкидывать половину активаций так, чтобы ничего не менялось.

109
00:22:09,925 --> 00:22:19,400
Вопрос из зала:

110
00:22:19,400 --> 00:22:23,720


111
00:22:23,720 --> 00:22:28,790


112
00:22:28,790 --> 00:22:35,000


113
00:22:35,000 --> 00:22:42,260


114
00:22:42,260 --> 00:22:47,690
Вопрос из зала: Можно ли задавать разные вероятности дропаута для разных слоёв?

115
00:22:47,690 --> 00:23:00,797
Да, в параметр ps можно передать массив, например ps=[0, 0.2] и xtra_fc=[512]

116
00:23:00,897 --> 00:23:13,650
значит отсутствие дропаута перед первым линейным слоем и дропаут с вероятностью p=0.2 перед вторым.

117
00:23:13,750 --> 00:23:28,100
Даже спустя несколько лет я до сих пор не понял, как разумно выставлять дропаут для разных слоёв,

118
00:23:30,260 --> 00:23:35,690
если кто-то из вас придумает хорошие правила, расскажите.

119
00:23:35,790 --> 00:23:52,450
Если вы не знаете, попробуйте либо дропаут с одинаковой вероятностью для всех слоёв, либо дропаут только на последнем слое.

120
00:23:53,670 --> 00:24:04,565
Вопрос из зала: Почему вы отслеживаете значения функции потерь, а не долю правильных ответов?

121
00:24:04,665 --> 00:24:13,920
Потому что потери известны и для обучающей, и для валидационной выборки.

122
00:24:13,920 --> 00:24:31,310
Дальше мы узнаем, что оптимизируется именно функция потерь, поэтому её проще отслеживать и понимать.

123
00:24:31,410 --> 00:24:44,225
Вопрос из зала: Дропаут добавляет случайный шум на каждой итерации, должна уменьшаться скорость обучения. Это надо корректировать?

124
00:24:44,325 --> 00:25:01,415
Теоретически да, скорость обучения должна уменьшаться, но на практике это ни на что не влияет.

125
00:25:01,515 --> 00:25:13,600
Давайте поговорим про анализ структурированных данных. Напомню, что мы анализировали данные соревнования Kaggle

126
00:25:13,600 --> 00:25:26,060
на данных Rossman, это немецкая сеть супермаркетов. Jupyter ноутбук называется lesson3-rossman.ipynb.

127
00:25:26,160 --> 00:25:36,880
Главный датасет содержал данные о продажах разных магазинов в разные дни,

128
00:25:36,880 --> 00:25:53,890
а также информацию о том, был ли открыт магазин, был ли в этот день праздник или акция и так далее.

129
00:25:53,890 --> 00:26:04,750
Была таблица о магазинах, содержащая тип магазина и ассортимента и информацию о ближайших конкурентах.

130
00:26:04,750 --> 00:26:11,590
Столбцы, или признаки, в таких таблицах можно поделить на категориальные и количественные.

131
00:26:11,590 --> 00:26:19,715
Категориальные признаки — это различные категории, например, ассортимент товара Assortment — 'a', 'b' или 'c'.

132
00:26:19,815 --> 00:26:27,910
Количественные признаки выражаются числом, например, расстояние до ближайшего конкурента CompetitionDistance —

133
00:26:27,910 --> 00:26:32,380
имеют значения различия между числами и их абсолютные величины.

134
00:26:32,380 --> 00:26:37,720
Эти два вида признаков обрабатываются по-разному.

135
00:26:37,720 --> 00:26:49,230
Тем, кто занимался машинным обучением, знакомы количественные признаки — на них легко построить линейную регрессию.

136
00:26:49,230 --> 00:26:54,850
С категориальными признаками чуть сложнее.

137
00:26:54,850 --> 00:27:06,944
Мы не будем смотреть на предобработку данных и выделение признаков, в итоге получается такая таблица и такие признаки.

138
00:27:07,044 --> 00:27:29,045
Напомню, что я не выделял признаки, это код обладателей третьего места в соревновании.

139
00:27:29,145 --> 00:27:46,660
Признаки разделены на категориальные и количественные. Год, месяц и день можно считать количественными признаками,

140
00:27:46,660 --> 00:27:52,179
потому что разница между, например, 2000 и 2003 имеет значение,

141
00:27:52,179 --> 00:28:14,570
но здесь они категориальные. Модель считает, что 2000, 2001 и 2002 года никак не связаны между собой.

142
00:28:14,670 --> 00:28:23,109
Если бы год был количественным признаком, модель строила бы гладкую функцию под все возможные значения.

143
00:28:23,209 --> 00:28:36,415
Поэтому количественные признаки с небольшим разбросом дискретных значений иногда удобнее считать категориальными.

144
00:28:36,515 --> 00:28:47,529
Ещё один пример — день недели. Можно пронумеровать дни от 0 до 6, потому что разница между 3 и 5, два дня, имеет смысл.

145
00:28:47,629 --> 00:28:56,609
Но если вдуматься, продажи могут различаться в зависимости от дня недели, а не его номера —

146
00:28:56,609 --> 00:29:05,700
в выходные один уровень продаж, по пятницам другой, в среду третий.

147
00:29:05,700 --> 00:29:12,909
Поэтому мы считаем день недели категориальным признаком.

148
00:29:13,009 --> 00:29:24,399
Вам придётся принимать решения о том, какой признак считать категориальным, а какой количественным.

149
00:29:24,499 --> 00:29:37,619
Если в столбце значения закодированы как 'B' или 'С', или как 'Джереми' и 'Янет', у вас нет выбора — это категориальный признак.

150
00:29:37,619 --> 00:29:51,415
Если признак закодирован числом, например, возраст или день недели, вам нужно решить, как это представить.

151
00:29:51,515 --> 00:29:59,199
Итак, если признак категориальный, он таким и останется, но количественный признак можно перевести в категориальный.

152
00:29:59,299 --> 00:30:15,594
Здесь я не принимал никаких решений — признаки были разделены на количественные и категориальные обладателями третьего места.

153
00:30:15,694 --> 00:30:29,545
Как видите, все количественные признаки содержат дробные числа, например, CompetitionDistance и температурные признаки.

154
00:30:29,645 --> 00:30:46,300
Эти признаки сложно сделать качественными, потому что почти все значения уникальны.

155
00:30:46,300 --> 00:30:54,520
Количество возможных значений признака обозначается как мощность.

156
00:30:54,520 --> 00:31:00,390
Мощность признака дня недели — семь, потому что различных дней недели семь.

157
00:31:02,010 --> 00:31:09,165
Вопрос из зала: Вы используете сегментацию признаков?

158
00:31:09,265 --> 00:31:13,033
Нет, не использую, хотя здесь

159
00:31:13,133 --> 00:31:23,140
максимальную температуру можно было бы разбить на категории 0-10, 10-20 и 20-30 и превратить в количественный признак.

160
00:31:23,140 --> 00:31:36,160
Неделю назад вышла статья, авторы которой утверждают, что сегментация признаков — это полезно, это первое известное мне практическое упоминание.

161
00:31:36,260 --> 00:31:44,507
Ещё неделю назад я сказал бы, что это плохая идея, но сейчас не уверен, возможно, это может быть полезно.

162
00:31:44,607 --> 00:32:00,880
Вопрос из зала: Если год — категориальный признак, что произойдёт, показать модели год, которого она ещё не видела?

163
00:32:00,880 --> 00:32:16,760
Мы это ещё обсудим. Короткий ответ — этот год будет считаться неизвестной категорией, pandas умеет с таким обращаться.

164
00:32:16,860 --> 00:32:24,840
По сути, это будет просто ещё одна категория «Неизвестная категория».

165
00:32:25,130 --> 00:32:35,980
Вопрос из зала: Если неизвестная категория будет в тестовой выборке, что предскажет модель?

166
00:32:36,080 --> 00:32:46,679
Ну, что-нибудь да предскажет. Если в обучающей выборке были неизвестные категории,

167
00:32:46,679 --> 00:32:55,164
она научится с таким обращаться, если нет — вставит случайный вектор.

168
00:32:55,264 --> 00:33:05,529
Это интересная тема, её стоит обсудить в этой части курса, и точно можно обсудить на форуме.

169
00:33:05,529 --> 00:33:21,764
Итак, мы разделили признаки на категориальные и количественные, в таблице 844338 записей.

170
00:33:21,864 --> 00:33:36,835
Мы преобразуем данные для признаков, которые решили сделать категориальными, методом astype('category').

171
00:33:36,935 --> 00:33:48,379
Я не буду рассказывать про pandas, про это есть куча хороших книг, например, Python for Data Analysis, я про неё уже говорил.

172
00:33:48,379 --> 00:33:53,564
Надеюсь, что и так понятно, что делает код.

173
00:33:53,664 --> 00:34:04,149
Итак, категориальные признаки преобразуются в тип category, а количественные — в 32-битное число с плавающей точкой,

174
00:34:04,249 --> 00:34:10,569
потому что с таким форматом чисел работает PyTorch.

175
00:34:10,669 --> 00:34:30,584
Некоторые количественные признаки, например, Promo, содержат значения 0 или 1, они тоже преобразуются в числа с плавающей точкой.

176
00:34:30,684 --> 00:34:44,339
Я стараюсь проделать большую часть обучения на маленьких датасетах — например, уменьшать изображения до размера 128 или 64.

177
00:34:44,439 --> 00:34:52,460
Структурированные данные нельзя уменьшить, вместо этого я случайно выбираю подмножество данных.

178
00:34:52,460 --> 00:35:07,609
Для этого я использую уже знакомую нам функцию get_cv_idxs() для получения индексов валидационной выборки.

179
00:35:07,609 --> 00:35:21,300
В итоге я получаю 150 тысяч случайных записей.

180
00:35:21,300 --> 00:35:32,910
Вот так они выглядят — есть булевые переменные, есть строки, есть целые числа.

181
00:35:37,020 --> 00:35:52,480
Несмотря на то, что я перевёл эти признаки в категориальные, они всё ещё показываются так, просто хранятся по-другому.

182
00:35:52,580 --> 00:36:05,070
В fastai есть функция proc_df() (processed dataframe), которая принимает датасет и название целевой переменной.

183
00:36:05,070 --> 00:36:22,589
Функция делает несколько вещей. Во-первых, она отделяет целевую переменную в объект y и датасет без неё в объект df.

184
00:36:22,589 --> 00:36:36,865
Она нормирует данные — нейронным сетям удобно, когда все данные близки к 0 со стандартным отклонением около 1.

185
00:36:36,965 --> 00:36:43,650
Для этого из каждого столбца вычитается среднее и все числа делятся на стандартное отклонение.

186
00:36:43,650 --> 00:36:51,900
Это регулируется параметром do_scale=True, при этом средние и стандартные отклонения сохраняются в объекте mapper,

187
00:36:51,900 --> 00:36:57,780
чтобы потом обработать тестовую выборку таким же образом.

188
00:36:57,780 --> 00:37:08,669
Функция вставляет пропущенные значения — неизвестные категории приобретают индекс 0 (известные нумеруются с 1),

189
00:37:08,769 --> 00:37:21,952
неизвестные количественные признаки заменяются медианой, и создаётся отдельный признак «Значение было пропущено».

190
00:37:22,052 --> 00:37:29,369
Я не буду в это углубляться, мы подробно обсудили это в курсе по машинному обучению.

191
00:37:29,369 --> 00:37:36,140
Если у вас есть какие-то вопросы на эту тему, изучите его, там нет ничего про глубокое обучение.

192
00:37:36,900 --> 00:37:42,330
Как видите, после обработки год 2014 стал годом номер 2.

193
00:37:42,330 --> 00:37:47,220
Все категориальные признаки были заменены на целые числа, начиная с нуля,

194
00:37:47,220 --> 00:37:59,560
так как эти признаки потом образуют таблицу, и мы не хотим, чтобы на год ушло 2014 столбцов, когда хватило бы двух.

195
00:37:59,660 --> 00:38:11,950
Как видите, таким же образом строки 'a' и 'c' преобразовались в числа 1 и 3.

196
00:38:12,050 --> 00:38:20,220
Окей, у нас есть датафрейм, не содержащий целевую переменную, где все значения — это числа.

197
00:38:20,220 --> 00:38:35,050
Всё, что мы делали до этого момента, не содержало никакого глубокого обучения и обсуждалось в курсе по машинному обучению.

198
00:38:35,150 --> 00:38:41,200
Одна из вещей, которые подробно обсуждались в курсе по машинному обучению — валидационные выборки.

199
00:38:41,300 --> 00:38:54,510
По условиям соревнования Kaggle нужно предсказать следующие две недели продаж.

200
00:38:54,510 --> 00:39:05,755
Я возьму последние две недели обучающей выборки в качестве валидационной, чтобы максимально приблизить её к тестовой.

201
00:39:05,855 --> 00:39:18,279
Рейчел недавно написала пост про создание валидационных выборок, она есть на fast.ai, мы добавим её в вики.

202
00:39:18,379 --> 00:39:34,460
По сути, это пересказ одной из лекций курса «Машинное обучение», к этой лекции есть видеозапись.

203
00:39:35,579 --> 00:39:42,125
Мы с Рейчел долго думали над тем, как обращаться с обучающей, валидационной и тестовой выборками, результаты в этом посте,

204
00:39:42,225 --> 00:39:50,710
но, опять же — там нет ничего про глубокое обучение, давайте уже к нему перейдём.

205
00:39:50,710 --> 00:40:05,479
В каждом соревновании Kaggle и вообще в любом проекте, связанном с машинным обучением, необходимо выбрать метрику.

206
00:40:05,579 --> 00:40:17,770
На соревновании Kaggle метрику оценки качества модели выбрали за нас — это средняя процентная ошибка (RMSPE).

207
00:40:17,770 --> 00:40:25,245
Функция ошибки возвращает корень из среднего квадрата процентного отклонения предсказания от истинной величины.

208
00:40:25,345 --> 00:40:26,460


209
00:40:26,560 --> 00:40:28,890


210
00:40:33,819 --> 00:40:37,839


211
00:40:37,839 --> 00:40:43,359


212
00:40:43,359 --> 00:40:47,989


213
00:40:48,089 --> 00:41:01,930
В PyTorch нет среднеквадратичной процентной ошибки, но мы можем написать её сами.

214
00:41:01,930 --> 00:41:22,380
Проще вспомнить логарифмы и понять, что частное а / b можно заменить на ln(a' / b') = ln(a') - ln(b').

215
00:41:22,380 --> 00:41:28,930


216
00:41:30,400 --> 00:41:35,430


217
00:41:39,880 --> 00:41:44,200


218
00:41:46,599 --> 00:41:50,740


219
00:41:53,770 --> 00:42:00,970


220
00:42:00,970 --> 00:42:04,510


221
00:42:04,510 --> 00:42:11,020
Эта часть тоже не относится к глубокому обучению, мы уже к нему переходим.

222
00:42:14,530 --> 00:42:19,162
Пока мы делаем всё то же самое, что и раньше:

223
00:42:19,262 --> 00:42:28,185
Создаём объект данных модели ModelData, который содержит валидационную, обучающую и иногда тестовую выборки,

224
00:42:28,285 --> 00:42:41,110
на его основе потом создастся learner, найдётся скорость обучения, обучится модель и так далее, это всё вы уже видели.

225
00:42:41,110 --> 00:42:55,030
Различие будет в том, что данные не разложены по папкам и не размечены в файле CSV,

226
00:42:55,030 --> 00:43:08,765
поэтому мы используем метод ColumnarModelData.from_data_frame(), он возвращает объект такой же структуры, как и на прошлых лекциях.

227
00:43:08,865 --> 00:43:24,590
Переменная PATH — директория, в которой хранятся необходимые файлы модели.

228
00:43:24,690 --> 00:43:29,075
Переменная val_idx содержит индексы валидационной выборки, мы объявили её выше.

229
00:43:29,175 --> 00:43:51,290
Переменная df — наш датафрейм. Переменная yl — логарифм целевой переменной y, она должна быть в модели в каком-то виде.

230
00:43:51,390 --> 00:44:03,380
Итак, это индексы валидационной выборки, датафрейм без целевой переменной, целевая переменная, а это —

231
00:44:03,380 --> 00:44:19,769
список категориальных признаков. Напомню, что сейчас все данные хранятся в виде чисел, поэтому важно его указать.

232
00:44:19,769 --> 00:44:30,469
В качестве списка категориальных признаков мы передаём переменную cat_vars, объявленную раньше.

233
00:44:30,469 --> 00:44:39,469
Также передаётся размер минибатча bs=128, этот параметр нам уже знаком.

234
00:44:39,569 --> 00:45:04,349
Теперь у нас есть объект данных модели md, содержащий уже упомянутые объекты train_dl, val_dl, train_ds, val_ds, размер выборки и всё остальное.

235
00:45:04,349 --> 00:45:18,709
После этого мы создадим из объекта данных модели алгоритм обучения методом get_learner().

236
00:45:21,769 --> 00:45:37,614
Эти параметры — начальная вероятность дропаута, количество активаций на каждом слое и дропаут для последних слоёв.

237
00:45:37,714 --> 00:45:53,519
Один из незнакомых параметров — emb_szc, вложения, сейчас их обсудим.

238
00:45:53,519 --> 00:46:05,549
Давайте забудем ненадолго про категориальные признаки и рассмотрим только количественные.

239
00:46:05,549 --> 00:46:29,740
Давайте возьмём все количественные признаки — минимальную и максимальную температуры, расстояние до ближайшего конкурента и так далее.

240
00:46:32,589 --> 00:46:36,700
Это всё числа с плавающей точкой.

241
00:46:36,700 --> 00:46:54,130
Нейронная сеть возьмёт этот одномерный массив, или вектор, или тензор первого ранга, и умножит его на матрицу.

242
00:46:54,130 --> 00:47:12,940
Допустим, у нас 20 признаков и мы хотим получить 100 новых, для этого матрица должна иметь размер 20x100.

243
00:47:12,940 --> 00:47:28,410
В итоге получается новый вектор длины 100, так работает линейный слой.

244
00:47:28,410 --> 00:47:39,670
Полученный вектор пропускается через выпрямитель, негативные значения заменяются нулями.

245
00:47:39,670 --> 00:47:59,349
После этого вектор умножается на ещё одну матрицу. Допустим, на этом нейронная сеть кончается, и мы хотим получить одно число.

246
00:47:59,349 --> 00:48:13,444
В таком случае матрица должна иметь размер 100x1. Это — нейронная сеть, состоящая из одного слоя.

247
00:48:13,544 --> 00:48:25,070
На практике обычно делают больше одного слоя, пусть матрица имеет размер 100x50.

248
00:48:25,170 --> 00:48:42,420
После умножения мы получаем новый вектор длиной 50, после умножения на матрицу 50x1 получается число.

249
00:48:42,420 --> 00:49:06,119
Напомню, что не нужно ставить выпрямитель перед Softmax, так как Softmax нужны негативные значения для низких вероятностей.

250
00:49:13,390 --> 00:49:22,400
Вот схема простейшей полносвязной нейронной сети.

251
00:49:22,400 --> 00:49:43,579
На входе — тензор первого ранга, дальше идут линейный слой, выпрямитель, ещё один линейный слой и Softmax.

252
00:49:43,579 --> 00:50:02,839
Можно добавить больше связок выпрямитель+линейный слой, можно добавить дропаут, в принципе, на этом вариации кончаются.

253
00:50:02,839 --> 00:50:12,619
Мы посмотрим на другие архитектуры, когда вернёмся к компьютерному зрению.

254
00:50:12,619 --> 00:50:27,170
Полносвязные нейронные сети —  это просто комбинация перемножения матриц и функций активаций —  например, выпрямителей и Softmax.

255
00:50:27,170 --> 00:50:42,434
Softmax необходим только в задачах классификации, здесь можно без него обойтись.

256
00:50:42,534 --> 00:50:51,440
Есть один нюанс, но в общем всё устроено именно так.

257
00:50:51,440 --> 00:50:56,420
Вернёмся к категориальным признакам.

258
00:50:56,420 --> 00:51:06,019
Один из категориальных признаков — день недели, кодируется числом от 0 до 6.

259
00:51:11,259 --> 00:51:24,252
Я хочу представить его в виде тензора первого ранга, чтобы использовать ту же архитектуру.

260
00:51:24,352 --> 00:51:45,327
Для этого создаётся новая таблица с семью строками и произвольным количеством столбцов, например, четырьмя.

261
00:51:45,427 --> 00:51:57,060
Допустим, в какой-то строке день недели — воскресенье. Вместо числа 0 (индекс воскресенья в категориальном признаке)

262
00:51:57,160 --> 00:52:12,239
мы вставим строку из матрицы 7x4, соответствующую воскресенью. Матрица заполнена числами с плавающей точкой.

263
00:52:12,339 --> 00:52:29,055
В итоге категориальный признак будет представим в виде массива из четырёх чисел, то есть тензора первого ранга.

264
00:52:29,155 --> 00:52:40,990
Изначально все числа в таблице случайные, но в процессе обучения модели изменятся.

265
00:52:41,090 --> 00:52:57,945
Итак, вместо категориального признака во входных данных мы вставляем тензор первого ранга.

266
00:52:58,045 --> 00:53:28,730
Архитектура нейронной сети не меняется. В конце вычисляется функция потерь, градиент, и эти четыре числа меняются соответственно.

267
00:53:28,730 --> 00:53:42,645
Процесс повторяется много раз, и числа в матрице приобретают смысл — мы получаем лучшие четыре числа для воскресенья,

268
00:53:42,745 --> 00:53:52,300
лучшие четыре числа для пятницы и так далее. По сути, создаётся новая матрица весов.

269
00:53:52,400 --> 00:54:01,480
Такие матрицы называются матрицы эмбеддинга.

270
00:54:03,640 --> 00:54:20,490
Количество строк матрицы эмбеддинга равно мощности признака, индекс строки равен номеру соответствующего значения признака.

271
00:54:20,590 --> 00:54:37,070
Например, для первого значения признака мы взяли бы первую строку таблицы и присоединили её к вектору количественных признаков.

272
00:54:37,070 --> 00:54:50,910
Например, если есть 5,000 различных почтовых индексов, матрица эмбеддинга может иметь размер 5,000x50.

273
00:54:51,010 --> 00:54:56,097
Допустим, необходимо преобразовать почтовый индекс 94003 под номером 4.

274
00:54:56,197 --> 00:55:04,905
Тогда нужно взять строку номер 4 матрицы эмбеддинга и добавить эти 50 чисел к вектору количественных признаков.

275
00:55:05,005 --> 00:55:10,635
После этого эти числа обрабатываются как количественные признаки.

276
00:55:10,735 --> 00:55:15,080
Вопрос из зала: Что значат эти четыре числа?

277
00:55:15,180 --> 00:55:34,965
Отличный вопрос, мы обсудим его, когда дойдём до коллаборативной фильтрации. Думайте о них как об ещё одних весах нейронной сети.

278
00:55:35,065 --> 00:55:47,270
Дальше мы узнаем, что в этих числах можно разглядеть смысл, но не их изначальное значение.

279
00:55:47,270 --> 00:55:52,099
Пока это просто случайные числа, которые мы меняем в процессе обучения.

280
00:55:52,099 --> 00:56:03,059
Вопрос из зала: Почему здесь именно 4 столбца? Для определения этого числа есть алгоритм?

281
00:56:03,140 --> 00:56:18,164
Да, есть. Сначала я составляю список всех признаков и их мощностей.

282
00:56:18,264 --> 00:56:29,089
Например, у нас больше тысячи магазинов и восемь дней недели.

283
00:56:29,089 --> 00:56:42,074
Дней недели восемь, а не семь, потому что нулевой зарезервирован под неизвестные значения, я всегда так делаю.

284
00:56:42,174 --> 00:56:47,835
Мощность признака год четыре — а различных годов три, и так далее.

285
00:56:47,935 --> 00:57:04,180
Для определения количества столбцов я делю мощность пополам и делаю так, чтобы это число было не больше 50.

286
00:57:04,180 --> 00:57:12,780
Вот размеры моих матриц эмбеддинга. В матрице эмбеддинга для магазинов 1116 строк по количеству магазинов и 50 столбцов,

287
00:57:12,880 --> 00:57:20,900
то есть вместо признака магазина добавляется 50 новых количественных признаков.

288
00:57:20,900 --> 00:57:29,190
В матрице для дня недели 8 строк и 4 столбца, каждому дню недели соответствует вектор длины 4.

289
00:57:31,220 --> 00:57:36,279
Вопрос из зала: И так делается для каждого категориального признака?

290
00:57:36,279 --> 00:57:56,310
Да, сначала я создал список, где каждой категории сопоставлялась её мощность, а потом преобразовал эту мощность в размер матрицы эмбеддинга.

291
00:57:56,410 --> 00:58:03,320
Размеры матриц эмбеддинга хранятся в переменной emb_szs (embedding sizes), мы передавали её в метод .get_learner().

292
00:58:03,320 --> 00:58:11,690
Каждому категориальному признаку соответствует матрица эмбеддинга.

293
00:58:11,690 --> 00:58:22,530
Вопрос из зала: Можно ли инициализировать матрицы эмбеддинга не случайными числами?

294
00:58:22,630 --> 00:58:28,920
Да, можно брать случайные матрицы эмбеддинга, а можно предобученные.

295
00:58:29,020 --> 00:58:36,094
Про предобученные мы ещё поговорим, если вкратце — если кто-то в Rossman обучил нейронную сеть

296
00:58:36,194 --> 00:58:45,829
для предсказывания продаж сыра и выложил свои матрицы эмбеддинга,

297
00:58:45,829 --> 00:58:52,790
их можно было бы использовать для предсказаний продаж алкоголя в Rossman.

298
00:58:53,480 --> 00:59:06,920
Например, так делают в Pinterest и Instacart — Instacart так выстраивает маршруты, Pinterest — решает, что показывать на странице.

299
00:59:06,920 --> 00:59:21,819
В обеих компаниях матрицы эмбеддинга — общие для всей компании.

300
00:59:23,260 --> 00:59:36,500
Вопрос из зала: Почему бы просто не использовать прямое кодирование?

301
00:59:36,500 --> 00:59:50,359
Да, можно было бы использовать 7 чисел вместо 4, из которых одно — 1, а остальные — 0, и записать их как числа с плавающей точкой.

302
00:59:52,309 --> 01:00:09,509
Именно так и делают в статистике, это называется фиктивные переменные.

303
01:00:09,609 --> 01:00:24,140
Проблема в том, что в этом случае воскресенью соответствовало бы всего одно число, и поведение было бы линейное.

304
01:00:24,140 --> 01:00:38,430
У нас воскресенье — точка в четырёхмерном пространстве, что позволяет передавать гораздо более сложные зависимости.

305
01:00:38,530 --> 01:00:57,349
Например, если выяснится, что продажи по выходным сильно отличаются, у субботы и воскресенья одно из чисел может совпадать.

306
01:00:57,349 --> 01:01:28,584
Скорее всего, выяснится, что в будни чаще покупают бензин или молоко, а перед выходными и праздниками — вино.

307
01:01:28,684 --> 01:01:41,185
Поэтому один из столбцов матрицы эмбеддинга может значит, например, насколько часто люди отдыхают в этот день.

308
01:01:41,285 --> 01:01:52,860
Наличие нескольких чисел для каждого значения признака позволяет нейронной сети уловить эти закономерности.

309
01:01:52,960 --> 01:02:04,005
Эмбеддинг — проявление идеи распределённого представления, основного принципа работы нейронных сетей.

310
01:02:04,105 --> 01:02:12,980
Нейронные сети обращаются с многомерными пространствами, которые сложно интерпретировать.

311
01:02:12,980 --> 01:02:18,255
Отдельный вес в строке матрицы эмбеддинга может не иметь определённого значения —

312
01:02:18,355 --> 01:02:26,595
оно может зависеть от значения остальных, и зависеть нелинейно.

313
01:02:26,695 --> 01:02:41,540
Это позволяет нейронным сетям улавливать сложные закономерности.

314
01:02:41,540 --> 01:03:07,280
Вопрос из зала: Эмбеддинг можно использовать только с какими-то определёнными категориальными признаками?

315
01:03:07,280 --> 01:03:20,385
Нет. Эмбеддинг подойдёт для любых категориальных признаков, кроме, возможно, признаков с очень большой мощностью.

316
01:03:20,485 --> 01:03:33,140
Но если мощность признака равна, например, 60,000, то это плохой категориальный признак в принципе.

317
01:03:33,140 --> 01:03:47,860
Обладатели третьего места в этом соревновании объявили категориальными все признаки с небольшой мощностью.

318
01:03:47,860 --> 01:03:56,170
Это хорошая практика — это позволит уловить максимальное количество зависимостей.

319
01:03:56,170 --> 01:04:05,470
Для всех количественных признаков нейронная сеть просто подберёт одномерную функцию.

320
01:04:05,470 --> 01:04:24,340
Янет: Применением эмбеддинга мы увеличиваем размерность, но на самом деле уменьшаем, потому что не используем прямое кодирование.

321
01:04:24,340 --> 01:04:39,250
Да, точно. Прямое кодирование увеличивает размерность матриц, но не вносит нового смысла.

322
01:04:39,250 --> 01:04:46,175
Янет: Использование эмбеддинга занимает меньше места, чем прямое кодирование, и полезнее по смыслу.

323
01:04:46,275 --> 01:04:52,720
Да. Давайте заглянем под капот и вспомним линейную алгебру.

324
01:04:52,720 --> 01:04:58,880
Если вы этого не поймёте, ничего, но некоторым это поможет.

325
01:04:58,980 --> 01:05:15,160
Воскресенье можно представить прямым кодированием в виде вектора длиной 8.

326
01:05:19,240 --> 01:05:30,540
Матрица эмбеддинга содержит 8 строк и 4 столбца.

327
01:05:32,440 --> 01:06:08,849
Выбор соответствующей строки матрицы можно представить в виде произведения вектора 1x8 на матрицу 8x4.

328
01:06:08,949 --> 01:06:23,450
Раньше некоторые люди так и воплощали эмбеддинг, некоторые из методов машинного обучения делают это до сих пор.

329
01:06:23,450 --> 01:06:34,490
Это очень неэффективно, поэтому современные библиотеки превращают номер класса в признаке в целое число и используют его как индекс.

330
01:06:37,069 --> 01:06:49,700
Идея перемножения матриц позволяет думать про эмбеддинг, как про ещё один линейный слой.

331
01:06:52,819 --> 01:06:59,510
Это незначительный нюанс, но, возможно, кому-то из вас поможет.

332
01:06:59,510 --> 01:07:06,234
Вопрос из зала: Можно ли преобразовать даты в категориальные признаки? Как при этом учитывается сезонность продаж?

333
01:07:06,334 --> 01:07:21,165
Отличный вопрос. Я подробно объяснял это в курсе «Машинное обучение», но тут тоже можно упомянуть.

334
01:07:21,265 --> 01:07:33,170
В fastai есть функция add_datepart(), которая принимает датафрейм, содержащий столбец с датами «Date»,

335
01:07:33,170 --> 01:07:46,170
удаляет его, если не поставить drop=False, и вставляет в датафрейм новые столбцы с информацией об этом дне —

336
01:07:46,270 --> 01:07:59,570
день недели, день месяца, месяц, год, является ли этот день началом или концом квартала — всё, что pandas знает про этот день.

337
01:07:59,570 --> 01:08:14,260
Вот эти признаки — год, месяц, неделя, день, день недели и так далее.

338
01:08:14,360 --> 01:08:34,500
Итак, для дня недели размер матрицы эмбеддинга — 8x4. Это позволяет создавать очень интересные модели.

339
01:08:34,600 --> 01:08:53,700
Например, если в Берлине с понедельника по среду скидка на молочные продукты, модель это учтёт.

340
01:08:53,800 --> 01:09:01,010
Это отличный способ использования времени в качестве признака, рад, что вы спросили.

341
01:09:01,010 --> 01:09:10,515
Главное — убедиться, что соответствующие признаки выделены в столбцы. Если в датафрейме нет признака дня недели,

342
01:09:10,615 --> 01:09:28,020
модели будет очень сложно увидеть связанные с днём недели закономерности.

343
01:09:28,120 --> 01:09:36,400
Стоит выделять наличие праздников и выходных в отдельные столбцы.

344
01:09:36,500 --> 01:09:44,720
Например, если вы предсказываете продажу напитков в Сан Франциско,

345
01:09:44,720 --> 01:09:52,080
стоит посмотреть расписание бейсбольных игр в AT&T-парке, потому что они влияют на продажи пива.

346
01:09:52,180 --> 01:10:03,820
Если вы добавите признаки периодических событий, нейронная сеть их учтёт.

347
01:10:03,820 --> 01:10:12,050
Я стараюсь пропускать вещи, не связанные с глубоким обучением.

348
01:10:12,050 --> 01:10:17,270
Итак, алгоритм обучения создаётся из объекта данных модели.

349
01:10:17,270 --> 01:10:37,220
В параметры передаются размеры матриц эмбеддинга emd_szs и количество количественных признаков,

350
01:10:37,220 --> 01:10:54,510
мы передаём разницу между количеством всех признаков и количеством категориальных.

351
01:10:54,610 --> 01:11:02,840
У матриц эмбеддинга есть свой дропаут, здесь он равен 0.04.

352
01:11:02,840 --> 01:11:12,690
1000 — количество активаций в первом линейном слое, 500 — во втором. 0.001 — дропаут для первого линейного слоя, 0.01 — для второго.

353
01:11:12,790 --> 01:11:26,610
Параметр y_range мы ещё обсудим. 1 — это количество чисел на выходе, у нас одно, потому что мы предсказываем продажи.

354
01:11:26,710 --> 01:11:35,000
У нас есть алгоритм обучения m. Вызываем его метод .lr_find() для нахождения скорости обучения, тут всё привычно.

355
01:11:35,000 --> 01:11:58,720
Дальше модель обучается как обычно. Здесь в качестве метрики передаётся заданная пользователем функция RMSPE.

356
01:11:58,720 --> 01:12:13,344
Функцию RSMPE мы задали выше, это средняя процентная ошибка, сначала берущая экспоненту от предсказаний, потому что модели возвращают логарифм.

357
01:12:13,444 --> 01:12:17,609
Метрика не влияет на процесс обучения, она выводится для нас.

358
01:12:17,709 --> 01:12:35,489
Здесь используется SGDR, которого не было у обладателей третьего места, интересно, что получится.

359
01:12:37,020 --> 01:12:49,619
Валидационная выборка не совпадает с тестовой, но она похожа — это последние две недели датасета.

360
01:12:52,530 --> 01:13:12,000
Потери на валидационной выборке равны 0.097, давайте посмотрим, какие потери на тестовой выборке у людей в рейтинге.

361
01:13:12,000 --> 01:13:19,779
Если бы потери на валидационной выборке совпадали с потерями на тестовой, мы были бы на вершине приватного рейтинга

362
01:13:19,879 --> 01:13:27,060
и среди первых 30-40 результатов публичного рейтинга.

363
01:13:27,060 --> 01:13:50,280
Я пробовал запускать код обладателей третьего места и потери были около 1.1, поэтому я думаю, что наша модель неплоха.

364
01:13:50,280 --> 01:14:04,230
Итак, есть техники для работы с временными признаками. Авторы этого подхода написали об этом статью, в Jupyter ноутбуке есть ссылка.

365
01:14:04,230 --> 01:14:14,336
По сравнению с этим подходом обладатели второго места выделили очень много признаков вручную.

366
01:14:14,436 --> 01:14:25,500
Победители соревнования — эксперты в области предсказания продаж, у них был готовый код для выделения признаков.

367
01:14:25,500 --> 01:14:35,190
Люди из Pinterest, строившие подобную модель для рекомендаций, говорили, что

368
01:14:35,190 --> 01:14:42,429
при переходе от градиентного бустинга к глубокому обучению сильно упростилось выделение признаков,

369
01:14:42,529 --> 01:14:47,149
получилась простая и легко поддерживаемая модель.

370
01:14:47,929 --> 01:15:00,619
Это одно из преимуществ глубокого обучения — результаты отличные, а работы мало.

371
01:15:01,989 --> 01:15:06,514
Вопрос из зала:

372
01:15:06,614 --> 01:15:11,140
Да, но не напрямую.

373
01:15:11,140 --> 01:15:21,100
У нас есть день недели, месяц и другие временные признаки, мы считаем их категориальными —

374
01:15:21,100 --> 01:15:28,719
мы строим распределённое представление января, распределённое представление воскресенья, Рождества и так далее.

375
01:15:28,719 --> 01:15:43,592
Мы не используем стандартных техник для обработки временных признаков, а используем матрицы эмбеддинга в полносвязных слоях.

376
01:15:43,692 --> 01:16:00,500
Это позволяет модели улавливать гораздо более глубокие закономерности, чем позволяют стандартные техники.

377
01:16:00,600 --> 01:16:13,820
Вопрос из зала:

378
01:16:14,380 --> 01:16:24,425
Во время вызова метода .fit() не вводятся новые параметры модели, только скорость обучения, количество циклов и метрика.

379
01:16:24,525 --> 01:16:31,475
Создание модели — другое дело.

380
01:16:31,575 --> 01:16:39,680
При классификации изображений мы вызывали метод ConvLearned.pretrained() и просто передавали туда данные.

381
01:16:39,780 --> 01:17:04,469
В нашем случае модель зависит от того, какие данные она использует, поэтому сначала строится объект данных модели, а из него создаётся алгоритм обучения.

382
01:17:04,639 --> 01:17:52,014
Вопрос из зала: Каков итоговый алгоритм? Выделить признаки, построить распределённое представление категориальных, и дать это всё модели?

383
01:17:52,114 --> 01:17:56,169
Да, алгоритм получается такой:

384
01:17:57,780 --> 01:18:07,855
1. Разделить признаки на количественные и категориальные и преобразовать данные в датафрейм.

385
01:18:07,955 --> 01:18:16,074
2. Создать валидационную выборку.

386
01:18:16,174 --> 01:18:28,645
3. Создать объект данных модели, можно просто скопировать эту строку.

387
01:18:28,745 --> 01:18:37,260
4. Создать массив размеров матриц эмбеддинга.

388
01:18:37,260 --> 01:18:45,239
5. Создать алгоритм обучения. Можете скопировать эту строку, будет переобучаться — поэкспериментируйте с параметрами.

389
01:18:45,339 --> 01:18:56,269
6. Обучить модель методом .fit().

390
01:18:59,620 --> 01:19:14,000
Вопрос из зала: Как выглядит дополнение данных в этом случае? И для чего здесь нужен дропаут?

391
01:19:14,000 --> 01:19:32,680
Про дополнение данных — понятия не имею. Я не видел статей на эту тему. Возможно, это можно сделать, но я не представляю, как.

392
01:19:32,680 --> 01:19:54,470
Дропаут делает то же самое, что и с изображениями — выбрасывает часть активаций линейных слоёв.

393
01:19:54,470 --> 01:20:12,860
У матриц эмбеддинга есть свой дропаут, он тоже выбрасывает часть активаций на каждый итерации.

394
01:20:12,860 --> 01:20:21,490
Давайте сделаем перерыв и продолжим в 8:05.
===========

395
01:20:24,590 --> 01:20:49,370
Прежде чем продолжить, отвечу на вопрос, который мне задали в перерыве - почему никто не работает со структурированными данными.

396
01:20:49,370 --> 01:21:10,100
Я думаю, это из-за того, что это выглядит скучно, поэтому это не публикуют и люди не знают, что это можно делать.

397
01:21:10,100 --> 01:21:26,802
Ещё одна важная причина - до появления библиотеки fastai не было способа делать это удобно, все этапы приходилось писать с нуля.

398
01:21:26,902 --> 01:21:42,025
С использованием библиотеки fastai создание и обучение модели занимает шесть шагов, примерно шесть строк кода.

399
01:21:42,125 --> 01:22:00,220
Я думаю, что это очень важная для индустрии и науки область, поэтому мне будет интересно услышать о ваших результатах -

400
01:22:00,320 --> 01:22:09,690
например, вы попробовали обучить модель на датасете со старых соревнований Kaggle и поняли, что выиграли бы,

401
01:22:09,690 --> 01:22:17,640
или использовали в своей работе градиентный бустинг или случайные леса, а потом попробовали глубокое обучение и вышло лучше.

402
01:22:17,640 --> 01:22:39,300
Я работаю со структурированными данными только с начала года, мой подход пока везде работал, я не знаю, какие у него недостатки.

403
01:22:39,300 --> 01:22:54,540
В этом классе впервые собралось больше десяти человек. Я надеюсь, мы многое вместе поймём и придумаем.

404
01:22:54,540 --> 01:23:06,085
Если вы хотите написать полезный пост - у Instacart есть пост про работу со структурированными данными;

405
01:23:06,185 --> 01:23:12,250
у Pinterest есть видео выступления на эту тему на O'Reilly Artificial Intelligence Conference,

406
01:23:12,350 --> 01:23:21,540
есть две научные статьи о победах в соревнованиях Kaggle - одна от команды Yoshua Bengio, они выиграли соревнование

407
01:23:21,540 --> 01:23:33,960
по предсказанию траекторий машин такси, и вторая - от победителей соревнования Rossman.

408
01:23:33,960 --> 01:23:46,760
Итак, обработка естественного языка (NLP). Это, возможно, самая многообещающая область глубокого обучения -

409
01:23:46,860 --> 01:23:59,960
она всего на пару лет моложе компьютерного зрения, это была вторая популярная область применения глубокого обучения.

410
01:23:59,960 --> 01:24:10,820
Большинство технологий компьютерного зрения в 2014 были уже отработаны,

411
01:24:10,820 --> 01:24:19,195
а в обработке естественного языка мы только сейчас подходим к этой точке.

412
01:24:19,295 --> 01:24:31,170
Что-то уже отработано, но в целом в обработке естественного языка всё гораздо более сырое, чем в компьютерном зрении.

413
01:24:31,270 --> 01:24:36,230
Всё, что мы будем обсуждать в этом курсе, будет менее проработано, чем компьютерное зрение.

414
01:24:39,620 --> 01:24:51,195
В последние месяцы полезные техники из компьютерного зрения появились и в обработке естественного языка.

415
01:24:51,295 --> 01:25:08,760
Я начну с описания одной из проблем NLP - их в NLP несколько, и все имеют свои названия.

416
01:25:08,860 --> 01:25:13,927
Мы рассмотрим проблему языкового моделирования (language modeling). В этом случае

417
01:25:14,027 --> 01:25:24,530
модель должна по нескольким словам из предложения угадывать следующее слово.

418
01:25:24,530 --> 01:25:30,900
Такие модели предлагают вам окончания фраз после нажимания пробела при наборе текста на смартфоне,

419
01:25:31,000 --> 01:25:39,310
например, так работает приложение SwiftKey, использующее глубокое обучение.

420
01:25:39,410 --> 01:25:47,322
Под фразой "языковое моделирование" мы подразумеваем модель, которая предсказывает следующее слово в предложении.

421
01:25:47,422 --> 01:25:57,785
Для примера я скачал научные статьи за 18 месяцев с сайта arxiv.org.

422
01:25:57,885 --> 01:26:09,007
arxiv.org - крупнейший архив препринтов научных статей в глубоком обучении и многих других областях.

423
01:26:09,107 --> 01:26:32,080
Из статей я выделил аннотации и темы. Тема <cat> этой статьи - Computer Science and Networking (csni), после <summ> идёт аннотация.

424
01:26:32,180 --> 01:26:38,560
Это пример текста для обучения языковой модели.

425
01:26:38,660 --> 01:26:46,740
Я обучил модель на полученном датасете с arXiv и создал тестовую функцию для демонстрации работы модели.

426
01:26:46,740 --> 01:26:53,040
В функцию передаётся тема и короткая фраза, а модель должна слово за словом дописать аннотацию по этим фразе и теме.

427
01:26:53,040 --> 01:27:19,540
Например, тема <CAT> - Computer Science and Networking (csni), часть аннотации <SUMM> - фраза "alrogithms that", ниже результат работы модели.

428
01:27:19,640 --> 01:27:30,970
Обученная на статьях с arXiv модель считает, что аннотация по теме Computer Science and Networking, начинающаяся со слов "algorithms that...", выглядит так.

429
01:27:31,070 --> 01:27:38,820
В начале обучения для каждого слова английского языка была создана случайная матрица эмбеддинга.

430
01:27:41,700 --> 01:27:49,090
В процессе чтения статей модель поняла, какие слова следуют за какими.

431
01:27:49,190 --> 01:28:10,225
Если в качестве темы выбрать Computer Science: Computer Vision (cscv), модель напишет аннотацию про свёрточные нейронные сети.

432
01:28:10,325 --> 01:28:20,190
Как видите, аннотация похожа на предыдущую, просто общие термины машинного обучения заменены на термины компьютерного зрения.

433
01:28:22,710 --> 01:28:29,875
В следующем примере я задал ту же тему - компьютерное зрение, аннотация состоит из одного слова "alrogithms.",

434
01:28:29,975 --> 01:28:42,180
и модели нужно дописать название статьи, начинающееся со слова "on...", вот результат, <eos> - символ конца строки.

435
01:28:42,180 --> 01:28:47,130
Если тема - Computer Science and Networking (csni), результат получится такой.

436
01:28:47,130 --> 01:29:00,085
Замена слова "on" на "towards" даёт слегка отличающиеся результаты.

437
01:29:00,185 --> 01:29:09,960
По-моему, это ошеломляющие результаты. Модель начала со случайных матриц, то есть не умела даже читать по-английски,

438
01:29:09,960 --> 01:29:24,530
а после обучения на статьях за 18 месяцев научилась ставить аббревиатуру CNN в скобках после фразы "свёрточные нейронные сети"

439
01:29:24,530 --> 01:29:40,902
и писать сложные аннотации вроде тех, которые я только что показал.

440
01:29:41,002 --> 01:29:55,290
Мы построим модель, способную на такие сложные и тонкие вещи, но не для этих целей,

441
01:29:55,290 --> 01:30:05,235
а для того, чтобы на её основе обучить другую модель, которая будет делить отзывы на IMDb на положительные и отрицательные.

442
01:30:05,335 --> 01:30:17,045
Задача похожа на задачу классификации кошек и собак, только вместо изображений - отзывы.

443
01:30:17,145 --> 01:30:28,160
Поэтому, как и в задаче с кошками и собаками, нам понадобится предобученная модель, которая умеет читать по-английски.

444
01:30:28,260 --> 01:30:33,860
Модель, которая может предсказать следующее слово в предложении, умеет читать по-английски.

445
01:30:33,960 --> 01:30:54,130
Мы возьмём её в качестве предобученной модели, добавим несколько новых слоёв и обучим её классифицировать отзывы.

446
01:30:54,130 --> 01:31:01,084
Когда я начал над этим работать, это была абсолютно новая идея. К сожалению, в последние пару месяцев

447
01:31:01,184 --> 01:31:14,219
появилось много статей на эту тему, и я больше не чувствую себя первоооткрывателем.

448
01:31:14,219 --> 01:31:21,410
Сейчас мы займёмся языковым моделированием для создания предобученной модели для задачи классификации отзывов.

449
01:31:21,510 --> 01:31:36,305
После этого мы примением всё, что узнали про компьютерное зрение, для последующей тонкой настройки предобученной модели.

450
01:31:36,405 --> 01:31:43,505
Янет: Почему нельзя обойтись без предобученной модели?

451
01:31:43,605 --> 01:31:54,400
Опыт показывает, что нельзя. Этому есть объяснение.

452
01:31:54,400 --> 01:32:11,195
Во-первых, предобученная модель - мощный инструмент, глупо его не использовать.

453
01:32:11,295 --> 01:32:24,920
Во-вторых, отзывы на IMDb очень длинные - до тысячи слов. Модель ничего не знает про английский язык,

454
01:32:24,920 --> 01:32:37,860
про то, что такое слова, что такое пунктуация и так далее, а обучается на данных о том, положительный отзыв или отрицательный.

455
01:32:37,960 --> 01:32:48,500
Из таких данных очень сложно получить структуру английского языка и одновременно научиться распознавать тональность.

456
01:32:48,500 --> 01:32:56,625
Поэтому мы используем нейронную сеть, которая как-то понимает английский язык, на котором написаны отзывы,

457
01:32:56,725 --> 01:33:03,860
и надеемся, что это знание поможет научиться различать тональность текстов.

458
01:33:05,030 --> 01:33:08,835
Отличный вопрос.

459
01:33:08,935 --> 01:33:16,920
Вопрос из зала: Это похоже на Char-RNN Андрея Карпатого?

460
01:33:17,020 --> 01:33:28,340
Да, похоже. Char-RNN предсказывает следующую букву слова, основываясь на предыдущих.

461
01:33:28,340 --> 01:33:42,710
Обычно (и в нашем курсе) языковые модели работают со словами как с единицами языка, а не с буквами.

462
01:33:42,710 --> 01:33:59,990
Вопрос из зала: Слова, написанные моделью - копии существующих слов или случайные сочетания букв? Как это понять?

463
01:33:59,990 --> 01:34:10,140
Слова - копии уже существующих слов, потому что модель рассматривает отдельные слова, а не буквы.

464
01:34:10,240 --> 01:34:15,015
С предложениями сложнее. Что-то можно понять, глядя на примеры:

465
01:34:15,115 --> 01:34:26,540
эти примеры отличаются только темой, и предложения схожи по структуре, различаются только терминами.

466
01:34:26,640 --> 01:34:45,260
Довольно сложно получить такие результаты просто нарезкой предложений, можете проверить, что это не так.

467
01:34:45,260 --> 01:34:53,669
Очевидно, на валидационной выборке модель предсказывает слова, которые не видела раньше.

468
01:34:54,569 --> 01:35:00,859
Поэтому, если модель хорошо предсказывает валидационную выборку, она должна хорошо генерировать текст.

469
01:35:01,849 --> 01:35:10,874
Наша цель - не сгенерировать текст, это была просто демонстрация работы, я не буду в это углубляться.

470
01:35:10,974 --> 01:35:19,269
Вы вполне можете написать свой Великий Генератор Повестей на этой неделе, если хотите.

471
01:35:19,269 --> 01:35:28,657
Есть пара техник, позволяющих моделям лучше генерировать текст, они довольно простые, их можно обсудить на форуме.

472
01:35:28,757 --> 01:35:40,905
Я хочу сконцентрироваться на классификации текстов, потому что это очень мощный инструмент для решения задач -

473
01:35:41,005 --> 01:35:57,309
например, вы возглавляете хедж-фонд и постоянно мониторите Twitter на предмет новостей о масштабных падениях рынка в прошлом,

474
01:35:57,409 --> 01:36:12,320
или хотите выделять запросы обслуживания клиентов, которые ведут к разрыву контракта.

475
01:36:12,320 --> 01:36:25,730
Эти проблемы решаются в рамках задачи классификации текстов.

476
01:36:25,730 --> 01:36:35,039
Ещё один пример - сортировка документов в суде на предмет судебного прецедента.

477
01:36:35,139 --> 01:36:41,564
Давайте посмотрим на используемые библиотеки.

478
01:36:41,664 --> 01:36:51,230
Из нового - библиотека torchtext, это библиотека PyTorch для обработки естественного языка.

479
01:36:51,230 --> 01:37:03,739
Библиотека fastai хорошо интегрирована с PyTorch, из неё мы импортируем несколько модулей для работы с текстом.

480
01:37:03,739 --> 01:37:20,290
Мы будем работать с датасетом отзывов на IMDb large movie review dataset, это известный датасет, с ним много кто работал.

481
01:37:20,290 --> 01:37:29,190
Он содержит 50,000 отзывов с яркой эмоциональной окраской - положительные или отрицательные.

482
01:37:29,290 --> 01:37:35,090
Сначала мы проигнорируем разметку тональности и обучим модель предсказывать слова.

483
01:37:35,090 --> 01:37:41,760
Это похоже на задачу с кошками и собаками - предобучим модель для одной задачи, дообучим для другой.

484
01:37:41,860 --> 01:37:54,150
Эта идя настолько нова, что никто до сих пор не выложил предобученную модель, придётся создавать её самим.

485
01:37:54,250 --> 01:38:04,080
Датасет можно скачать по ссылке, после этого мы стандартным образом задаём пути к выборкам.

486
01:38:04,180 --> 01:38:14,720
Всё как в компьютерном зрении - директории с обучающей и тестовой выборками. Валидационная выборка не выделена.

487
01:38:14,720 --> 01:38:28,970
Как и в компьютерном зрении, обучающая выборка содержит отдельные файлы - только не изображения, а отзывы.

488
01:38:28,970 --> 01:39:02,410
Можно посмотреть на один из отзывов.

489
01:39:02,530 --> 01:39:09,395
После этого можно посмотреть размеры датасета, используя стандартные команды UNIX.

490
01:39:09,495 --> 01:39:18,005
Обучающая выборка содержит 17.5 миллионов слов, тестовая - 5.6 миллионов слов.

491
01:39:18,105 --> 01:39:29,920
Отзывы взяты с IMDb - это отзывы случайных людей, а не официальные обзоры New York Times.

492
01:39:29,920 --> 01:39:40,870
Перед тем, как начать что-то делать с текстом, необходимо превратить его в список токенов.

493
01:39:40,870 --> 01:39:47,315
Токен - это единица текста, в данном случае слово. После этого токены преобразуются в числа.

494
01:39:47,415 --> 01:39:51,915
Процесс превращения текста в список токенов называется токенизация.

495
01:39:52,315 --> 01:39:57,605
В NLP много жаргонных слов, мы будем постепенно их узнавать.

496
01:39:57,705 --> 01:40:07,985
Этот текст был разбит на токены и соединён обратно с пробелом в качестве разделителя.

497
01:40:08,085 --> 01:40:24,430
Слово "wasn't" - это два токена "was" и "n't", многоточие - один токен, много восклицательных знаков - много токенов.

498
01:40:24,430 --> 01:40:43,380
Хороший алгоритм токенизации учитывает такие различия - правильно разделяет знаки препинания и составные слова.

499
01:40:43,380 --> 01:40:58,600
Лучший известный мне алгоритм токенизации - австралийский инструмент spaCy, с ним работают torchtext и fastai.

500
01:40:58,600 --> 01:41:15,610
Для токенизации текста в torchtext создаётся объект Field конструктором .Field(),

501
01:41:15,610 --> 01:41:23,975
в него передаётся параметр lowercase=True, отвечающий за перевод текста в нижний регистр, и алгоритм токенизации tokenize=spacy_tok.

502
01:41:24,075 --> 01:41:37,300
При создании объекта не выполняется токенизация, только правильна её выполнения сохраняются в объект TEXT.

503
01:41:37,300 --> 01:41:44,680
Это - часть torchtext, а не fastai, это есть в документации на их сайте. torchtext пока плохо документирован,

504
01:41:44,680 --> 01:41:51,845
потому что он совсем новый, возможно, больше всего вы узнаете из этой лекции, но что-то есть и в документации.

505
01:41:51,945 --> 01:42:04,447
После этого можно создать обычную модель fastai.

506
01:42:04,547 --> 01:42:17,050
Объект FILES содержит пути к обучающей, валидационной и тестовой выборкам -

507
01:42:17,050 --> 01:42:22,745
у меня нет тестовой выборки, поэтому вместо неё я использую валидационную.

508
01:42:22,845 --> 01:42:29,240
Модель создаётся конструктором LanguageModelData(), в него передаётся путь к файлам PATH,

509
01:42:29,340 --> 01:42:47,805
объект torchtext с инструкциями по обработке текста TEXT, список файлов FILES, размер минибатча bs.

510
01:42:47,905 --> 01:42:57,130
Здесь два непривычных параметра. min_freq - это минимальная частота.

511
01:42:57,130 --> 01:43:08,350
После обработки каждый токен будет заменён на число, индекс токена в списке уникальных токенов.

512
01:43:08,350 --> 01:43:21,929
Параметр min_freq=10 значит, что все слова, встречающиеся реже 10 раз, будут считаться незнакомым словом, сейчас мы это обсудим.

513
01:43:21,929 --> 01:43:29,904
Мы также обсудим параметр bptt (Backpropagation through time)- он определяет максимальную длину фразы,

514
01:43:30,004 --> 01:43:40,749
которая обрабатывается за одну итерацию. Длинные предложения будут разбиваться на фразы по 70 токенов или меньше.

515
01:43:40,849 --> 01:43:46,330
Всё сейчас обсудим.

516
01:43:46,430 --> 01:43:57,789
После построения модели в объекте TEXT появится поле vocab.

517
01:43:57,889 --> 01:44:06,640
Это очень важное понятие в NLP, таких понятий много, и приходится вываливать их на вас горой, но они будут повторяться.

518
01:44:06,740 --> 01:44:19,710
Поле vocab содержит словарь, то есть список уникальных слов и их индексы.

519
01:44:19,710 --> 01:44:35,885
Список слов лежит в TEXT.vocab.itos, itos = integer to string. Индексу 0 соответствует неизвестное слово, индексу 1 - отступ,

520
01:44:35,985 --> 01:44:43,434
индексу 2 - слово "the", дальше идут запятая, точка, слово "and" и так далее.

521
01:44:43,534 --> 01:44:57,580
Это первые двенадцать слов в словаре, словарь отсортирован по частотности, не считая первых двух специальных токенов.

522
01:44:57,680 --> 01:45:04,280
Можно посмотреть обратное соответствие в объекте TEXT.vocab.stoi, stoi = string to integer.

523
01:45:04,580 --> 01:45:09,640
Индексу 2 в TEXT.vocab.itos соответствовало слово "the", слову "the" в TEXT.vocab.stoi соответствует индекс 2.

524
01:45:09,740 --> 01:45:20,790
Словарь позволяет сопоставить каждому слову его индекс.

525
01:45:20,790 --> 01:45:31,800
Можно взять первые 12 слов какого-то текста и превратить их в 12 индексов в соответствии со словарём.

526
01:45:31,900 --> 01:45:39,950
Например, "of" и "the" имеют индексы 7 и 2.

527
01:45:40,050 --> 01:45:46,955
Мы будем работать с представлением в виде индексов.

528
01:45:47,055 --> 01:45:53,030
Вопрос из зала: Используется ли здесь стемминг или лемматизация?

529
01:45:53,130 --> 01:46:08,420
Нет, мы не хотим углубляться в детали языка, а просто смотрим, какие токены следуют за какими.

530
01:46:08,520 --> 01:46:25,300
Мы не знаем, какие зависимости будут интересными. Практика показывает, что стемминг и лемматизация не нужны.

531
01:46:25,300 --> 01:46:34,685
Эта область ещё совсем новая, я могу чего-то не знать.

532
01:46:34,785 --> 01:46:43,110
Вопрос из зала: Не теряется ли контекст, когда мы рассматриваем отдельные слова?

533
01:46:43,110 --> 01:46:59,840
Мы не рассматриваем отдельные слова, мы сохраняем их порядок.

534
01:46:59,940 --> 01:47:05,870
Когда мы заменяем их на числа, порядок сохраняется.

535
01:47:05,970 --> 01:47:16,570
Есть подход "мешок слов", в котором порядок не учитывается, мы рассматривали его в курсе "Машинное обучение".

536
01:47:18,190 --> 01:47:32,075
Я считаю, что этот подход отслужил своё, и глубокое обучение уже позволяет сохранять контекст.

537
01:47:32,175 --> 01:47:38,135
Мы научились это делать совсем недавно, буквально несколько месяцев назад.

538
01:47:38,235 --> 01:47:49,590
Давайте вернёмся к параметрам размера минибатча bs и обратного распространения во времени bptt.

539
01:47:52,650 --> 01:48:15,490
У нас есть текст, это набор слов. Тексты разбиты по отдельным файлам, но при обучении они склеиваются в один огромный текст.

540
01:48:15,490 --> 01:48:25,330
По сути, предсказывается следующее слово в огромном длинном тексте из всех отзывов датасета.

541
01:48:25,330 --> 01:48:35,644
Этот огромный текст содержит десятки миллионов слов, для обучения мы разбиваем его на минибатчи.

542
01:48:35,744 --> 01:48:47,949
Мы указали размер минибатча 64, это значит, что весь текст разбивается на 64 части.

543
01:48:50,739 --> 01:49:40,180
После этого эти части складываются одна под другой в огромную таблицу с 64 строками, и таблица транспонируется.

544
01:49:40,280 --> 01:49:51,929
Допустим, размер текста был 64 миллиона, тогда транспонированная таблица содержит миллион строк.

545
01:49:51,929 --> 01:49:59,790
Каждый столбец содержит 1/64 всего датасета.

546
01:49:59,790 --> 01:50:21,472
Каждый минибатч содержит количество строк таблицы, примерно равное значению параметра bptt, у нас bptt=70.

547
01:50:21,572 --> 01:50:37,619
Таким образом, минибатч всегда содержит 64 столбца и примерно 70 строк.

548
01:50:37,619 --> 01:50:59,585
Давайте получим один минибатч командой next(inter(md.trn_dl)), этой же командой минибатчи получает нейронная сеть.

549
01:50:59,685 --> 01:51:08,645
Команда вернула массив размера 75x64 - 64 столбца и примерно 70 строк.

550
01:51:08,745 --> 01:51:22,190
Примерное количество строк - довольно интересная вещь: torchtext специально меняет каждый раз высоту минибатча,

551
01:51:25,800 --> 01:51:33,330
чтобы каждую эпоху модель видела различные куски данных.

552
01:51:33,330 --> 01:51:49,475
Это напоминает перемешивание изображений в компьютерном зрении - нельзя перемешивать слова, но можно по-разному их нарезать.

553
01:51:49,575 --> 01:52:08,650
Первый столбец содержит первые 75 слов первого текста.

554
01:52:08,750 --> 01:52:19,049
Второй столбец содержит первые 75 слов второго сегмента из 64 - то есть начинается где-то после 1 миллиона слов.

555
01:52:19,149 --> 01:52:23,489
Последний столбец содержит первые 75 слов последнего сегмента из 64.

556
01:52:27,450 --> 01:53:08,365
Следующий объект - целевая переменная, это тот же массив, но сдвинутый вниз на единицу. По техническим причинам он хранится в виде вектора.

557
01:53:08,465 --> 01:53:21,850
Библиотека fastai при создании модели создаёт минибатчи размера (примерно bppt)x(размер минибатча)

558
01:53:21,950 --> 01:53:40,050
и соответствующие целевые переменные, так как наша цель - предсказать следующее слово.

559
01:53:40,860 --> 01:54:08,975
Вопрос из зала: Разве не было бы эфективнее разбивать текст на минибатчи по предложениям?

560
01:54:09,075 --> 01:54:15,460
Нет, не особо. Текст разбит на 64 столбца, каждый столбец содержит около миллиона слов.

561
01:54:15,460 --> 01:54:23,735
Столбцы не всегда оканчиваются концом предложения, но они настолько длинные, что это неважно.

562
01:54:23,835 --> 01:54:38,260
Длина каждого столбца - около миллиона слов, это очень много предложений.

563
01:54:38,260 --> 01:54:45,690
Напомню, столбцы получились нарезанием изначального текста на 64 куска.

564
01:54:47,159 --> 01:55:03,709
Эти матрицы в языковых моделях довольно сложно осознать, не расстраивайтесь, если не поймёте сразу.

565
01:55:03,809 --> 01:55:16,804
Задавайте вопросы на форуме, прослушайте несколько раз в записи то, что я только что сказал,

566
01:55:16,904 --> 01:55:23,380
попробуйте повторить этот процесс с парой предложений в Excel. Может, у вас получится лучше это объяснить.

567
01:55:23,380 --> 01:55:46,110
То, что я показал - это совместная работа torchtext и fastai, они тесно связаны и не всегда понятно, где проходит граница.

568
01:55:46,110 --> 01:55:58,640
После создания объекта данных модели можно создать саму модель.

569
01:55:58,740 --> 01:56:05,600
Для этого нужно создать матрицу эмбеддинга.

570
01:56:05,700 --> 01:56:29,285
Давайте посмотрим на размеры. В обучающей выборке 4602 батча - это примерно равно (длина всего текста) / (размер минибатча * bptt).

571
01:56:29,385 --> 01:56:47,119
Объект md.nt содержит уникальные токены, повторяющиеся больше 10 раз - их 34,945.

572
01:56:47,219 --> 01:56:53,944
Все слова с частотностью меньше 10 заменены токеном <unk>, неизвестное слово.

573
01:56:54,044 --> 01:57:02,650
Длина обучающей выборки - 1, потому что все тексты склеены в один огромный текст,

574
01:57:05,170 --> 01:57:12,330
и в этом огромном тексте 20,621,966 слов.

575
01:57:12,330 --> 01:57:32,600
Из 34,945 уникальных токенов создаётся матрица эмбеддинга. В ней 34,945 строк, нулевая соответствует токену <unk>,

576
01:57:32,700 --> 01:57:38,719
первая - <pad>, дальше идут точка, запятая, "the" и так далее.

577
01:57:38,819 --> 01:57:53,284
Каждому токену соответствует вектор в матрице эмбеддинга. Это то же самое, что мы делали до перерыва,

578
01:57:53,384 --> 01:58:00,850
токен - это категориальный признак с очень высокой мощностью, при этом единственный.

579
01:58:00,850 --> 01:58:19,090
Это очень распространенная ситуация в NLP: слово - единственный признак, он категориальный и его мощность равна 34,945.

580
01:58:19,090 --> 01:58:25,360
Ширина матрицы эмбеддинга, то есть количество столбцов, задаётся в переменной em_sz=200.

581
01:58:28,210 --> 01:58:35,497
Это сильно больше, чем при работе со структурированными данными, потому что слово языка -

582
01:58:35,597 --> 01:58:45,400
гораздо более сложная смысловая единица, чем понятие воскресенья или магазин в Берлине.

583
01:58:45,400 --> 01:58:53,105
Обычно количество столбцов в матрице эмбеддинга при работе с текстом будет от 50 до 600, мы взяли число посередине.

584
01:58:53,205 --> 01:59:04,900
Как обычно, нужно указать количество активаций в слое nh=500 и количество слоёв nl=3.

585
01:59:04,900 --> 01:59:20,587
Позже мы обсудим алгоритм оптимизации Adam, его значения по умолчанию не идеальны, мы их меняем.

586
01:59:20,687 --> 01:59:30,740
В каждой задаче по обработке естественного языка имеет смысл вставлять эту строку кода, она полезная.

587
01:59:30,840 --> 01:59:41,060
После этого можно создать модель. Она принимает в качестве параметров алгоритм оптимизации opt_fn,

588
01:59:41,160 --> 01:59:51,400
ширину матрицы эмбеддинга em_sz, количество активаций в слое nh, количество слоёв nl, различные вероятности дропаута.

589
01:59:51,500 --> 02:00:00,690
fastai использует языковую модель AWD LSTM, разработанную Стивеном Мерити, это новейшая разработка.

590
02:00:03,690 --> 02:00:12,715
Её отличительная особенность - много различных видов дропаута в разных частях модели.

591
02:00:12,815 --> 02:00:22,540
Мы подробно обсудим архтектуру этой модели на последней лекции, сейчас не будем на этом останавливаться.

592
02:00:22,640 --> 02:00:29,610
Обращайтесь с этими вероятностями дропаута как обычно: если модель недообучается - пропорционально уменьшите их,

593
02:00:29,710 --> 02:00:40,100
если переобучается - пропорционально увеличьте. Я так и делаю.

594
02:00:40,200 --> 02:00:53,820
Модель совсем новая, мало кем опробованная, оптимальных значений пока нет, но мы со Стивеном используем такие.

595
02:00:53,820 --> 02:00:59,905
Есть ещё один способ избежать переобучения, мы обсудим его на последней лекции,

596
02:01:00,005 --> 02:01:09,000
а пока вы можете просто вставлять эту строчку во все задачи в NLP и не задумываться об этом.

597
02:01:09,000 --> 02:01:16,700
Поле .clip мы тоже обсудим на последней лекции, если вкратце -

598
02:01:17,060 --> 02:01:36,870
при умножении градиента на скорость обучения полученное число не может быть больше 0.3.

599
02:01:36,870 --> 02:01:57,690
Это полезно, потому что позволяет избежать расхождения алгоритма обучения при больших скоростях обучения, мы это обсуждали.

600
02:02:01,320 --> 02:02:12,540
При ограничении в 0.3 слишком большие шаги будут обрезаться до значения 0.3 и расхождения не будет.

601
02:02:12,540 --> 02:02:17,580
Полный смысл параметров сейчас не важен, можете просто использовать то, что есть.

602
02:02:17,680 --> 02:02:23,190
После этого модель обучается методом .fit(), как обычно.

603
02:02:26,160 --> 02:02:50,310
Янет: Существуют готовые эмбеддинги вроде Word2vec или GloVe. Почему вы их не используете?

604
02:02:50,310 --> 02:03:15,330
Отличный вопрос. Word2vec и GloVe предоставляют готовые матрицы эмбеддинга, это не предобученные модели.

605
02:03:15,330 --> 02:03:31,885
Их можно скачать и использовать. Я пробовал использовать только их для построения предобученной модели, улучшения не заметил,

606
02:03:31,985 --> 02:03:36,190
созданная с нуля предобученная модель работала гораздо лучше.

607
02:03:36,290 --> 02:03:46,120
Word2vec вызвал большой резонанс при появлении, но я думаю, что мой подход работает лучше.

608
02:03:46,220 --> 02:03:54,010
Я думаю, можно попробовать скомбинировать word2vec и такие модели и посмотреть, станет ли лучше.

609
02:03:54,110 --> 02:04:01,010
Вопрос из зала: Как выглядит архитектура используемой модели?

610
02:04:01,010 --> 02:04:15,415
Это рекуррентная нейронная сеть c использованием долгой краткосрочной памяти (LSTM), мы обсудим её на последней лекции.

611
02:04:15,515 --> 02:04:22,585
Я опускаю многие технические детали, но они сейчас не так важны.

612
02:04:22,685 --> 02:04:43,230
Можно переходить к обучению. Модель обучалась долго, поэтому я несколько раз сохранял её в процессе.

613
02:04:45,540 --> 02:04:52,780
В какой-то момент у меня кончилось терпение и я перестал обучать.

614
02:04:52,880 --> 02:05:05,300
Потом я прогнал тест, на который мы уже смотрели. Модель работает неплохо.

615
02:05:05,400 --> 02:05:18,600
Теперь можно использовать её в качестве предобученной модели для анализа тональности текста.

616
02:05:18,600 --> 02:05:40,015
Для анализа тональности текста мне потребуется словарь перевода слов в числа, поэтому первым делом я загружаю его в объект TEXT.

617
02:05:40,115 --> 02:05:50,650
Если выполнять ячейки Jupyter ноутбука параллельно, словарь и так будет в памяти, его не нужно будет заново загружать.

618
02:05:50,750 --> 02:06:08,075
После этого я добавляю целевую переменную IMDB_LABEL, которая говорит, положительный отзыв или отрицательный.

619
02:06:08,175 --> 02:06:20,910
Теперь мне нужно разбить огромный текст обратно на отзывы, потому что у всех отзывов разная тональность.

620
02:06:20,910 --> 02:06:29,095
В torchtext уже есть датасет, с которым мы работаем, потэому я просто вызываю его.

621
02:06:29,195 --> 02:06:43,492
После этого можно посмотреть на пример текста, этот отзыв - позитивный.

622
02:06:43,592 --> 02:06:55,830
В этих строках не используется fastai, только torchtext. Мы ещё их обсудим, пока можете почитать документацию.

623
02:06:55,830 --> 02:07:06,540
Всё, что вам нужно знать - объект, созданный методом .splits() в torchtext, передаётся в метод TextData.from_splits() в fastai.

624
02:07:06,540 --> 02:07:14,635
Это конвертирует объект torchtext в объект данных модели fastai, после чего его можно использовать для обучения.

625
02:07:14,735 --> 02:07:20,425
Алгоритм обучения создаётся из объекта данных модели методом .get_model().

626
02:07:20,525 --> 02:07:26,590
После создания модели в неё загружается предобученная языковая модель.

627
02:07:26,690 --> 02:07:41,665
После этого выполняются привычные шаги - заморозка слоёв, обучение, разморозка, снова обучение.

628
02:07:41,765 --> 02:07:52,045
Модель обучается очень быстро при наличии предобученной модели - каждая эпоха занимает пару минут.

629
02:07:52,145 --> 02:08:05,953
Максимальное значение доли правильных ответов в 94.5% достигнуто на 10 эпохе, то есть после 20 минут обучения.

630
02:08:06,053 --> 02:08:10,590
Насколько хороша доля правильных ответов 94.5%?

631
02:08:10,590 --> 02:08:15,600
Один из коллег Стивена Мерити, Джеймс Брэдбери, написал обзорную статью

632
02:08:15,600 --> 02:08:22,810


633
02:08:22,810 --> 02:08:25,690


634
02:08:28,720 --> 02:08:36,310


635
02:08:36,310 --> 02:08:40,750


636
02:08:40,750 --> 02:08:47,980


637
02:08:47,980 --> 02:08:55,120


638
02:08:55,120 --> 02:09:01,150


639
02:09:01,150 --> 02:09:05,620


640
02:09:05,620 --> 02:09:12,070


641
02:09:12,070 --> 02:09:17,370


642
02:09:17,370 --> 02:09:21,340


643
02:09:21,340 --> 02:09:24,900


644
02:09:27,370 --> 02:09:32,200


645
02:09:32,200 --> 02:09:39,400


646
02:09:39,400 --> 02:09:42,790


647
02:09:42,790 --> 02:09:50,320


648
02:09:50,320 --> 02:09:55,030


649
02:09:55,030 --> 02:09:59,530


650
02:09:59,530 --> 02:10:03,310


651
02:10:03,310 --> 02:10:08,680


652
02:10:08,680 --> 02:10:13,030


653
02:10:15,760 --> 02:10:21,670


654
02:10:21,670 --> 02:10:26,830


655
02:10:26,830 --> 02:10:30,340


656
02:10:30,340 --> 02:10:36,460


657
02:10:36,460 --> 02:10:41,500


658
02:10:41,500 --> 02:10:47,170


659
02:10:47,170 --> 02:10:51,640


660
02:10:51,640 --> 02:10:56,560


661
02:10:56,560 --> 02:11:02,920


662
02:11:02,920 --> 02:11:07,989


663
02:11:10,620 --> 02:11:15,250


664
02:11:15,250 --> 02:11:20,530


665
02:11:20,530 --> 02:11:25,570


666
02:11:25,570 --> 02:11:32,560


667
02:11:32,560 --> 02:11:36,730


668
02:11:43,030 --> 02:11:48,670


669
02:11:48,670 --> 02:11:52,060


670
02:11:52,060 --> 02:11:55,900


671
02:11:59,500 --> 02:12:04,030


672
02:12:04,030 --> 02:12:09,100


673
02:12:09,100 --> 02:12:13,150


674
02:12:15,969 --> 02:12:19,690


675
02:12:19,690 --> 02:12:25,840


676
02:12:25,840 --> 02:12:28,930


677
02:12:28,930 --> 02:12:32,080


678
02:12:32,080 --> 02:12:36,400


679
02:12:36,400 --> 02:12:39,730


680
02:12:39,730 --> 02:12:46,780


681
02:12:46,780 --> 02:12:50,260


682
02:12:50,260 --> 02:12:56,619


683
02:12:58,599 --> 02:13:05,289


684
02:13:05,289 --> 02:13:08,170


685
02:13:08,170 --> 02:13:12,519


686
02:13:12,519 --> 02:13:17,559


687
02:13:17,559 --> 02:13:22,599


688
02:13:22,599 --> 02:13:26,650


689
02:13:28,449 --> 02:13:31,809


690
02:13:31,809 --> 02:13:36,480


691
02:13:39,010 --> 02:13:42,760


692
02:13:42,760 --> 02:13:46,239


693
02:13:47,829 --> 02:13:51,699


694
02:13:51,699 --> 02:13:57,670


695
02:13:57,670 --> 02:14:03,940


696
02:14:06,150 --> 02:14:12,579


697
02:14:12,579 --> 02:14:16,000


698
02:14:16,000 --> 02:14:19,329


699
02:14:19,329 --> 02:14:23,199


700
02:14:23,199 --> 02:14:27,820


701
02:14:27,820 --> 02:14:32,819


