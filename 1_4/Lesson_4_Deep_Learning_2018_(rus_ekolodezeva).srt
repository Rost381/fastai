1
00:00:00,060 --> 00:00:04,944
Добро пожаловать на четвёртую лекцию.

2
00:00:05,044 --> 00:00:23,330
Эта неделя на форуме была активной. Вы написали несколько полезных статей, как и на прошлой неделе, давайте на них посмотрим.

3
00:00:24,480 --> 00:00:35,910
Vitaly Bushaev написал один из лучших постов за последнее время про скорость обучения и SGDR.

4
00:00:39,960 --> 00:00:49,569
Он проделал отличную работу, объяснив базовые идеи понятным всем языком,

5
00:00:49,669 --> 00:00:55,079
но также добавил ссылки на научные статьи для тех, кто хочет узнать больше,

6
00:00:55,079 --> 00:01:08,755
в посте много примеров и детальных объяснений, я считаю, что такие посты очень полезны в нашей области.

7
00:01:08,855 --> 00:01:32,939
Мне очень нравится, что материал перед публикацией обсуждается и улучшается на форуме.

8
00:01:32,939 --> 00:01:49,590
Циклическая скорость обучения — популярная тема, Anand Saha тоже написал про это пост,

9
00:01:49,590 --> 00:02:01,670
опять же — отличная работа, много картинок, ссылок на научные статьи и кода, объясняющего, как всё работает.

10
00:02:01,950 --> 00:02:07,600
Mark Hoffman написал хорошее введение в SGDR.

11
00:02:09,580 --> 00:02:17,950
Manikanta Yadunanda написал про дифференциальные скорости обучения

12
00:02:18,290 --> 00:02:31,019
и их применение в переносе обучения, добавив введение в перенос обучения.

13
00:02:31,019 --> 00:02:59,829
Arjun Rajkumar написал пост о компьютерном зрении, мне нравится, что он упомянул реальные применения этой технологии.

14
00:02:59,829 --> 00:03:06,904
Есть другие отличные посты, спасибо всем вам за такую активность.

15
00:03:07,004 --> 00:03:28,350
Повторюсь — если вы хотите написать что-то своё, но боитесь — не стоит, на форуме люди дружелюбны и готовы вам помочь.

16
00:03:30,690 --> 00:03:37,930
Сегодня будет интересная лекция, много информации на совершенно разные темы.

17
00:03:38,030 --> 00:03:51,090
Мы потратили довольно много времени на компьютерное зрение, а сегодня обсудим сразу три области.

18
00:03:51,090 --> 00:04:09,180
Начнём с анализа структурированных данных, то есть данных, хранящихся в таблицах, как базы данных.

19
00:04:09,180 --> 00:04:21,954
Затем посмотрим на обработку естественного языка и на рекомендательные системы.

20
00:04:22,054 --> 00:04:31,080
Мы рассмотрим эти темы на достаточно высоком уровне, но акцент будет на практической части, а не теоретической.

21
00:04:33,510 --> 00:04:46,675
Теоретическая часть будет в следующих трёх лекциях, там же мы ещё раз обсудим компьютерное зрение.

22
00:04:46,775 --> 00:04:58,000
Сегодня мы сфокусируемся на практическом применении этих идей и введём несколько новых понятий.

23
00:04:58,100 --> 00:05:14,980
Одно из таких понятий — дропаут, вы наверняка уже слышали про него, это полезная техника.

24
00:05:15,080 --> 00:05:21,145
Я продемонстрирую её на примере соревнования Kaggle про породы собак.

25
00:05:21,245 --> 00:05:40,770
Я создал модель и поставил precompute=True, чтобы получить предвычисленные активации на последнем свёрточном слое.

26
00:05:40,770 --> 00:05:57,960
Напоминаю, что активация — это число, вычисленное с помощью весов, или параметров, в свёрточных фильтрах,

27
00:05:57,960 --> 00:06:09,460
которые применяются к предыдущему слою, содержащему активации либо входные данные.

28
00:06:09,560 --> 00:06:14,210
Помните, что активация — это число. Итак, мы используем предвычисленные активации,

29
00:06:17,550 --> 00:06:27,480
а затем добавляем несколько полносвязных слоёв, заполненных случайными числами,

30
00:06:27,480 --> 00:06:43,009
на которые будут умножаться активации, как в нашей демонстрации в Excel.

31
00:06:43,009 --> 00:06:58,870
Если вы выведете содержимое объекта learn, увидите, какие слои мы добавили в модель.

32
00:06:58,870 --> 00:07:02,900
Нормализацию минибатчей мы обсудим на последней лекции.

33
00:07:02,900 --> 00:07:18,505
Линейный слой — это просто матрица 1024x512, то есть слой принимает 1024 активации и возвращает 512.

34
00:07:18,605 --> 00:07:26,750
Это выпрямитель, он заменяет отрицательные числа на нули.

35
00:07:26,750 --> 00:07:38,594
Второй линейный слой берёт 512 активаций с предыдущего слоя, умножает их на матрицу 512x120 и получает 120 новых активаций.

36
00:07:38,694 --> 00:07:48,829
Результат передаётся в функцию Softmax, которую мы обсуждали на прошлой неделе:

37
00:07:48,929 --> 00:08:02,180
она берёт экспоненту от числа и делит её на сумму экспонент от всех чисел, получает вероятность.

38
00:08:02,180 --> 00:08:09,850
Все вероятности в сумме дают 1 и каждая вероятность лежит между 0 и 1.

39
00:08:09,950 --> 00:08:16,595
Итак, это слои, добавленные поверх предвычисленных свёрточных слоёв, они обучаются при precompute=True.

40
00:08:16,695 --> 00:08:24,404
Давайте обсудим, что такое дропаут и что значит параметр p.

41
00:08:24,504 --> 00:08:53,085
Дропаут с параметром p=0.5 значит, что случайная половина всех активаций слоя уничтожается.

42
00:08:53,185 --> 00:09:03,970
Параметр p — вероятность удаления активации, p=0.5 значит удаление 50% активаций слоя.

43
00:09:03,970 --> 00:09:14,924
При удалении половины активаций следующий слой почти не меняется,

44
00:09:15,024 --> 00:09:25,100
так как этот слой — подвыборка максимумом, и что-то меняется, только если удалённая ячейка была максимальной из четырёх.

45
00:09:25,100 --> 00:09:31,360
Если следующий слой — свёрточный, он тоже меняется несильно, потому что свёрточные фильтры рассматривают области 3x3.

46
00:09:31,460 --> 00:09:53,060
Результат интересный. Важно понимать, что после каждого минибатча выбрасывается случайная половина активаций.

47
00:09:53,060 --> 00:10:08,279
Это помогает избежать переобучения. Если какая-то группа активаций очень хорошо предсказывает определённую кошку или собаку,

48
00:10:08,379 --> 00:10:18,405
то после исчезновения этих активаций модель больше не сможет сказать, что находится на этом изображении,

49
00:10:18,505 --> 00:10:31,485
поэтому ей придётся находить признаки, которые будут работать, даже если половина активаций каждый раз выбрасывается.

50
00:10:31,585 --> 00:10:50,310
Этой технике три или четыре года, и она почти полностью решает проблему переобучения.

51
00:10:50,410 --> 00:11:12,079
До того, как появился дропаут, люди не знали, что делать с переобучением, когда дополнение данных и новые данные перестают помогать.

52
00:11:12,079 --> 00:11:35,870
Джеффри Хинтон и его коллеги изобрели дропаут, вдохновляясь работой человеческого мозга.

53
00:11:35,870 --> 00:11:57,539
Если вероятность p=0.01, это значит, что 1% случайных активаций выбрасывается. Это почти ничего не изменит и не спасёт от переобучения.

54
00:11:57,639 --> 00:12:17,805
Если p=0.99, выбросится 99% активаций, переобучения точно не будет, но доля правильных ответов на обучающей выборке будет низкой.

55
00:12:17,905 --> 00:12:32,509
Высокие p позволят хорошо обобщать, но снизят долю правильных ответов на обучающей выборке, низкие — наоборот.

56
00:12:32,509 --> 00:12:43,589
Дропаут объясняет, почему в начале обучения потери на обучающей выборке были больше потерь на валидационной.

57
00:12:43,689 --> 00:12:59,870
Это может показаться странным, но объяснение простое — на валидационной выборке не используется дропаут.

58
00:12:59,870 --> 00:13:10,529
При предсказаниях валидационной выборки хочется использовать наилучшую из возможных моделей, поэтому активации не выбрасывают.

59
00:13:10,629 --> 00:13:26,520
Поэтому иногда, особенно в начале обучения, доля правильных ответов и функция потерь на обучающей выборке хуже, чем на валидационной.

60
00:13:26,520 --> 00:13:30,460
Янет: Надо ли чем-то заменять выброшенные активации?

61
00:13:30,460 --> 00:13:41,375
При использовании дропаута PyTorch делает две вещи. Положим p=0.5.

62
00:13:41,475 --> 00:13:47,600
PyTorch под капотом, во-первых, выкидывает половину активаций, во-вторых, дублирует оставшиеся,

63
00:13:47,700 --> 00:13:56,885
поэтому в среднем активации не меняются, это удобно.

64
00:13:56,985 --> 00:14:06,520
Вам не нужно об этом думать, это делается автоматически.

65
00:14:06,520 --> 00:14:18,370
Вероятность дропаута для всех новых слоёв задаётся параметром ps метода ConvLearner.pretrained().

66
00:14:18,370 --> 00:14:27,670
Дропаут в предобученных слоях не изменится, потому что эти слои уже были обучены с какой-то вероятностью дропаута,

67
00:14:29,740 --> 00:14:33,220
настраивается дропаут только в новых слоях.

68
00:14:33,220 --> 00:14:40,839
Здесь ps=0.5, поэтому в объекте learn первый и второй дропауты оба имеют вероятность p=0.5.

69
00:14:40,839 --> 00:14:46,390
Напомню, что на вход этих слоёв подаются результаты последнего свёрточного слоя предобученной сети,

70
00:14:48,820 --> 00:14:55,804
половина активаций выбрасывается, оставшееся подаётся в линейный слой, пропускается через выпрямитель,

71
00:14:55,904 --> 00:15:05,230
снова половина активаций выбрасывается, оставшееся подаётся в линейный слой, а потом подаётся в функцию Softmax.

72
00:15:05,230 --> 00:15:11,950
Для небольшого увеличения точности расчётов имеет смысл брать логарифм Softmax, а не просто Softmax,

73
00:15:12,520 --> 00:15:22,480
потому что потом от этого берётся экспонента, но это неважно.

74
00:15:22,480 --> 00:15:36,380
Если убрать дропаут, поставив ps=0, доля правильных ответов в первую эпоху увеличится с 0.76 до 0.8,

75
00:15:36,380 --> 00:15:45,649
что неудивительно, потому что мы перестали выбрасывать важные результаты в начале обучения.

76
00:15:45,649 --> 00:15:52,350
Но на третьей эпохе результаты другие — доля правильных ответов была 84.8%, а стала 84.1%.

77
00:15:52,450 --> 00:16:02,489
Модель сильно переобучилась уже после трёх эпох — потери равны 0.35 на обучающей выборке и 0.55 на валидационной.

78
00:16:02,589 --> 00:16:15,975
Как видите, в модели нет дропаута, если ps=0, эти слои даже не добавляются.

79
00:16:16,075 --> 00:16:29,450
В модели два линейных слоя — это не обязательно.

80
00:16:29,450 --> 00:16:41,720
В параметр xtra_fc (extra fully connected layers) передаётся количество полносвязных слоёв и их размеры.

81
00:16:41,720 --> 00:16:51,170
Должен быть хотя бы один полносвязный слой, который примет на вход результат работы последнего свёрточного слоя,

82
00:16:51,170 --> 00:16:56,660
здесь размер последнего слоя 1024, и вернёт вектор длиной в количество классов —

83
00:16:56,660 --> 00:17:05,024
2 класса в задаче классификации собак и кошек, 120 пород собак и 17 типов спутниковых снимков.

84
00:17:05,124 --> 00:17:14,445
Размеры единственного линейного слоя нельзя выбрать, они определяются задачей, но можно выбрать размеры дополнительных.

85
00:17:14,545 --> 00:17:25,880
Здесь мы задаём xtra_fc=[], в модели будет только один линейный слой. Вероятность дропаута ps=0.

86
00:17:25,880 --> 00:17:35,080
Это минимальный набор дополнительных слоёв.

87
00:17:36,800 --> 00:17:53,059
Даже так мы получаем неплохие результаты, потому что обучение было недолгим и предобученная модель подходит к датасету.

88
00:17:53,059 --> 00:18:01,640
Янет: Какое значение вероятности дропаута использовать по умолчанию?

89
00:18:01,640 --> 00:18:20,710
По умолчанию fastai выставляет p=0.25 для первого слоя дропаута и p=0.5 для второго, это обычно работает.

90
00:18:20,710 --> 00:18:38,080
Если ваша модель переобучается. поставьте обе вероятности ps=0.5, не помогло — ps=0.7 и так далее.

91
00:18:38,080 --> 00:18:47,320
Если модель недообучается, попробуйте уменьшить вероятность, скорее всего, сильно уменьшать не придётся.

92
00:18:47,420 --> 00:19:06,089
Вероятность дропаута редко приходится уменьшать, обычно приходится увеличивать, и значения 0.6-0.7 для меня оптимальны.

93
00:19:06,189 --> 00:19:21,689
Я увеличил вероятность дропаута ps=0.5 в Jupyter ноутбуке с породами собак, когда перешёл с ResNet34 на ResNet50 —

94
00:19:21,789 --> 00:19:32,984
У ResNet34 меньше параметров и она реже переобучается, в отличие от ResNet50, которая начала переобучаться.

95
00:19:33,084 --> 00:19:41,350
С большими моделями обычно сложнее.

96
00:19:42,000 --> 00:19:49,169
Вопрос из зала: Параметр p=0.5 значит вероятность в 50%?
Да.

97
00:19:49,169 --> 00:20:01,174
Вопрос из зала: Как понять, что модель переобучается?

98
00:20:01,474 --> 00:20:11,889
Если потери на обучающей выборке меньше потерь на валидационной.

99
00:20:11,889 --> 00:20:23,914
Не стоит концентрироваться на переобучении, просто старайтесь уменьшить потери на валидационной выборке.

100
00:20:24,014 --> 00:20:38,919
Экспериментируйте и научитесь понимать, что работает лучше и когда модель сильно переобучена.

101
00:20:38,919 --> 00:20:51,376
Окей, так устроен дропаут, не забывайте, что он есть по умолчанию. Ещё вопрос?

102
00:20:51,476 --> 00:21:07,809
Вопрос из зала: Вероятность дропаута p=0.5 удаляет каждую активацию с вероятностью 50% или выбирает половину и удаляет её?

103
00:21:07,809 --> 00:21:12,639
Удаляет каждую активацию с вероятностью 50%.

104
00:21:12,639 --> 00:21:19,960
Вопрос из зала: Почему имеет значение среднее активаций?

105
00:21:19,960 --> 00:21:40,139
Вернёмся к демонстрации в Excel. Это число получено перемножением четырёх групп чисел по 9.

106
00:21:40,139 --> 00:21:52,624
Если выкинуть половину чисел на одном слое, числа на другом слое будут примерно вдвое меньше, и так на каждом слое после.

107
00:21:52,724 --> 00:22:03,970
Если раньше ухо считалось пушистым при значении выше 0.6, теперь это значение — 0.3, меняется значение активаций,

108
00:22:04,070 --> 00:22:09,825
а хочется выкидывать половину активаций так, чтобы ничего не менялось.

109
00:22:09,925 --> 00:22:19,400
Вопрос из зала: Почему первый слой — линейный?

110
00:22:19,400 --> 00:22:42,260
Потому что предобученная модель возвращает вектор, и после последнего свёрточного слоя можно поставить только линейный.

111
00:22:42,260 --> 00:22:47,690
Вопрос из зала: Можно ли задавать разные вероятности дропаута для разных слоёв?

112
00:22:47,690 --> 00:23:00,797
Да, в параметр ps можно передать массив, например ps=[0, 0.2] и xtra_fc=[512]

113
00:23:00,897 --> 00:23:13,650
значит отсутствие дропаута перед первым линейным слоем и дропаут с вероятностью p=0.2 перед вторым.

114
00:23:13,750 --> 00:23:28,100
Даже спустя несколько лет я до сих пор не понял, как разумно выставлять дропаут для разных слоёв,

115
00:23:30,260 --> 00:23:35,690
если кто-то из вас придумает хорошие правила, расскажите.

116
00:23:35,790 --> 00:23:52,450
Если вы не знаете, попробуйте либо дропаут с одинаковой вероятностью для всех слоёв, либо дропаут только на последнем слое.

117
00:23:53,670 --> 00:24:04,565
Вопрос из зала: Почему вы отслеживаете значения функции потерь, а не долю правильных ответов?

118
00:24:04,665 --> 00:24:13,920
Потому что потери известны и для обучающей, и для валидационной выборки.

119
00:24:13,920 --> 00:24:31,310
Дальше мы узнаем, что оптимизируется именно функция потерь, поэтому её проще отслеживать и понимать.

120
00:24:31,410 --> 00:24:44,225
Вопрос из зала: Дропаут добавляет случайный шум на каждой итерации, должна уменьшаться скорость обучения. Это надо корректировать?

121
00:24:44,325 --> 00:25:01,415
Теоретически да, скорость обучения должна уменьшаться, но на практике это ни на что не влияет.

122
00:25:01,515 --> 00:25:13,600
Давайте поговорим про анализ структурированных данных. Напомню, что мы анализировали данные соревнования Kaggle

123
00:25:13,600 --> 00:25:26,060
на данных Rossman, это немецкая сеть супермаркетов. Jupyter ноутбук называется lesson3-rossman.ipynb.

124
00:25:26,160 --> 00:25:36,880
Главный датасет содержал данные о продажах разных магазинов в разные дни,

125
00:25:36,880 --> 00:25:53,890
а также информацию о том, был ли открыт магазин, был ли в этот день праздник или акция и так далее.

126
00:25:53,890 --> 00:26:04,750
Была таблица о магазинах, содержащая тип магазина и ассортимента и информацию о ближайших конкурентах.

127
00:26:04,750 --> 00:26:11,590
Столбцы, или признаки, в таких таблицах можно поделить на категориальные и количественные.

128
00:26:11,590 --> 00:26:19,715
Категориальные признаки — это различные категории, например, ассортимент товара Assortment — 'a', 'b' или 'c'.

129
00:26:19,815 --> 00:26:27,910
Количественные признаки выражаются числом, например, расстояние до ближайшего конкурента CompetitionDistance —

130
00:26:27,910 --> 00:26:32,380
имеют значения различия между числами и их абсолютные величины.

131
00:26:32,380 --> 00:26:37,720
Эти два вида признаков обрабатываются по-разному.

132
00:26:37,720 --> 00:26:49,230
Тем, кто занимался машинным обучением, знакомы количественные признаки — на них легко построить линейную регрессию.

133
00:26:49,230 --> 00:26:54,850
С категориальными признаками чуть сложнее.

134
00:26:54,850 --> 00:27:06,944
Мы не будем смотреть на предобработку данных и выделение признаков, в итоге получается такая таблица и такие признаки.

135
00:27:07,044 --> 00:27:29,045
Напомню, что я не выделял признаки, это код обладателей третьего места в соревновании.

136
00:27:29,145 --> 00:27:46,660
Признаки разделены на категориальные и количественные. Год, месяц и день можно считать количественными признаками,

137
00:27:46,660 --> 00:27:52,179
потому что разница между, например, 2000 и 2003 имеет значение,

138
00:27:52,179 --> 00:28:14,570
но здесь они категориальные. Модель считает, что 2000, 2001 и 2002 года никак не связаны между собой.

139
00:28:14,670 --> 00:28:23,109
Если бы год был количественным признаком, модель строила бы гладкую функцию под все возможные значения.

140
00:28:23,209 --> 00:28:36,415
Поэтому количественные признаки с небольшим разбросом дискретных значений иногда удобнее считать категориальными.

141
00:28:36,515 --> 00:28:47,529
Ещё один пример — день недели. Можно пронумеровать дни от 0 до 6, потому что разница между 3 и 5, два дня, имеет смысл.

142
00:28:47,629 --> 00:28:56,609
Но если вдуматься, продажи могут различаться в зависимости от дня недели, а не его номера —

143
00:28:56,609 --> 00:29:05,700
в выходные один уровень продаж, по пятницам другой, в среду третий.

144
00:29:05,700 --> 00:29:12,909
Поэтому мы считаем день недели категориальным признаком.

145
00:29:13,009 --> 00:29:24,399
Вам придётся принимать решения о том, какой признак считать категориальным, а какой количественным.

146
00:29:24,499 --> 00:29:37,619
Если в столбце значения закодированы как 'B' или 'С', или как 'Джереми' и 'Янет', у вас нет выбора — это категориальный признак.

147
00:29:37,619 --> 00:29:51,415
Если признак закодирован числом, например, возраст или день недели, вам нужно решить, как это представить.

148
00:29:51,515 --> 00:29:59,199
Итак, если признак категориальный, он таким и останется, но количественный признак можно перевести в категориальный.

149
00:29:59,299 --> 00:30:15,594
Здесь я не принимал никаких решений — признаки были разделены на количественные и категориальные обладателями третьего места.

150
00:30:15,694 --> 00:30:29,545
Как видите, все количественные признаки содержат дробные числа, например, CompetitionDistance и температурные признаки.

151
00:30:29,645 --> 00:30:46,300
Эти признаки сложно сделать качественными, потому что почти все значения уникальны.

152
00:30:46,300 --> 00:30:54,520
Количество возможных значений признака обозначается как мощность.

153
00:30:54,520 --> 00:31:00,390
Мощность признака дня недели — семь, потому что различных дней недели семь.

154
00:31:02,010 --> 00:31:09,165
Вопрос из зала: Вы используете сегментацию признаков?

155
00:31:09,265 --> 00:31:13,033
Нет, не использую, хотя здесь

156
00:31:13,133 --> 00:31:23,140
максимальную температуру можно было бы разбить на категории 0-10, 10-20 и 20-30 и превратить в количественный признак.

157
00:31:23,140 --> 00:31:36,160
Неделю назад вышла статья, авторы которой утверждают, что сегментация признаков — это полезно, это первое известное мне практическое упоминание.

158
00:31:36,260 --> 00:31:44,507
Ещё неделю назад я сказал бы, что это плохая идея, но сейчас не уверен, возможно, это может быть полезно.

159
00:31:44,607 --> 00:32:00,880
Вопрос из зала: Если год — категориальный признак, что произойдёт, показать модели год, которого она ещё не видела?

160
00:32:00,880 --> 00:32:16,760
Мы это ещё обсудим. Короткий ответ — этот год будет считаться неизвестной категорией, pandas умеет с таким обращаться.

161
00:32:16,860 --> 00:32:24,840
По сути, это будет просто ещё одна категория «Неизвестная категория».

162
00:32:25,130 --> 00:32:35,980
Вопрос из зала: Если неизвестная категория будет в тестовой выборке, что предскажет модель?

163
00:32:36,080 --> 00:32:46,679
Ну, что-нибудь да предскажет. Если в обучающей выборке были неизвестные категории,

164
00:32:46,679 --> 00:32:55,164
она научится с таким обращаться, если нет — вставит случайный вектор.

165
00:32:55,264 --> 00:33:05,529
Это интересная тема, её стоит обсудить в этой части курса, и точно можно обсудить на форуме.

166
00:33:05,529 --> 00:33:21,764
Итак, мы разделили признаки на категориальные и количественные, в таблице 844,338 записей.

167
00:33:21,864 --> 00:33:36,835
Мы преобразуем данные признаков, которые решили сделать категориальными, методом astype('category').

168
00:33:36,935 --> 00:33:48,379
Я не буду рассказывать про pandas, про это есть куча хороших книг, например, Python for Data Analysis, я про неё уже говорил.

169
00:33:48,379 --> 00:33:53,564
Надеюсь, что и так понятно, что делает код.

170
00:33:53,664 --> 00:34:04,149
Итак, категориальные признаки преобразуются в тип category, а количественные — в 32-битное число с плавающей точкой,

171
00:34:04,249 --> 00:34:10,569
потому что с таким форматом чисел работает PyTorch.

172
00:34:10,669 --> 00:34:30,584
Некоторые количественные признаки, например, Promo, содержат значения 0 или 1, они тоже преобразуются в числа с плавающей точкой.

173
00:34:30,684 --> 00:34:44,339
Я стараюсь проделать большую часть обучения на маленьких датасетах — например, уменьшать изображения до размера 128 или 64.

174
00:34:44,439 --> 00:34:52,460
Структурированные данные нельзя уменьшить, вместо этого я случайно выбираю подмножество данных.

175
00:34:52,460 --> 00:35:07,609
Для этого я использую уже знакомую нам функцию get_cv_idxs() для получения индексов валидационной выборки.

176
00:35:07,609 --> 00:35:21,300
В итоге я получаю 150 тысяч случайных записей.

177
00:35:21,300 --> 00:35:32,910
Вот так они выглядят — есть булевые переменные, есть строки, есть целые числа.

178
00:35:37,020 --> 00:35:52,480
Несмотря на то, что я перевёл эти признаки в категориальные, они всё ещё показываются так, просто хранятся по-другому.

179
00:35:52,580 --> 00:36:05,070
В fastai есть функция proc_df() (processed dataframe), которая принимает датасет и название целевой переменной.

180
00:36:05,070 --> 00:36:22,589
Функция делает несколько вещей. Во-первых, она отделяет целевую переменную в объект y и датасет без неё в объект df.

181
00:36:22,589 --> 00:36:36,865
Она нормирует данные — нейронным сетям удобно, когда все данные близки к 0 со стандартным отклонением около 1.

182
00:36:36,965 --> 00:36:43,650
Для этого из каждого столбца вычитается среднее и все числа делятся на стандартное отклонение.

183
00:36:43,650 --> 00:36:51,900
Это регулируется параметром do_scale=True, при этом средние и стандартные отклонения сохраняются в объекте mapper,

184
00:36:51,900 --> 00:36:57,780
чтобы потом обработать тестовую выборку таким же образом.

185
00:36:57,780 --> 00:37:08,669
Функция вставляет пропущенные значения — неизвестные категории приобретают индекс 0 (известные нумеруются с 1),

186
00:37:08,769 --> 00:37:21,952
неизвестные количественные признаки заменяются медианой, и создаётся отдельный признак «Значение было пропущено».

187
00:37:22,052 --> 00:37:29,369
Я не буду в это углубляться, мы подробно обсудили это в курсе по машинному обучению.

188
00:37:29,369 --> 00:37:36,140
Если у вас есть какие-то вопросы на эту тему, изучите его, там нет ничего про глубокое обучение.

189
00:37:36,900 --> 00:37:42,330
Как видите, после обработки год 2014 стал годом номер 2.

190
00:37:42,330 --> 00:37:47,220
Все категориальные признаки были заменены на целые числа, начиная с нуля,

191
00:37:47,220 --> 00:37:59,560
так как эти признаки потом образуют таблицу, и мы не хотим, чтобы на год ушло 2014 столбцов, когда хватило бы двух.

192
00:37:59,660 --> 00:38:11,950
Как видите, таким же образом строки 'a' и 'c' преобразовались в числа 1 и 3.

193
00:38:12,050 --> 00:38:20,220
Окей, у нас есть датафрейм, не содержащий целевую переменную, где все значения — это числа.

194
00:38:20,220 --> 00:38:35,050
Всё, что мы делали до этого момента, не содержало никакого глубокого обучения и обсуждалось в курсе по машинному обучению.

195
00:38:35,150 --> 00:38:41,200
Одна из вещей, которые подробно обсуждались в курсе по машинному обучению — валидационные выборки.

196
00:38:41,300 --> 00:38:54,510
По условиям соревнования Kaggle нужно предсказать следующие две недели продаж.

197
00:38:54,510 --> 00:39:05,755
Я возьму последние две недели обучающей выборки в качестве валидационной, чтобы максимально приблизить её к тестовой.

198
00:39:05,855 --> 00:39:18,279
Рейчел недавно написала пост про создание валидационных выборок, он есть на fast.ai, мы добавим ссылку в вики.

199
00:39:18,379 --> 00:39:34,460
По сути, это пересказ одной из лекций курса «Машинное обучение», к этой лекции есть видеозапись.

200
00:39:35,579 --> 00:39:42,125
Мы с Рейчел долго думали над тем, как обращаться с обучающей, валидационной и тестовой выборками, результаты в этом посте,

201
00:39:42,225 --> 00:39:50,710
но, опять же — там нет ничего про глубокое обучение, давайте уже к нему перейдём.

202
00:39:50,710 --> 00:40:05,479
В каждом соревновании Kaggle и вообще в любом проекте, связанном с машинным обучением, необходимо выбрать метрику.

203
00:40:05,579 --> 00:40:17,770
На соревновании Kaggle метрику оценки качества модели выбрали за нас — это средняя процентная ошибка (RMSPE).

204
00:40:17,770 --> 00:40:25,245
Функция ошибки возвращает корень из среднего квадрата процентного отклонения предсказания от истинной величины.

205
00:40:25,345 --> 00:40:47,989
В этой метрике имеет значение среднее отношение предсказания и истинного значения.

206
00:40:48,089 --> 00:41:01,930
В PyTorch нет среднеквадратичной процентной ошибки, но мы можем написать её сами.

207
00:41:01,930 --> 00:41:22,380
Проще вспомнить логарифмы и понять, что частное а / b можно заменить на ln(a' / b') = ln(a') - ln(b').

208
00:41:22,380 --> 00:41:41,990
Раньше  мы взяли логарифм от предсказаний и истинных значений, поэтому в функции exp_rmspe() необходимо взять от них экспоненту функцией inv_y().

209
00:41:42,090 --> 00:42:04,510
После взятия экспоненты мы записываем формулу RSMPE и получаем готовую метрику.

210
00:42:04,510 --> 00:42:11,020
Эта часть тоже не относится к глубокому обучению, мы уже к нему переходим.

211
00:42:14,530 --> 00:42:19,162
Пока мы делаем всё то же самое, что и раньше:

212
00:42:19,262 --> 00:42:28,185
Создаём объект данных модели ModelData, который содержит валидационную, обучающую и иногда тестовую выборки,

213
00:42:28,285 --> 00:42:41,110
на его основе создаём алгоритм обучения, ищем скорость обучения, обучаем модель и так далее, это всё вы уже видели.

214
00:42:41,110 --> 00:42:55,030
Различие будет в том, что данные не разложены по папкам и не размечены в файле CSV,

215
00:42:55,030 --> 00:43:08,765
поэтому мы используем метод ColumnarModelData.from_data_frame(), он возвращает объект такой же структуры, как и на прошлых лекциях.

216
00:43:08,865 --> 00:43:24,590
Переменная PATH — директория, в которой хранятся необходимые файлы модели.

217
00:43:24,690 --> 00:43:29,075
Переменная val_idx содержит индексы валидационной выборки, мы объявили её выше.

218
00:43:29,175 --> 00:43:51,290
Переменная df — наш датафрейм. Переменная yl — логарифм целевой переменной y, она должна быть в модели в каком-то виде.

219
00:43:51,390 --> 00:44:03,380
Итак, это индексы валидационной выборки, датафрейм без целевой переменной, целевая переменная, а это —

220
00:44:03,380 --> 00:44:19,769
список категориальных признаков. Напомню, что сейчас все данные хранятся в виде чисел, поэтому важно его указать.

221
00:44:19,769 --> 00:44:30,469
В качестве списка категориальных признаков мы передаём переменную cat_vars, объявленную раньше.

222
00:44:30,469 --> 00:44:39,469
Также передаётся размер минибатча bs=128, этот параметр нам уже знаком.

223
00:44:39,569 --> 00:45:04,349
Теперь у нас есть объект данных модели md, содержащий уже упомянутые объекты train_dl, val_dl, train_ds, val_ds, размер выборки и всё остальное.

224
00:45:04,349 --> 00:45:18,709
После этого мы создадим из объекта данных модели алгоритм обучения методом .get_learner().

225
00:45:21,769 --> 00:45:37,614
Эти параметры — начальная вероятность дропаута, количество активаций на каждом слое и дропаут для последних слоёв.

226
00:45:37,714 --> 00:45:53,519
Один из незнакомых параметров — emb_szc, это эмбеддинг, сейчас мы его обсудим.

227
00:45:53,519 --> 00:46:05,549
Давайте забудем ненадолго про категориальные признаки и рассмотрим только количественные.

228
00:46:05,549 --> 00:46:29,740
Давайте возьмём все количественные признаки — минимальную и максимальную температуры, расстояние до ближайшего конкурента и так далее.

229
00:46:32,589 --> 00:46:36,700
Это всё числа с плавающей точкой.

230
00:46:36,700 --> 00:46:54,130
Нейронная сеть возьмёт этот одномерный массив, или вектор, или тензор первого ранга, и умножит его на матрицу.

231
00:46:54,130 --> 00:47:12,940
Допустим, у нас 20 признаков и мы хотим получить 100 новых, для этого матрица должна иметь размер 20x100.

232
00:47:12,940 --> 00:47:28,410
В итоге получается новый вектор длины 100, так работает линейный слой.

233
00:47:28,410 --> 00:47:39,670
Полученный вектор пропускается через выпрямитель, негативные значения заменяются нулями.

234
00:47:39,670 --> 00:47:59,349
После этого вектор умножается на ещё одну матрицу. Допустим, на этом нейронная сеть кончается, и мы хотим получить одно число.

235
00:47:59,349 --> 00:48:13,444
В таком случае матрица должна иметь размер 100x1. Это — нейронная сеть, состоящая из одного слоя.

236
00:48:13,544 --> 00:48:25,070
На практике обычно делают больше одного слоя, пусть матрица имеет размер 100x50.

237
00:48:25,170 --> 00:48:42,420
После умножения мы получаем новый вектор длиной 50, после умножения на матрицу 50x1 получается число.

238
00:48:42,420 --> 00:49:06,119
Напомню, что не нужно ставить выпрямитель перед Softmax, так как Softmax нужны негативные значения для низких вероятностей.

239
00:49:13,390 --> 00:49:22,400
Вот схема простейшей полносвязной нейронной сети.

240
00:49:22,400 --> 00:49:43,579
На входе — тензор первого ранга, дальше идут линейный слой, выпрямитель, ещё один линейный слой и Softmax.

241
00:49:43,579 --> 00:50:02,839
Можно добавить больше связок выпрямитель+линейный слой, можно добавить дропаут, в принципе, на этом вариации кончаются.

242
00:50:02,839 --> 00:50:12,619
Мы посмотрим на другие архитектуры, когда вернёмся к компьютерному зрению.

243
00:50:12,619 --> 00:50:27,170
Полносвязные нейронные сети —  это просто комбинация перемножения матриц и функций активаций —  например, выпрямителей и Softmax.

244
00:50:27,170 --> 00:50:42,434
Softmax необходим только в задачах классификации, здесь можно без него обойтись.

245
00:50:42,534 --> 00:50:51,440
Есть один нюанс, но в общем всё устроено именно так.

246
00:50:51,440 --> 00:50:56,420
Вернёмся к категориальным признакам.

247
00:50:56,420 --> 00:51:06,019
Один из категориальных признаков — день недели, кодируется числом от 0 до 6.

248
00:51:11,259 --> 00:51:24,252
Я хочу представить его в виде тензора первого ранга, чтобы использовать ту же архитектуру.

249
00:51:24,352 --> 00:51:45,327
Для этого создаётся новая таблица с семью строками и произвольным количеством столбцов, например, четырьмя.

250
00:51:45,427 --> 00:51:57,060
Допустим, в какой-то строке день недели — воскресенье. Вместо числа 0 (индекс воскресенья в категориальном признаке)

251
00:51:57,160 --> 00:52:12,239
мы вставим строку из матрицы 7x4, соответствующую воскресенью. Матрица заполнена числами с плавающей точкой.

252
00:52:12,339 --> 00:52:29,055
В итоге категориальный признак будет представим в виде массива из четырёх чисел, то есть тензора первого ранга.

253
00:52:29,155 --> 00:52:40,990
Изначально все числа в таблице случайные, но в процессе обучения модели изменятся.

254
00:52:41,090 --> 00:52:57,945
Итак, вместо категориального признака во входных данных мы вставляем тензор первого ранга.

255
00:52:58,045 --> 00:53:28,730
Архитектура нейронной сети не меняется. На каждой итерации вычисляются функция потерь и градиент, и эти четыре числа меняются соответственно.

256
00:53:28,730 --> 00:53:42,645
Процесс повторяется много раз, и числа в матрице приобретают смысл — мы получаем лучшие четыре числа для воскресенья,

257
00:53:42,745 --> 00:53:52,300
лучшие четыре числа для пятницы и так далее. По сути, создаётся новая матрица весов.

258
00:53:52,400 --> 00:54:01,480
Такие матрицы называются матрицы эмбеддинга.

259
00:54:03,640 --> 00:54:20,490
Количество строк матрицы эмбеддинга равно мощности признака, индекс строки равен номеру соответствующего значения признака.

260
00:54:20,590 --> 00:54:37,070
Например, для первого значения признака мы взяли бы первую строку таблицы и присоединили её к вектору количественных признаков.

261
00:54:37,070 --> 00:54:50,910
Например, если есть 5,000 различных почтовых индексов, матрица эмбеддинга может иметь размер 5,000x50.

262
00:54:51,010 --> 00:54:56,097
Допустим, необходимо преобразовать почтовый индекс 94003 под номером 4.

263
00:54:56,197 --> 00:55:04,905
Тогда нужно взять строку номер 4 матрицы эмбеддинга и добавить эти 50 чисел к вектору количественных признаков.

264
00:55:05,005 --> 00:55:10,635
После этого эти числа обрабатываются как количественные признаки.

265
00:55:10,735 --> 00:55:15,080
Вопрос из зала: Что значат эти четыре числа?

266
00:55:15,180 --> 00:55:34,965
Отличный вопрос, мы обсудим его, когда дойдём до коллаборативной фильтрации. Думайте о них как об ещё одних весах нейронной сети.

267
00:55:35,065 --> 00:55:47,270
Дальше мы узнаем, что в этих числах можно разглядеть смысл, но не их изначальное значение.

268
00:55:47,270 --> 00:55:52,099
Пока это просто случайные числа, которые мы меняем в процессе обучения.

269
00:55:52,099 --> 00:56:03,059
Вопрос из зала: Почему здесь именно 4 столбца? Для определения этого числа есть алгоритм?

270
00:56:03,140 --> 00:56:18,164
Да, есть. Сначала я составляю список всех признаков и их мощностей.

271
00:56:18,264 --> 00:56:29,089
Например, у нас больше тысячи магазинов и восемь дней недели.

272
00:56:29,089 --> 00:56:42,074
Дней недели восемь, а не семь, потому что нулевой зарезервирован под неизвестные значения, я всегда так делаю.

273
00:56:42,174 --> 00:56:47,835
Мощность признака год четыре — а различных годов три, и так далее.

274
00:56:47,935 --> 00:57:04,180
Для определения количества столбцов я делю мощность пополам и делаю так, чтобы это число было не больше 50.

275
00:57:04,180 --> 00:57:12,780
Вот размеры моих матриц эмбеддинга. В матрице эмбеддинга для магазинов 1116 строк по количеству магазинов и 50 столбцов,

276
00:57:12,880 --> 00:57:20,900
то есть вместо признака магазина добавляется 50 новых количественных признаков.

277
00:57:20,900 --> 00:57:29,190
В матрице для дня недели 8 строк и 4 столбца, каждому дню недели соответствует вектор длины 4.

278
00:57:31,220 --> 00:57:36,279
Вопрос из зала: И так делается для каждого категориального признака?

279
00:57:36,279 --> 00:57:56,310
Да, сначала я создал список, где каждой категории сопоставлялась её мощность, а потом преобразовал эту мощность в размер матрицы эмбеддинга.

280
00:57:56,410 --> 00:58:03,320
Размеры матриц эмбеддинга хранятся в переменной emb_szs (embedding sizes), мы передавали её в метод .get_learner().

281
00:58:03,320 --> 00:58:11,690
Каждому категориальному признаку соответствует матрица эмбеддинга.

282
00:58:11,690 --> 00:58:22,530
Вопрос из зала: Можно ли инициализировать матрицы эмбеддинга не случайными числами?

283
00:58:22,630 --> 00:58:28,920
Да, можно брать случайные матрицы эмбеддинга, а можно предобученные.

284
00:58:29,020 --> 00:58:36,094
Про предобученные мы ещё поговорим, если вкратце — если кто-то в Rossman обучил нейронную сеть

285
00:58:36,194 --> 00:58:45,829
для предсказывания продаж сыра и выложил свои матрицы эмбеддинга,

286
00:58:45,829 --> 00:58:52,790
их можно было бы использовать для предсказаний продаж алкоголя в Rossman.

287
00:58:53,480 --> 00:59:06,920
Например, так делают в Pinterest и Instacart — Instacart так выстраивает маршруты, Pinterest — решает, что показывать на странице.

288
00:59:06,920 --> 00:59:21,819
В обеих компаниях матрицы эмбеддинга — общие для всей компании.

289
00:59:23,260 --> 00:59:36,500
Вопрос из зала: Почему бы просто не использовать прямое кодирование?

290
00:59:36,500 --> 00:59:50,359
Да, можно было бы использовать 7 чисел вместо 4, из которых одно — 1, а остальные — 0, и записать их как числа с плавающей точкой.

291
00:59:52,309 --> 01:00:09,509
Именно так и делают в статистике, это называется фиктивные переменные.

292
01:00:09,609 --> 01:00:24,140
Проблема в том, что в этом случае воскресенью соответствовало бы всего одно число, и поведение было бы линейное.

293
01:00:24,140 --> 01:00:38,430
У нас воскресенье — точка в четырёхмерном пространстве, что позволяет передавать гораздо более сложные зависимости.

294
01:00:38,530 --> 01:00:57,349
Например, если выяснится, что продажи по выходным сильно отличаются, у субботы и воскресенья одно из чисел может совпадать.

295
01:00:57,349 --> 01:01:28,584
Скорее всего, выяснится, что в будни чаще покупают бензин или молоко, а перед выходными и праздниками — вино.

296
01:01:28,684 --> 01:01:41,185
Поэтому один из столбцов матрицы эмбеддинга может значить, например, насколько часто люди отдыхают в этот день.

297
01:01:41,285 --> 01:01:52,860
Наличие нескольких чисел для каждого значения признака позволяет нейронной сети уловить эти закономерности.

298
01:01:52,960 --> 01:02:04,005
Эмбеддинг — проявление идеи распределённого представления, основного принципа работы нейронных сетей.

299
01:02:04,105 --> 01:02:12,980
Нейронные сети обращаются с многомерными пространствами, которые сложно интерпретировать.

300
01:02:12,980 --> 01:02:18,255
Отдельный вес в строке матрицы эмбеддинга может не иметь определённого значения —

301
01:02:18,355 --> 01:02:26,595
оно может зависеть от значения остальных, и зависеть нелинейно.

302
01:02:26,695 --> 01:02:41,540
Это позволяет нейронным сетям улавливать сложные закономерности.

303
01:02:41,540 --> 01:03:07,280
Вопрос из зала: Эмбеддинг можно использовать только с какими-то определёнными категориальными признаками?

304
01:03:07,280 --> 01:03:20,385
Нет. Эмбеддинг подойдёт для любых категориальных признаков, кроме, возможно, признаков с очень большой мощностью.

305
01:03:20,485 --> 01:03:33,140
Но если мощность признака равна, например, 60,000, то это плохой категориальный признак в принципе.

306
01:03:33,140 --> 01:03:47,860
Обладатели третьего места в этом соревновании объявили категориальными все признаки с небольшой мощностью.

307
01:03:47,860 --> 01:03:56,170
Это хорошая практика — это позволит уловить максимальное количество зависимостей.

308
01:03:56,170 --> 01:04:05,470
Для всех количественных признаков нейронная сеть просто подберёт одномерную функцию.

309
01:04:05,470 --> 01:04:24,340
Янет: Применением эмбеддинга мы увеличиваем размерность, но на самом деле уменьшаем, потому что не используем прямое кодирование.

310
01:04:24,340 --> 01:04:39,250
Да, точно. Прямое кодирование увеличивает размерность матриц, но не вносит нового смысла.

311
01:04:39,250 --> 01:04:46,175
Янет: Использование эмбеддинга занимает меньше места, чем прямое кодирование, и полезнее по смыслу.

312
01:04:46,275 --> 01:04:52,720
Да. Давайте заглянем под капот и вспомним линейную алгебру.

313
01:04:52,720 --> 01:04:58,880
Если вы этого не поймёте, ничего, но некоторым это поможет.

314
01:04:58,980 --> 01:05:15,160
Воскресенье можно представить прямым кодированием в виде вектора длиной 8.

315
01:05:19,240 --> 01:05:30,540
Матрица эмбеддинга содержит 8 строк и 4 столбца.

316
01:05:32,440 --> 01:06:08,849
Выбор соответствующей строки матрицы можно представить в виде произведения вектора 1x8 на матрицу 8x4.

317
01:06:08,949 --> 01:06:23,450
Раньше некоторые люди так и воплощали эмбеддинг, некоторые из методов машинного обучения делают это до сих пор.

318
01:06:23,450 --> 01:06:34,490
Это очень неэффективно, поэтому современные библиотеки превращают номер класса в признаке в целое число и используют его как индекс.

319
01:06:37,069 --> 01:06:49,700
Идея перемножения матриц позволяет думать про эмбеддинг, как про ещё один линейный слой.

320
01:06:52,819 --> 01:06:59,510
Это незначительный нюанс, но, возможно, кому-то из вас поможет.

321
01:06:59,510 --> 01:07:06,234
Вопрос из зала: Можно ли преобразовать даты в категориальные признаки? Как при этом учитывается сезонность продаж?

322
01:07:06,334 --> 01:07:21,165
Отличный вопрос. Я подробно объяснял это в курсе «Машинное обучение», но тут тоже можно упомянуть.

323
01:07:21,265 --> 01:07:33,170
В fastai есть функция add_datepart(), которая принимает датафрейм, содержащий столбец с датами «Date»,

324
01:07:33,170 --> 01:07:46,170
удаляет его, если не поставить drop=False, и вставляет в датафрейм новые столбцы с информацией об этой дате —

325
01:07:46,270 --> 01:07:59,570
день недели, день месяца, месяц, год, является ли этот день началом или концом квартала — всё, что pandas знает про этот день.

326
01:07:59,570 --> 01:08:14,260
Вот эти признаки — год, месяц, неделя, день, день недели и так далее.

327
01:08:14,360 --> 01:08:34,500
Итак, для дня недели размер матрицы эмбеддинга — 8x4. Это позволяет создавать очень интересные модели.

328
01:08:34,600 --> 01:08:53,700
Например, если в Берлине с понедельника по среду скидка на молочные продукты, модель это учтёт.

329
01:08:53,800 --> 01:09:01,010
Это отличный способ использования времени в качестве признака, рад, что вы спросили.

330
01:09:01,010 --> 01:09:10,515
Главное — убедиться, что соответствующие признаки выделены в столбцы. Если в датафрейме нет признака дня недели,

331
01:09:10,615 --> 01:09:28,020
модели будет очень сложно увидеть связанные с днём недели закономерности.

332
01:09:28,120 --> 01:09:36,400
Стоит выделять наличие праздников и выходных в отдельные столбцы.

333
01:09:36,500 --> 01:09:44,720
Например, если вы предсказываете продажу напитков в Сан Франциско,

334
01:09:44,720 --> 01:09:52,080
стоит посмотреть расписание бейсбольных игр в AT&T-парке, потому что они влияют на продажи пива.

335
01:09:52,180 --> 01:10:03,820
Если вы добавите признаки периодических событий, нейронная сеть их учтёт.

336
01:10:03,820 --> 01:10:12,050
Я стараюсь пропускать вещи, не связанные с глубоким обучением.

337
01:10:12,050 --> 01:10:17,270
Итак, алгоритм обучения создаётся из объекта данных модели.

338
01:10:17,270 --> 01:10:37,220
В параметры передаются размеры матриц эмбеддинга emd_szs и количество количественных признаков,

339
01:10:37,220 --> 01:10:54,510
мы передаём разницу между количеством всех признаков и количеством категориальных.

340
01:10:54,610 --> 01:11:02,840
У матриц эмбеддинга есть свой дропаут, здесь он равен 0.04.

341
01:11:02,840 --> 01:11:12,690
1000 — количество активаций в первом линейном слое, 500 — во втором. 0.001 — дропаут для первого линейного слоя, 0.01 — для второго.

342
01:11:12,790 --> 01:11:26,610
Параметр y_range мы ещё обсудим. 1 — это количество чисел на выходе, у нас одно, потому что мы предсказываем продажи.

343
01:11:26,710 --> 01:11:35,000
У нас есть алгоритм обучения m. Вызываем его метод .lr_find() для нахождения скорости обучения, тут всё привычно.

344
01:11:35,000 --> 01:11:58,720
Дальше модель обучается как обычно. В качестве метрики можно использовать свои функции.

345
01:11:58,720 --> 01:12:13,344
Функцию exp_rsmpe() мы задали выше, это средняя процентная ошибка, сначала берущая экспоненту от предсказаний, потому что модели возвращают логарифм.

346
01:12:13,444 --> 01:12:17,609
Метрика не влияет на процесс обучения, она выводится для нас.

347
01:12:17,709 --> 01:12:35,489
Здесь используется SGDR, которого не было у обладателей третьего места, интересно, что получится.

348
01:12:37,020 --> 01:12:49,619
Валидационная выборка не совпадает с тестовой, но она похожа — это последние две недели датасета.

349
01:12:52,530 --> 01:13:12,000
Потери на валидационной выборке равны 0.097, давайте посмотрим, какие потери на тестовой выборке у людей в рейтинге.

350
01:13:12,000 --> 01:13:19,779
Если бы потери на валидационной выборке совпадали с потерями на тестовой, мы были бы на вершине приватного рейтинга

351
01:13:19,879 --> 01:13:27,060
и среди первых 30-40 результатов публичного рейтинга.

352
01:13:27,060 --> 01:13:50,280
Я пробовал запускать код обладателей третьего места и потери были больше 0.1, поэтому я думаю, что наша модель неплоха.

353
01:13:50,280 --> 01:14:04,230
Итак, есть техники для работы с временными признаками. Авторы этого подхода написали об этом статью, в Jupyter ноутбуке есть ссылка.

354
01:14:04,230 --> 01:14:14,336
По сравнению с этим подходом обладатели второго места выделили очень много признаков вручную.

355
01:14:14,436 --> 01:14:25,500
Победители соревнования — эксперты в области предсказания продаж, у них был готовый код для выделения признаков.

356
01:14:25,500 --> 01:14:35,190
Люди из Pinterest, строившие подобную модель для рекомендаций, говорили, что

357
01:14:35,190 --> 01:14:42,429
при переходе от градиентного бустинга к глубокому обучению сильно упростилось выделение признаков,

358
01:14:42,529 --> 01:14:47,149
получилась простая и легко поддерживаемая модель.

359
01:14:47,929 --> 01:15:00,619
Это одно из преимуществ глубокого обучения — результаты отличные, а работы мало.

360
01:15:01,989 --> 01:15:06,514
Вопрос из зала: Здесь используются временные признаки?

361
01:15:06,614 --> 01:15:11,140
Да, но не напрямую.

362
01:15:11,140 --> 01:15:21,100
У нас есть день недели, месяц и другие временные признаки, мы считаем их категориальными —

363
01:15:21,100 --> 01:15:28,719
мы строим распределённое представление января, распределённое представление воскресенья, Рождества и так далее.

364
01:15:28,719 --> 01:15:43,592
Мы не используем стандартные техники для обработки временных признаков, только матрицы эмбеддинга в полносвязных слоях.

365
01:15:43,692 --> 01:16:00,500
Это позволяет модели улавливать гораздо более глубокие закономерности, чем позволяют стандартные техники.

366
01:16:00,600 --> 01:16:13,820
Вопрос из зала: В предыдущих моделях метрика передавалась раньше метода .fit(), а здесь — в метод .fit(), почему?

367
01:16:14,380 --> 01:16:24,425
Во время вызова метода .fit() не вводятся новые параметры модели, только скорость обучения, количество циклов и метрика.

368
01:16:24,525 --> 01:16:31,475
Создание модели — другое дело.

369
01:16:31,575 --> 01:16:39,680
При классификации изображений мы вызывали метод ConvLearned.pretrained() и просто передавали туда данные.

370
01:16:39,780 --> 01:17:04,469
В нашем случае модель зависит от того, какие данные она использует, поэтому сначала строится объект данных модели, а из него создаётся алгоритм обучения.

371
01:17:04,639 --> 01:17:52,014
Вопрос из зала: Каков итоговый алгоритм? Выделить признаки, построить распределённое представление категориальных, и дать это всё модели?

372
01:17:52,114 --> 01:17:56,169
Да, алгоритм получается такой:

373
01:17:57,780 --> 01:18:07,855
1. Разделить признаки на количественные и категориальные и преобразовать данные в датафрейм.

374
01:18:07,955 --> 01:18:16,074
2. Создать валидационную выборку.

375
01:18:16,174 --> 01:18:28,645
3. Создать объект данных модели, можно просто скопировать эту строку.

376
01:18:28,745 --> 01:18:37,260
4. Создать массив размеров матриц эмбеддинга.

377
01:18:37,260 --> 01:18:45,239
5. Создать алгоритм обучения. Можете скопировать эту строку, будет переобучаться — поэкспериментируйте с параметрами.

378
01:18:45,339 --> 01:18:56,269
6. Обучить модель методом .fit().

379
01:18:59,620 --> 01:19:14,000
Вопрос из зала: Как выглядит дополнение данных в этом случае? И для чего здесь нужен дропаут?

380
01:19:14,000 --> 01:19:32,680
Про дополнение данных — понятия не имею. Я не видел статей на эту тему. Возможно, это можно сделать, но я не представляю, как.

381
01:19:32,680 --> 01:19:54,470
Дропаут делает то же самое, что и с изображениями — выбрасывает часть активаций линейных слоёв.

382
01:19:54,470 --> 01:20:12,860
У матриц эмбеддинга есть свой дропаут, он тоже выбрасывает часть активаций на каждый итерации.

383
01:20:12,860 --> 01:20:21,490
Давайте сделаем перерыв и продолжим в 8:05.

384
01:20:24,590 --> 01:20:49,370
Прежде чем продолжить, отвечу на вопрос, который мне задали в перерыве — почему никто не работает со структурированными данными.

385
01:20:49,370 --> 01:21:10,100
Я думаю, это из-за того, что это выглядит скучно, поэтому это не публикуют и люди не знают, что это можно делать.

386
01:21:10,100 --> 01:21:26,802
Ещё одна важная причина — до появления библиотеки fastai не было способа делать это просто, все этапы приходилось писать с нуля.

387
01:21:26,902 --> 01:21:42,025
С использованием библиотеки fastai создание и обучение модели занимает шесть шагов, примерно шесть строк кода.

388
01:21:42,125 --> 01:22:00,220
Я думаю, что это очень важная для индустрии и науки область, поэтому мне будет интересно услышать о ваших результатах —

389
01:22:00,320 --> 01:22:09,690
например, вы попробовали обучить модель на датасете со старых соревнований Kaggle и поняли, что выиграли бы,

390
01:22:09,690 --> 01:22:17,640
или использовали в своей работе градиентный бустинг или случайные леса, а потом попробовали глубокое обучение и вышло лучше.

391
01:22:17,640 --> 01:22:39,300
Я работаю со структурированными данными только с начала года, мой подход пока везде работал, я не знаю, какие у него недостатки.

392
01:22:39,300 --> 01:22:54,540
В этом классе впервые собралось больше десяти человек. Я надеюсь, мы многое вместе поймём и придумаем.

393
01:22:54,540 --> 01:23:06,085
Если вы хотите написать полезный пост — у Instacart есть пост про работу со структурированными данными;

394
01:23:06,185 --> 01:23:12,250
у Pinterest есть видео выступления на эту тему на O'Reilly Artificial Intelligence Conference;

395
01:23:12,350 --> 01:23:21,540
есть две научные статьи о победах в соревнованиях Kaggle — одна от команды Yoshua Bengio, они выиграли соревнование

396
01:23:21,540 --> 01:23:33,960
по предсказанию траекторий машин такси, и вторая — от победителей соревнования Rossman.

397
01:23:33,960 --> 01:23:46,760
Итак, обработка естественного языка (NLP). Это, возможно, самая многообещающая область глубокого обучения —

398
01:23:46,860 --> 01:23:59,960
она всего на пару лет моложе компьютерного зрения, это вторая популярная область применения глубокого обучения.

399
01:23:59,960 --> 01:24:10,820
Большинство технологий компьютерного зрения в 2014 были уже отработаны,

400
01:24:10,820 --> 01:24:19,195
а в обработке естественного языка мы только сейчас подходим к этой точке.

401
01:24:19,295 --> 01:24:31,170
Что-то уже отработано, но в целом в обработке естественного языка всё гораздо более сырое, чем в компьютерном зрении.

402
01:24:31,270 --> 01:24:36,230
Всё, что мы будем обсуждать в этом курсе, будет менее проработано, чем компьютерное зрение.

403
01:24:39,620 --> 01:24:51,195
В последние месяцы полезные техники из компьютерного зрения появились и в обработке естественного языка.

404
01:24:51,295 --> 01:25:08,760
Я начну с описания одной из проблем NLP — их в NLP несколько, и все имеют свои названия.

405
01:25:08,860 --> 01:25:13,927
Мы рассмотрим проблему языкового моделирования (language modeling). В этом случае

406
01:25:14,027 --> 01:25:24,530
модель должна по нескольким словам из предложения угадывать следующее слово.

407
01:25:24,530 --> 01:25:30,900
Такие модели предлагают вам окончания фраз после нажимания пробела при наборе текста на смартфоне,

408
01:25:31,000 --> 01:25:39,310
например, так работает приложение SwiftKey, использующее глубокое обучение.

409
01:25:39,410 --> 01:25:47,322
Под фразой «языковое моделирование» мы подразумеваем модель, которая предсказывает следующее слово в предложении.

410
01:25:47,422 --> 01:25:57,785
Для примера я скачал научные статьи за 18 месяцев с сайта arxiv.org.

411
01:25:57,885 --> 01:26:09,007
arxiv.org — крупнейший архив препринтов научных статей по глубокому обучению и многим другим областям.

412
01:26:09,107 --> 01:26:32,080
Из статей я выделил аннотации и темы. Тема <cat> этой статьи — Computer Science and Networking (csni), после <summ> идёт аннотация.

413
01:26:32,180 --> 01:26:38,560
Это пример текста для обучения языковой модели.

414
01:26:38,660 --> 01:26:46,740
Я обучил модель на полученном датасете с arXiv и создал тестовую функцию для демонстрации работы модели.

415
01:26:46,740 --> 01:26:53,040
В функцию передаётся тема и короткая фраза, а модель должна слово за словом дописать аннотацию по этим фразе и теме.

416
01:26:53,040 --> 01:27:19,540
Например, тема <CAT> — Computer Science and Networking (csni), часть аннотации <SUMM> — фраза «alrogithms that», ниже результат работы модели.

417
01:27:19,640 --> 01:27:30,970
Обученная на статьях с arXiv модель считает, что аннотация по теме Computer Science and Networking, начинающаяся со слов «algorithms that», выглядит так.

418
01:27:31,070 --> 01:27:38,820
В начале обучения для каждого слова английского языка была создана случайная матрица эмбеддинга.

419
01:27:41,700 --> 01:27:49,090
В процессе чтения статей модель поняла, какие слова следуют за какими.

420
01:27:49,190 --> 01:28:10,225
Если в качестве темы выбрать Computer Science: Computer Vision (cscv), модель напишет аннотацию про свёрточные нейронные сети.

421
01:28:10,325 --> 01:28:20,190
Как видите, аннотация похожа на предыдущую, просто общие термины машинного обучения заменены на термины компьютерного зрения.

422
01:28:22,710 --> 01:28:29,875
В следующем примере я задал ту же тему — компьютерное зрение, аннотация состоит из одного слова «alrogithms.»,

423
01:28:29,975 --> 01:28:42,180
и модели нужно дописать название статьи, начинающееся со слова «on», вот результат, <eos> — символ конца строки.

424
01:28:42,180 --> 01:28:47,130
Если тема — Computer Science and Networking (csni), результат получится такой.

425
01:28:47,130 --> 01:29:00,085
Замена слова «on» на «towards» даёт слегка отличающиеся результаты.

426
01:29:00,185 --> 01:29:09,960
По-моему, это ошеломляющие результаты. Модель начала со случайных матриц, то есть не умела даже читать по-английски,

427
01:29:09,960 --> 01:29:24,530
а после обучения на статьях за 18 месяцев научилась ставить аббревиатуру CNN в скобках после фразы «свёрточные нейронные сети»

428
01:29:24,530 --> 01:29:40,902
и писать сложные аннотации вроде тех, которые я только что показал.

429
01:29:41,002 --> 01:29:55,290
Мы построим модель, способную на такие сложные и тонкие вещи, но не для этих целей,

430
01:29:55,290 --> 01:30:05,235
а для того, чтобы на её основе обучить другую модель, которая будет делить отзывы на IMDb на положительные и отрицательные.

431
01:30:05,335 --> 01:30:17,045
Задача похожа на задачу классификации кошек и собак, только вместо изображений — отзывы.

432
01:30:17,145 --> 01:30:28,160
Поэтому, как и в задаче с кошками и собаками, нам понадобится предобученная модель, которая умеет читать по-английски.

433
01:30:28,260 --> 01:30:33,860
Модель, которая может предсказать следующее слово в предложении, умеет читать по-английски.

434
01:30:33,960 --> 01:30:54,130
Мы возьмём её в качестве предобученной модели, добавим несколько новых слоёв и обучим классифицировать отзывы.

435
01:30:54,130 --> 01:31:01,084
Когда я начал над этим работать, это была абсолютно новая идея. К сожалению, в последние пару месяцев

436
01:31:01,184 --> 01:31:14,219
появилось много статей на эту тему, и я больше не чувствую себя первооткрывателем.

437
01:31:14,219 --> 01:31:21,410
Сейчас мы займёмся языковым моделированием для создания предобученной модели для задачи классификации отзывов.

438
01:31:21,510 --> 01:31:36,305
После этого мы применим всё, что узнали про компьютерное зрение, для последующей тонкой настройки предобученной модели.

439
01:31:36,405 --> 01:31:43,505
Янет: Почему нельзя обойтись без предобученной модели?

440
01:31:43,605 --> 01:31:54,400
Опыт показывает, что нельзя. Этому есть объяснение.

441
01:31:54,400 --> 01:32:11,195
Во-первых, предобученная модель — мощный инструмент, глупо его не использовать.

442
01:32:11,295 --> 01:32:24,920
Во-вторых, отзывы на IMDb очень длинные — до тысячи слов. Модель ничего не знает про английский язык,

443
01:32:24,920 --> 01:32:37,860
про то, что такое слова, что такое пунктуация и так далее, а обучается на данных о том, положительный отзыв или отрицательный.

444
01:32:37,960 --> 01:32:48,500
Из таких данных очень сложно получить структуру английского языка и одновременно научиться распознавать тональность.

445
01:32:48,500 --> 01:32:56,625
Поэтому мы используем нейронную сеть, которая как-то понимает английский язык, на котором написаны отзывы,

446
01:32:56,725 --> 01:33:03,860
и надеемся, что это знание поможет научиться различать тональность текстов.

447
01:33:05,030 --> 01:33:08,835
Отличный вопрос.

448
01:33:08,935 --> 01:33:16,920
Вопрос из зала: Это похоже на Char-RNN Андрея Карпатого?

449
01:33:17,020 --> 01:33:28,340
Да, похоже. Char-RNN предсказывает следующую букву слова, основываясь на предыдущих.

450
01:33:28,340 --> 01:33:42,710
Обычно (и в нашем курсе) языковые модели работают со словами как с единицами языка, а не с буквами.

451
01:33:42,710 --> 01:33:59,990
Вопрос из зала: Слова, написанные моделью — копии существующих слов или случайные сочетания букв? Как это понять?

452
01:33:59,990 --> 01:34:10,140
Слова — копии уже существующих слов, потому что модель рассматривает отдельные слова, а не буквы.

453
01:34:10,240 --> 01:34:15,015
С предложениями сложнее. Что-то можно понять, глядя на примеры:

454
01:34:15,115 --> 01:34:26,540
эти примеры отличаются только темой, и предложения схожи по структуре, различаются только терминами.

455
01:34:26,640 --> 01:34:45,260
Довольно сложно получить такие результаты просто нарезкой предложений, можете проверить, что это не так.

456
01:34:45,260 --> 01:34:53,669
Очевидно, на валидационной выборке модель предсказывает слова, которые не видела раньше.

457
01:34:54,569 --> 01:35:00,859
Поэтому, если модель хорошо предсказывает валидационную выборку, она должна хорошо генерировать текст.

458
01:35:01,849 --> 01:35:10,874
Наша цель — не генерация текста, это была просто демонстрация работы, я не буду в это углубляться.

459
01:35:10,974 --> 01:35:19,269
Вы вполне можете написать свой Великий Генератор Повестей на этой неделе, если хотите.

460
01:35:19,269 --> 01:35:28,657
Есть пара техник, позволяющих моделям лучше генерировать текст, они довольно простые, их можно обсудить на форуме.

461
01:35:28,757 --> 01:35:40,905
Я хочу сконцентрироваться на классификации текстов, потому что это очень мощный инструмент для решения задач —

462
01:35:41,005 --> 01:35:57,309
например, вы возглавляете хедж-фонд и постоянно мониторите Twitter на предмет новостей о масштабных падениях рынка в прошлом,

463
01:35:57,409 --> 01:36:12,320
или хотите выделять запросы обслуживания клиентов, которые ведут к разрыву контракта.

464
01:36:12,320 --> 01:36:25,730
Эти проблемы решаются в рамках задачи классификации текстов.

465
01:36:25,730 --> 01:36:35,039
Ещё один пример — сортировка документов в суде на предмет судебного прецедента.

466
01:36:35,139 --> 01:36:41,564
Давайте посмотрим на используемые библиотеки.

467
01:36:41,664 --> 01:36:51,230
Из нового — библиотека torchtext, это библиотека PyTorch для обработки естественного языка.

468
01:36:51,230 --> 01:37:03,739
Библиотека fastai хорошо интегрирована с PyTorch, из неё мы импортируем несколько модулей для работы с текстом.

469
01:37:03,739 --> 01:37:20,290
Мы будем работать с датасетом отзывов на IMDb large movie review dataset, это известный датасет, с ним много кто работал.

470
01:37:20,290 --> 01:37:29,190
Он содержит 50,000 отзывов с яркой эмоциональной окраской — положительные или отрицательные.

471
01:37:29,290 --> 01:37:35,090
Сначала мы проигнорируем разметку тональности и обучим модель предсказывать слова.

472
01:37:35,090 --> 01:37:41,760
Это похоже на задачу с кошками и собаками — предобучим модель для одной задачи, дообучим для другой.

473
01:37:41,860 --> 01:37:54,150
Эта идея настолько нова, что никто до сих пор не выложил предобученную модель, придётся создавать её самим.

474
01:37:54,250 --> 01:38:04,080
Датасет можно скачать по ссылке, после этого мы стандартным образом задаём пути к выборкам.

475
01:38:04,180 --> 01:38:14,720
Всё как в компьютерном зрении — директории с обучающей и тестовой выборками. Валидационная выборка не выделена.

476
01:38:14,720 --> 01:38:28,970
Как и в компьютерном зрении, обучающая выборка содержит отдельные файлы — только не изображения, а отзывы.

477
01:38:28,970 --> 01:39:02,410
Можно посмотреть на один из отзывов.

478
01:39:02,530 --> 01:39:09,395
После этого можно посмотреть размеры датасета, используя стандартные команды UNIX.

479
01:39:09,495 --> 01:39:18,005
Обучающая выборка содержит 17.5 миллионов слов, тестовая — 5.6 миллионов слов.

480
01:39:18,105 --> 01:39:29,920
Отзывы взяты с IMDb — это отзывы случайных людей, а не официальные обзоры New York Times.

481
01:39:29,920 --> 01:39:40,870
Перед тем, как начать что-то делать с текстом, необходимо превратить его в список токенов.

482
01:39:40,870 --> 01:39:47,315
Токен — это единица текста, в данном случае слово. После этого токены преобразуются в числа.

483
01:39:47,415 --> 01:39:51,915
Процесс превращения текста в список токенов называется токенизация.

484
01:39:52,315 --> 01:39:57,605
В NLP много жаргонных слов, мы будем постепенно их узнавать.

485
01:39:57,705 --> 01:40:07,985
Этот текст был разбит на токены и соединён обратно с пробелом в качестве разделителя.

486
01:40:08,085 --> 01:40:24,430
Слово «wasn't» — это два токена «was» и «n't», многоточие — один токен, много восклицательных знаков — много токенов.

487
01:40:24,430 --> 01:40:43,380
Хороший алгоритм токенизации учитывает такие различия — правильно разделяет знаки препинания и составные слова.

488
01:40:43,380 --> 01:40:58,600
Лучший известный мне алгоритм токенизации — австралийский инструмент spaCy, с ним работают torchtext и fastai.

489
01:40:58,600 --> 01:41:15,610
Для токенизации текста в torchtext создаётся объект Field конструктором .Field(),

490
01:41:15,610 --> 01:41:23,975
в него передаётся параметр lowercase=True, отвечающий за перевод текста в нижний регистр, и алгоритм токенизации tokenize=spacy_tok.

491
01:41:24,075 --> 01:41:37,300
При создании объекта не выполняется токенизация, только правила её выполнения сохраняются в объект TEXT.

492
01:41:37,300 --> 01:41:44,680
Это — часть torchtext, а не fastai, это есть в документации на их сайте. torchtext пока плохо документирован,

493
01:41:44,680 --> 01:41:51,845
потому что он совсем новый, возможно, больше всего вы узнаете из этой лекции, но что-то есть и в документации.

494
01:41:51,945 --> 01:42:04,447
После этого можно создать обычную модель fastai.

495
01:42:04,547 --> 01:42:17,050
Объект FILES содержит пути к обучающей, валидационной и тестовой выборкам —

496
01:42:17,050 --> 01:42:22,745
у меня нет тестовой выборки, поэтому вместо неё я использую валидационную.

497
01:42:22,845 --> 01:42:29,240
Модель создаётся конструктором LanguageModelData(), в него передаётся путь к файлам PATH,

498
01:42:29,340 --> 01:42:47,805
объект torchtext с инструкциями по обработке текста TEXT, список файлов FILES, размер минибатча bs.

499
01:42:47,905 --> 01:42:57,130
Здесь два непривычных параметра. min_freq — это минимальная частота.

500
01:42:57,130 --> 01:43:08,350
После обработки каждый токен будет заменён на число, индекс токена в списке уникальных токенов.

501
01:43:08,350 --> 01:43:21,929
Параметр min_freq=10 значит, что все слова, встречающиеся реже 10 раз, будут считаться незнакомым словом, сейчас мы это обсудим.

502
01:43:21,929 --> 01:43:29,904
Мы также обсудим параметр bptt (backpropagation through time)- он определяет максимальную длину фразы,

503
01:43:30,004 --> 01:43:40,749
которая обрабатывается за одну итерацию. Длинные предложения будут разбиваться на фразы по 70 токенов или меньше.

504
01:43:40,849 --> 01:43:46,330
Всё сейчас обсудим.

505
01:43:46,430 --> 01:43:57,789
После построения модели в объекте TEXT появится поле vocab.

506
01:43:57,889 --> 01:44:06,640
Это очень важное понятие в NLP, таких понятий много, и приходится вываливать их на вас горой, но они будут повторяться.

507
01:44:06,740 --> 01:44:19,710
Поле vocab содержит словарь, то есть список уникальных слов и их индексы.

508
01:44:19,710 --> 01:44:35,885
Список слов лежит в TEXT.vocab.itos, itos = integer to string. Индексу 0 соответствует неизвестное слово, индексу 1 — отступ,

509
01:44:35,985 --> 01:44:43,434
индексу 2 — слово «the», дальше идут запятая, точка, слово «and» и так далее.

510
01:44:43,534 --> 01:44:57,580
Это первые двенадцать слов в словаре, словарь отсортирован по частотности, не считая первых двух специальных токенов.

511
01:44:57,680 --> 01:45:04,280
Можно посмотреть обратное соответствие в объекте TEXT.vocab.stoi, stoi = string to integer.

512
01:45:04,580 --> 01:45:09,640
Индексу 2 в TEXT.vocab.itos соответствовало слово «the», слову «the» в TEXT.vocab.stoi соответствует индекс 2.

513
01:45:09,740 --> 01:45:20,790
Словарь позволяет сопоставить каждому слову его индекс.

514
01:45:20,790 --> 01:45:31,800
Можно взять первые 12 слов какого-то текста и превратить их в 12 индексов в соответствии со словарём.

515
01:45:31,900 --> 01:45:39,950
Например, «of» и «the» имеют индексы 7 и 2.

516
01:45:40,050 --> 01:45:46,955
Мы будем работать с представлением в виде индексов.

517
01:45:47,055 --> 01:45:53,030
Вопрос из зала: Используется ли здесь стемминг или лемматизация?

518
01:45:53,130 --> 01:46:08,420
Нет, мы не хотим углубляться в детали языка, а просто смотрим, какие токены следуют за какими.

519
01:46:08,520 --> 01:46:25,300
Мы не знаем, какие зависимости будут интересными. Практика показывает, что стемминг и лемматизация не нужны.

520
01:46:25,300 --> 01:46:34,685
Эта область ещё совсем новая, я могу чего-то не знать.

521
01:46:34,785 --> 01:46:43,110
Вопрос из зала: Не теряется ли контекст, когда мы рассматриваем отдельные слова?

522
01:46:43,110 --> 01:46:59,840
Мы не рассматриваем отдельные слова, мы сохраняем их порядок.

523
01:46:59,940 --> 01:47:05,870
Когда мы заменяем их на числа, порядок сохраняется.

524
01:47:05,970 --> 01:47:16,570
Есть подход «мешок слов», в котором порядок не учитывается, мы рассматривали его в курсе «Машинное обучение».

525
01:47:18,190 --> 01:47:32,075
Я считаю, что этот подход отслужил своё, и глубокое обучение уже позволяет сохранять контекст.

526
01:47:32,175 --> 01:47:38,135
Мы научились это делать совсем недавно, буквально несколько месяцев назад.

527
01:47:38,235 --> 01:47:49,590
Давайте вернёмся к параметрам размера минибатча bs и обратного распространения во времени bptt.

528
01:47:52,650 --> 01:48:15,490
У нас есть текст, это набор слов. Тексты разбиты по отдельным файлам, но при обучении они склеиваются в один огромный текст.

529
01:48:15,490 --> 01:48:25,330
По сути, предсказывается следующее слово в огромном длинном тексте из всех отзывов датасета.

530
01:48:25,330 --> 01:48:35,644
Этот огромный текст содержит десятки миллионов слов, для обучения мы разбиваем его на минибатчи.

531
01:48:35,744 --> 01:48:47,949
Мы указали размер минибатча 64, это значит, что весь текст разбивается на 64 части.

532
01:48:50,739 --> 01:49:40,180
После этого эти части складываются одна под другой в огромную таблицу с 64 строками, и таблица транспонируется.

533
01:49:40,280 --> 01:49:51,929
Допустим, размер текста был 64 миллиона, тогда транспонированная таблица содержит миллион строк.

534
01:49:51,929 --> 01:49:59,790
Каждый столбец содержит 1/64 всего датасета.

535
01:49:59,790 --> 01:50:21,472
Каждый минибатч содержит количество строк таблицы, примерно равное значению параметра bptt, у нас bptt=70.

536
01:50:21,572 --> 01:50:37,619
Таким образом, минибатч всегда содержит 64 столбца и примерно 70 строк.

537
01:50:37,619 --> 01:50:59,585
Давайте получим один минибатч командой next(iter(md.trn_dl)), этой же командой минибатчи получает нейронная сеть.

538
01:50:59,685 --> 01:51:08,645
Команда вернула массив размера 75x64 — 64 столбца и примерно 70 строк.

539
01:51:08,745 --> 01:51:22,190
Примерное количество строк — довольно интересная вещь: torchtext специально меняет каждый раз высоту минибатча,

540
01:51:25,800 --> 01:51:33,330
чтобы каждую эпоху модель видела различные куски данных.

541
01:51:33,330 --> 01:51:49,475
Это напоминает перемешивание изображений в компьютерном зрении — нельзя перемешивать слова, но можно по-разному их нарезать.

542
01:51:49,575 --> 01:52:08,650
Первый столбец содержит первые 75 слов первого текста.

543
01:52:08,750 --> 01:52:19,049
Второй столбец содержит первые 75 слов второго сегмента из 64 — то есть начинается где-то после 1 миллиона слов.

544
01:52:19,149 --> 01:52:23,489
Последний столбец содержит первые 75 слов последнего сегмента из 64.

545
01:52:27,450 --> 01:53:08,365
Следующий объект — целевая переменная, это тот же массив, но сдвинутый вниз на единицу. По техническим причинам он хранится в виде вектора.

546
01:53:08,465 --> 01:53:21,850
Библиотека fastai при создании модели создаёт минибатчи размера (примерно bppt)x(размер минибатча)

547
01:53:21,950 --> 01:53:40,050
и соответствующие целевые переменные, так как наша цель — предсказать следующее слово.

548
01:53:40,860 --> 01:54:08,975
Вопрос из зала: Разве не было бы эффективнее разбивать текст на минибатчи по предложениям?

549
01:54:09,075 --> 01:54:15,460
Нет, не особо. Текст разбит на 64 столбца, каждый столбец содержит около миллиона слов.

550
01:54:15,460 --> 01:54:23,735
Столбцы не всегда оканчиваются концом предложения, но они настолько длинные, что это неважно.

551
01:54:23,835 --> 01:54:38,260
Длина каждого столбца — около миллиона слов, это очень много предложений.

552
01:54:38,260 --> 01:54:45,690
Напомню, столбцы получились нарезанием изначального текста на 64 куска.

553
01:54:47,159 --> 01:55:03,709
Эти матрицы в языковых моделях довольно сложно осознать, не расстраивайтесь, если не поймёте сразу.

554
01:55:03,809 --> 01:55:16,804
Задавайте вопросы на форуме, прослушайте несколько раз в записи то, что я только что сказал,

555
01:55:16,904 --> 01:55:23,380
попробуйте повторить этот процесс с парой предложений в Excel. Может, у вас получится лучше это объяснить.

556
01:55:23,380 --> 01:55:46,110
То, что я показал — это совместная работа torchtext и fastai, они тесно связаны и не всегда понятно, где проходит граница.

557
01:55:46,110 --> 01:55:58,640
После создания объекта данных модели можно создать саму модель.

558
01:55:58,740 --> 01:56:05,600
Для этого нужно создать матрицу эмбеддинга.

559
01:56:05,700 --> 01:56:29,285
Давайте посмотрим на размеры. В обучающей выборке 4602 батча — это примерно равно (длина всего текста) / (размер минибатча * bptt).

560
01:56:29,385 --> 01:56:47,119
Объект md.nt содержит уникальные токены, повторяющиеся больше 10 раз — их 34,945.

561
01:56:47,219 --> 01:56:53,944
Все слова с частотностью меньше 10 заменены токеном <unk>, неизвестное слово.

562
01:56:54,044 --> 01:57:02,650
Длина обучающей выборки — 1, потому что все тексты склеены в один огромный текст,

563
01:57:05,170 --> 01:57:12,330
и в этом огромном тексте 20,621,966 слов.

564
01:57:12,330 --> 01:57:32,600
Из 34,945 уникальных токенов создаётся матрица эмбеддинга. В ней 34,945 строк, нулевая соответствует токену <unk>,

565
01:57:32,700 --> 01:57:38,719
первая — <pad>, дальше идут точка, запятая, «the» и так далее.

566
01:57:38,819 --> 01:57:53,284
Каждому токену соответствует вектор в матрице эмбеддинга. Это то же самое, что мы делали до перерыва,

567
01:57:53,384 --> 01:58:00,850
токен — это категориальный признак с очень высокой мощностью, при этом единственный.

568
01:58:00,850 --> 01:58:19,090
Это очень распространённая ситуация в NLP: слово — единственный признак, он категориальный и его мощность равна 34,945.

569
01:58:19,090 --> 01:58:25,360
Ширина матрицы эмбеддинга, то есть количество столбцов, задаётся в переменной em_sz=200.

570
01:58:28,210 --> 01:58:35,497
Это сильно больше, чем при работе со структурированными данными, потому что слово языка —

571
01:58:35,597 --> 01:58:45,400
гораздо более сложная смысловая единица, чем понятие воскресенья или магазин в Берлине.

572
01:58:45,400 --> 01:58:53,105
Обычно количество столбцов в матрице эмбеддинга при работе с текстом будет от 50 до 600, мы взяли число посередине.

573
01:58:53,205 --> 01:59:04,900
Как обычно, нужно указать количество активаций в слое nh=500 и количество слоёв nl=3.

574
01:59:04,900 --> 01:59:20,587
Позже мы обсудим алгоритм оптимизации Adam, его значения по умолчанию не идеальны, мы их меняем.

575
01:59:20,687 --> 01:59:30,740
В каждой задаче по обработке естественного языка имеет смысл вставлять эту строку кода, она полезная.

576
01:59:30,840 --> 01:59:41,060
После этого можно создать модель. Она принимает в качестве параметров алгоритм оптимизации opt_fn,

577
01:59:41,160 --> 01:59:51,400
ширину матрицы эмбеддинга em_sz, количество активаций в слое nh, количество слоёв nl, различные вероятности дропаута.

578
01:59:51,500 --> 02:00:00,690
fastai использует языковую модель AWD LSTM, разработанную Стивеном Мерити, это новейшая разработка.

579
02:00:03,690 --> 02:00:12,715
Её отличительная особенность — много различных видов дропаута в разных частях модели.

580
02:00:12,815 --> 02:00:22,540
Мы подробно обсудим архитектуру этой модели на последней лекции, сейчас не будем на этом останавливаться.

581
02:00:22,640 --> 02:00:29,610
Обращайтесь с этими вероятностями дропаута как обычно: если модель недообучается — пропорционально уменьшите их,

582
02:00:29,710 --> 02:00:40,100
если переобучается — пропорционально увеличьте. Я так и делаю.

583
02:00:40,200 --> 02:00:53,820
Модель совсем новая, мало кем опробованная, оптимальных значений пока нет, но мы со Стивеном используем такие.

584
02:00:53,820 --> 02:00:59,905
Есть ещё один способ избежать переобучения, мы обсудим его на последней лекции,

585
02:01:00,005 --> 02:01:09,000
а пока вы можете просто вставлять эту строку во все задачи в NLP и не задумываться об этом.

586
02:01:09,000 --> 02:01:16,700
Поле .clip мы тоже обсудим на последней лекции, если вкратце —

587
02:01:17,060 --> 02:01:36,870
при умножении градиента на скорость обучения полученное число не может быть больше 0.3.

588
02:01:36,870 --> 02:01:57,690
Это полезно, потому что позволяет избежать расхождения алгоритма обучения при больших скоростях обучения, мы это обсуждали.

589
02:02:01,320 --> 02:02:12,540
При ограничении в 0.3 слишком большие шаги будут обрезаться до значения 0.3 и расхождения не будет.

590
02:02:12,540 --> 02:02:17,580
Полный смысл параметров сейчас не важен, можете просто использовать то, что есть.

591
02:02:17,680 --> 02:02:23,190
После этого модель обучается методом .fit(), как обычно.

592
02:02:26,160 --> 02:02:50,310
Янет: Существуют готовые эмбеддинги вроде Word2vec или GloVe. Почему вы их не используете?

593
02:02:50,310 --> 02:03:15,330
Отличный вопрос. Word2vec и GloVe предоставляют готовые матрицы эмбеддинга, это не предобученные модели.

594
02:03:15,330 --> 02:03:31,885
Их можно скачать и использовать. Я пробовал использовать только их для построения предобученной модели, улучшения не заметил,

595
02:03:31,985 --> 02:03:36,190
созданная с нуля предобученная модель работала гораздо лучше.

596
02:03:36,290 --> 02:03:46,120
Word2vec вызвал большой резонанс при появлении, но я думаю, что мой подход работает лучше.

597
02:03:46,220 --> 02:03:54,010
Я думаю, можно попробовать скомбинировать Word2vec c такими моделями и посмотреть, станет ли лучше.

598
02:03:54,110 --> 02:04:01,010
Вопрос из зала: Как выглядит архитектура используемой модели?

599
02:04:01,010 --> 02:04:15,415
Это рекуррентная нейронная сеть c использованием долгой краткосрочной памяти (LSTM), мы обсудим её на последней лекции.

600
02:04:15,515 --> 02:04:22,585
Я опускаю многие технические детали, но они сейчас не так важны.

601
02:04:22,685 --> 02:04:43,230
Можно переходить к обучению. Модель обучалась долго, поэтому я несколько раз сохранял её в процессе.

602
02:04:45,540 --> 02:04:52,780
В какой-то момент у меня кончилось терпение и я перестал обучать.

603
02:04:52,880 --> 02:05:05,300
Потом я прогнал тест, на который мы уже смотрели. Модель работает неплохо.

604
02:05:05,400 --> 02:05:18,600
Теперь можно использовать её в качестве предобученной модели для анализа тональности текста.

605
02:05:18,600 --> 02:05:40,015
Для анализа тональности текста мне потребуется словарь перевода слов в числа, поэтому первым делом я загружаю его в объект TEXT.

606
02:05:40,115 --> 02:05:50,650
Если выполнять ячейки Jupyter ноутбука параллельно, словарь и так будет в памяти, его не нужно будет заново загружать.

607
02:05:50,750 --> 02:06:08,075
После этого я добавляю целевую переменную IMDB_LABEL, которая говорит, положительный отзыв или отрицательный.

608
02:06:08,175 --> 02:06:20,910
Теперь мне нужно разбить огромный текст обратно на отзывы, потому что у всех отзывов разная тональность.

609
02:06:20,910 --> 02:06:29,095
В torchtext уже есть датасет, с которым мы работаем, поэтому я просто вызываю его.

610
02:06:29,195 --> 02:06:43,492
После этого можно посмотреть на пример текста, этот отзыв — позитивный.

611
02:06:43,592 --> 02:06:55,830
В этих строках не используется fastai, только torchtext. Мы ещё их обсудим, пока можете почитать документацию.

612
02:06:55,830 --> 02:07:06,540
Всё, что вам нужно знать — объект, созданный методом .splits() в torchtext, передаётся в метод TextData.from_splits() в fastai.

613
02:07:06,540 --> 02:07:14,635
Это конвертирует объект torchtext в объект данных модели fastai, после чего его можно использовать для обучения.

614
02:07:14,735 --> 02:07:20,425
Алгоритм обучения создаётся из объекта данных модели методом .get_model().

615
02:07:20,525 --> 02:07:26,590
После создания модели в неё загружается предобученная языковая модель.

616
02:07:26,690 --> 02:07:41,665
После этого выполняются привычные шаги — заморозка слоёв, обучение, разморозка, снова обучение.

617
02:07:41,765 --> 02:07:52,045
Модель обучается очень быстро при наличии предобученной модели — каждая эпоха занимает пару минут.

618
02:07:52,145 --> 02:08:05,953
Максимальное значение доли правильных ответов в 94.5% достигнуто на 10 эпохе, то есть после 20 минут обучения.

619
02:08:06,053 --> 02:08:10,590
Насколько хороша доля правильных ответов 94.5%?

620
02:08:10,590 --> 02:08:23,455
Один из коллег Стивена Мерити, Джеймс Брэдбери, написал обзорную статью по задачам NLP,

621
02:08:23,555 --> 02:08:36,310
в том числе и обзор на статьи, в которых описывается предсказание слов в датасете IMDb. Вот список лучших результатов со всего мира.

622
02:08:36,310 --> 02:08:44,315
В этих статьях использовались специализированные модели для анализа тональности текста, лучшая доля правильных ответов — 94.1%.

623
02:08:44,415 --> 02:08:55,120
Мы получили долю правильных ответов в 94.5%, это лучше, чем всё, что пока придумали для этой задачи.

624
02:08:55,120 --> 02:09:08,795
Когда я говорю, что это многообещающая техника, я имею в виду то, что в мире пока никто её не использует,

625
02:09:08,895 --> 02:09:22,155
ни IBM, ни CERN, никакие большие компании. Если бы кто-то использовал эту технику, они опубликовали бы статью на эту тему,

626
02:09:22,255 --> 02:09:29,735
чтобы у всех был доступ к лучшим алгоритмам анализа тональности текста.

627
02:09:29,835 --> 02:09:42,790
Я надеюсь, что вы поэкспериментируете с этой моделью, посмотрите, где она работает хорошо, а где плохо.

628
02:09:42,790 --> 02:09:57,230
Для обучения использовалось около 25 тысяч документов, я думаю, обучать модель на меньших датасетах будет сложнее.

629
02:09:57,330 --> 02:10:05,945
Мы не будем много заниматься этим в первой части курса, но будем во второй.

630
02:10:06,045 --> 02:10:13,030
Можно обучить языковую модель на текстах из медицинских журналов и выложить её в общий доступ,

631
02:10:15,760 --> 02:10:24,200
чтобы на её основе можно было обучить модель на, например, медицинских текстах о раке простаты.

632
02:10:24,300 --> 02:10:33,350
Возможностей очень много. Можно принять предложение Янет и использовать Word2vec.

633
02:10:33,450 --> 02:10:47,170
Даже без этого можно обучить языковую модель на текстах Википедии, на её основе обучить языковую модель на текстах IMDb,

634
02:10:47,170 --> 02:10:54,050
а потом дообучить эту модель для анализа тональности текстов для классификации отзывов. Возможно, получится лучше, чем сейчас.

635
02:10:54,150 --> 02:11:04,112
Это всё верхушка айсберга. Я разговаривал с очень крутым исследователем по имени Себастьян Рудер,

636
02:11:04,212 --> 02:11:09,989
насколько мне известно, он единственный исследователь NLP, который много пишет про

637
02:11:10,620 --> 02:11:15,250
предобученные модели, тонкую настройку и перенос обучения.

638
02:11:15,250 --> 02:11:23,000
Я спросил его, почему в NLP редко используются предобученные модели, и он сказал, что для этого нет удобных средств разработки.

639
02:11:23,100 --> 02:11:34,595
Я собирался показать ему эту лекцию, потому что считаю, что fastai сильно в этом поможет.

640
02:11:34,695 --> 02:11:50,315
У нас кончается время, поэтому я быстро покажу введение в коллаборативную фильтрацию, а закончим мы её в следующий раз.

641
02:11:50,415 --> 02:11:55,900
В коллаборативной фильтрации будет мало нового материала, потому что основные детали мы уже знаем.

642
02:11:59,500 --> 02:12:10,037
На следующей неделе мы всё очень подробно разберём с нуля —

643
02:12:10,137 --> 02:12:17,779
разберёмся со стохастическим градиентным спуском, поймём, как задать функцию потерь и как она работает.

644
02:12:17,879 --> 02:12:30,455
После этого мы пройдёмся по уже изученным темам, но глубже — это анализ структурированных данных, свёрточные и рекуррентные нейронные сети.

645
02:12:30,555 --> 02:12:36,400
Надеюсь, мы успеем обсудить все этапы создания таких моделей с нуля.

646
02:12:36,400 --> 02:12:43,205
Мы рассмотрим датасет MovieLens, изучим основополагающую теорию и разберёмся с необходимой математикой.

647
02:12:43,305 --> 02:12:53,389
Вот так выглядит датасет. В первой записи написано, что пользователь номер 1 посмотрел фильм номер 31,

648
02:12:53,489 --> 02:13:01,894
оценил его оценкой 2.5 и это случилось в определённый момент времени.

649
02:13:01,994 --> 02:13:12,519
Фильм номер 1029 имеет оценку 3.0, фильм номер 1172 — оценку 4.0 и так далее.

650
02:13:12,519 --> 02:13:24,574
Задача состоит в том, чтобы предсказать оценку фильма для новых пар «пользователь — фильм».

651
02:13:24,674 --> 02:13:31,809
Так строятся рекомендательные системы — например, так Amazon понимает, какие книги вам могут понравиться,

652
02:13:31,809 --> 02:13:34,094
Netflix рекомендует вам интересные фильмы к просмотру, и так далее.

653
02:13:34,194 --> 02:13:54,634
Я также скачал названия и жанры фильмов, чтобы мы смогли интерпретировать числа в матрице эмбеддинга.

654
02:13:54,734 --> 02:13:59,162
Мы создадим таблицу, где каждая строка соответствует отдельному пользователю, каждый столбец — фильму.

655
02:13:59,262 --> 02:14:03,940
Можете просмотреть Jupyter ноутбук заранее, там всё привычное — 

656
02:14:06,150 --> 02:14:12,579
считывание данных методом CollabFilterDataset.from_csv(), создание алгоритма обучения методом .get_learner(), обучение методом .fit().

657
02:14:12,579 --> 02:14:19,329
Мы опять сравним полученные результаты с результатами схожих соревнований, они опять окажутся лучше.

658
02:14:19,329 --> 02:14:25,459
Мы пробежимся по этим шагам, а потом углубимся в теорию.

659
02:14:25,559 --> 02:14:32,819
Увидимся на следующей неделе.

