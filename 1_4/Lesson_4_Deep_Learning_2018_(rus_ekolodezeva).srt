1
00:00:00,060 --> 00:00:04,944
Добро пожаловать на четвёртую лекцию.

2
00:00:05,044 --> 00:00:23,330
Эта неделя на форуме была активной. Вы написали несколько полезных статей, как и на прошлой неделе, давайте на них посмотрим.

3
00:00:24,480 --> 00:00:35,910
Vitaly Bushaev написал один из лучших постов за последнее время про скорость обучения и SGDR.

4
00:00:39,960 --> 00:00:49,569
Он проделал отличную работу, объяснив базовые идеи понятным всем языком,

5
00:00:49,669 --> 00:00:55,079
но также добавил ссылки на научные статьи для тех, кто хочет узнать больше,

6
00:00:55,079 --> 00:01:08,755
в посте много примеров и детальных объяснений, я считаю, что такие посты очень полезны в нашей сфере.

7
00:01:08,855 --> 00:01:32,939
Мне очень нравится, что материал перед публикацией обсуждается и улучшается на форуме.

8
00:01:32,939 --> 00:01:49,590
Циклическая скорость обучения - популярная тема, Anand Saha тоже написал про это пост,

9
00:01:49,590 --> 00:02:01,670
опять же - отличная работа, много картинок, ссылок на научные статьи и кода, объясняющего, как всё работает.

10
00:02:01,950 --> 00:02:07,600
Mark Hoffman написал хорошее введение в SGDR.

11
00:02:09,580 --> 00:02:16,450
Manikanta Yadunanda написал про дифференциальные скорости обучения

12
00:02:19,390 --> 00:02:31,019
и их применение в переносе обучения, добавив введение в перенос обучения.

13
00:02:31,019 --> 00:02:59,829
Arjun Rajkumar написал пост о компьютерном зрении, мне нравится, что он упомянул реальные применения этой технологии.

14
00:02:59,829 --> 00:03:06,904
Есть другие отличные посты, спасибо всем вам за такую активность.

15
00:03:07,004 --> 00:03:28,350
Повторюсь - если вы хотите написать что-то своё, но боитесь - не стоит, на форуме люди дружелюбны и готовы вам помочь.

16
00:03:30,690 --> 00:03:37,930
Сегодня будет интересная лекция, много информации на совершенно разные темы.

17
00:03:38,030 --> 00:03:51,090
Мы потратили довольно много времени на компьютерное зрение, а сегодня обсудим сразу три области.

18
00:03:51,090 --> 00:04:09,180
Начнём с анализа структурированных данных, то есть данных, хранящихся в таблицах, как базы данных.

19
00:04:09,180 --> 00:04:21,954
Затем посмотрим на обработку естественного языка и на рекомендательные системы.

20
00:04:22,054 --> 00:04:31,080
Мы рассмотрим эти темы на достаточно высоком уровне, но акцент будет на практической части, а не теоретической.

21
00:04:33,510 --> 00:04:46,675
Теоретическая часть будет в следующих трёх лекциях, там же мы ещё раз обсудим компьютерное зрение.

22
00:04:46,775 --> 00:04:58,000
Сегодня мы сфокусируемся на практическом применении этих идей и введём несколько необходимых новых понятий.

23
00:04:58,100 --> 00:05:14,980
Одно из таких понятий - дропаут, вы наверняка уже слышали про него, это полезная техника.

24
00:05:15,080 --> 00:05:21,145
Я продемонстрирую её на примере соревнования Kaggle про породы собак.

25
00:05:21,245 --> 00:05:40,770
Я создал модель и поставил precompute=True, чтобы получить предвычисленные активации на последнем свёрточном слое.

26
00:05:40,770 --> 00:05:57,960
Напоминаю, что активация - это число, вычисленное с помощью весов, или параметров, в свёрточных фильтрах,

27
00:05:57,960 --> 00:06:09,460
которые применяются к предыдущему слою, содержащему активации либо входные данные.

28
00:06:09,560 --> 00:06:14,210
Помните, что активация - это число. Итак, мы используем предвычисленные активации,

29
00:06:17,550 --> 00:06:27,480
а затем добавляем несколько полносвязных слоёв, заполненных случайными числами,

30
00:06:27,480 --> 00:06:43,009
на которые будут умножаться активации, как в нашей демонстрации в Excel.

31
00:06:43,009 --> 00:06:58,870
Если вы выведете содержимое объекта learn, увидите, какие слои мы добавили в модель.

32
00:06:58,870 --> 00:07:02,900
Нормализацию минибатчей мы обсудим на последней лекции.

33
00:07:02,900 --> 00:07:18,505
Линейный слой - это просто матрица 1024x512, то есть слой принимает 1024 активации и возвращает 512.

34
00:07:18,605 --> 00:07:26,750
Это выпрямитель, он заменяет отрицательные числа на нули.

35
00:07:26,750 --> 00:07:38,594
Второй линейный слой берёт 512 активаций с предыдущего слоя, умножает их на матрицу 512x120 и получает 120 новых активаций.

36
00:07:38,694 --> 00:07:48,829
Результат передаётся в функцию Softmax, которую мы обсуждали на прошлой неделе:

37
00:07:48,929 --> 00:08:02,180
она берёт экспоненту от числа и делит её на сумму экспонент от всех чисел, получает вероятность.

38
00:08:02,180 --> 00:08:09,850
Все вероятности в сумме дают 1 и каждая вероятность лежит между 0 и 1.

39
00:08:09,950 --> 00:08:16,595
Итак, это слои, добавленные поверх предвычисленных свёрточных слоёв, они обучаются при precompute=True.

40
00:08:16,695 --> 00:08:24,404
Давайте обсудим, что такое дропаут и что значит параметр p.

41
00:08:24,504 --> 00:08:53,085
Дропаут с параметром p=0.5 значит, что случайная половина всех активаций слоя уничтожается.

42
00:08:53,185 --> 00:09:03,970
Параметр p - вероятность удаления активации, p=0.5 значит удаление 50% активаций слоя.

43
00:09:03,970 --> 00:09:14,924
При удалении половины активаций следующий слой почти не меняется,

44
00:09:15,024 --> 00:09:25,100
так как этот слой - подвыборка максимумом, что-то меняется, только если удалённая ячейка была максимальной из четырёх.

45
00:09:25,100 --> 00:09:31,360
Если следующий слой - свёрточный, тоже меняется немного, потому что свёрточные фильтры рассматривают области 3x3.

46
00:09:31,460 --> 00:09:53,060
Результат интересный. Важно понимать, что после каждого минибатча выбрасывается случайная половина активаций.

47
00:09:53,060 --> 00:10:08,279
Это помогает избежать переобучения. Если какая-то группа активаций очень хорошо предсказывает определённую кошку или собаку,

48
00:10:08,379 --> 00:10:18,405
то после исчезновения этих активаций модель больше не сможет сказать, что находится на этом изображении,

49
00:10:18,505 --> 00:10:31,485
поэтому ей придётся находить признаки, которые будут работать, даже если половина активаций каждый раз выбрасывается.

50
00:10:31,585 --> 00:10:50,310
Этой технике три или четыре года, и она почти полностью решает проблему переобучения.

51
00:10:50,410 --> 00:11:12,079
До того, как появился дропаут, люди не знали, что делать с переобучением, когда дополнение данных и новые данные перестают помогать.

52
00:11:12,079 --> 00:11:35,870
Джеффри Хинтон и его коллеги изобрели дропаут, вдохновляясь работой человеческого мозга.

53
00:11:35,870 --> 00:11:57,539
Если вероятность p=0.01, это значит, что 1% случайных активаций выбрасывается. Это почти ничего не изменит и не спасёт от переобучения.

54
00:11:57,639 --> 00:12:17,805
Если p=0.99, выбросится 99% активаций, переобучения точно не будет, но доля правильных ответов на обучающей выборке будет низкой.

55
00:12:17,905 --> 00:12:32,509
Высокие p позволят хорошо обобщать, но снизят долю правильных ответов на обучающей выборке, низкие - наоборот.

56
00:12:32,509 --> 00:12:43,589
Дропаут объясняет, почему в начале обучения потери на обучающей выборке были больше потерь на валидационной.

57
00:12:43,689 --> 00:12:59,870
Это может показаться странным, но объяснение простое - на валидационной выборке не используется дропаут.

58
00:12:59,870 --> 00:13:10,529
При предсказаниях валидационной выборки хочется использовать наилучшую из возможных моделей, поэтому активации не выбрасывают.

59
00:13:10,629 --> 00:13:26,520
Поэтому иногда, особенно в начале обучения, доля правильных ответов и функция потерь на обучающей выборке хуже, чем на валидационной.

60
00:13:26,520 --> 00:13:30,460
Янет: Надо ли чем-то заменять выброшенные активации?

61
00:13:30,460 --> 00:13:41,375
При использовании дропаута PyTorch делает две вещи. Положим p=0.5.

62
00:13:41,475 --> 00:13:47,600
PyTorch под капотом, во-первых, выкидывает половину активаций, во-вторых, дублирует оставшиеся,

63
00:13:47,700 --> 00:13:56,885
поэтому в среднем активации не меняются, это удобно.

64
00:13:56,985 --> 00:14:06,520
Вам не нужно об этом думать, это делается автоматически.

65
00:14:06,520 --> 00:14:18,370
Вероятность дропаута для всех новых слоёв передаётся параметром ps метода ConvLearner.pretrained().

66
00:14:18,370 --> 00:14:27,670
Дропаут в предобученных слоях не изменится, потому что эти слои уже были обучены с какой-то вероятностью дропаута,

67
00:14:29,740 --> 00:14:33,220
настраивается дропаут только в новых слоях.

68
00:14:33,220 --> 00:14:40,839
Здесь ps=0.5, поэтому в объекте learn первый и второй дропауты оба имеют вероятность p=0.5.

69
00:14:40,839 --> 00:14:46,390
Напомню, что на вход этих слоёв подаются результаты последнего свёрточного слоя преобученной сети,

70
00:14:48,820 --> 00:14:55,804
половина активаций выбрасывается, оставшееся подаётся в линейный слой, пропускается через выпрямитель,

71
00:14:55,904 --> 00:15:05,230
снова половина активаций выбрасывается, оставшееся подаётся в линейный слой, а потом подаётся в функцию Softmax.

72
00:15:05,230 --> 00:15:11,950
Для небольшого увеличения точности расчётов имеет смысл брать логарифм Softmax, а не просто Softmax,

73
00:15:12,520 --> 00:15:22,480
потому что потом от этого берётся экспонента, но это неважно.

74
00:15:22,480 --> 00:15:36,380
Если убрать дропаут, поставив ps=0, доля правильных ответов в первую эпоху увеличится с 0.76 до 0.8,

75
00:15:36,380 --> 00:15:45,649
что неудивительно, потому что мы перестали выбрасывать важные результаты в начале обучения.

76
00:15:45,649 --> 00:15:52,350
Но на третьей эпохе результаты другие - доля правильных ответов была 84.8%, а стала 84.1%.

77
00:15:52,450 --> 00:16:02,489
Модель сильно переобучилась уже после трёх эпох - потери равны 0.35 на обучающей выборке и 0.55 на валидационной.

78
00:16:02,589 --> 00:16:15,975
Как видите, в модели нет дропаута, если ps=0, эти слои даже не добавляются.

79
00:16:16,075 --> 00:16:29,450
В модели два дополнительных линейных слоя - это не обязательно.

80
00:16:29,450 --> 00:16:41,720
В параметр xtra_fc (extra fully connected layers) передаётся количество дополнительных полносвязных слоёв и их размеры.

81
00:16:41,720 --> 00:16:51,170
Должен быть хотя бы один полносвязный слой, который примет на вход результат работы последнего свёрточного слоя,

82
00:16:51,170 --> 00:16:56,660
здесь размер последнего слоя 1024, и вернёт вектор длиной в количество классов -

83
00:16:56,660 --> 00:17:05,024
2 класса в задаче классификации собак и кошек, 120 пород собак и 17 типов спутниковых снимков.

84
00:17:05,124 --> 00:17:14,445
Размеры единственного линейного слоя нельзя выбрать, они определяются задачей, но можно выбрать размеры дополнительных.

85
00:17:14,545 --> 00:17:25,880
Здесь мы задаём xtra_fc=[], в модели будет только один линейный слой. Вероятность дропаута ps=0.

86
00:17:25,880 --> 00:17:35,080
Это минимальный набор дополнительных слоёв.

87
00:17:36,800 --> 00:17:53,059
Даже так мы получаем неплохие результаты, потому что обучение было недолгим и предобученная модель подходит к датасету.

88
00:17:53,059 --> 00:18:01,640
Янет: Какое значение вероятности дропаута использовать по умолчанию?

89
00:18:01,640 --> 00:18:20,710
По умолчанию fastai выставляет p=0.25 для первого слоя дропаута и p=0.5 для второго, это обычно работает.

90
00:18:20,710 --> 00:18:38,080
Если ваша модель переобучается. поставьте обе вероятности ps=0.5, не помогло - ps=0.7 и так далее.

91
00:18:38,080 --> 00:18:47,320
Если модель недообучается, попробуйте уменьшить вероятность, скорее всего, сильно уменьшать не придётся.

92
00:18:47,420 --> 00:19:06,089
Вероятность дропаута редко приходится уменьшать, обычно приходится увеличивать, и значения 0.6-0.7 для меня оптимальны.

93
00:19:06,189 --> 00:19:21,689
Я увеличил вероятность дропаута ps=0.5 в Jupyter ноутбуке с породами собак, когда перешёл с ResNet34 на ResNet50 -

94
00:19:21,789 --> 00:19:32,984
У ResNet34 меньше параметров и она хуже переобучается, в отличие от ResNet50, которая начала переобучаться.

95
00:19:33,084 --> 00:19:41,350
С большими моделями обычно сложнее.

96
00:19:42,000 --> 00:19:49,169
Вопрос из зала: Параметр p=0.5 значит вероятность в 50%?
Да.

97
00:19:49,169 --> 00:20:01,174
Вопрос из зала: Как понять, что модель переобучается?

98
00:20:01,474 --> 00:20:11,889
Если потери на обучающей выборке меньше потерь на валидационной.

99
00:20:11,889 --> 00:20:23,914
Не стоит концентрироваться на переобучении, просто старайтесь уменьшить потери на валидационной выборке.

100
00:20:24,014 --> 00:20:38,919
Экспериментируйте и научитесь понимать, что работает лучше и когда модель сильно переобучена.

101
00:20:38,919 --> 00:20:51,376
Окей, так устроен дропаут, не забывайте, что он есть по умолчанию. Ещё вопрос?

102
00:20:51,476 --> 00:21:07,809
Вопрос из зала: Вероятность дропаута p=0.5 удаляет каждую активацию с вероятностью 50% или выбирает половину и удаляет её?

103
00:21:07,809 --> 00:21:12,639
Удаляет каждую активацию с вероятностью 50%.

104
00:21:12,639 --> 00:21:19,960
Вопрос из зала: Почему имеет значение среднее активаций?

105
00:21:19,960 --> 00:21:40,139
Вернёмся к демонстрации в Excel. Это число получено перемножением четырёх групп чисел по 9.

106
00:21:40,139 --> 00:21:52,624
Если выкинуть половину чисел на одном слое, числа на другом слое будут примерно вдвое меньше, и так на каждом слое после.

107
00:21:52,724 --> 00:22:03,970
Если раньше ухо считалось пушистым при значении выше 0.6, теперь это значение - 0.3, меняется значение активаций,

108
00:22:04,070 --> 00:22:09,825
а хочется выкидывать половину активаций так, чтобы ничего не менялось.

109
00:22:09,925 --> 00:22:19,400
Вопрос из зала:

110
00:22:19,400 --> 00:22:23,720


111
00:22:23,720 --> 00:22:28,790


112
00:22:28,790 --> 00:22:35,000


113
00:22:35,000 --> 00:22:42,260


114
00:22:42,260 --> 00:22:47,690
Вопрос из зала: Можно ли задавать разные вероятности дропаута для разных слоёв?

115
00:22:47,690 --> 00:23:00,797
Да, в параметр ps можно передать массив, например ps=[0, 0.2] и xtra_fc=[512]

116
00:23:00,897 --> 00:23:13,650
значит отсутствие дропаута перед первым линейным слоем и дропаут с вероятностью p=0.5 перед вторым.

117
00:23:13,750 --> 00:23:28,100
Даже спустя несколько лет я до сих пор не понял, как разумно выставлять дропаут для разных слоёв,

118
00:23:30,260 --> 00:23:35,690
если кто-то из вас придумает хорошие правила, расскажите.

119
00:23:35,790 --> 00:23:52,450
Если вы не знаете, попробуйте либо дропаут с одинаковой вероятностью для всех слоёв, либо дропаут только на последнем слое.

120
00:23:53,670 --> 00:24:04,565
Вопрос из зала: Почему вы отслеживаете значения функции потерь, а не долю правильных ответов?

121
00:24:04,665 --> 00:24:13,920
Потому что потери известны и для обучающей, и для валидационной выборки.

122
00:24:13,920 --> 00:24:31,310
Дальше мы узнаем, что оптимизируется именно функция потерь, поэтому её проще отслеживать и понимать.

123
00:24:31,410 --> 00:24:44,225
Вопрос из зала: Дропаут добавляет случайный шум на каждой итерации, как это влияет на скорость обучения?

124
00:24:44,325 --> 00:25:01,415
Теоретически может, но я на практике никогда не замечал.

125
00:25:01,515 --> 00:25:13,600
Давайте поговорим про анализ структурированных данных. Напомню, что мы анализировали данные соревнования Kaggle

126
00:25:13,600 --> 00:25:26,060
на данных Rossman, это немецкая сеть супермаркетов. Jupyter ноубтук называется lesson3-rossman.ipynb.

127
00:25:26,160 --> 00:25:36,880
Главный датасет содержал данные о продажах разных магазинов в разные дни,

128
00:25:36,880 --> 00:25:53,890
а также информацию о том, был ли открыт магазин, был ли в этот день праздник или акция и так далее.

129
00:25:53,890 --> 00:26:04,750
Была таблица о магазинах, содержащая тип магазина и ассортимента и информацию о ближайших конкурентах.

130
00:26:04,750 --> 00:26:11,590
Столбцы, или признаки, в таких таблицах можно поделить на категориальные и количественные.

131
00:26:11,590 --> 00:26:19,715
Категориальные признаки - это различные категории, например, ассортимент товара Assortment - 'a', 'b' или 'c'.

132
00:26:19,815 --> 00:26:27,910
Количественные признаки выражаются числом, например, расстояние до ближайшего конкурента CompetitionDistance -

133
00:26:27,910 --> 00:26:32,380
имеют значения различия между числами и их абсолютные величины.

134
00:26:32,380 --> 00:26:37,720
Эти два вида признаков обрабатываются по-разному.

135
00:26:37,720 --> 00:26:49,230
Тем, кто занимался машинным обучением, знакомы количественные признаки - на них легко построить линейную регрессию.

136
00:26:49,230 --> 00:26:54,850
С категориальными признаками чуть сложнее.

137
00:26:54,850 --> 00:27:06,944
Мы не будем смотреть на предобработку данных и выделение признаков, в итоге получается такая таблица и такие признаки.

138
00:27:07,044 --> 00:27:29,045
Напомню, что я не выделял признаки, это код обладателей третьего места в соревновании.

139
00:27:29,145 --> 00:27:46,660
Признаки разделены на категориальные и количественные. Год, месяц и день можно считать количественными признаками,

140
00:27:46,660 --> 00:27:52,179
потому что разница между, например, 2000 и 2003 имеет значение,

141
00:27:52,179 --> 00:28:14,570
но здесь они категориальные. Модель считает, что 2000, 2001 и 2002 года никак не связаны между собой.

142
00:28:14,670 --> 00:28:23,109
Если бы год был количественным признаком, модель строила бы гладкую функцию под все возможные значения.

143
00:28:23,209 --> 00:28:36,415
Поэтому количественные признаки с небольшим разбросом дискретных значений иногда удобнее считать категориальными.

144
00:28:36,515 --> 00:28:47,529
Ещё один пример - день недели. Можно пронумеровать дни от 0 до 6, потому что разница между 3 и 5, два дня, имеет смысл.

145
00:28:47,629 --> 00:28:56,609
Но если вдуматься, продажи могут различаться в зависимости от дня недели, а не его номера -

146
00:28:56,609 --> 00:29:05,700
в выходные один уровень продаж, по пятницам другой, в среду третий.

147
00:29:05,700 --> 00:29:12,909
Поэтому мы считаем день недели категориальным признаком.

148
00:29:13,009 --> 00:29:24,399
Вам придётся принимать решения о том, какой признак считать категориальным, а какой количественным.

149
00:29:24,499 --> 00:29:37,619
Если в столбце значения закодированы как 'B' или 'С', или как 'Джереми' и 'Янет', у вас нет выбора - это категориальный признак.

150
00:29:37,619 --> 00:29:51,415
Если признак закодирован числом, например, возраст или день недели, вам нужно решить, как это представить.

151
00:29:51,515 --> 00:29:59,199
Итак, если признак категориальный, он таким и останется, но количественный признак можно перевести в категориальный.

152
00:29:59,299 --> 00:30:15,594
Здесь я не принимал никаких решений - признаки были разделены на количественные и категориальные обладателями третьего места.

153
00:30:15,694 --> 00:30:29,545
Как видите, все количественные признаки содержат дробные числа, например, CompetitionDistance и температурные признаки.

154
00:30:29,645 --> 00:30:46,300
Эти признаки сложно сделать качественными, потому что почти все значения уникальны.

155
00:30:46,300 --> 00:30:54,520
Количество возможных значений признака обозначается как мощность.

156
00:30:54,520 --> 00:31:00,390
Мощность признака дня недели - семь, потому что различных дней недели семь.

157
00:31:02,010 --> 00:31:09,165
Вопрос из зала: Вы используете сегментацию признаков?

158
00:31:09,265 --> 00:31:13,033
Нет, не использую, хотя здесь

159
00:31:13,133 --> 00:31:23,140
максимальную температуру можно было бы разбить на категории 0-10, 10-20 и 20-30 и превратить в количественный признак.

160
00:31:23,140 --> 00:31:36,160
Неделю назад вышла статья, авторы которой утверждают, что сегментация признаков - это полезно, это первое известное мне практическое упоминание.

161
00:31:36,260 --> 00:31:44,507
Ещё неделю назад я сказал бы, что это плохая идея, но сейчас не уверен, возможно, это может быть полезно.

162
00:31:44,607 --> 00:32:00,880
Вопрос из зала: Если год - категориальный признак, что произойдёт, показать модели год, которого она ещё не видела?

163
00:32:00,880 --> 00:32:16,760
Мы это ещё обсудим. Короткий ответ - этот год будет считаться неизвестной категорией, pandas умеет с таким обращаться.

164
00:32:16,860 --> 00:32:24,840
По сути, это будет просто ещё одна категория "Неизвестная категория".

165
00:32:25,130 --> 00:32:35,980
Вопрос из зала: Если неизвестная категория будет в тестовой выборке, что предскажет модель?

166
00:32:36,080 --> 00:32:46,679
Ну, что-нибудь да предскажет. Если в обучающей выборке были неизвестные категории,

167
00:32:46,679 --> 00:32:55,164
она научится с таким обращаться, если нет - вставит случайный вектор.

168
00:32:55,264 --> 00:33:05,529
Это интересная тема, её стоит обсудить в этой части курса, и точно можно обсудить на форуме.

169
00:33:05,529 --> 00:33:21,764
Итак, мы разделили признаки на категориальные и количественные, в таблице 844338 записей.

170
00:33:21,864 --> 00:33:36,835
Мы преобразуем данные для признаков, которые решили сделать категориальными, методом astype('category').

171
00:33:36,935 --> 00:33:48,379
Я не буду рассказывать про pandas, про это есть куча хороших книг, например, Python for Data Analysis, я про неё уже говорил.

172
00:33:48,379 --> 00:33:53,564
Надеюсь, что и так понятно, что делает код.

173
00:33:53,664 --> 00:34:04,149
Итак, категориальные признаки преобразуются в тип category, а количественные - в 32-битное число с плавающей точкой,

174
00:34:04,249 --> 00:34:10,569
потому что с таким форматом чисел работает PyTorch.

175
00:34:10,669 --> 00:34:30,584
Некоторые количественные признаки, например, Promo, содержат значения 0 или 1, они тоже преобразуются в числа с плавающей точкой.

176
00:34:30,684 --> 00:34:44,339
Я стараюсь проделать большую часть обучения на маленьких датасетах - например, уменьшать изображения до размера 128 или 64.

177
00:34:44,439 --> 00:34:52,460
Структурированные данные нельзя уменьшить, вместо этого я случайно выбираю подмножество данных.

178
00:34:52,460 --> 00:35:07,609
Для этого я использую уже знакомую нам функцию get_cv_idxs() для получения индексов валидационной выборки.

179
00:35:07,609 --> 00:35:21,300
В итоге я получаю 150 тысяч случайных записей.

180
00:35:21,300 --> 00:35:32,910
Вот так они выглядят - есть булевские переменные, есть строки, есть целые числа.

181
00:35:37,020 --> 00:35:52,480
Несмотря на то, что я перевёл эти признаки в категориальные, они всё ещё показываются так, просто хранятся по-другому.

182
00:35:52,580 --> 00:36:05,070
В fastai есть функция proc_df() (processed dataframe), которая принимает датасет и название целевой переменной.

183
00:36:05,070 --> 00:36:22,589
Функция делает несколько вещей. Во-первых, она отделяет целевую переменную в объект y и датасет без неё в объект df.

184
00:36:22,589 --> 00:36:36,865
Она нормирует данные - нейронным сетям удобно, когда все данные близки к 0 со стандартным отклонением около 1.

185
00:36:36,965 --> 00:36:43,650
Для этого из каждого столбца вычитается среднее и все числа делятся на стандартное отклонение.

186
00:36:43,650 --> 00:36:51,900
Это регулируется параметром do_scale=True, при этом средние и стандартные отклонения сохраняются в объекте mapper,

187
00:36:51,900 --> 00:36:57,780
чтобы потом обработать тестовую выборку таким же образом.

188
00:36:57,780 --> 00:37:08,669
Функция вставляет пропущенные значения - неизвестные категории приобретают индекс 0 (известные нумеруются с 1),

189
00:37:08,769 --> 00:37:21,952
неизвестные количественные признаки заменяются медианой, и создаётся отдельный признак "Значение было пропущено".

190
00:37:22,052 --> 00:37:29,369
Я не буду в это углубляться, мы подробно обсудили это в курсе по машинному обучению.

191
00:37:29,369 --> 00:37:36,140
Если у вас есть какие-то вопросы на эту тему, изучите его, там нет ничего про глубокое обучение.

192
00:37:36,900 --> 00:37:42,330
Как видите, после обработки год 2014 стал годом номер 2.

193
00:37:42,330 --> 00:37:47,220
Все категориальные признаки были заменены на целые числа, начиная с нуля,

194
00:37:47,220 --> 00:37:59,560
так как эти признаки потом образуют таблицу, и мы не хотим, чтобы на год ушло 2014 столбцов, когда хватило бы двух.

195
00:37:59,660 --> 00:38:11,950
Как видите, таким же образом строки 'a' и 'c' преобразовались в числа 1 и 3.

196
00:38:12,050 --> 00:38:20,220
Окей, у нас есть датафрейм, не содержащий целевую переменную, где все значения - это числа.

197
00:38:20,220 --> 00:38:35,050
Всё, что мы делали до этого момента, не содержало никакого глубокого обучения и обсуждалось в курсе по машинному обучению.

198
00:38:35,150 --> 00:38:41,200
Одна из вещей, которые подробно обсуждались в курсе по машинному обучению - валидационные выборки.

199
00:38:41,300 --> 00:38:54,510
По условиям соревнования Kaggle нужно предсказать следующие две недели продаж.

200
00:38:54,510 --> 00:39:05,755
Я возьму последние две недели обучающей выборки в качестве валидационной, чтобы максимально приблизить её к тестовой.

201
00:39:05,855 --> 00:39:18,279
Рейяер недавно написала пост про создание валидационных выборок, она есть на fast.ai, мы добавим её в вики.

202
00:39:18,379 --> 00:39:34,460
По сути, это пересказ одной из лекций курса "Машинное обучение", к этой лекции есть видеозапись.

203
00:39:35,579 --> 00:39:42,125
Мы с Рейчел долго думали над тем, как обращаться с обучающей, валидационной и тестовой выборками, результаты в этом посте,

204
00:39:42,225 --> 00:39:50,710
но, опять же - там нет ничего про глубокое обучение, давайте уже к нему перейдём.

205
00:39:50,710 --> 00:40:05,479
В каждом соревновании Kaggle и вообще в любом проекте, связанном с машинным обучением, необходимо выбрать метрику.

206
00:40:05,579 --> 00:40:17,770
На соревновании Kaggle метрику оценки качества модели выбрали за нас - это средняя процентная ошибка (RMSPE).

207
00:40:17,770 --> 00:40:25,245
Функция ошибки возвращает корень из среднего квадрата процентного отклонения предсказания от истинной величины.

208
00:40:25,345 --> 00:40:26,460


209
00:40:26,560 --> 00:40:28,890


210
00:40:33,819 --> 00:40:37,839


211
00:40:37,839 --> 00:40:43,359


212
00:40:43,359 --> 00:40:47,989


213
00:40:48,089 --> 00:41:01,930
В PyTorch нет среднеквадратичной процентной ошибки, но мы можем написать её сами.

214
00:41:01,930 --> 00:41:22,380
Проще вспомнить логарифмы и понять, что частное а / b можно заменить на ln(a' / b') = ln(a') - ln(b').

215
00:41:22,380 --> 00:41:28,930


216
00:41:30,400 --> 00:41:35,430


217
00:41:39,880 --> 00:41:44,200


218
00:41:46,599 --> 00:41:50,740


219
00:41:53,770 --> 00:42:00,970


220
00:42:00,970 --> 00:42:04,510


221
00:42:04,510 --> 00:42:11,020
Эта часть тоже не относится к глубокому обучению, мы уже к нему переходим.

222
00:42:14,530 --> 00:42:19,162
Пока мы делаем всё то же самое, что и раньше:

223
00:42:19,262 --> 00:42:28,185
Создаём объект данных модели ModelData, который содержит валидационную, обучающую и иногда тестовую выборки,

224
00:42:28,285 --> 00:42:41,110
на его основе потом создастся learner, найдётся скорость обучения, обучится модель и так далее, это всё вы уже видели.

225
00:42:41,110 --> 00:42:55,030
Различие будет в том, что данные не разложены по папкам и не размечены в файле CSV,

226
00:42:55,030 --> 00:43:08,765
поэтому мы используем метод ColumnarModelData.from_data_frame(), он возвращает объект такой же структуры, как и на прошлых лекциях.

227
00:43:08,865 --> 00:43:24,590
Переменная PATH - директория, в которой хранятся необходимые файлы модели.

228
00:43:24,690 --> 00:43:29,075
Переменная val_idx содержит индексы валидационной выборки, мы объявили её выше.

229
00:43:29,175 --> 00:43:51,290
Переменная df - наш датафрейм. Переменная yl - логарифм целевой переменной y, она должна быть в модели в каком-то виде.

230
00:43:51,390 --> 00:44:03,380
Итак, это индексы валидационной выборки, датафрейм без целевой переменной, целевая переменная, а это -

231
00:44:03,380 --> 00:44:19,769
список категориальных признаков. Напомню, что сейчас все данные хранятся в виде чисел, поэтому важно его указать.

232
00:44:19,769 --> 00:44:30,469
В качестве списка категориальных признаков мы передаём переменную cat_vars, объявленную раньше.

233
00:44:30,469 --> 00:44:39,469
Также передаётся размер минибатча bs=128, этот параметр нам уже знаком.

234
00:44:39,569 --> 00:45:04,349
Теперь у нас есть объект данных модели md, содержащий уже упомянутые объекты train_dl, val_dl, train_ds, val_ds, размер выборки и всё остальное.

235
00:45:04,349 --> 00:45:18,709
После этого мы создадим из объекта данных модели алгоритм обучания методом get_learner().

236
00:45:21,769 --> 00:45:37,614
Эти параметры - начальная вероятность дропаута, количество активаций на каждом слое и дропаут для последних слоёв.

237
00:45:37,714 --> 00:45:53,519
Один из незнакомых параметров - emb_szc, вложения, сейчас их обсудим.

238
00:45:53,519 --> 00:46:05,549
Давайте забудем ненадолго про категориальные признаки и рассмотрим только количественные.

239
00:46:05,549 --> 00:46:29,740
Давайте возьмём все количественные признаки - минимальную и максимальную температуры, расстояние до ближайшего конкурента и так далее.

240
00:46:32,589 --> 00:46:36,700
Это всё числа с плавающей точкой.

241
00:46:36,700 --> 00:46:54,130
Нейронная сеть возьмёт этот одномерный массив, или вектор, или тензор первого ранга, и умножит его на матрицу.

242
00:46:54,130 --> 00:47:12,940
Допустим, у нас 20 признаков и мы хотим получить 100 новых, для этого матрица должна иметь размер 20x100.

243
00:47:12,940 --> 00:47:28,410
В итоге получается новый вектор длины 100, так работает линейный слой.

244
00:47:28,410 --> 00:47:39,670
Полученный вектор пропускается через выпрямитель, негативные значения заменяются нулями.

245
00:47:39,670 --> 00:47:59,349
После этого вектор умножается на ещё одну матрицу. Допустим, на этом нейронная сеть кончается, и мы хотим получить одно число.

246
00:47:59,349 --> 00:48:13,444
В таком случае матрица должна иметь размер 100x1. Это - нейронная сеть, состоящая из одного слоя.

247
00:48:13,544 --> 00:48:25,070
На практике обычно делают больше одного слоя, пусть матрица имеет размер 100x50.

248
00:48:25,170 --> 00:48:42,420
После умножения мы получаем новый вектор длиной 50, после умножения на матрицу 50x1 получается число.

249
00:48:42,420 --> 00:49:06,119
Напомню, что не нужно ставить выпрямитель перед Softmax, так как Softmax нужны негативные значения для низких вероятностей.

250
00:49:13,390 --> 00:49:22,400
Вот схема простейшей полносвязной нейронной сети.

251
00:49:22,400 --> 00:49:43,579
На входе - тензор первого ранга, дальше идут линейный слой, выпрямитель, ещё один линейный слой и Softmax.

252
00:49:43,579 --> 00:50:02,839
Можно добавить больше связок выпрямитель+линейный слой, можно добавить дропаут, в принципе, на этом вариации кончаются.

253
00:50:02,839 --> 00:50:12,619
Мы посмотрим на другие архитектуры, когда вернёмся к компьютерному зрению.

254
00:50:12,619 --> 00:50:27,170
Полносвязные нейронные сети -  это просто комбинация перемножения матриц и функций активаций -  например, выпрямителей и Softmax.

255
00:50:27,170 --> 00:50:42,434
Softmax необходим только в задачах классификации, здесь можно без него обойтись.

256
00:50:42,534 --> 00:50:51,440
Есть один нюанс, но в общем всё устроено именно так.

257
00:50:51,440 --> 00:50:56,420
Вернёмся к категориальным признакам.

258
00:50:56,420 --> 00:51:06,019


259
00:51:11,259 --> 00:51:19,430


260
00:51:19,430 --> 00:51:22,940


261
00:51:22,940 --> 00:51:28,490


262
00:51:35,690 --> 00:51:43,130


263
00:51:43,130 --> 00:51:52,220


264
00:51:53,690 --> 00:52:00,530


265
00:52:00,530 --> 00:52:06,680


266
00:52:08,869 --> 00:52:15,710


267
00:52:15,710 --> 00:52:20,720


268
00:52:26,480 --> 00:52:31,730


269
00:52:31,730 --> 00:52:37,510


270
00:52:37,510 --> 00:52:44,570


271
00:52:44,570 --> 00:52:50,150


272
00:52:50,150 --> 00:52:54,920


273
00:52:54,920 --> 00:53:01,070


274
00:53:01,070 --> 00:53:05,960


275
00:53:05,960 --> 00:53:11,840


276
00:53:11,840 --> 00:53:16,010


277
00:53:16,010 --> 00:53:20,150


278
00:53:20,150 --> 00:53:23,510


279
00:53:23,510 --> 00:53:28,730


280
00:53:28,730 --> 00:53:31,520


281
00:53:31,520 --> 00:53:36,320


282
00:53:36,320 --> 00:53:40,520


283
00:53:40,520 --> 00:53:44,870


284
00:53:44,870 --> 00:53:48,520


285
00:53:48,520 --> 00:53:56,180


286
00:53:56,180 --> 00:54:01,480


287
00:54:03,640 --> 00:54:10,600


288
00:54:10,600 --> 00:54:17,540


289
00:54:17,540 --> 00:54:23,540


290
00:54:23,540 --> 00:54:30,410


291
00:54:30,410 --> 00:54:37,070


292
00:54:37,070 --> 00:54:41,960


293
00:54:41,960 --> 00:54:46,910


294
00:54:46,910 --> 00:54:55,010


295
00:54:55,010 --> 00:54:59,660


296
00:55:02,360 --> 00:55:07,550


297
00:55:07,550 --> 00:55:13,820


298
00:55:13,820 --> 00:55:19,160


299
00:55:20,680 --> 00:55:26,930


300
00:55:26,930 --> 00:55:32,090


301
00:55:32,090 --> 00:55:37,940


302
00:55:37,940 --> 00:55:41,660


303
00:55:41,660 --> 00:55:47,270


304
00:55:47,270 --> 00:55:52,099


305
00:55:52,099 --> 00:55:59,359


306
00:56:03,140 --> 00:56:15,829


307
00:56:15,829 --> 00:56:20,599


308
00:56:20,599 --> 00:56:24,130


309
00:56:24,130 --> 00:56:29,089


310
00:56:29,089 --> 00:56:33,619


311
00:56:33,619 --> 00:56:37,490


312
00:56:39,619 --> 00:56:44,630


313
00:56:44,630 --> 00:56:51,140


314
00:56:55,490 --> 00:57:04,180


315
00:57:04,180 --> 00:57:10,210


316
00:57:10,210 --> 00:57:15,200


317
00:57:15,200 --> 00:57:20,900


318
00:57:20,900 --> 00:57:25,490


319
00:57:31,220 --> 00:57:36,279


320
00:57:36,279 --> 00:57:43,700


321
00:57:43,700 --> 00:57:53,450


322
00:57:53,450 --> 00:57:59,270


323
00:57:59,270 --> 00:58:03,320


324
00:58:03,320 --> 00:58:07,250


325
00:58:07,250 --> 00:58:11,690


326
00:58:11,690 --> 00:58:19,190


327
00:58:19,190 --> 00:58:25,970


328
00:58:26,660 --> 00:58:31,280


329
00:58:31,280 --> 00:58:35,030


330
00:58:35,030 --> 00:58:39,589


331
00:58:41,510 --> 00:58:45,829


332
00:58:45,829 --> 00:58:50,390


333
00:58:53,480 --> 00:58:59,990


334
00:59:02,000 --> 00:59:06,920


335
00:59:06,920 --> 00:59:12,500


336
00:59:12,500 --> 00:59:18,230


337
00:59:18,230 --> 00:59:21,819


338
00:59:23,260 --> 00:59:31,039


339
00:59:31,039 --> 00:59:36,500


340
00:59:36,500 --> 00:59:43,789


341
00:59:43,789 --> 00:59:50,359


342
00:59:52,309 --> 01:00:00,619


343
01:00:00,619 --> 01:00:05,299


344
01:00:05,299 --> 01:00:13,819


345
01:00:13,819 --> 01:00:19,549


346
01:00:19,549 --> 01:00:24,140


347
01:00:24,140 --> 01:00:29,599


348
01:00:29,599 --> 01:00:34,760


349
01:00:34,760 --> 01:00:42,200


350
01:00:45,890 --> 01:00:52,309


351
01:00:52,309 --> 01:00:57,349


352
01:00:57,349 --> 01:01:05,230


353
01:01:06,460 --> 01:01:11,029


354
01:01:11,029 --> 01:01:18,799


355
01:01:18,799 --> 01:01:26,029


356
01:01:26,029 --> 01:01:31,240


357
01:01:31,240 --> 01:01:37,960


358
01:01:37,960 --> 01:01:44,510


359
01:01:44,510 --> 01:01:49,820


360
01:01:49,820 --> 01:01:56,000


361
01:01:56,000 --> 01:02:01,130


362
01:02:01,130 --> 01:02:06,980


363
01:02:06,980 --> 01:02:12,980


364
01:02:12,980 --> 01:02:17,390


365
01:02:17,390 --> 01:02:21,080


366
01:02:21,080 --> 01:02:23,630


367
01:02:23,630 --> 01:02:29,660


368
01:02:33,770 --> 01:02:41,540


369
01:02:41,540 --> 01:02:50,300


370
01:02:50,300 --> 01:02:55,310


371
01:02:57,470 --> 01:03:02,990


372
01:03:02,990 --> 01:03:07,280


373
01:03:07,280 --> 01:03:13,550


374
01:03:13,550 --> 01:03:18,440


375
01:03:18,440 --> 01:03:22,430


376
01:03:22,430 --> 01:03:26,330


377
01:03:26,330 --> 01:03:33,140


378
01:03:33,140 --> 01:03:38,660


379
01:03:38,660 --> 01:03:45,200


380
01:03:45,200 --> 01:03:47,860


381
01:03:47,860 --> 01:03:51,850


382
01:03:51,850 --> 01:03:56,170


383
01:03:56,170 --> 01:04:00,370


384
01:04:00,370 --> 01:04:05,470


385
01:04:05,470 --> 01:04:12,400


386
01:04:12,400 --> 01:04:18,610


387
01:04:18,610 --> 01:04:24,340


388
01:04:24,340 --> 01:04:31,000


389
01:04:31,000 --> 01:04:36,160


390
01:04:36,160 --> 01:04:39,250


391
01:04:39,250 --> 01:04:43,120


392
01:04:43,120 --> 01:04:49,330


393
01:04:49,330 --> 01:04:52,720


394
01:04:52,720 --> 01:04:56,620


395
01:04:56,620 --> 01:05:01,240


396
01:05:01,240 --> 01:05:08,260


397
01:05:08,260 --> 01:05:15,160


398
01:05:19,240 --> 01:05:27,670


399
01:05:27,670 --> 01:05:30,540


400
01:05:32,440 --> 01:05:38,059


401
01:05:38,059 --> 01:05:43,789


402
01:05:43,789 --> 01:05:49,220


403
01:05:51,980 --> 01:05:58,460


404
01:05:58,460 --> 01:06:04,789


405
01:06:04,789 --> 01:06:13,010


406
01:06:13,010 --> 01:06:18,710


407
01:06:18,710 --> 01:06:23,450


408
01:06:23,450 --> 01:06:29,480


409
01:06:29,480 --> 01:06:34,490


410
01:06:37,069 --> 01:06:40,400


411
01:06:40,400 --> 01:06:45,170


412
01:06:45,170 --> 01:06:49,700


413
01:06:52,819 --> 01:06:59,510


414
01:06:59,510 --> 01:07:03,460


415
01:07:03,460 --> 01:07:09,109


416
01:07:09,109 --> 01:07:16,220


417
01:07:16,220 --> 01:07:26,210


418
01:07:26,210 --> 01:07:33,170


419
01:07:33,170 --> 01:07:39,079


420
01:07:39,079 --> 01:07:43,610


421
01:07:43,610 --> 01:07:48,830


422
01:07:48,830 --> 01:07:52,970


423
01:07:52,970 --> 01:07:59,570


424
01:07:59,570 --> 01:08:04,910


425
01:08:08,900 --> 01:08:19,720


426
01:08:19,720 --> 01:08:26,180


427
01:08:26,180 --> 01:08:32,210


428
01:08:32,210 --> 01:08:36,890


429
01:08:36,890 --> 01:08:43,070


430
01:08:45,500 --> 01:08:50,300


431
01:08:50,300 --> 01:08:57,200


432
01:08:57,200 --> 01:09:01,010


433
01:09:01,010 --> 01:09:08,300


434
01:09:08,300 --> 01:09:12,830


435
01:09:12,830 --> 01:09:18,380


436
01:09:18,380 --> 01:09:22,220


437
01:09:22,220 --> 01:09:26,600


438
01:09:26,600 --> 01:09:29,540


439
01:09:29,540 --> 01:09:32,890


440
01:09:33,310 --> 01:09:39,590


441
01:09:39,590 --> 01:09:44,720


442
01:09:44,720 --> 01:09:49,430


443
01:09:49,430 --> 01:09:54,830


444
01:09:54,830 --> 01:09:59,840


445
01:09:59,840 --> 01:10:03,820


446
01:10:03,820 --> 01:10:12,050


447
01:10:12,050 --> 01:10:17,270


448
01:10:17,270 --> 01:10:23,360


449
01:10:26,890 --> 01:10:32,540


450
01:10:32,540 --> 01:10:37,220


451
01:10:37,220 --> 01:10:41,600


452
01:10:41,600 --> 01:10:47,630


453
01:10:47,630 --> 01:10:51,200


454
01:10:51,200 --> 01:10:57,920


455
01:10:57,920 --> 01:11:02,840


456
01:11:02,840 --> 01:11:06,320


457
01:11:06,320 --> 01:11:10,820


458
01:11:10,820 --> 01:11:14,660


459
01:11:14,660 --> 01:11:19,130


460
01:11:19,130 --> 01:11:22,910


461
01:11:22,910 --> 01:11:30,410


462
01:11:30,410 --> 01:11:35,000


463
01:11:35,000 --> 01:11:41,750


464
01:11:41,750 --> 01:11:49,550


465
01:11:49,550 --> 01:11:54,020


466
01:11:54,020 --> 01:11:58,720


467
01:11:58,720 --> 01:12:03,860


468
01:12:05,780 --> 01:12:10,860


469
01:12:10,860 --> 01:12:15,929


470
01:12:15,929 --> 01:12:22,949


471
01:12:25,500 --> 01:12:31,800


472
01:12:31,800 --> 01:12:35,489


473
01:12:37,020 --> 01:12:45,060


474
01:12:45,060 --> 01:12:49,619


475
01:12:52,530 --> 01:12:58,760


476
01:12:58,760 --> 01:13:12,000


477
01:13:12,000 --> 01:13:18,179


478
01:13:18,179 --> 01:13:21,480


479
01:13:24,210 --> 01:13:27,060


480
01:13:27,060 --> 01:13:35,100


481
01:13:35,100 --> 01:13:41,070


482
01:13:44,219 --> 01:13:50,280


483
01:13:50,280 --> 01:13:55,230


484
01:13:55,230 --> 01:14:01,500


485
01:14:01,500 --> 01:14:04,230


486
01:14:04,230 --> 01:14:11,190


487
01:14:11,190 --> 01:14:16,230


488
01:14:16,230 --> 01:14:20,460


489
01:14:20,460 --> 01:14:25,500


490
01:14:25,500 --> 01:14:31,199


491
01:14:31,199 --> 01:14:35,190


492
01:14:35,190 --> 01:14:39,510


493
01:14:39,510 --> 01:14:45,449


494
01:14:47,929 --> 01:14:51,840


495
01:14:51,840 --> 01:15:00,619


496
01:15:01,989 --> 01:15:11,140


497
01:15:11,140 --> 01:15:16,680


498
01:15:16,680 --> 01:15:21,100


499
01:15:21,100 --> 01:15:24,610


500
01:15:26,620 --> 01:15:28,719


501
01:15:28,719 --> 01:15:36,360


502
01:15:36,360 --> 01:15:42,310


503
01:15:42,310 --> 01:15:47,739


504
01:15:50,380 --> 01:15:58,000


505
01:15:58,000 --> 01:16:03,100


506
01:16:03,100 --> 01:16:10,120


507
01:16:14,380 --> 01:16:18,989


508
01:16:21,520 --> 01:16:27,430


509
01:16:27,430 --> 01:16:35,620


510
01:16:35,620 --> 01:16:43,840


511
01:16:43,840 --> 01:16:49,239


512
01:16:51,790 --> 01:16:56,830


513
01:16:56,830 --> 01:17:01,930


514
01:17:01,930 --> 01:17:04,469


515
01:17:04,639 --> 01:17:10,800


516
01:17:10,800 --> 01:17:16,260


517
01:17:16,260 --> 01:17:25,159


518
01:17:32,599 --> 01:17:38,099


519
01:17:38,099 --> 01:17:49,260


520
01:17:49,260 --> 01:17:54,869


521
01:17:57,780 --> 01:18:04,050


522
01:18:04,050 --> 01:18:11,760


523
01:18:11,760 --> 01:18:20,489


524
01:18:25,860 --> 01:18:31,530


525
01:18:31,530 --> 01:18:37,260


526
01:18:37,260 --> 01:18:44,340


527
01:18:44,340 --> 01:18:48,239


528
01:18:53,099 --> 01:18:56,269


529
01:18:59,620 --> 01:19:07,400


530
01:19:07,400 --> 01:19:14,000


531
01:19:14,000 --> 01:19:17,630


532
01:19:17,630 --> 01:19:20,080


533
01:19:21,200 --> 01:19:25,220


534
01:19:25,220 --> 01:19:29,960


535
01:19:29,960 --> 01:19:32,680


536
01:19:32,680 --> 01:19:45,950


537
01:19:48,020 --> 01:19:54,470


538
01:19:54,470 --> 01:20:00,110


539
01:20:01,820 --> 01:20:12,860


540
01:20:12,860 --> 01:20:30,220


541
01:20:30,220 --> 01:20:36,350


542
01:20:36,350 --> 01:20:41,960


543
01:20:41,960 --> 01:20:49,370


544
01:20:49,370 --> 01:20:55,910


545
01:20:55,910 --> 01:20:59,480


546
01:20:59,480 --> 01:21:06,500


547
01:21:06,500 --> 01:21:10,100


548
01:21:10,100 --> 01:21:16,220


549
01:21:16,220 --> 01:21:23,030


550
01:21:25,550 --> 01:21:30,860


551
01:21:33,470 --> 01:21:39,270


552
01:21:39,270 --> 01:21:44,880


553
01:21:44,880 --> 01:21:51,570


554
01:21:51,570 --> 01:21:57,810


555
01:21:57,810 --> 01:22:02,730


556
01:22:06,210 --> 01:22:09,690


557
01:22:09,690 --> 01:22:13,950


558
01:22:13,950 --> 01:22:17,640


559
01:22:17,640 --> 01:22:25,050


560
01:22:25,050 --> 01:22:29,640


561
01:22:29,640 --> 01:22:34,500


562
01:22:34,500 --> 01:22:39,300


563
01:22:39,300 --> 01:22:46,590


564
01:22:46,590 --> 01:22:50,670


565
01:22:50,670 --> 01:22:54,540


566
01:22:54,540 --> 01:22:56,790


567
01:22:56,790 --> 01:23:02,730


568
01:23:02,730 --> 01:23:09,540


569
01:23:09,540 --> 01:23:15,060


570
01:23:15,060 --> 01:23:21,540


571
01:23:21,540 --> 01:23:28,640


572
01:23:28,640 --> 01:23:33,960


573
01:23:33,960 --> 01:23:44,790


574
01:23:44,790 --> 01:23:48,830


575
01:23:48,830 --> 01:23:54,530


576
01:23:54,530 --> 01:23:59,960


577
01:23:59,960 --> 01:24:06,080


578
01:24:06,080 --> 01:24:10,820


579
01:24:10,820 --> 01:24:17,180


580
01:24:17,180 --> 01:24:21,310


581
01:24:21,310 --> 01:24:28,550


582
01:24:28,550 --> 01:24:33,890


583
01:24:33,890 --> 01:24:36,230


584
01:24:39,620 --> 01:24:45,290


585
01:24:45,290 --> 01:24:49,760


586
01:24:49,760 --> 01:24:52,730


587
01:24:57,860 --> 01:25:03,380


588
01:25:03,380 --> 01:25:06,770


589
01:25:06,770 --> 01:25:10,850


590
01:25:10,850 --> 01:25:15,650


591
01:25:19,280 --> 01:25:24,530


592
01:25:24,530 --> 01:25:28,820


593
01:25:28,820 --> 01:25:33,080


594
01:25:33,080 --> 01:25:36,910


595
01:25:36,910 --> 01:25:41,810


596
01:25:41,810 --> 01:25:46,550


597
01:25:46,550 --> 01:25:49,940


598
01:25:54,620 --> 01:26:01,050


599
01:26:01,050 --> 01:26:07,140


600
01:26:07,140 --> 01:26:14,910


601
01:26:16,920 --> 01:26:21,480


602
01:26:24,420 --> 01:26:28,590


603
01:26:28,590 --> 01:26:35,670


604
01:26:35,670 --> 01:26:41,550


605
01:26:41,550 --> 01:26:46,740


606
01:26:46,740 --> 01:26:53,040


607
01:26:53,040 --> 01:26:57,630


608
01:26:57,630 --> 01:27:04,530


609
01:27:04,530 --> 01:27:08,450


610
01:27:08,450 --> 01:27:13,230


611
01:27:13,230 --> 01:27:16,890


612
01:27:16,890 --> 01:27:22,290


613
01:27:22,290 --> 01:27:27,990


614
01:27:27,990 --> 01:27:34,050


615
01:27:34,050 --> 01:27:38,820


616
01:27:41,700 --> 01:27:46,950


617
01:27:46,950 --> 01:27:51,330


618
01:27:51,330 --> 01:27:58,860


619
01:27:58,860 --> 01:28:01,290


620
01:28:01,290 --> 01:28:04,620


621
01:28:04,620 --> 01:28:07,560


622
01:28:07,560 --> 01:28:12,990


623
01:28:15,630 --> 01:28:20,190


624
01:28:22,710 --> 01:28:26,940


625
01:28:26,940 --> 01:28:32,910


626
01:28:32,910 --> 01:28:36,630


627
01:28:36,630 --> 01:28:42,180


628
01:28:42,180 --> 01:28:47,130


629
01:28:47,130 --> 01:28:52,740


630
01:28:52,740 --> 01:28:57,330


631
01:28:57,330 --> 01:29:02,940


632
01:29:02,940 --> 01:29:09,960


633
01:29:09,960 --> 01:29:15,660


634
01:29:15,660 --> 01:29:20,790


635
01:29:20,790 --> 01:29:24,530


636
01:29:24,530 --> 01:29:28,800


637
01:29:28,800 --> 01:29:33,780


638
01:29:33,780 --> 01:29:38,850


639
01:29:38,850 --> 01:29:47,360


640
01:29:49,560 --> 01:29:55,290


641
01:29:55,290 --> 01:29:58,740


642
01:29:58,740 --> 01:30:04,140


643
01:30:04,140 --> 01:30:08,820


644
01:30:09,840 --> 01:30:14,380


645
01:30:14,380 --> 01:30:19,810


646
01:30:19,810 --> 01:30:24,790


647
01:30:24,790 --> 01:30:31,630


648
01:30:31,630 --> 01:30:36,190


649
01:30:39,790 --> 01:30:44,409


650
01:30:44,409 --> 01:30:48,849


651
01:30:48,849 --> 01:30:54,130


652
01:30:54,130 --> 01:30:59,829


653
01:30:59,829 --> 01:31:02,440


654
01:31:02,440 --> 01:31:06,250


655
01:31:06,250 --> 01:31:14,219


656
01:31:14,219 --> 01:31:18,940


657
01:31:18,940 --> 01:31:23,980


658
01:31:23,980 --> 01:31:28,000


659
01:31:28,000 --> 01:31:32,710


660
01:31:32,710 --> 01:31:40,000


661
01:31:40,000 --> 01:31:47,110


662
01:31:47,110 --> 01:31:54,400


663
01:31:54,400 --> 01:32:01,360


664
01:32:01,360 --> 01:32:06,610


665
01:32:06,610 --> 01:32:15,880


666
01:32:15,880 --> 01:32:19,540


667
01:32:19,540 --> 01:32:24,920


668
01:32:24,920 --> 01:32:30,140


669
01:32:30,140 --> 01:32:35,420


670
01:32:35,420 --> 01:32:40,400


671
01:32:40,400 --> 01:32:44,060


672
01:32:44,060 --> 01:32:48,500


673
01:32:48,500 --> 01:32:53,540


674
01:32:53,540 --> 01:32:59,810


675
01:32:59,810 --> 01:33:03,860


676
01:33:05,030 --> 01:33:12,740


677
01:33:12,740 --> 01:33:21,200


678
01:33:21,200 --> 01:33:28,340


679
01:33:28,340 --> 01:33:33,020


680
01:33:33,020 --> 01:33:38,810


681
01:33:38,810 --> 01:33:42,710


682
01:33:42,710 --> 01:33:50,870


683
01:33:50,870 --> 01:33:56,150


684
01:33:56,150 --> 01:33:59,990


685
01:33:59,990 --> 01:34:04,130


686
01:34:04,130 --> 01:34:07,070


687
01:34:07,070 --> 01:34:13,310


688
01:34:13,310 --> 01:34:16,820


689
01:34:19,220 --> 01:34:25,340


690
01:34:25,340 --> 01:34:30,440


691
01:34:33,020 --> 01:34:37,789


692
01:34:39,679 --> 01:34:45,260


693
01:34:45,260 --> 01:34:49,130


694
01:34:49,130 --> 01:34:53,449


695
01:34:55,639 --> 01:34:59,659


696
01:35:01,849 --> 01:35:06,110


697
01:35:08,059 --> 01:35:13,789


698
01:35:13,789 --> 01:35:19,269


699
01:35:19,269 --> 01:35:24,909


700
01:35:27,469 --> 01:35:31,820


701
01:35:31,820 --> 01:35:37,880


702
01:35:37,880 --> 01:35:44,030


703
01:35:44,030 --> 01:35:49,099


704
01:35:49,099 --> 01:35:54,500


705
01:35:54,500 --> 01:36:00,219


706
01:36:00,219 --> 01:36:06,110


707
01:36:06,110 --> 01:36:12,320


708
01:36:12,320 --> 01:36:16,489


709
01:36:16,489 --> 01:36:25,730


710
01:36:25,730 --> 01:36:30,980


711
01:36:30,980 --> 01:36:39,199


712
01:36:39,199 --> 01:36:44,030


713
01:36:44,030 --> 01:36:51,230


714
01:36:51,230 --> 01:36:57,110


715
01:36:57,110 --> 01:37:03,739


716
01:37:03,739 --> 01:37:09,800


717
01:37:09,800 --> 01:37:16,280


718
01:37:16,280 --> 01:37:20,290


719
01:37:20,290 --> 01:37:26,390


720
01:37:26,390 --> 01:37:32,090


721
01:37:32,090 --> 01:37:35,090


722
01:37:35,090 --> 01:37:39,200


723
01:37:39,200 --> 01:37:44,420


724
01:37:44,420 --> 01:37:50,900


725
01:37:50,900 --> 01:37:57,500


726
01:37:57,500 --> 01:38:01,700


727
01:38:01,700 --> 01:38:06,560


728
01:38:06,560 --> 01:38:10,880


729
01:38:10,880 --> 01:38:14,720


730
01:38:14,720 --> 01:38:22,100


731
01:38:22,100 --> 01:38:28,970


732
01:38:28,970 --> 01:38:36,140


733
01:38:36,140 --> 01:38:41,660


734
01:38:43,580 --> 01:38:50,840


735
01:38:50,840 --> 01:38:54,290


736
01:38:54,290 --> 01:38:58,010


737
01:38:58,010 --> 01:39:02,410


738
01:39:02,530 --> 01:39:06,880


739
01:39:06,880 --> 01:39:12,010


740
01:39:12,010 --> 01:39:24,100


741
01:39:24,100 --> 01:39:29,920


742
01:39:29,920 --> 01:39:40,870


743
01:39:40,870 --> 01:39:45,670


744
01:39:45,670 --> 01:39:49,060


745
01:39:49,060 --> 01:39:52,870


746
01:39:55,600 --> 01:39:59,710


747
01:39:59,710 --> 01:40:05,440


748
01:40:05,440 --> 01:40:10,630


749
01:40:10,630 --> 01:40:19,240


750
01:40:19,240 --> 01:40:24,430


751
01:40:24,430 --> 01:40:30,720


752
01:40:30,720 --> 01:40:35,410


753
01:40:35,410 --> 01:40:43,380


754
01:40:43,380 --> 01:40:47,590


755
01:40:47,590 --> 01:40:53,470


756
01:40:53,470 --> 01:40:58,600


757
01:40:58,600 --> 01:41:05,440


758
01:41:07,210 --> 01:41:12,250


759
01:41:12,250 --> 01:41:15,610


760
01:41:15,610 --> 01:41:21,490


761
01:41:21,490 --> 01:41:26,560


762
01:41:26,560 --> 01:41:30,930


763
01:41:30,930 --> 01:41:37,300


764
01:41:37,300 --> 01:41:40,750


765
01:41:40,750 --> 01:41:44,680


766
01:41:44,680 --> 01:41:49,720


767
01:41:49,720 --> 01:41:54,070


768
01:41:57,850 --> 01:42:05,380


769
01:42:05,380 --> 01:42:08,980


770
01:42:10,900 --> 01:42:17,050


771
01:42:17,050 --> 01:42:20,770


772
01:42:20,770 --> 01:42:24,820


773
01:42:26,350 --> 01:42:32,230


774
01:42:34,540 --> 01:42:39,910


775
01:42:39,910 --> 01:42:45,120


776
01:42:45,120 --> 01:42:50,590


777
01:42:50,590 --> 01:42:57,130


778
01:42:57,130 --> 01:43:03,460


779
01:43:03,460 --> 01:43:08,350


780
01:43:08,350 --> 01:43:14,190


781
01:43:17,680 --> 01:43:21,929


782
01:43:21,929 --> 01:43:26,250


783
01:43:26,250 --> 01:43:33,659


784
01:43:33,659 --> 01:43:37,409


785
01:43:37,409 --> 01:43:44,190


786
01:43:44,190 --> 01:43:48,570


787
01:43:48,570 --> 01:43:55,199


788
01:43:55,199 --> 01:44:00,480


789
01:44:00,480 --> 01:44:03,840


790
01:44:03,840 --> 01:44:09,540


791
01:44:09,540 --> 01:44:15,210


792
01:44:15,210 --> 01:44:19,710


793
01:44:19,710 --> 01:44:26,940


794
01:44:26,940 --> 01:44:32,330


795
01:44:32,330 --> 01:44:39,540


796
01:44:39,540 --> 01:44:47,429


797
01:44:47,429 --> 01:44:55,110


798
01:44:55,110 --> 01:45:00,150


799
01:45:00,150 --> 01:45:05,280


800
01:45:05,280 --> 01:45:14,100


801
01:45:14,100 --> 01:45:20,790


802
01:45:20,790 --> 01:45:27,960


803
01:45:27,960 --> 01:45:35,740


804
01:45:35,740 --> 01:45:44,260


805
01:45:44,260 --> 01:45:49,750


806
01:45:49,750 --> 01:45:56,410


807
01:45:56,410 --> 01:46:01,630


808
01:46:01,630 --> 01:46:06,610


809
01:46:06,610 --> 01:46:10,330


810
01:46:10,330 --> 01:46:18,550


811
01:46:18,550 --> 01:46:25,300


812
01:46:25,300 --> 01:46:29,380


813
01:46:29,380 --> 01:46:32,470


814
01:46:32,470 --> 01:46:37,000


815
01:46:37,000 --> 01:46:43,110


816
01:46:43,110 --> 01:46:52,150


817
01:46:52,150 --> 01:46:57,040


818
01:46:57,040 --> 01:47:02,740


819
01:47:02,740 --> 01:47:09,100


820
01:47:09,100 --> 01:47:13,180


821
01:47:13,180 --> 01:47:16,570


822
01:47:18,190 --> 01:47:24,790


823
01:47:24,790 --> 01:47:28,780


824
01:47:28,780 --> 01:47:35,470


825
01:47:35,470 --> 01:47:40,900


826
01:47:44,830 --> 01:47:49,590


827
01:47:52,650 --> 01:48:00,610


828
01:48:00,610 --> 01:48:07,030


829
01:48:07,030 --> 01:48:10,510


830
01:48:10,510 --> 01:48:15,490


831
01:48:15,490 --> 01:48:21,760


832
01:48:21,760 --> 01:48:25,330


833
01:48:25,330 --> 01:48:31,750


834
01:48:31,750 --> 01:48:39,639


835
01:48:39,639 --> 01:48:47,949


836
01:48:50,739 --> 01:49:03,570


837
01:49:04,040 --> 01:49:10,960


838
01:49:10,960 --> 01:49:22,960


839
01:49:25,340 --> 01:49:27,400


840
01:49:28,550 --> 01:49:34,409


841
01:49:36,000 --> 01:49:44,460


842
01:49:44,460 --> 01:49:51,929


843
01:49:51,929 --> 01:49:59,790


844
01:49:59,790 --> 01:50:06,810


845
01:50:06,810 --> 01:50:14,040


846
01:50:14,040 --> 01:50:20,010


847
01:50:20,010 --> 01:50:26,159


848
01:50:29,909 --> 01:50:37,619


849
01:50:37,619 --> 01:50:44,130


850
01:50:44,130 --> 01:50:48,300


851
01:50:48,300 --> 01:50:54,150


852
01:50:55,710 --> 01:51:03,560


853
01:51:03,560 --> 01:51:13,830


854
01:51:16,380 --> 01:51:22,190


855
01:51:25,800 --> 01:51:33,330


856
01:51:33,330 --> 01:51:38,520


857
01:51:38,520 --> 01:51:42,270


858
01:51:42,270 --> 01:51:46,680


859
01:51:46,680 --> 01:51:52,370


860
01:51:54,510 --> 01:52:01,440


861
01:52:05,190 --> 01:52:12,210


862
01:52:12,210 --> 01:52:17,670


863
01:52:17,670 --> 01:52:23,489


864
01:52:27,450 --> 01:52:41,400


865
01:52:41,400 --> 01:52:48,360


866
01:52:48,360 --> 01:52:52,380


867
01:52:52,380 --> 01:52:58,020


868
01:52:58,020 --> 01:53:05,250


869
01:53:05,250 --> 01:53:11,580


870
01:53:11,580 --> 01:53:17,100


871
01:53:17,100 --> 01:53:26,700


872
01:53:29,489 --> 01:53:34,200


873
01:53:34,200 --> 01:53:40,050


874
01:53:40,860 --> 01:53:49,750


875
01:53:49,750 --> 01:53:56,770


876
01:53:56,770 --> 01:54:03,100


877
01:54:03,100 --> 01:54:07,180


878
01:54:07,180 --> 01:54:10,870


879
01:54:10,870 --> 01:54:15,460


880
01:54:15,460 --> 01:54:20,020


881
01:54:20,020 --> 01:54:27,550


882
01:54:27,550 --> 01:54:33,160


883
01:54:33,160 --> 01:54:38,260


884
01:54:38,260 --> 01:54:42,160


885
01:54:42,160 --> 01:54:45,690


886
01:54:47,159 --> 01:54:55,480


887
01:54:55,480 --> 01:55:01,599


888
01:55:01,599 --> 01:55:05,920


889
01:55:05,920 --> 01:55:11,710


890
01:55:11,710 --> 01:55:14,949


891
01:55:14,949 --> 01:55:18,760


892
01:55:18,760 --> 01:55:23,380


893
01:55:23,380 --> 01:55:30,159


894
01:55:31,869 --> 01:55:35,949


895
01:55:35,949 --> 01:55:39,670


896
01:55:39,670 --> 01:55:46,110


897
01:55:46,110 --> 01:55:55,750


898
01:55:55,750 --> 01:56:01,630


899
01:56:01,630 --> 01:56:09,670


900
01:56:09,670 --> 01:56:15,820


901
01:56:15,820 --> 01:56:20,860


902
01:56:20,860 --> 01:56:26,560


903
01:56:26,560 --> 01:56:32,110


904
01:56:34,000 --> 01:56:39,070


905
01:56:39,070 --> 01:56:44,739


906
01:56:44,739 --> 01:56:49,599


907
01:56:49,599 --> 01:56:58,389


908
01:56:58,389 --> 01:57:02,650


909
01:57:05,170 --> 01:57:12,330


910
01:57:12,330 --> 01:57:17,800


911
01:57:17,800 --> 01:57:28,960


912
01:57:28,960 --> 01:57:36,340


913
01:57:36,340 --> 01:57:41,199


914
01:57:44,440 --> 01:57:50,469


915
01:57:50,469 --> 01:57:56,199


916
01:57:56,199 --> 01:58:00,850


917
01:58:00,850 --> 01:58:07,540


918
01:58:07,540 --> 01:58:13,900


919
01:58:13,900 --> 01:58:19,090


920
01:58:19,090 --> 01:58:25,360


921
01:58:28,210 --> 01:58:32,440


922
01:58:32,440 --> 01:58:37,960


923
01:58:37,960 --> 01:58:45,400


924
01:58:45,400 --> 01:58:49,600


925
01:58:49,600 --> 01:58:56,710


926
01:58:56,710 --> 01:59:01,030


927
01:59:01,030 --> 01:59:04,900


928
01:59:04,900 --> 01:59:11,220


929
01:59:11,220 --> 01:59:16,170


930
01:59:16,170 --> 01:59:19,470


931
01:59:19,470 --> 01:59:24,240


932
01:59:27,560 --> 01:59:34,020


933
01:59:34,020 --> 01:59:38,280


934
01:59:38,960 --> 01:59:43,260


935
01:59:43,260 --> 01:59:47,640


936
01:59:47,640 --> 01:59:55,260


937
01:59:55,260 --> 02:00:00,690


938
02:00:03,690 --> 02:00:09,810


939
02:00:09,810 --> 02:00:15,720


940
02:00:15,720 --> 02:00:18,780


941
02:00:20,760 --> 02:00:24,420


942
02:00:24,420 --> 02:00:28,350


943
02:00:28,350 --> 02:00:33,690


944
02:00:36,110 --> 02:00:44,190


945
02:00:44,190 --> 02:00:47,400


946
02:00:47,400 --> 02:00:53,820


947
02:00:53,820 --> 02:00:57,780


948
02:00:57,780 --> 02:01:02,130


949
02:01:02,130 --> 02:01:09,000


950
02:01:09,000 --> 02:01:12,870


951
02:01:12,870 --> 02:01:16,700


952
02:01:17,060 --> 02:01:21,690


953
02:01:23,760 --> 02:01:29,880


954
02:01:29,880 --> 02:01:36,870


955
02:01:36,870 --> 02:01:43,850


956
02:01:43,850 --> 02:01:49,110


957
02:01:51,570 --> 02:01:53,810


958
02:01:54,390 --> 02:01:57,690


959
02:02:01,320 --> 02:02:05,220


960
02:02:05,220 --> 02:02:12,540


961
02:02:12,540 --> 02:02:15,810


962
02:02:15,810 --> 02:02:23,190


963
02:02:30,960 --> 02:02:38,820


964
02:02:38,820 --> 02:02:44,730


965
02:02:44,730 --> 02:02:50,310


966
02:02:50,310 --> 02:02:57,830


967
02:03:00,960 --> 02:03:04,830


968
02:03:04,830 --> 02:03:09,000


969
02:03:09,000 --> 02:03:15,330


970
02:03:15,330 --> 02:03:24,480


971
02:03:24,480 --> 02:03:30,150


972
02:03:30,150 --> 02:03:33,720


973
02:03:33,720 --> 02:03:38,760


974
02:03:38,760 --> 02:03:43,350


975
02:03:43,350 --> 02:03:48,990


976
02:03:51,120 --> 02:03:57,000


977
02:03:57,000 --> 02:04:01,010


978
02:04:01,010 --> 02:04:05,640


979
02:04:05,640 --> 02:04:10,800


980
02:04:10,800 --> 02:04:20,130


981
02:04:20,130 --> 02:04:25,140


982
02:04:25,140 --> 02:04:29,820


983
02:04:31,710 --> 02:04:37,020


984
02:04:37,020 --> 02:04:43,230


985
02:04:45,540 --> 02:04:49,830


986
02:04:49,830 --> 02:04:55,830


987
02:04:55,830 --> 02:04:59,430


988
02:04:59,430 --> 02:05:03,090


989
02:05:03,090 --> 02:05:07,610


990
02:05:13,470 --> 02:05:18,600


991
02:05:18,600 --> 02:05:22,320


992
02:05:25,050 --> 02:05:29,760


993
02:05:29,760 --> 02:05:37,830


994
02:05:37,830 --> 02:05:42,300


995
02:05:42,300 --> 02:05:46,260


996
02:05:46,260 --> 02:05:55,140


997
02:05:55,140 --> 02:05:59,790


998
02:05:59,790 --> 02:06:04,010


999
02:06:04,010 --> 02:06:12,240


1000
02:06:12,240 --> 02:06:16,830


1001
02:06:16,830 --> 02:06:20,910


1002
02:06:20,910 --> 02:06:25,830


1003
02:06:25,830 --> 02:06:32,460


1004
02:06:32,460 --> 02:06:36,600


1005
02:06:36,600 --> 02:06:41,760


1006
02:06:41,760 --> 02:06:48,989


1007
02:06:50,730 --> 02:06:55,830


1008
02:06:55,830 --> 02:07:00,390


1009
02:07:00,390 --> 02:07:06,540


1010
02:07:06,540 --> 02:07:12,270


1011
02:07:12,270 --> 02:07:17,100


1012
02:07:17,100 --> 02:07:23,850


1013
02:07:23,850 --> 02:07:29,430


1014
02:07:29,430 --> 02:07:34,260


1015
02:07:34,260 --> 02:07:39,120


1016
02:07:39,120 --> 02:07:44,310


1017
02:07:44,310 --> 02:07:49,020


1018
02:07:49,020 --> 02:07:55,170


1019
02:07:55,170 --> 02:07:59,910


1020
02:07:59,910 --> 02:08:10,590


1021
02:08:10,590 --> 02:08:15,600


1022
02:08:15,600 --> 02:08:22,810


1023
02:08:22,810 --> 02:08:25,690


1024
02:08:28,720 --> 02:08:36,310


1025
02:08:36,310 --> 02:08:40,750


1026
02:08:40,750 --> 02:08:47,980


1027
02:08:47,980 --> 02:08:55,120


1028
02:08:55,120 --> 02:09:01,150


1029
02:09:01,150 --> 02:09:05,620


1030
02:09:05,620 --> 02:09:12,070


1031
02:09:12,070 --> 02:09:17,370


1032
02:09:17,370 --> 02:09:21,340


1033
02:09:21,340 --> 02:09:24,900


1034
02:09:27,370 --> 02:09:32,200


1035
02:09:32,200 --> 02:09:39,400


1036
02:09:39,400 --> 02:09:42,790


1037
02:09:42,790 --> 02:09:50,320


1038
02:09:50,320 --> 02:09:55,030


1039
02:09:55,030 --> 02:09:59,530


1040
02:09:59,530 --> 02:10:03,310


1041
02:10:03,310 --> 02:10:08,680


1042
02:10:08,680 --> 02:10:13,030


1043
02:10:15,760 --> 02:10:21,670


1044
02:10:21,670 --> 02:10:26,830


1045
02:10:26,830 --> 02:10:30,340


1046
02:10:30,340 --> 02:10:36,460


1047
02:10:36,460 --> 02:10:41,500


1048
02:10:41,500 --> 02:10:47,170


1049
02:10:47,170 --> 02:10:51,640


1050
02:10:51,640 --> 02:10:56,560


1051
02:10:56,560 --> 02:11:02,920


1052
02:11:02,920 --> 02:11:07,989


1053
02:11:10,620 --> 02:11:15,250


1054
02:11:15,250 --> 02:11:20,530


1055
02:11:20,530 --> 02:11:25,570


1056
02:11:25,570 --> 02:11:32,560


1057
02:11:32,560 --> 02:11:36,730


1058
02:11:43,030 --> 02:11:48,670


1059
02:11:48,670 --> 02:11:52,060


1060
02:11:52,060 --> 02:11:55,900


1061
02:11:59,500 --> 02:12:04,030


1062
02:12:04,030 --> 02:12:09,100


1063
02:12:09,100 --> 02:12:13,150


1064
02:12:15,969 --> 02:12:19,690


1065
02:12:19,690 --> 02:12:25,840


1066
02:12:25,840 --> 02:12:28,930


1067
02:12:28,930 --> 02:12:32,080


1068
02:12:32,080 --> 02:12:36,400


1069
02:12:36,400 --> 02:12:39,730


1070
02:12:39,730 --> 02:12:46,780


1071
02:12:46,780 --> 02:12:50,260


1072
02:12:50,260 --> 02:12:56,619


1073
02:12:58,599 --> 02:13:05,289


1074
02:13:05,289 --> 02:13:08,170


1075
02:13:08,170 --> 02:13:12,519


1076
02:13:12,519 --> 02:13:17,559


1077
02:13:17,559 --> 02:13:22,599


1078
02:13:22,599 --> 02:13:26,650


1079
02:13:28,449 --> 02:13:31,809


1080
02:13:31,809 --> 02:13:36,480


1081
02:13:39,010 --> 02:13:42,760


1082
02:13:42,760 --> 02:13:46,239


1083
02:13:47,829 --> 02:13:51,699


1084
02:13:51,699 --> 02:13:57,670


1085
02:13:57,670 --> 02:14:03,940


1086
02:14:06,150 --> 02:14:12,579


1087
02:14:12,579 --> 02:14:16,000


1088
02:14:16,000 --> 02:14:19,329


1089
02:14:19,329 --> 02:14:23,199


1090
02:14:23,199 --> 02:14:27,820


1091
02:14:27,820 --> 02:14:32,819


