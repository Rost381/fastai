1
00:00:00,060 --> 00:00:04,944
Добро пожаловать на четвёртую лекцию.

2
00:00:05,044 --> 00:00:23,330
Эта неделя на форуме была активной. Вы написали несколько полезных статей, как и на прошлой неделе, давайте на них посмотрим.

3
00:00:24,480 --> 00:00:35,910
Vitaly Bushaev написал один из лучших постов за последнее время про скорость обучения и SGDR.

4
00:00:39,960 --> 00:00:49,569
Он проделал отличную работу, объяснив базовые идеи понятным всем языком,

5
00:00:49,669 --> 00:00:55,079
но также добавил ссылки на научные статьи для тех, кто хочет узнать больше,

6
00:00:55,079 --> 00:01:08,755
в посте много примеров и детальных объяснений, я считаю, что такие посты очень полезны в нашей сфере.

7
00:01:08,855 --> 00:01:32,939
Мне очень нравится, что материал перед публикацией обсуждается и улучшается на форуме.

8
00:01:32,939 --> 00:01:49,590
Циклическая скорость обучения — популярная тема, Anand Saha тоже написал про это пост,

9
00:01:49,590 --> 00:02:01,670
опять же — отличная работа, много картинок, ссылок на научные статьи и кода, объясняющего, как всё работает.

10
00:02:01,950 --> 00:02:07,600
Mark Hoffman написал хорошее введение в SGDR.

11
00:02:09,580 --> 00:02:16,450
Manikanta Yadunanda написал про дифференциальные скорости обучения

12
00:02:19,390 --> 00:02:31,019
и их применение в переносе обучения, добавив введение в перенос обучения.

13
00:02:31,019 --> 00:02:59,829
Arjun Rajkumar написал пост о компьютерном зрении, мне нравится, что он упомянул реальные применения этой технологии.

14
00:02:59,829 --> 00:03:06,904
Есть другие отличные посты, спасибо всем вам за такую активность.

15
00:03:07,004 --> 00:03:28,350
Повторюсь — если вы хотите написать что-то своё, но боитесь — не стоит, на форуме люди дружелюбны и готовы вам помочь.

16
00:03:30,690 --> 00:03:37,930
Сегодня будет интересная лекция, много информации на совершенно разные темы.

17
00:03:38,030 --> 00:03:51,090
Мы потратили довольно много времени на компьютерное зрение, а сегодня обсудим сразу три области.

18
00:03:51,090 --> 00:04:09,180
Начнём с анализа структурированных данных, то есть данных, хранящихся в таблицах, как базы данных.

19
00:04:09,180 --> 00:04:21,954
Затем посмотрим на обработку естественного языка и на рекомендательные системы.

20
00:04:22,054 --> 00:04:31,080
Мы рассмотрим эти темы на достаточно высоком уровне, но акцент будет на практической части, а не теоретической.

21
00:04:33,510 --> 00:04:46,675
Теоретическая часть будет в следующих трёх лекциях, там же мы ещё раз обсудим компьютерное зрение.

22
00:04:46,775 --> 00:04:58,000
Сегодня мы сфокусируемся на практическом применении этих идей и введём несколько необходимых новых понятий.

23
00:04:58,100 --> 00:05:14,980
Одно из таких понятий — дропаут, вы наверняка уже слышали про него, это полезная техника.

24
00:05:15,080 --> 00:05:21,145
Я продемонстрирую её на примере соревнования Kaggle про породы собак.

25
00:05:21,245 --> 00:05:40,770
Я создал модель и поставил precompute=True, чтобы получить предвычисленные активации на последнем свёрточном слое.

26
00:05:40,770 --> 00:05:57,960
Напоминаю, что активация — это число, вычисленное с помощью весов, или параметров, в свёрточных фильтрах,

27
00:05:57,960 --> 00:06:09,460
которые применяются к предыдущему слою, содержащему активации либо входные данные.

28
00:06:09,560 --> 00:06:14,210
Помните, что активация — это число. Итак, мы используем предвычисленные активации,

29
00:06:17,550 --> 00:06:27,480
а затем добавляем несколько полносвязных слоёв, заполненных случайными числами,

30
00:06:27,480 --> 00:06:43,009
на которые будут умножаться активации, как в нашей демонстрации в Excel.

31
00:06:43,009 --> 00:06:58,870
Если вы выведете содержимое объекта learn, увидите, какие слои мы добавили в модель.

32
00:06:58,870 --> 00:07:02,900
Нормализацию минибатчей мы обсудим на последней лекции.

33
00:07:02,900 --> 00:07:18,505
Линейный слой — это просто матрица 1024x512, то есть слой принимает 1024 активации и возвращает 512.

34
00:07:18,605 --> 00:07:26,750
Это выпрямитель, он заменяет отрицательные числа на нули.

35
00:07:26,750 --> 00:07:38,594
Второй линейный слой берёт 512 активаций с предыдущего слоя, умножает их на матрицу 512x120 и получает 120 новых активаций.

36
00:07:38,694 --> 00:07:48,829
Результат передаётся в функцию Softmax, которую мы обсуждали на прошлой неделе:

37
00:07:48,929 --> 00:08:02,180
она берёт экспоненту от числа и делит её на сумму экспонент от всех чисел, получает вероятность.

38
00:08:02,180 --> 00:08:09,850
Все вероятности в сумме дают 1 и каждая вероятность лежит между 0 и 1.

39
00:08:09,950 --> 00:08:16,595
Итак, это слои, добавленные поверх предвычисленных свёрточных слоёв, они обучаются при precompute=True.

40
00:08:16,695 --> 00:08:24,404
Давайте обсудим, что такое дропаут и что значит параметр p.

41
00:08:24,504 --> 00:08:53,085
Дропаут с параметром p=0.5 значит, что случайная половина всех активаций слоя уничтожается.

42
00:08:53,185 --> 00:09:03,970
Параметр p — вероятность удаления активации, p=0.5 значит удаление 50% активаций слоя.

43
00:09:03,970 --> 00:09:14,924
При удалении половины активаций следующий слой почти не меняется,

44
00:09:15,024 --> 00:09:25,100
так как этот слой — подвыборка максимумом, что-то меняется, только если удалённая ячейка была максимальной из четырёх.

45
00:09:25,100 --> 00:09:31,360
Если следующий слой — свёрточный, тоже меняется немного, потому что свёрточные фильтры рассматривают области 3x3.

46
00:09:31,460 --> 00:09:53,060
Результат интересный. Важно понимать, что после каждого минибатча выбрасывается случайная половина активаций.

47
00:09:53,060 --> 00:10:08,279
Это помогает избежать переобучения. Если какая-то группа активаций очень хорошо предсказывает определённую кошку или собаку,

48
00:10:08,379 --> 00:10:18,405
то после исчезновения этих активаций модель больше не сможет сказать, что находится на этом изображении,

49
00:10:18,505 --> 00:10:31,485
поэтому ей придётся находить признаки, которые будут работать, даже если половина активаций каждый раз выбрасывается.

50
00:10:31,585 --> 00:10:50,310
Этой технике три или четыре года, и она почти полностью решает проблему переобучения.

51
00:10:50,410 --> 00:11:12,079
До того, как появился дропаут, люди не знали, что делать с переобучением, когда дополнение данных и новые данные перестают помогать.

52
00:11:12,079 --> 00:11:35,870
Джеффри Хинтон и его коллеги изобрели дропаут, вдохновляясь работой человеческого мозга.

53
00:11:35,870 --> 00:11:57,539
Если вероятность p=0.01, это значит, что 1% случайных активаций выбрасывается. Это почти ничего не изменит и не спасёт от переобучения.

54
00:11:57,639 --> 00:12:17,805
Если p=0.99, выбросится 99% активаций, переобучения точно не будет, но доля правильных ответов на обучающей выборке будет низкой.

55
00:12:17,905 --> 00:12:32,509
Высокие p позволят хорошо обобщать, но снизят долю правильных ответов на обучающей выборке, низкие — наоборот.

56
00:12:32,509 --> 00:12:43,589
Дропаут объясняет, почему в начале обучения потери на обучающей выборке были больше потерь на валидационной.

57
00:12:43,689 --> 00:12:59,870
Это может показаться странным, но объяснение простое — на валидационной выборке не используется дропаут.

58
00:12:59,870 --> 00:13:10,529
При предсказаниях валидационной выборки хочется использовать наилучшую из возможных моделей, поэтому активации не выбрасывают.

59
00:13:10,629 --> 00:13:26,520
Поэтому иногда, особенно в начале обучения, доля правильных ответов и функция потерь на обучающей выборке хуже, чем на валидационной.

60
00:13:26,520 --> 00:13:30,460
Янет: Надо ли чем-то заменять выброшенные активации?

61
00:13:30,460 --> 00:13:41,375
При использовании дропаута PyTorch делает две вещи. Положим p=0.5.

62
00:13:41,475 --> 00:13:47,600
PyTorch под капотом, во-первых, выкидывает половину активаций, во-вторых, дублирует оставшиеся,

63
00:13:47,700 --> 00:13:56,885
поэтому в среднем активации не меняются, это удобно.

64
00:13:56,985 --> 00:14:06,520
Вам не нужно об этом думать, это делается автоматически.

65
00:14:06,520 --> 00:14:18,370
Вероятность дропаута для всех новых слоёв передаётся параметром ps метода ConvLearner.pretrained().

66
00:14:18,370 --> 00:14:27,670
Дропаут в предобученных слоях не изменится, потому что эти слои уже были обучены с какой-то вероятностью дропаута,

67
00:14:29,740 --> 00:14:33,220
настраивается дропаут только в новых слоях.

68
00:14:33,220 --> 00:14:40,839
Здесь ps=0.5, поэтому в объекте learn первый и второй дропауты оба имеют вероятность p=0.5.

69
00:14:40,839 --> 00:14:46,390
Напомню, что на вход этих слоёв подаются результаты последнего свёрточного слоя предобученной сети,

70
00:14:48,820 --> 00:14:55,804
половина активаций выбрасывается, оставшееся подаётся в линейный слой, пропускается через выпрямитель,

71
00:14:55,904 --> 00:15:05,230
снова половина активаций выбрасывается, оставшееся подаётся в линейный слой, а потом подаётся в функцию Softmax.

72
00:15:05,230 --> 00:15:11,950
Для небольшого увеличения точности расчётов имеет смысл брать логарифм Softmax, а не просто Softmax,

73
00:15:12,520 --> 00:15:22,480
потому что потом от этого берётся экспонента, но это неважно.

74
00:15:22,480 --> 00:15:36,380
Если убрать дропаут, поставив ps=0, доля правильных ответов в первую эпоху увеличится с 0.76 до 0.8,

75
00:15:36,380 --> 00:15:45,649
что неудивительно, потому что мы перестали выбрасывать важные результаты в начале обучения.

76
00:15:45,649 --> 00:15:52,350
Но на третьей эпохе результаты другие — доля правильных ответов была 84.8%, а стала 84.1%.

77
00:15:52,450 --> 00:16:02,489
Модель сильно переобучилась уже после трёх эпох — потери равны 0.35 на обучающей выборке и 0.55 на валидационной.

78
00:16:02,589 --> 00:16:15,975
Как видите, в модели нет дропаута, если ps=0, эти слои даже не добавляются.

79
00:16:16,075 --> 00:16:29,450
В модели два дополнительных линейных слоя — это не обязательно.

80
00:16:29,450 --> 00:16:41,720
В параметр xtra_fc (extra fully connected layers) передаётся количество дополнительных полносвязных слоёв и их размеры.

81
00:16:41,720 --> 00:16:51,170
Должен быть хотя бы один полносвязный слой, который примет на вход результат работы последнего свёрточного слоя,

82
00:16:51,170 --> 00:16:56,660
здесь размер последнего слоя 1024, и вернёт вектор длиной в количество классов —

83
00:16:56,660 --> 00:17:05,024
2 класса в задаче классификации собак и кошек, 120 пород собак и 17 типов спутниковых снимков.

84
00:17:05,124 --> 00:17:14,445
Размеры единственного линейного слоя нельзя выбрать, они определяются задачей, но можно выбрать размеры дополнительных.

85
00:17:14,545 --> 00:17:25,880
Здесь мы задаём xtra_fc=[], в модели будет только один линейный слой. Вероятность дропаута ps=0.

86
00:17:25,880 --> 00:17:35,080
Это минимальный набор дополнительных слоёв.

87
00:17:36,800 --> 00:17:53,059
Даже так мы получаем неплохие результаты, потому что обучение было недолгим и предобученная модель подходит к датасету.

88
00:17:53,059 --> 00:18:01,640
Янет: Какое значение вероятности дропаута использовать по умолчанию?

89
00:18:01,640 --> 00:18:20,710
По умолчанию fastai выставляет p=0.25 для первого слоя дропаута и p=0.5 для второго, это обычно работает.

90
00:18:20,710 --> 00:18:38,080
Если ваша модель переобучается. поставьте обе вероятности ps=0.5, не помогло — ps=0.7 и так далее.

91
00:18:38,080 --> 00:18:47,320
Если модель недообучается, попробуйте уменьшить вероятность, скорее всего, сильно уменьшать не придётся.

92
00:18:47,420 --> 00:19:06,089
Вероятность дропаута редко приходится уменьшать, обычно приходится увеличивать, и значения 0.6-0.7 для меня оптимальны.

93
00:19:06,189 --> 00:19:21,689
Я увеличил вероятность дропаута ps=0.5 в Jupyter ноутбуке с породами собак, когда перешёл с ResNet34 на ResNet50 —

94
00:19:21,789 --> 00:19:32,984
У ResNet34 меньше параметров и она хуже переобучается, в отличие от ResNet50, которая начала переобучаться.

95
00:19:33,084 --> 00:19:41,350
С большими моделями обычно сложнее.

96
00:19:42,000 --> 00:19:49,169
Вопрос из зала: Параметр p=0.5 значит вероятность в 50%?
Да.

97
00:19:49,169 --> 00:20:01,174
Вопрос из зала: Как понять, что модель переобучается?

98
00:20:01,474 --> 00:20:11,889
Если потери на обучающей выборке меньше потерь на валидационной.

99
00:20:11,889 --> 00:20:23,914
Не стоит концентрироваться на переобучении, просто старайтесь уменьшить потери на валидационной выборке.

100
00:20:24,014 --> 00:20:38,919
Экспериментируйте и научитесь понимать, что работает лучше и когда модель сильно переобучена.

101
00:20:38,919 --> 00:20:51,376
Окей, так устроен дропаут, не забывайте, что он есть по умолчанию. Ещё вопрос?

102
00:20:51,476 --> 00:21:07,809
Вопрос из зала: Вероятность дропаута p=0.5 удаляет каждую активацию с вероятностью 50% или выбирает половину и удаляет её?

103
00:21:07,809 --> 00:21:12,639
Удаляет каждую активацию с вероятностью 50%.

104
00:21:12,639 --> 00:21:19,960
Вопрос из зала: Почему имеет значение среднее активаций?

105
00:21:19,960 --> 00:21:40,139
Вернёмся к демонстрации в Excel. Это число получено перемножением четырёх групп чисел по 9.

106
00:21:40,139 --> 00:21:52,624
Если выкинуть половину чисел на одном слое, числа на другом слое будут примерно вдвое меньше, и так на каждом слое после.

107
00:21:52,724 --> 00:22:03,970
Если раньше ухо считалось пушистым при значении выше 0.6, теперь это значение — 0.3, меняется значение активаций,

108
00:22:04,070 --> 00:22:09,825
а хочется выкидывать половину активаций так, чтобы ничего не менялось.

109
00:22:09,925 --> 00:22:19,400
Вопрос из зала:

110
00:22:19,400 --> 00:22:23,720


111
00:22:23,720 --> 00:22:28,790


112
00:22:28,790 --> 00:22:35,000


113
00:22:35,000 --> 00:22:42,260


114
00:22:42,260 --> 00:22:47,690
Вопрос из зала: Можно ли задавать разные вероятности дропаута для разных слоёв?

115
00:22:47,690 --> 00:23:00,797
Да, в параметр ps можно передать массив, например ps=[0, 0.2] и xtra_fc=[512]

116
00:23:00,897 --> 00:23:13,650
значит отсутствие дропаута перед первым линейным слоем и дропаут с вероятностью p=0.5 перед вторым.

117
00:23:13,750 --> 00:23:28,100
Даже спустя несколько лет я до сих пор не понял, как разумно выставлять дропаут для разных слоёв,

118
00:23:30,260 --> 00:23:35,690
если кто-то из вас придумает хорошие правила, расскажите.

119
00:23:35,790 --> 00:23:52,450
Если вы не знаете, попробуйте либо дропаут с одинаковой вероятностью для всех слоёв, либо дропаут только на последнем слое.

120
00:23:53,670 --> 00:24:04,565
Вопрос из зала: Почему вы отслеживаете значения функции потерь, а не долю правильных ответов?

121
00:24:04,665 --> 00:24:13,920
Потому что потери известны и для обучающей, и для валидационной выборки.

122
00:24:13,920 --> 00:24:31,310
Дальше мы узнаем, что оптимизируется именно функция потерь, поэтому её проще отслеживать и понимать.

123
00:24:31,410 --> 00:24:44,225
Вопрос из зала: Дропаут добавляет случайный шум на каждой итерации, как это влияет на скорость обучения?

124
00:24:44,325 --> 00:25:01,415
Теоретически может, но я на практике никогда не замечал.

125
00:25:01,515 --> 00:25:13,600
Давайте поговорим про анализ структурированных данных. Напомню, что мы анализировали данные соревнования Kaggle

126
00:25:13,600 --> 00:25:26,060
на данных Rossman, это немецкая сеть супермаркетов. Jupyter ноутбук называется lesson3-rossman.ipynb.

127
00:25:26,160 --> 00:25:36,880
Главный датасет содержал данные о продажах разных магазинов в разные дни,

128
00:25:36,880 --> 00:25:53,890
а также информацию о том, был ли открыт магазин, был ли в этот день праздник или акция и так далее.

129
00:25:53,890 --> 00:26:04,750
Была таблица о магазинах, содержащая тип магазина и ассортимента и информацию о ближайших конкурентах.

130
00:26:04,750 --> 00:26:11,590
Столбцы, или признаки, в таких таблицах можно поделить на категориальные и количественные.

131
00:26:11,590 --> 00:26:19,715
Категориальные признаки — это различные категории, например, ассортимент товара Assortment — 'a', 'b' или 'c'.

132
00:26:19,815 --> 00:26:27,910
Количественные признаки выражаются числом, например, расстояние до ближайшего конкурента CompetitionDistance —

133
00:26:27,910 --> 00:26:32,380
имеют значения различия между числами и их абсолютные величины.

134
00:26:32,380 --> 00:26:37,720
Эти два вида признаков обрабатываются по-разному.

135
00:26:37,720 --> 00:26:49,230
Тем, кто занимался машинным обучением, знакомы количественные признаки — на них легко построить линейную регрессию.

136
00:26:49,230 --> 00:26:54,850
С категориальными признаками чуть сложнее.

137
00:26:54,850 --> 00:27:06,944
Мы не будем смотреть на предобработку данных и выделение признаков, в итоге получается такая таблица и такие признаки.

138
00:27:07,044 --> 00:27:29,045
Напомню, что я не выделял признаки, это код обладателей третьего места в соревновании.

139
00:27:29,145 --> 00:27:46,660
Признаки разделены на категориальные и количественные. Год, месяц и день можно считать количественными признаками,

140
00:27:46,660 --> 00:27:52,179
потому что разница между, например, 2000 и 2003 имеет значение,

141
00:27:52,179 --> 00:28:14,570
но здесь они категориальные. Модель считает, что 2000, 2001 и 2002 года никак не связаны между собой.

142
00:28:14,670 --> 00:28:23,109
Если бы год был количественным признаком, модель строила бы гладкую функцию под все возможные значения.

143
00:28:23,209 --> 00:28:36,415
Поэтому количественные признаки с небольшим разбросом дискретных значений иногда удобнее считать категориальными.

144
00:28:36,515 --> 00:28:47,529
Ещё один пример — день недели. Можно пронумеровать дни от 0 до 6, потому что разница между 3 и 5, два дня, имеет смысл.

145
00:28:47,629 --> 00:28:56,609
Но если вдуматься, продажи могут различаться в зависимости от дня недели, а не его номера —

146
00:28:56,609 --> 00:29:05,700
в выходные один уровень продаж, по пятницам другой, в среду третий.

147
00:29:05,700 --> 00:29:12,909
Поэтому мы считаем день недели категориальным признаком.

148
00:29:13,009 --> 00:29:24,399
Вам придётся принимать решения о том, какой признак считать категориальным, а какой количественным.

149
00:29:24,499 --> 00:29:37,619
Если в столбце значения закодированы как 'B' или 'С', или как 'Джереми' и 'Янет', у вас нет выбора — это категориальный признак.

150
00:29:37,619 --> 00:29:51,415
Если признак закодирован числом, например, возраст или день недели, вам нужно решить, как это представить.

151
00:29:51,515 --> 00:29:59,199
Итак, если признак категориальный, он таким и останется, но количественный признак можно перевести в категориальный.

152
00:29:59,299 --> 00:30:15,594
Здесь я не принимал никаких решений — признаки были разделены на количественные и категориальные обладателями третьего места.

153
00:30:15,694 --> 00:30:29,545
Как видите, все количественные признаки содержат дробные числа, например, CompetitionDistance и температурные признаки.

154
00:30:29,645 --> 00:30:46,300
Эти признаки сложно сделать качественными, потому что почти все значения уникальны.

155
00:30:46,300 --> 00:30:54,520
Количество возможных значений признака обозначается как мощность.

156
00:30:54,520 --> 00:31:00,390
Мощность признака дня недели — семь, потому что различных дней недели семь.

157
00:31:02,010 --> 00:31:09,165
Вопрос из зала: Вы используете сегментацию признаков?

158
00:31:09,265 --> 00:31:13,033
Нет, не использую, хотя здесь

159
00:31:13,133 --> 00:31:23,140
максимальную температуру можно было бы разбить на категории 0-10, 10-20 и 20-30 и превратить в количественный признак.

160
00:31:23,140 --> 00:31:36,160
Неделю назад вышла статья, авторы которой утверждают, что сегментация признаков — это полезно, это первое известное мне практическое упоминание.

161
00:31:36,260 --> 00:31:44,507
Ещё неделю назад я сказал бы, что это плохая идея, но сейчас не уверен, возможно, это может быть полезно.

162
00:31:44,607 --> 00:32:00,880
Вопрос из зала: Если год — категориальный признак, что произойдёт, показать модели год, которого она ещё не видела?

163
00:32:00,880 --> 00:32:16,760
Мы это ещё обсудим. Короткий ответ — этот год будет считаться неизвестной категорией, pandas умеет с таким обращаться.

164
00:32:16,860 --> 00:32:24,840
По сути, это будет просто ещё одна категория «Неизвестная категория».

165
00:32:25,130 --> 00:32:35,980
Вопрос из зала: Если неизвестная категория будет в тестовой выборке, что предскажет модель?

166
00:32:36,080 --> 00:32:46,679
Ну, что-нибудь да предскажет. Если в обучающей выборке были неизвестные категории,

167
00:32:46,679 --> 00:32:55,164
она научится с таким обращаться, если нет — вставит случайный вектор.

168
00:32:55,264 --> 00:33:05,529
Это интересная тема, её стоит обсудить в этой части курса, и точно можно обсудить на форуме.

169
00:33:05,529 --> 00:33:21,764
Итак, мы разделили признаки на категориальные и количественные, в таблице 844338 записей.

170
00:33:21,864 --> 00:33:36,835
Мы преобразуем данные для признаков, которые решили сделать категориальными, методом astype('category').

171
00:33:36,935 --> 00:33:48,379
Я не буду рассказывать про pandas, про это есть куча хороших книг, например, Python for Data Analysis, я про неё уже говорил.

172
00:33:48,379 --> 00:33:53,564
Надеюсь, что и так понятно, что делает код.

173
00:33:53,664 --> 00:34:04,149
Итак, категориальные признаки преобразуются в тип category, а количественные — в 32-битное число с плавающей точкой,

174
00:34:04,249 --> 00:34:10,569
потому что с таким форматом чисел работает PyTorch.

175
00:34:10,669 --> 00:34:30,584
Некоторые количественные признаки, например, Promo, содержат значения 0 или 1, они тоже преобразуются в числа с плавающей точкой.

176
00:34:30,684 --> 00:34:44,339
Я стараюсь проделать большую часть обучения на маленьких датасетах — например, уменьшать изображения до размера 128 или 64.

177
00:34:44,439 --> 00:34:52,460
Структурированные данные нельзя уменьшить, вместо этого я случайно выбираю подмножество данных.

178
00:34:52,460 --> 00:35:07,609
Для этого я использую уже знакомую нам функцию get_cv_idxs() для получения индексов валидационной выборки.

179
00:35:07,609 --> 00:35:21,300
В итоге я получаю 150 тысяч случайных записей.

180
00:35:21,300 --> 00:35:32,910
Вот так они выглядят — есть булевые переменные, есть строки, есть целые числа.

181
00:35:37,020 --> 00:35:52,480
Несмотря на то, что я перевёл эти признаки в категориальные, они всё ещё показываются так, просто хранятся по-другому.

182
00:35:52,580 --> 00:36:05,070
В fastai есть функция proc_df() (processed dataframe), которая принимает датасет и название целевой переменной.

183
00:36:05,070 --> 00:36:22,589
Функция делает несколько вещей. Во-первых, она отделяет целевую переменную в объект y и датасет без неё в объект df.

184
00:36:22,589 --> 00:36:36,865
Она нормирует данные — нейронным сетям удобно, когда все данные близки к 0 со стандартным отклонением около 1.

185
00:36:36,965 --> 00:36:43,650
Для этого из каждого столбца вычитается среднее и все числа делятся на стандартное отклонение.

186
00:36:43,650 --> 00:36:51,900
Это регулируется параметром do_scale=True, при этом средние и стандартные отклонения сохраняются в объекте mapper,

187
00:36:51,900 --> 00:36:57,780
чтобы потом обработать тестовую выборку таким же образом.

188
00:36:57,780 --> 00:37:08,669
Функция вставляет пропущенные значения — неизвестные категории приобретают индекс 0 (известные нумеруются с 1),

189
00:37:08,769 --> 00:37:21,952
неизвестные количественные признаки заменяются медианой, и создаётся отдельный признак «Значение было пропущено».

190
00:37:22,052 --> 00:37:29,369
Я не буду в это углубляться, мы подробно обсудили это в курсе по машинному обучению.

191
00:37:29,369 --> 00:37:36,140
Если у вас есть какие-то вопросы на эту тему, изучите его, там нет ничего про глубокое обучение.

192
00:37:36,900 --> 00:37:42,330
Как видите, после обработки год 2014 стал годом номер 2.

193
00:37:42,330 --> 00:37:47,220
Все категориальные признаки были заменены на целые числа, начиная с нуля,

194
00:37:47,220 --> 00:37:59,560
так как эти признаки потом образуют таблицу, и мы не хотим, чтобы на год ушло 2014 столбцов, когда хватило бы двух.

195
00:37:59,660 --> 00:38:11,950
Как видите, таким же образом строки 'a' и 'c' преобразовались в числа 1 и 3.

196
00:38:12,050 --> 00:38:20,220
Окей, у нас есть датафрейм, не содержащий целевую переменную, где все значения — это числа.

197
00:38:20,220 --> 00:38:35,050
Всё, что мы делали до этого момента, не содержало никакого глубокого обучения и обсуждалось в курсе по машинному обучению.

198
00:38:35,150 --> 00:38:41,200
Одна из вещей, которые подробно обсуждались в курсе по машинному обучению — валидационные выборки.

199
00:38:41,300 --> 00:38:54,510
По условиям соревнования Kaggle нужно предсказать следующие две недели продаж.

200
00:38:54,510 --> 00:39:05,755
Я возьму последние две недели обучающей выборки в качестве валидационной, чтобы максимально приблизить её к тестовой.

201
00:39:05,855 --> 00:39:18,279
Рейчел недавно написала пост про создание валидационных выборок, она есть на fast.ai, мы добавим её в вики.

202
00:39:18,379 --> 00:39:34,460
По сути, это пересказ одной из лекций курса «Машинное обучение», к этой лекции есть видеозапись.

203
00:39:35,579 --> 00:39:42,125
Мы с Рейчел долго думали над тем, как обращаться с обучающей, валидационной и тестовой выборками, результаты в этом посте,

204
00:39:42,225 --> 00:39:50,710
но, опять же — там нет ничего про глубокое обучение, давайте уже к нему перейдём.

205
00:39:50,710 --> 00:40:05,479
В каждом соревновании Kaggle и вообще в любом проекте, связанном с машинным обучением, необходимо выбрать метрику.

206
00:40:05,579 --> 00:40:17,770
На соревновании Kaggle метрику оценки качества модели выбрали за нас — это средняя процентная ошибка (RMSPE).

207
00:40:17,770 --> 00:40:25,245
Функция ошибки возвращает корень из среднего квадрата процентного отклонения предсказания от истинной величины.

208
00:40:25,345 --> 00:40:26,460


209
00:40:26,560 --> 00:40:28,890


210
00:40:33,819 --> 00:40:37,839


211
00:40:37,839 --> 00:40:43,359


212
00:40:43,359 --> 00:40:47,989


213
00:40:48,089 --> 00:41:01,930
В PyTorch нет среднеквадратичной процентной ошибки, но мы можем написать её сами.

214
00:41:01,930 --> 00:41:22,380
Проще вспомнить логарифмы и понять, что частное а / b можно заменить на ln(a' / b') = ln(a') - ln(b').

215
00:41:22,380 --> 00:41:28,930


216
00:41:30,400 --> 00:41:35,430


217
00:41:39,880 --> 00:41:44,200


218
00:41:46,599 --> 00:41:50,740


219
00:41:53,770 --> 00:42:00,970


220
00:42:00,970 --> 00:42:04,510


221
00:42:04,510 --> 00:42:11,020
Эта часть тоже не относится к глубокому обучению, мы уже к нему переходим.

222
00:42:14,530 --> 00:42:19,162
Пока мы делаем всё то же самое, что и раньше:

223
00:42:19,262 --> 00:42:28,185
Создаём объект данных модели ModelData, который содержит валидационную, обучающую и иногда тестовую выборки,

224
00:42:28,285 --> 00:42:41,110
на его основе потом создастся learner, найдётся скорость обучения, обучится модель и так далее, это всё вы уже видели.

225
00:42:41,110 --> 00:42:55,030
Различие будет в том, что данные не разложены по папкам и не размечены в файле CSV,

226
00:42:55,030 --> 00:43:08,765
поэтому мы используем метод ColumnarModelData.from_data_frame(), он возвращает объект такой же структуры, как и на прошлых лекциях.

227
00:43:08,865 --> 00:43:24,590
Переменная PATH — директория, в которой хранятся необходимые файлы модели.

228
00:43:24,690 --> 00:43:29,075
Переменная val_idx содержит индексы валидационной выборки, мы объявили её выше.

229
00:43:29,175 --> 00:43:51,290
Переменная df — наш датафрейм. Переменная yl — логарифм целевой переменной y, она должна быть в модели в каком-то виде.

230
00:43:51,390 --> 00:44:03,380
Итак, это индексы валидационной выборки, датафрейм без целевой переменной, целевая переменная, а это —

231
00:44:03,380 --> 00:44:19,769
список категориальных признаков. Напомню, что сейчас все данные хранятся в виде чисел, поэтому важно его указать.

232
00:44:19,769 --> 00:44:30,469
В качестве списка категориальных признаков мы передаём переменную cat_vars, объявленную раньше.

233
00:44:30,469 --> 00:44:39,469
Также передаётся размер минибатча bs=128, этот параметр нам уже знаком.

234
00:44:39,569 --> 00:45:04,349
Теперь у нас есть объект данных модели md, содержащий уже упомянутые объекты train_dl, val_dl, train_ds, val_ds, размер выборки и всё остальное.

235
00:45:04,349 --> 00:45:18,709
После этого мы создадим из объекта данных модели алгоритм обучения методом get_learner().

236
00:45:21,769 --> 00:45:37,614
Эти параметры — начальная вероятность дропаута, количество активаций на каждом слое и дропаут для последних слоёв.

237
00:45:37,714 --> 00:45:53,519
Один из незнакомых параметров — emb_szc, вложения, сейчас их обсудим.

238
00:45:53,519 --> 00:46:05,549
Давайте забудем ненадолго про категориальные признаки и рассмотрим только количественные.

239
00:46:05,549 --> 00:46:29,740
Давайте возьмём все количественные признаки — минимальную и максимальную температуры, расстояние до ближайшего конкурента и так далее.

240
00:46:32,589 --> 00:46:36,700
Это всё числа с плавающей точкой.

241
00:46:36,700 --> 00:46:54,130
Нейронная сеть возьмёт этот одномерный массив, или вектор, или тензор первого ранга, и умножит его на матрицу.

242
00:46:54,130 --> 00:47:12,940
Допустим, у нас 20 признаков и мы хотим получить 100 новых, для этого матрица должна иметь размер 20x100.

243
00:47:12,940 --> 00:47:28,410
В итоге получается новый вектор длины 100, так работает линейный слой.

244
00:47:28,410 --> 00:47:39,670
Полученный вектор пропускается через выпрямитель, негативные значения заменяются нулями.

245
00:47:39,670 --> 00:47:59,349
После этого вектор умножается на ещё одну матрицу. Допустим, на этом нейронная сеть кончается, и мы хотим получить одно число.

246
00:47:59,349 --> 00:48:13,444
В таком случае матрица должна иметь размер 100x1. Это — нейронная сеть, состоящая из одного слоя.

247
00:48:13,544 --> 00:48:25,070
На практике обычно делают больше одного слоя, пусть матрица имеет размер 100x50.

248
00:48:25,170 --> 00:48:42,420
После умножения мы получаем новый вектор длиной 50, после умножения на матрицу 50x1 получается число.

249
00:48:42,420 --> 00:49:06,119
Напомню, что не нужно ставить выпрямитель перед Softmax, так как Softmax нужны негативные значения для низких вероятностей.

250
00:49:13,390 --> 00:49:22,400
Вот схема простейшей полносвязной нейронной сети.

251
00:49:22,400 --> 00:49:43,579
На входе — тензор первого ранга, дальше идут линейный слой, выпрямитель, ещё один линейный слой и Softmax.

252
00:49:43,579 --> 00:50:02,839
Можно добавить больше связок выпрямитель+линейный слой, можно добавить дропаут, в принципе, на этом вариации кончаются.

253
00:50:02,839 --> 00:50:12,619
Мы посмотрим на другие архитектуры, когда вернёмся к компьютерному зрению.

254
00:50:12,619 --> 00:50:27,170
Полносвязные нейронные сети —  это просто комбинация перемножения матриц и функций активаций —  например, выпрямителей и Softmax.

255
00:50:27,170 --> 00:50:42,434
Softmax необходим только в задачах классификации, здесь можно без него обойтись.

256
00:50:42,534 --> 00:50:51,440
Есть один нюанс, но в общем всё устроено именно так.

257
00:50:51,440 --> 00:50:56,420
Вернёмся к категориальным признакам.

258
00:50:56,420 --> 00:51:06,019
Один из категориальных признаков — день недели, кодируется числом от 0 до 6.

259
00:51:11,259 --> 00:51:24,252
Я хочу представить его в виде тензора первого ранга, чтобы использовать ту же архитектуру.

260
00:51:24,352 --> 00:51:45,327
Для этого создаётся новая таблица с семью строками и произвольным количеством столбцов, например, четырьмя.

261
00:51:45,427 --> 00:51:57,060
Допустим, в какой-то строке день недели — воскресенье. Вместо числа 0 (индекс воскресенья в категориальном признаке)

262
00:51:57,160 --> 00:52:12,239
мы вставим строку из матрицы 7x4, соответствующую воскресенью. Матрица заполнена числами с плавающей точкой.

263
00:52:12,339 --> 00:52:29,055
В итоге категориальный признак будет представим в виде массива из четырёх чисел, то есть тензора первого ранга.

264
00:52:29,155 --> 00:52:40,990
Изначально все числа в таблице случайные, но в процессе обучения модели изменятся.

265
00:52:41,090 --> 00:52:57,945
Итак, вместо категориального признака во входных данных мы вставляем тензор первого ранга.

266
00:52:58,045 --> 00:53:28,730
Архитектура нейронной сети не меняется. В конце вычисляется функция потерь, градиент, и эти четыре числа меняются соответственно.

267
00:53:28,730 --> 00:53:42,645
Процесс повторяется много раз, и числа в матрице приобретают смысл — мы получаем лучшие четыре числа для воскресенья,

268
00:53:42,745 --> 00:53:52,300
лучшие четыре числа для пятницы и так далее. По сути, создаётся новая матрица весов.

269
00:53:52,400 --> 00:54:01,480
Такие матрицы называются матрицы эмбеддинга.

270
00:54:03,640 --> 00:54:20,490
Количество строк матрицы эмбеддинга равно мощности признака, индекс строки равен номеру соответствующего значения признака.

271
00:54:20,590 --> 00:54:37,070
Например, для первого значения признака мы взяли бы первую строку таблицы и присоединили её к вектору количественных признаков.

272
00:54:37,070 --> 00:54:50,910
Например, если есть 5,000 различных почтовых индексов, матрица эмбеддинга может иметь размер 5,000x50.

273
00:54:51,010 --> 00:54:56,097
Допустим, необходимо преобразовать почтовый индекс 94003 под номером 4.

274
00:54:56,197 --> 00:55:04,905
Тогда нужно взять строку номер 4 матрицы эмбеддинга и добавить эти 50 чисел к вектору количественных признаков.

275
00:55:05,005 --> 00:55:10,635
После этого эти числа обрабатываются как количественные признаки.

276
00:55:10,735 --> 00:55:15,080
Вопрос из зала: Что значат эти четыре числа?

277
00:55:15,180 --> 00:55:34,965
Отличный вопрос, мы обсудим его, когда дойдём до коллаборативной фильтрации. Думайте о них как об ещё одних весах нейронной сети.

278
00:55:35,065 --> 00:55:47,270
Дальше мы узнаем, что в этих числах можно разглядеть смысл, но не их изначальное значение.

279
00:55:47,270 --> 00:55:52,099
Пока это просто случайные числа, которые мы меняем в процессе обучения.

280
00:55:52,099 --> 00:56:03,059
Вопрос из зала: Почему здесь именно 4 столбца? Для определения этого числа есть алгоритм?

281
00:56:03,140 --> 00:56:18,164
Да, есть. Сначала я составляю список всех признаков и их мощностей.

282
00:56:18,264 --> 00:56:29,089
Например, у нас больше тысячи магазинов и восемь дней недели.

283
00:56:29,089 --> 00:56:42,074
Дней недели восемь, а не семь, потому что нулевой зарезервирован под неизвестные значения, я всегда так делаю.

284
00:56:42,174 --> 00:56:47,835
Мощность признака год четыре — а различных годов три, и так далее.

285
00:56:47,935 --> 00:57:04,180
Для определения количества столбцов я делю мощность пополам и делаю так, чтобы это число было не больше 50.

286
00:57:04,180 --> 00:57:12,780
Вот размеры моих матриц эмбеддинга. В матрице эмбеддинга для магазинов 1116 строк по количеству магазинов и 50 столбцов,

287
00:57:12,880 --> 00:57:20,900
то есть вместо признака магазина добавляется 50 новых количественных признаков.

288
00:57:20,900 --> 00:57:29,190
В матрице для дня недели 8 строк и 4 столбца, каждому дню недели соответствует вектор длины 4.

289
00:57:31,220 --> 00:57:36,279
Вопрос из зала: И так делается для каждого категориального признака?

290
00:57:36,279 --> 00:57:56,310
Да, сначала я создал список, где каждой категории сопоставлялась её мощность, а потом преобразовал эту мощность в размер матрицы эмбеддинга.

291
00:57:56,410 --> 00:58:03,320
Размеры матриц эмбеддинга хранятся в переменной emb_szs (embedding sizes), мы передавали её в метод .get_learner().

292
00:58:03,320 --> 00:58:11,690
Каждому категориальному признаку соответствует матрица эмбеддинга.

293
00:58:11,690 --> 00:58:22,530
Вопрос из зала: Можно ли инициализировать матрицы эмбеддинга не случайными числами?

294
00:58:22,630 --> 00:58:28,920
Да, можно брать случайные матрицы эмбеддинга, а можно предобученные.

295
00:58:29,020 --> 00:58:36,094
Про предобученные мы ещё поговорим, если вкратце — если кто-то в Rossman обучил нейронную сеть

296
00:58:36,194 --> 00:58:45,829
для предсказывания продаж сыра и выложил свои матрицы эмбеддинга,

297
00:58:45,829 --> 00:58:52,790
их можно было бы использовать для предсказаний продаж алкоголя в Rossman.

298
00:58:53,480 --> 00:59:06,920
Например, так делают в Pinterest и Instacart — Instacart так выстраивает маршруты, Pinterest — решает, что показывать на странице.

299
00:59:06,920 --> 00:59:21,819
В обеих компаниях матрицы эмбеддинга — общие для всей компании.

300
00:59:23,260 --> 00:59:36,500
Вопрос из зала: Почему бы просто не использовать прямое кодирование?

301
00:59:36,500 --> 00:59:50,359
Да, можно было бы использовать 7 чисел вместо 4, из которых одно — 1, а остальные — 0, и записать их как числа с плавающей точкой.

302
00:59:52,309 --> 01:00:09,509
Именно так и делают в статистике, это называется фиктивные переменные.

303
01:00:09,609 --> 01:00:24,140
Проблема в том, что в этом случае воскресенью соответствовало бы всего одно число, и поведение было бы линейное.

304
01:00:24,140 --> 01:00:38,430
У нас воскресенье — точка в четырёхмерном пространстве, что позволяет передавать гораздо более сложные зависимости.

305
01:00:38,530 --> 01:00:57,349
Например, если выяснится, что продажи по выходным сильно отличаются, у субботы и воскресенья одно из чисел может совпадать.

306
01:00:57,349 --> 01:01:28,584
Скорее всего, выяснится, что в будни чаще покупают бензин или молоко, а перед выходными и праздниками — вино.

307
01:01:28,684 --> 01:01:41,185
Поэтому один из столбцов матрицы эмбеддинга может значит, например, насколько часто люди отдыхают в этот день.

308
01:01:41,285 --> 01:01:52,860
Наличие нескольких чисел для каждого значения признака позволяет нейронной сети уловить эти закономерности.

309
01:01:52,960 --> 01:02:04,005
Эмбеддинг — проявление идеи распределённого представления, основного принципа работы нейронных сетей.

310
01:02:04,105 --> 01:02:12,980
Нейронные сети обращаются с многомерными пространствами, которые сложно интерпретировать.

311
01:02:12,980 --> 01:02:18,255
Отдельный вес в строке матрицы эмбеддинга может не иметь определённого значения —

312
01:02:18,355 --> 01:02:26,595
оно может зависеть от значения остальных, и зависеть нелинейно.

313
01:02:26,695 --> 01:02:41,540
Это позволяет нейронным сетям улавливать сложные закономерности.

314
01:02:41,540 --> 01:03:07,280
Вопрос из зала: Эмбеддинг можно использовать только с какими-то определёнными категориальными признаками?

315
01:03:07,280 --> 01:03:20,385
Нет. Эмбеддинг подойдёт для любых категориальных признаков, кроме, возможно, признаков с очень большой мощностью.

316
01:03:20,485 --> 01:03:33,140
Но если мощность признака равна, например, 60,000, то это плохой категориальный признак в принципе.

317
01:03:33,140 --> 01:03:47,860
Обладатели третьего места в этом соревновании объявили категориальными все признаки с небольшой мощностью.

318
01:03:47,860 --> 01:03:56,170
Это хорошая практика — это позволит уловить максимальное количество зависимостей.

319
01:03:56,170 --> 01:04:05,470
Для всех количественных признаков нейронная сеть просто подберёт одномерную функцию.

320
01:04:05,470 --> 01:04:24,340
Янет: Применением эмбеддинга мы увеличиваем размерность, но на самом деле уменьшаем, потому что не используем прямое кодирование.

321
01:04:24,340 --> 01:04:39,250
Да, точно. Прямое кодирование увеличивает размерность матриц, но не вносит нового смысла.

322
01:04:39,250 --> 01:04:46,175
Янет: Использование эмбеддинга занимает меньше места, чем прямое кодирование, и полезнее по смыслу.

323
01:04:46,275 --> 01:04:52,720
Да. Давайте заглянем под капот и вспомним линейную алгебру.

324
01:04:52,720 --> 01:04:58,880
Если вы этого не поймёте, ничего, но некоторым это поможет.

325
01:04:58,980 --> 01:05:15,160
Воскресенье можно представить прямым кодированием в виде вектора длиной 8.

326
01:05:19,240 --> 01:05:30,540
Матрица эмбеддинга содержит 8 строк и 4 столбца.

327
01:05:32,440 --> 01:06:08,849
Выбор соответствующей строки матрицы можно представить в виде произведения вектора 1x8 на матрицу 8x4.

328
01:06:08,949 --> 01:06:23,450
Раньше некоторые люди так и воплощали эмбеддинг, некоторые из методов машинного обучения делают это до сих пор.

329
01:06:23,450 --> 01:06:34,490
Это очень неэффективно, поэтому современные библиотеки превращают номер класса в признаке в целое число и используют его как индекс.

330
01:06:37,069 --> 01:06:49,700
Идея перемножения матриц позволяет думать про эмбеддинг, как про ещё один линейный слой.

331
01:06:52,819 --> 01:06:59,510
Это незначительный нюанс, но, возможно, кому-то из вас поможет.

332
01:06:59,510 --> 01:07:06,234
Вопрос из зала: Можно ли преобразовать даты в категориальные признаки? Как при этом учитывается сезонность продаж?

333
01:07:06,334 --> 01:07:21,165
Отличный вопрос. Я подробно объяснял это в курсе «Машинное обучение», но тут тоже можно упомянуть.

334
01:07:21,265 --> 01:07:33,170
В fastai есть функция add_datepart(), которая принимает датафрейм, содержащий столбец с датами «Date»,

335
01:07:33,170 --> 01:07:46,170
удаляет его, если не поставить drop=False, и вставляет в датафрейм новые столбцы с информацией об этом дне —

336
01:07:46,270 --> 01:07:59,570
день недели, день месяца, месяц, год, является ли этот день началом или концом квартала — всё, что pandas знает про этот день.

337
01:07:59,570 --> 01:08:14,260
Вот эти признаки — год, месяц, неделя, день, день недели и так далее.

338
01:08:14,360 --> 01:08:34,500
Итак, для дня недели размер матрицы эмбеддинга — 8x4. Это позволяет создавать очень интересные модели.

339
01:08:34,600 --> 01:08:53,700
Например, если в Берлине с понедельника по среду скидка на молочные продукты, модель это учтёт.

340
01:08:53,800 --> 01:09:01,010
Это отличный способ использования времени в качестве признака, рад, что вы спросили.

341
01:09:01,010 --> 01:09:10,515
Главное — убедиться, что соответствующие признаки выделены в столбцы. Если в датафрейме нет признака дня недели,

342
01:09:10,615 --> 01:09:28,020
модели будет очень сложно увидеть связанные с днём недели закономерности.

343
01:09:28,120 --> 01:09:36,400
Стоит выделять наличие праздников и выходных в отдельные столбцы.

344
01:09:36,500 --> 01:09:44,720
Например, если вы предсказываете продажу напитков в Сан Франциско,

345
01:09:44,720 --> 01:09:52,080
стоит посмотреть расписание бейсбольных игр в AT&T-парке, потому что они влияют на продажи пива.

346
01:09:52,180 --> 01:10:03,820
Если вы добавите признаки периодических событий, нейронная сеть их учтёт.

347
01:10:03,820 --> 01:10:12,050
Я стараюсь пропускать вещи, не связанные с глубоким обучением.
=========================

348
01:10:12,050 --> 01:10:17,270
Итак, алгоритм обучения создаётся из объекта данных модели.

349
01:10:17,270 --> 01:10:37,220
В параметры передаются размеры матриц эмбеддинга emd_szs и количество количественных признаков,

350
01:10:37,220 --> 01:10:54,510
мы передаём разницу между количеством всех признаков и количеством категориальных.

351
01:10:54,610 --> 01:11:02,840
У матриц эмбеддинга есть свой дропаут, здесь он равен 0.04.

352
01:11:02,840 --> 01:11:12,690
1000 - количество активаций в первом линейном слое, 500 - во втором. 0.001 - дропаут для первого линейного слоя, 0.01 - для второго.

353
01:11:12,790 --> 01:11:26,610
Параметр y_range мы ещё обсудим. 1 - это количество чисел на выходе, у нас одно, потому что мы предсказываем продажи.

354
01:11:26,710 --> 01:11:35,000
У нас есть алгоритм обучения m. Вызываем его метод .lr_find() для нахождения скорости обучения, тут всё привычно.

355
01:11:35,000 --> 01:11:58,720
Дальше модель обучается как обычно. Здесь в качестве метрики передаётся заданная пользователем функция RMSPE.

356
01:11:58,720 --> 01:12:13,344
Функцию RSMPE мы задали выше, это средняя процентная ошибка, сначала берущая экспоненту от предсказаний, потому что модели возвращают логарифм.

357
01:12:13,444 --> 01:12:17,609
Метрика не влияет на процесс обучения, она выводится для нас.

358
01:12:17,709 --> 01:12:35,489
Здесь используется SGDR, которого не было у обладателей третьего места, интересно, что получится.

359
01:12:37,020 --> 01:12:49,619
Валидационная выборка не совпадает с тестовой, но она похожа - это последние две недели датасета.

360
01:12:52,530 --> 01:12:58,760


361
01:12:58,760 --> 01:13:12,000


362
01:13:12,000 --> 01:13:18,179


363
01:13:18,179 --> 01:13:21,480


364
01:13:24,210 --> 01:13:27,060


365
01:13:27,060 --> 01:13:35,100


366
01:13:35,100 --> 01:13:41,070


367
01:13:44,219 --> 01:13:50,280


368
01:13:50,280 --> 01:13:55,230


369
01:13:55,230 --> 01:14:01,500


370
01:14:01,500 --> 01:14:04,230


371
01:14:04,230 --> 01:14:11,190


372
01:14:11,190 --> 01:14:16,230


373
01:14:16,230 --> 01:14:20,460


374
01:14:20,460 --> 01:14:25,500


375
01:14:25,500 --> 01:14:31,199


376
01:14:31,199 --> 01:14:35,190


377
01:14:35,190 --> 01:14:39,510


378
01:14:39,510 --> 01:14:45,449


379
01:14:47,929 --> 01:14:51,840


380
01:14:51,840 --> 01:15:00,619


381
01:15:01,989 --> 01:15:11,140


382
01:15:11,140 --> 01:15:16,680


383
01:15:16,680 --> 01:15:21,100


384
01:15:21,100 --> 01:15:24,610


385
01:15:26,620 --> 01:15:28,719


386
01:15:28,719 --> 01:15:36,360


387
01:15:36,360 --> 01:15:42,310


388
01:15:42,310 --> 01:15:47,739


389
01:15:50,380 --> 01:15:58,000


390
01:15:58,000 --> 01:16:03,100


391
01:16:03,100 --> 01:16:10,120


392
01:16:14,380 --> 01:16:18,989


393
01:16:21,520 --> 01:16:27,430


394
01:16:27,430 --> 01:16:35,620


395
01:16:35,620 --> 01:16:43,840


396
01:16:43,840 --> 01:16:49,239


397
01:16:51,790 --> 01:16:56,830


398
01:16:56,830 --> 01:17:01,930


399
01:17:01,930 --> 01:17:04,469


400
01:17:04,639 --> 01:17:10,800


401
01:17:10,800 --> 01:17:16,260


402
01:17:16,260 --> 01:17:25,159


403
01:17:32,599 --> 01:17:38,099


404
01:17:38,099 --> 01:17:49,260


405
01:17:49,260 --> 01:17:54,869


406
01:17:57,780 --> 01:18:04,050


407
01:18:04,050 --> 01:18:11,760


408
01:18:11,760 --> 01:18:20,489


409
01:18:25,860 --> 01:18:31,530


410
01:18:31,530 --> 01:18:37,260


411
01:18:37,260 --> 01:18:44,340


412
01:18:44,340 --> 01:18:48,239


413
01:18:53,099 --> 01:18:56,269


414
01:18:59,620 --> 01:19:07,400


415
01:19:07,400 --> 01:19:14,000


416
01:19:14,000 --> 01:19:17,630


417
01:19:17,630 --> 01:19:20,080


418
01:19:21,200 --> 01:19:25,220


419
01:19:25,220 --> 01:19:29,960


420
01:19:29,960 --> 01:19:32,680


421
01:19:32,680 --> 01:19:45,950


422
01:19:48,020 --> 01:19:54,470


423
01:19:54,470 --> 01:20:00,110


424
01:20:01,820 --> 01:20:12,860


425
01:20:12,860 --> 01:20:30,220


426
01:20:30,220 --> 01:20:36,350


427
01:20:36,350 --> 01:20:41,960


428
01:20:41,960 --> 01:20:49,370


429
01:20:49,370 --> 01:20:55,910


430
01:20:55,910 --> 01:20:59,480


431
01:20:59,480 --> 01:21:06,500


432
01:21:06,500 --> 01:21:10,100


433
01:21:10,100 --> 01:21:16,220


434
01:21:16,220 --> 01:21:23,030


435
01:21:25,550 --> 01:21:30,860


436
01:21:33,470 --> 01:21:39,270


437
01:21:39,270 --> 01:21:44,880


438
01:21:44,880 --> 01:21:51,570


439
01:21:51,570 --> 01:21:57,810


440
01:21:57,810 --> 01:22:02,730


441
01:22:06,210 --> 01:22:09,690


442
01:22:09,690 --> 01:22:13,950


443
01:22:13,950 --> 01:22:17,640


444
01:22:17,640 --> 01:22:25,050


445
01:22:25,050 --> 01:22:29,640


446
01:22:29,640 --> 01:22:34,500


447
01:22:34,500 --> 01:22:39,300


448
01:22:39,300 --> 01:22:46,590


449
01:22:46,590 --> 01:22:50,670


450
01:22:50,670 --> 01:22:54,540


451
01:22:54,540 --> 01:22:56,790


452
01:22:56,790 --> 01:23:02,730


453
01:23:02,730 --> 01:23:09,540


454
01:23:09,540 --> 01:23:15,060


455
01:23:15,060 --> 01:23:21,540


456
01:23:21,540 --> 01:23:28,640


457
01:23:28,640 --> 01:23:33,960


458
01:23:33,960 --> 01:23:44,790


459
01:23:44,790 --> 01:23:48,830


460
01:23:48,830 --> 01:23:54,530


461
01:23:54,530 --> 01:23:59,960


462
01:23:59,960 --> 01:24:06,080


463
01:24:06,080 --> 01:24:10,820


464
01:24:10,820 --> 01:24:17,180


465
01:24:17,180 --> 01:24:21,310


466
01:24:21,310 --> 01:24:28,550


467
01:24:28,550 --> 01:24:33,890


468
01:24:33,890 --> 01:24:36,230


469
01:24:39,620 --> 01:24:45,290


470
01:24:45,290 --> 01:24:49,760


471
01:24:49,760 --> 01:24:52,730


472
01:24:57,860 --> 01:25:03,380


473
01:25:03,380 --> 01:25:06,770


474
01:25:06,770 --> 01:25:10,850


475
01:25:10,850 --> 01:25:15,650


476
01:25:19,280 --> 01:25:24,530


477
01:25:24,530 --> 01:25:28,820


478
01:25:28,820 --> 01:25:33,080


479
01:25:33,080 --> 01:25:36,910


480
01:25:36,910 --> 01:25:41,810


481
01:25:41,810 --> 01:25:46,550


482
01:25:46,550 --> 01:25:49,940


483
01:25:54,620 --> 01:26:01,050


484
01:26:01,050 --> 01:26:07,140


485
01:26:07,140 --> 01:26:14,910


486
01:26:16,920 --> 01:26:21,480


487
01:26:24,420 --> 01:26:28,590


488
01:26:28,590 --> 01:26:35,670


489
01:26:35,670 --> 01:26:41,550


490
01:26:41,550 --> 01:26:46,740


491
01:26:46,740 --> 01:26:53,040


492
01:26:53,040 --> 01:26:57,630


493
01:26:57,630 --> 01:27:04,530


494
01:27:04,530 --> 01:27:08,450


495
01:27:08,450 --> 01:27:13,230


496
01:27:13,230 --> 01:27:16,890


497
01:27:16,890 --> 01:27:22,290


498
01:27:22,290 --> 01:27:27,990


499
01:27:27,990 --> 01:27:34,050


500
01:27:34,050 --> 01:27:38,820


501
01:27:41,700 --> 01:27:46,950


502
01:27:46,950 --> 01:27:51,330


503
01:27:51,330 --> 01:27:58,860


504
01:27:58,860 --> 01:28:01,290


505
01:28:01,290 --> 01:28:04,620


506
01:28:04,620 --> 01:28:07,560


507
01:28:07,560 --> 01:28:12,990


508
01:28:15,630 --> 01:28:20,190


509
01:28:22,710 --> 01:28:26,940


510
01:28:26,940 --> 01:28:32,910


511
01:28:32,910 --> 01:28:36,630


512
01:28:36,630 --> 01:28:42,180


513
01:28:42,180 --> 01:28:47,130


514
01:28:47,130 --> 01:28:52,740


515
01:28:52,740 --> 01:28:57,330


516
01:28:57,330 --> 01:29:02,940


517
01:29:02,940 --> 01:29:09,960


518
01:29:09,960 --> 01:29:15,660


519
01:29:15,660 --> 01:29:20,790


520
01:29:20,790 --> 01:29:24,530


521
01:29:24,530 --> 01:29:28,800


522
01:29:28,800 --> 01:29:33,780


523
01:29:33,780 --> 01:29:38,850


524
01:29:38,850 --> 01:29:47,360


525
01:29:49,560 --> 01:29:55,290


526
01:29:55,290 --> 01:29:58,740


527
01:29:58,740 --> 01:30:04,140


528
01:30:04,140 --> 01:30:08,820


529
01:30:09,840 --> 01:30:14,380


530
01:30:14,380 --> 01:30:19,810


531
01:30:19,810 --> 01:30:24,790


532
01:30:24,790 --> 01:30:31,630


533
01:30:31,630 --> 01:30:36,190


534
01:30:39,790 --> 01:30:44,409


535
01:30:44,409 --> 01:30:48,849


536
01:30:48,849 --> 01:30:54,130


537
01:30:54,130 --> 01:30:59,829


538
01:30:59,829 --> 01:31:02,440


539
01:31:02,440 --> 01:31:06,250


540
01:31:06,250 --> 01:31:14,219


541
01:31:14,219 --> 01:31:18,940


542
01:31:18,940 --> 01:31:23,980


543
01:31:23,980 --> 01:31:28,000


544
01:31:28,000 --> 01:31:32,710


545
01:31:32,710 --> 01:31:40,000


546
01:31:40,000 --> 01:31:47,110


547
01:31:47,110 --> 01:31:54,400


548
01:31:54,400 --> 01:32:01,360


549
01:32:01,360 --> 01:32:06,610


550
01:32:06,610 --> 01:32:15,880


551
01:32:15,880 --> 01:32:19,540


552
01:32:19,540 --> 01:32:24,920


553
01:32:24,920 --> 01:32:30,140


554
01:32:30,140 --> 01:32:35,420


555
01:32:35,420 --> 01:32:40,400


556
01:32:40,400 --> 01:32:44,060


557
01:32:44,060 --> 01:32:48,500


558
01:32:48,500 --> 01:32:53,540


559
01:32:53,540 --> 01:32:59,810


560
01:32:59,810 --> 01:33:03,860


561
01:33:05,030 --> 01:33:12,740


562
01:33:12,740 --> 01:33:21,200


563
01:33:21,200 --> 01:33:28,340


564
01:33:28,340 --> 01:33:33,020


565
01:33:33,020 --> 01:33:38,810


566
01:33:38,810 --> 01:33:42,710


567
01:33:42,710 --> 01:33:50,870


568
01:33:50,870 --> 01:33:56,150


569
01:33:56,150 --> 01:33:59,990


570
01:33:59,990 --> 01:34:04,130


571
01:34:04,130 --> 01:34:07,070


572
01:34:07,070 --> 01:34:13,310


573
01:34:13,310 --> 01:34:16,820


574
01:34:19,220 --> 01:34:25,340


575
01:34:25,340 --> 01:34:30,440


576
01:34:33,020 --> 01:34:37,789


577
01:34:39,679 --> 01:34:45,260


578
01:34:45,260 --> 01:34:49,130


579
01:34:49,130 --> 01:34:53,449


580
01:34:55,639 --> 01:34:59,659


581
01:35:01,849 --> 01:35:06,110


582
01:35:08,059 --> 01:35:13,789


583
01:35:13,789 --> 01:35:19,269


584
01:35:19,269 --> 01:35:24,909


585
01:35:27,469 --> 01:35:31,820


586
01:35:31,820 --> 01:35:37,880


587
01:35:37,880 --> 01:35:44,030


588
01:35:44,030 --> 01:35:49,099


589
01:35:49,099 --> 01:35:54,500


590
01:35:54,500 --> 01:36:00,219


591
01:36:00,219 --> 01:36:06,110


592
01:36:06,110 --> 01:36:12,320


593
01:36:12,320 --> 01:36:16,489


594
01:36:16,489 --> 01:36:25,730


595
01:36:25,730 --> 01:36:30,980


596
01:36:30,980 --> 01:36:39,199


597
01:36:39,199 --> 01:36:44,030


598
01:36:44,030 --> 01:36:51,230


599
01:36:51,230 --> 01:36:57,110


600
01:36:57,110 --> 01:37:03,739


601
01:37:03,739 --> 01:37:09,800


602
01:37:09,800 --> 01:37:16,280


603
01:37:16,280 --> 01:37:20,290


604
01:37:20,290 --> 01:37:26,390


605
01:37:26,390 --> 01:37:32,090


606
01:37:32,090 --> 01:37:35,090


607
01:37:35,090 --> 01:37:39,200


608
01:37:39,200 --> 01:37:44,420


609
01:37:44,420 --> 01:37:50,900


610
01:37:50,900 --> 01:37:57,500


611
01:37:57,500 --> 01:38:01,700


612
01:38:01,700 --> 01:38:06,560


613
01:38:06,560 --> 01:38:10,880


614
01:38:10,880 --> 01:38:14,720


615
01:38:14,720 --> 01:38:22,100


616
01:38:22,100 --> 01:38:28,970


617
01:38:28,970 --> 01:38:36,140


618
01:38:36,140 --> 01:38:41,660


619
01:38:43,580 --> 01:38:50,840


620
01:38:50,840 --> 01:38:54,290


621
01:38:54,290 --> 01:38:58,010


622
01:38:58,010 --> 01:39:02,410


623
01:39:02,530 --> 01:39:06,880


624
01:39:06,880 --> 01:39:12,010


625
01:39:12,010 --> 01:39:24,100


626
01:39:24,100 --> 01:39:29,920


627
01:39:29,920 --> 01:39:40,870


628
01:39:40,870 --> 01:39:45,670


629
01:39:45,670 --> 01:39:49,060


630
01:39:49,060 --> 01:39:52,870


631
01:39:55,600 --> 01:39:59,710


632
01:39:59,710 --> 01:40:05,440


633
01:40:05,440 --> 01:40:10,630


634
01:40:10,630 --> 01:40:19,240


635
01:40:19,240 --> 01:40:24,430


636
01:40:24,430 --> 01:40:30,720


637
01:40:30,720 --> 01:40:35,410


638
01:40:35,410 --> 01:40:43,380


639
01:40:43,380 --> 01:40:47,590


640
01:40:47,590 --> 01:40:53,470


641
01:40:53,470 --> 01:40:58,600


642
01:40:58,600 --> 01:41:05,440


643
01:41:07,210 --> 01:41:12,250


644
01:41:12,250 --> 01:41:15,610


645
01:41:15,610 --> 01:41:21,490


646
01:41:21,490 --> 01:41:26,560


647
01:41:26,560 --> 01:41:30,930


648
01:41:30,930 --> 01:41:37,300


649
01:41:37,300 --> 01:41:40,750


650
01:41:40,750 --> 01:41:44,680


651
01:41:44,680 --> 01:41:49,720


652
01:41:49,720 --> 01:41:54,070


653
01:41:57,850 --> 01:42:05,380


654
01:42:05,380 --> 01:42:08,980


655
01:42:10,900 --> 01:42:17,050


656
01:42:17,050 --> 01:42:20,770


657
01:42:20,770 --> 01:42:24,820


658
01:42:26,350 --> 01:42:32,230


659
01:42:34,540 --> 01:42:39,910


660
01:42:39,910 --> 01:42:45,120


661
01:42:45,120 --> 01:42:50,590


662
01:42:50,590 --> 01:42:57,130


663
01:42:57,130 --> 01:43:03,460


664
01:43:03,460 --> 01:43:08,350


665
01:43:08,350 --> 01:43:14,190


666
01:43:17,680 --> 01:43:21,929


667
01:43:21,929 --> 01:43:26,250


668
01:43:26,250 --> 01:43:33,659


669
01:43:33,659 --> 01:43:37,409


670
01:43:37,409 --> 01:43:44,190


671
01:43:44,190 --> 01:43:48,570


672
01:43:48,570 --> 01:43:55,199


673
01:43:55,199 --> 01:44:00,480


674
01:44:00,480 --> 01:44:03,840


675
01:44:03,840 --> 01:44:09,540


676
01:44:09,540 --> 01:44:15,210


677
01:44:15,210 --> 01:44:19,710


678
01:44:19,710 --> 01:44:26,940


679
01:44:26,940 --> 01:44:32,330


680
01:44:32,330 --> 01:44:39,540


681
01:44:39,540 --> 01:44:47,429


682
01:44:47,429 --> 01:44:55,110


683
01:44:55,110 --> 01:45:00,150


684
01:45:00,150 --> 01:45:05,280


685
01:45:05,280 --> 01:45:14,100


686
01:45:14,100 --> 01:45:20,790


687
01:45:20,790 --> 01:45:27,960


688
01:45:27,960 --> 01:45:35,740


689
01:45:35,740 --> 01:45:44,260


690
01:45:44,260 --> 01:45:49,750


691
01:45:49,750 --> 01:45:56,410


692
01:45:56,410 --> 01:46:01,630


693
01:46:01,630 --> 01:46:06,610


694
01:46:06,610 --> 01:46:10,330


695
01:46:10,330 --> 01:46:18,550


696
01:46:18,550 --> 01:46:25,300


697
01:46:25,300 --> 01:46:29,380


698
01:46:29,380 --> 01:46:32,470


699
01:46:32,470 --> 01:46:37,000


700
01:46:37,000 --> 01:46:43,110


701
01:46:43,110 --> 01:46:52,150


702
01:46:52,150 --> 01:46:57,040


703
01:46:57,040 --> 01:47:02,740


704
01:47:02,740 --> 01:47:09,100


705
01:47:09,100 --> 01:47:13,180


706
01:47:13,180 --> 01:47:16,570


707
01:47:18,190 --> 01:47:24,790


708
01:47:24,790 --> 01:47:28,780


709
01:47:28,780 --> 01:47:35,470


710
01:47:35,470 --> 01:47:40,900


711
01:47:44,830 --> 01:47:49,590


712
01:47:52,650 --> 01:48:00,610


713
01:48:00,610 --> 01:48:07,030


714
01:48:07,030 --> 01:48:10,510


715
01:48:10,510 --> 01:48:15,490


716
01:48:15,490 --> 01:48:21,760


717
01:48:21,760 --> 01:48:25,330


718
01:48:25,330 --> 01:48:31,750


719
01:48:31,750 --> 01:48:39,639


720
01:48:39,639 --> 01:48:47,949


721
01:48:50,739 --> 01:49:03,570


722
01:49:04,040 --> 01:49:10,960


723
01:49:10,960 --> 01:49:22,960


724
01:49:25,340 --> 01:49:27,400


725
01:49:28,550 --> 01:49:34,409


726
01:49:36,000 --> 01:49:44,460


727
01:49:44,460 --> 01:49:51,929


728
01:49:51,929 --> 01:49:59,790


729
01:49:59,790 --> 01:50:06,810


730
01:50:06,810 --> 01:50:14,040


731
01:50:14,040 --> 01:50:20,010


732
01:50:20,010 --> 01:50:26,159


733
01:50:29,909 --> 01:50:37,619


734
01:50:37,619 --> 01:50:44,130


735
01:50:44,130 --> 01:50:48,300


736
01:50:48,300 --> 01:50:54,150


737
01:50:55,710 --> 01:51:03,560


738
01:51:03,560 --> 01:51:13,830


739
01:51:16,380 --> 01:51:22,190


740
01:51:25,800 --> 01:51:33,330


741
01:51:33,330 --> 01:51:38,520


742
01:51:38,520 --> 01:51:42,270


743
01:51:42,270 --> 01:51:46,680


744
01:51:46,680 --> 01:51:52,370


745
01:51:54,510 --> 01:52:01,440


746
01:52:05,190 --> 01:52:12,210


747
01:52:12,210 --> 01:52:17,670


748
01:52:17,670 --> 01:52:23,489


749
01:52:27,450 --> 01:52:41,400


750
01:52:41,400 --> 01:52:48,360


751
01:52:48,360 --> 01:52:52,380


752
01:52:52,380 --> 01:52:58,020


753
01:52:58,020 --> 01:53:05,250


754
01:53:05,250 --> 01:53:11,580


755
01:53:11,580 --> 01:53:17,100


756
01:53:17,100 --> 01:53:26,700


757
01:53:29,489 --> 01:53:34,200


758
01:53:34,200 --> 01:53:40,050


759
01:53:40,860 --> 01:53:49,750


760
01:53:49,750 --> 01:53:56,770


761
01:53:56,770 --> 01:54:03,100


762
01:54:03,100 --> 01:54:07,180


763
01:54:07,180 --> 01:54:10,870


764
01:54:10,870 --> 01:54:15,460


765
01:54:15,460 --> 01:54:20,020


766
01:54:20,020 --> 01:54:27,550


767
01:54:27,550 --> 01:54:33,160


768
01:54:33,160 --> 01:54:38,260


769
01:54:38,260 --> 01:54:42,160


770
01:54:42,160 --> 01:54:45,690


771
01:54:47,159 --> 01:54:55,480


772
01:54:55,480 --> 01:55:01,599


773
01:55:01,599 --> 01:55:05,920


774
01:55:05,920 --> 01:55:11,710


775
01:55:11,710 --> 01:55:14,949


776
01:55:14,949 --> 01:55:18,760


777
01:55:18,760 --> 01:55:23,380


778
01:55:23,380 --> 01:55:30,159


779
01:55:31,869 --> 01:55:35,949


780
01:55:35,949 --> 01:55:39,670


781
01:55:39,670 --> 01:55:46,110


782
01:55:46,110 --> 01:55:55,750


783
01:55:55,750 --> 01:56:01,630


784
01:56:01,630 --> 01:56:09,670


785
01:56:09,670 --> 01:56:15,820


786
01:56:15,820 --> 01:56:20,860


787
01:56:20,860 --> 01:56:26,560


788
01:56:26,560 --> 01:56:32,110


789
01:56:34,000 --> 01:56:39,070


790
01:56:39,070 --> 01:56:44,739


791
01:56:44,739 --> 01:56:49,599


792
01:56:49,599 --> 01:56:58,389


793
01:56:58,389 --> 01:57:02,650


794
01:57:05,170 --> 01:57:12,330


795
01:57:12,330 --> 01:57:17,800


796
01:57:17,800 --> 01:57:28,960


797
01:57:28,960 --> 01:57:36,340


798
01:57:36,340 --> 01:57:41,199


799
01:57:44,440 --> 01:57:50,469


800
01:57:50,469 --> 01:57:56,199


801
01:57:56,199 --> 01:58:00,850


802
01:58:00,850 --> 01:58:07,540


803
01:58:07,540 --> 01:58:13,900


804
01:58:13,900 --> 01:58:19,090


805
01:58:19,090 --> 01:58:25,360


806
01:58:28,210 --> 01:58:32,440


807
01:58:32,440 --> 01:58:37,960


808
01:58:37,960 --> 01:58:45,400


809
01:58:45,400 --> 01:58:49,600


810
01:58:49,600 --> 01:58:56,710


811
01:58:56,710 --> 01:59:01,030


812
01:59:01,030 --> 01:59:04,900


813
01:59:04,900 --> 01:59:11,220


814
01:59:11,220 --> 01:59:16,170


815
01:59:16,170 --> 01:59:19,470


816
01:59:19,470 --> 01:59:24,240


817
01:59:27,560 --> 01:59:34,020


818
01:59:34,020 --> 01:59:38,280


819
01:59:38,960 --> 01:59:43,260


820
01:59:43,260 --> 01:59:47,640


821
01:59:47,640 --> 01:59:55,260


822
01:59:55,260 --> 02:00:00,690


823
02:00:03,690 --> 02:00:09,810


824
02:00:09,810 --> 02:00:15,720


825
02:00:15,720 --> 02:00:18,780


826
02:00:20,760 --> 02:00:24,420


827
02:00:24,420 --> 02:00:28,350


828
02:00:28,350 --> 02:00:33,690


829
02:00:36,110 --> 02:00:44,190


830
02:00:44,190 --> 02:00:47,400


831
02:00:47,400 --> 02:00:53,820


832
02:00:53,820 --> 02:00:57,780


833
02:00:57,780 --> 02:01:02,130


834
02:01:02,130 --> 02:01:09,000


835
02:01:09,000 --> 02:01:12,870


836
02:01:12,870 --> 02:01:16,700


837
02:01:17,060 --> 02:01:21,690


838
02:01:23,760 --> 02:01:29,880


839
02:01:29,880 --> 02:01:36,870


840
02:01:36,870 --> 02:01:43,850


841
02:01:43,850 --> 02:01:49,110


842
02:01:51,570 --> 02:01:53,810


843
02:01:54,390 --> 02:01:57,690


844
02:02:01,320 --> 02:02:05,220


845
02:02:05,220 --> 02:02:12,540


846
02:02:12,540 --> 02:02:15,810


847
02:02:15,810 --> 02:02:23,190


848
02:02:30,960 --> 02:02:38,820


849
02:02:38,820 --> 02:02:44,730


850
02:02:44,730 --> 02:02:50,310


851
02:02:50,310 --> 02:02:57,830


852
02:03:00,960 --> 02:03:04,830


853
02:03:04,830 --> 02:03:09,000


854
02:03:09,000 --> 02:03:15,330


855
02:03:15,330 --> 02:03:24,480


856
02:03:24,480 --> 02:03:30,150


857
02:03:30,150 --> 02:03:33,720


858
02:03:33,720 --> 02:03:38,760


859
02:03:38,760 --> 02:03:43,350


860
02:03:43,350 --> 02:03:48,990


861
02:03:51,120 --> 02:03:57,000


862
02:03:57,000 --> 02:04:01,010


863
02:04:01,010 --> 02:04:05,640


864
02:04:05,640 --> 02:04:10,800


865
02:04:10,800 --> 02:04:20,130


866
02:04:20,130 --> 02:04:25,140


867
02:04:25,140 --> 02:04:29,820


868
02:04:31,710 --> 02:04:37,020


869
02:04:37,020 --> 02:04:43,230


870
02:04:45,540 --> 02:04:49,830


871
02:04:49,830 --> 02:04:55,830


872
02:04:55,830 --> 02:04:59,430


873
02:04:59,430 --> 02:05:03,090


874
02:05:03,090 --> 02:05:07,610


875
02:05:13,470 --> 02:05:18,600


876
02:05:18,600 --> 02:05:22,320


877
02:05:25,050 --> 02:05:29,760


878
02:05:29,760 --> 02:05:37,830


879
02:05:37,830 --> 02:05:42,300


880
02:05:42,300 --> 02:05:46,260


881
02:05:46,260 --> 02:05:55,140


882
02:05:55,140 --> 02:05:59,790


883
02:05:59,790 --> 02:06:04,010


884
02:06:04,010 --> 02:06:12,240


885
02:06:12,240 --> 02:06:16,830


886
02:06:16,830 --> 02:06:20,910


887
02:06:20,910 --> 02:06:25,830


888
02:06:25,830 --> 02:06:32,460


889
02:06:32,460 --> 02:06:36,600


890
02:06:36,600 --> 02:06:41,760


891
02:06:41,760 --> 02:06:48,989


892
02:06:50,730 --> 02:06:55,830


893
02:06:55,830 --> 02:07:00,390


894
02:07:00,390 --> 02:07:06,540


895
02:07:06,540 --> 02:07:12,270


896
02:07:12,270 --> 02:07:17,100


897
02:07:17,100 --> 02:07:23,850


898
02:07:23,850 --> 02:07:29,430


899
02:07:29,430 --> 02:07:34,260


900
02:07:34,260 --> 02:07:39,120


901
02:07:39,120 --> 02:07:44,310


902
02:07:44,310 --> 02:07:49,020


903
02:07:49,020 --> 02:07:55,170


904
02:07:55,170 --> 02:07:59,910


905
02:07:59,910 --> 02:08:10,590


906
02:08:10,590 --> 02:08:15,600


907
02:08:15,600 --> 02:08:22,810


908
02:08:22,810 --> 02:08:25,690


909
02:08:28,720 --> 02:08:36,310


910
02:08:36,310 --> 02:08:40,750


911
02:08:40,750 --> 02:08:47,980


912
02:08:47,980 --> 02:08:55,120


913
02:08:55,120 --> 02:09:01,150


914
02:09:01,150 --> 02:09:05,620


915
02:09:05,620 --> 02:09:12,070


916
02:09:12,070 --> 02:09:17,370


917
02:09:17,370 --> 02:09:21,340


918
02:09:21,340 --> 02:09:24,900


919
02:09:27,370 --> 02:09:32,200


920
02:09:32,200 --> 02:09:39,400


921
02:09:39,400 --> 02:09:42,790


922
02:09:42,790 --> 02:09:50,320


923
02:09:50,320 --> 02:09:55,030


924
02:09:55,030 --> 02:09:59,530


925
02:09:59,530 --> 02:10:03,310


926
02:10:03,310 --> 02:10:08,680


927
02:10:08,680 --> 02:10:13,030


928
02:10:15,760 --> 02:10:21,670


929
02:10:21,670 --> 02:10:26,830


930
02:10:26,830 --> 02:10:30,340


931
02:10:30,340 --> 02:10:36,460


932
02:10:36,460 --> 02:10:41,500


933
02:10:41,500 --> 02:10:47,170


934
02:10:47,170 --> 02:10:51,640


935
02:10:51,640 --> 02:10:56,560


936
02:10:56,560 --> 02:11:02,920


937
02:11:02,920 --> 02:11:07,989


938
02:11:10,620 --> 02:11:15,250


939
02:11:15,250 --> 02:11:20,530


940
02:11:20,530 --> 02:11:25,570


941
02:11:25,570 --> 02:11:32,560


942
02:11:32,560 --> 02:11:36,730


943
02:11:43,030 --> 02:11:48,670


944
02:11:48,670 --> 02:11:52,060


945
02:11:52,060 --> 02:11:55,900


946
02:11:59,500 --> 02:12:04,030


947
02:12:04,030 --> 02:12:09,100


948
02:12:09,100 --> 02:12:13,150


949
02:12:15,969 --> 02:12:19,690


950
02:12:19,690 --> 02:12:25,840


951
02:12:25,840 --> 02:12:28,930


952
02:12:28,930 --> 02:12:32,080


953
02:12:32,080 --> 02:12:36,400


954
02:12:36,400 --> 02:12:39,730


955
02:12:39,730 --> 02:12:46,780


956
02:12:46,780 --> 02:12:50,260


957
02:12:50,260 --> 02:12:56,619


958
02:12:58,599 --> 02:13:05,289


959
02:13:05,289 --> 02:13:08,170


960
02:13:08,170 --> 02:13:12,519


961
02:13:12,519 --> 02:13:17,559


962
02:13:17,559 --> 02:13:22,599


963
02:13:22,599 --> 02:13:26,650


964
02:13:28,449 --> 02:13:31,809


965
02:13:31,809 --> 02:13:36,480


966
02:13:39,010 --> 02:13:42,760


967
02:13:42,760 --> 02:13:46,239


968
02:13:47,829 --> 02:13:51,699


969
02:13:51,699 --> 02:13:57,670


970
02:13:57,670 --> 02:14:03,940


971
02:14:06,150 --> 02:14:12,579


972
02:14:12,579 --> 02:14:16,000


973
02:14:16,000 --> 02:14:19,329


974
02:14:19,329 --> 02:14:23,199


975
02:14:23,199 --> 02:14:27,820


976
02:14:27,820 --> 02:14:32,819


