1
00:00:00,149 --> 00:00:03,600
welcome back everybody
[Music]

2
00:00:03,600 --> 00:00:09,599
I'm sure you've noticed but there's been
a lot of cool activity on the forum this

3
00:00:09,599 --> 00:00:25,920
week and one of the things that's been
really great to see is that a lot of you
have started creating really helpful
materials both for your classmates to
better understand stuff and also for you
to better understand stuff by trying to
teach what you've learned I just wanted

4
00:00:25,920 --> 00:00:32,340
to highlight a few i've actually posted
to the wiki thread of a few of these but

5
00:00:32,340 --> 00:00:42,480
there's there's lots more Russian has
posted a whole bunch of nice
introductory tutorials so for example if
you're having any trouble getting
connected with AWS she's got a whole

6
00:00:46,079 --> 00:00:57,750
step-by-step how to go about logging in
and getting everything working which i
think is a really terrific thing and so
it's a kind of thing that if you are

7
00:00:57,750 --> 00:01:05,460
writing some notes for yourself to
remind you how to do it
you may as well post them for others to
do it as well and by using a markdown
file like this and it's actually good

8
00:01:07,439 --> 00:01:13,789
practice if you haven't used github
before if you put it up on github
everybody can now use it or of course
you can just put it in the forum so more

9
00:01:13,889 --> 00:01:25,140
advanced a thing that Reshma wrote up
about is she noticed that I like using
Tmax which is a handy little thing which

10
00:01:25,140 --> 00:01:39,869
lets me lets me basically have a window
I'll show you so as soon as I log into
my computer if I run Tmax you'll see
that all of my windows pop straight up

11
00:01:39,869 --> 00:01:48,210
basically and I can like continue
running stuff in the background and I
can like I've got them over here and I
can kind of zoom into it or I can move

12
00:01:48,210 --> 00:01:53,579
over to the top which is here so Jupiter
colonel running and so forth so if that

13
00:01:53,579 --> 00:02:03,060
sounds interesting
Reshma has a tutorial here on how you
can use two maps and it's actually got a
whole bunch of stuff in her github so

14
00:02:03,060 --> 00:02:28,810
that's that's really cool I built among
has written a very nice kind of summary
basically of our last lesson
which kind of covers what are the key
things we did and why did we do them so
if you are a kind of
wondering like how does it fit together
I think this is a really helpful summary
like what if those couple of hours look

15
00:02:29,610 --> 00:02:45,069
like if we summarize it all into a page
or two I also really like Pavel has dad
kind of done a deep dive on the learning
rate finder which is a topic that a lot

16
00:02:45,069 --> 00:02:59,500
of you have been interested in learning
more about particularly those of you who
have done deep learning before I
realized that this is like a solution to
a problem that you've been having for a
long time and haven't seen before and so
it's kind of something which hasn't
really been blogged about before so this

17
00:02:59,500 --> 00:03:13,560
is the first I've seen it's logged about
so when I put this on Twitter a link to
pebbles post it's been shared you know
hundreds of times it's been really
really popular and viewed many thousands
of times so that's some great content

18
00:03:13,560 --> 00:03:20,779
radec has posted lots of cool stuff I
really like this practitioners guide to
apply torch which again this is more for

19
00:03:20,879 --> 00:03:32,260
more advanced students but it's like
digging into people who have never used
hi torch before but know a bit about
numerical programming in general and
it's a quick introduction to how high
torch is different and then there's been

20
00:03:35,560 --> 00:03:40,624
some interesting little bits of research
like what's the relationship between
learning rate and batch sites so one of

21
00:03:40,724 --> 00:03:46,030
the students actually asked me this
before class and I said oh well one of
the other students has written an

22
00:03:46,030 --> 00:04:01,235
analysis of exactly that so what he's
done is basically looked through and
tried different batch sizes and
different learning rates and tried to
see how they seemed to relate together
and these are all like cool experiments
which you know you can try yourself

23
00:04:01,335 --> 00:04:15,586
I predict again he's written something
again a kind of a research into this
question I made a claim that the the
stochastic gradient descent with
restarts finds more generalizable parts
of the function surface because they're

24
00:04:15,686 --> 00:04:25,147
kind of flatter and he's been trying to
figure out is there a way to measure
that more directly not quite successful
yet but a really interesting piece of

25
00:04:25,247 --> 00:04:32,999
research got some introductions to
convolutional neural networks and then

26
00:04:32,999 --> 00:04:57,059
something that we'll be learning about
towards the end of this course but I'm
sure you've noticed we're using
something called ResNet and a nonce aha
actually posted a pretty impressive
analysis of like watts arrest net and
why is it interesting and this one's
actually being very already shared very
widely around the internet I've seen
also so some more advanced students who
are interested in jumping ahead can look

27
00:04:57,059 --> 00:05:04,349
at that and uphill Tamang also has done
something similar so lots of yeah lots
of stuff going on on the forums

28
00:05:06,059 --> 00:05:17,479
I'm sure you've also noticed we have a
beginner forum now specifically for you
know asking questions which you know it
is always the case that there are no
dumb questions but when there's lots of

29
00:05:19,860 --> 00:05:29,819
people around you talking about advanced
topics it might not feel that way so
hopefully the beginners forum is just a
less intimidating space and if there are

30
00:05:29,819 --> 00:05:38,632
more advanced student who can help
answer those questions please do but
remember when you do answer those
questions try to answer in a way that's
friendly to people that maybe you know

31
00:05:38,732 --> 00:05:47,934
have no more than a year of programming
experience you haven't done any machine
learning before so you know I hope other

32
00:05:48,034 --> 00:05:56,159
people in the class feel like you can
contribute as well and just remember all
of the people we just looked at or many

33
00:05:56,159 --> 00:06:04,769
of them I believe have never hosted
anything to the internet before right I
mean you don't have to be a particular
kind of person to be allowed to block

34
00:06:04,769 --> 00:06:20,399
something you can just jot down your
notes throw it up there and one handy
thing is if you just put it on the forum
and you're not quite sure of some of the
details then then you know you have an
opportunity to get feedback and say like

35
00:06:20,399 --> 00:06:29,439
oh well that's not quite how that works
you know actually it works this way
instead or oh that's a really
interesting insight have you thought
about taking this further and so forth

36
00:06:29,539 --> 00:06:38,969
so what we've done so far is a kind of
an injury an introduction as a just as a
practitioner to convolutional neural

37
00:06:38,969 --> 00:06:59,319
networks for images and we haven't
really talked much at all about
the theory or why they work or the math
of them but on the other hand what we
have done is seen how to build a model
which actually works exceptionally well
compact world-class level models and

38
00:06:59,319 --> 00:07:12,280
we'll kind of review a little bit of
that today and then also today we're
going to dig in a little quite a lot
more actually into the underlying theory
of like what is a what is a CNN what's a

39
00:07:12,280 --> 00:07:16,000
convolution how does this work and then
we're going to kind of go through this

40
00:07:16,000 --> 00:07:25,930
this cycle where we're going to dig
we're going to do a little intro into a
whole bunch of application areas using
neural nets for structured data so kind

41
00:07:25,930 --> 00:07:31,449
of like logistics or forecasting or you
know financial data or that kind of

42
00:07:31,449 --> 00:07:45,550
thing and then looking at language
applications and LP applications using
recurrent neural Nets and then
collaborative filtering for
recommendations and systems and so these

43
00:07:45,550 --> 00:07:53,020
will all be like similar to what we've
done for cnn's for images would be like
here's how you can get a
state-of-the-art result without digging
into the theory but but knowing how to

44
00:07:55,150 --> 00:08:01,984
actually make a work and then we're kind
of going to go back through those almost
in reverse order so then we're going to

45
00:08:02,084 --> 00:08:09,949
dig right into collaborative filtering
in a lot of detail and see how how to
write the code underneath and how the
math works underneath and then we're

46
00:08:10,049 --> 00:08:20,620
going to do the same thing for the
structured data analysis we're going to
do the same thing for comp nets for
images and finally an in depth deep dive
into apparent neural networks so that's

47
00:08:20,620 --> 00:08:37,078
kind of where we're okay so let's start
by doing a little bit of a review and I
want to also provide a bit more detail
on some on some steps that we only
briefly slipped over so I want to make

48
00:08:37,178 --> 00:08:55,889
sure that we're all able to complete
kind of last week's assignment which was
that the dog breeze I mean to basically
apply what you've learned with another
data set and I thought the easiest one
to do with me the dog breeds cattle
competition and so I want to make sure
everybody has everything you need to do
this right now so and the

49
00:08:55,889 --> 00:09:11,339
first thing is to make sure that you
know how to download data and so there's
there's two main places at the moment
we're kind of downloading data from one
is from cattle and the other is from
like anywhere else and so I'll first of

50
00:09:11,339 --> 00:09:37,960
all do the the casual version so to
download from cattle we use something
called cattle CLI which is gear and to
install what I think it's already in the
system will shake yeah so it should
already be in your environment but to

51
00:09:38,060 --> 00:09:53,279
make sure one thing that happens is
because this is downloading from the
cattle website through experience rating
every time cap will change us the
website it breaks so anytime you try to
use it and if the cattles websites
changed recent when you'll need to make

52
00:09:53,279 --> 00:10:13,434
sure you get the most recent version so
you can always go to pip install cable -
CL I - - upgrade and so that'll just
make sure that you've got the latest
version of of it and everything that it
depends on okay and so then having done

53
00:10:13,534 --> 00:10:25,765
that you can follow the instructions
actually I think Reshma was kind enough
to they go there's a cable CLI you know
like everything you need to know can be
under Reshma's github so basically to do

54
00:10:25,865 --> 00:10:43,509
that at the next step you go kg download
and then you provide your username with
- you you provide your password with - P
and then - see it the competition name

55
00:10:43,609 --> 00:11:02,100
and a lot of people in the forum is
being confused about what to enter here
and so the key thing to note is that
when you're at a capital competition
after the /c there's a specific name
planet - understanding - etcetera right
that's the name you need okay the other

56
00:11:02,100 --> 00:11:12,650
thing you'll need to make sure is that
you've on your own computer have
attempted to click download at least
once because when you do
ask you to accept the rules if you've

57
00:11:12,750 --> 00:11:18,850
forgotten to do that kg download will
give you a hint it'll say oh it looks
like you might have forgotten the rules

58
00:11:18,850 --> 00:11:31,085
if you log into cattle with like a
Google account like anything other than
a username password this won't work so
you'll need to click forgot password on
Kaggle and get them to send you a normal
password so that's the cattle version

59
00:11:31,185 --> 00:11:38,775
right and so when you do that you end up
with a whole folder created for you with
all of that competition and data in it

60
00:11:38,875 --> 00:11:52,600
so a couple of reasons you might want to
not use that the first years that you're
using a data set that's not on cattle
the second is that you don't want all of
the data sets in a cattle competition

61
00:11:52,600 --> 00:12:11,890
for example the planet competition that
we've been looking at a little bit we'll
look at again today has data in two
formats TIFF and JPEG the TIFF is 19
gigabytes and the JPEG is 600 megabytes
so you probably don't want to download
both so I'll show you a really cool kit

62
00:12:11,890 --> 00:12:32,480
which actually somebody on the forum
taught me I think was one of the young
MSN students here at USF there's a
Chrome extension cord curl W get so you
can just search for a curl W get and
then you install it by just clicking on
installed and having an extension before

63
00:12:32,580 --> 00:13:02,944
and then from now on every time you try
to download something so I'll try and
download this file
and I'll just go ahead and cancel it
right and now you see this little yellow
button that's added up here there's a
whole command here right so I can copy
that and paste it into my window and hit
go and it's there cuz okay so what that

64
00:13:03,044 --> 00:13:10,449
does is like all of your cookies and
headers and everything else needed to
download that file is like say so this

65
00:13:10,449 --> 00:13:19,420
is not just useful for downloading data
it's also useful if you like trying to
download some I don't know TV show or
something anything where you're it's

66
00:13:19,420 --> 00:13:33,610
hidden behind a login or something you
can you can grab it and actually that is
very useful for data science because
quite often we want to analyze things
like videos on our on our consoles so
this is a good trick alright so there's

67
00:13:33,610 --> 00:13:44,019
two ways to get the data so then having
got the data you then need to build your

68
00:13:44,019 --> 00:13:57,670
model right so what I tend to do like
you'll notice that I tend to assume that
the data is in a directory called data
that's a subdirectory of wherever your
notebook is right now you don't

69
00:13:57,670 --> 00:14:04,990
necessarily actually want to put your
data there you might want to put it
directly in your home directory or you
might wanna put it on another drive or

70
00:14:04,990 --> 00:14:17,949
whatever so what I do is if you look
inside my courses do one folder you'll
see that data is actually a symbolic
link to a different drive alright so you

71
00:14:17,949 --> 00:14:24,109
can put it anywhere you like and then
you can just add a symbolic link or you
can just put it there directly it's up

72
00:14:24,209 --> 00:14:35,829
to you if you haven't used symlinks
before they're like aliases or shortcuts
on the mac or windows very handy and
there's some threads on the forum about
how to use them if you want help with

73
00:14:35,829 --> 00:14:45,490
that that for example is also how we
actually have the fast AI modules
available from the same place as our
notebooks it's just a symlink

74
00:14:45,490 --> 00:15:06,845
to where they come from anytime you want
to see like where things actually point
to in Linux you can just use the
- L flag - listing a directory and
that'll show you where the symlinks
exist still lost I'll show you which
scenes the directories so forth okay so

75
00:15:06,945 --> 00:15:20,315
one thing which may be a little unclear
based on what we've done so far is like
how little code you actually need to do
this end to it so what I've got here is

76
00:15:20,415 --> 00:15:28,292
is in a single window is an entire
end-to-end process to get a
state-of-the-art result put cats versus
dogs all right I've only step I've

77
00:15:28,392 --> 00:15:36,197
skipped is the bit where we've
downloaded it from title and then where
we unzip it all right so these are

78
00:15:36,297 --> 00:15:51,130
literally all the steps and so we import
our libraries and actually if you import
this one Kampf loner that basically
imports everything else so that's that

79
00:15:51,130 --> 00:15:57,860
we need to tell at the path of where
things are the size that we want the
batch size that we want right so then

80
00:15:57,960 --> 00:16:10,940
and we're going to learn a lot more
about what these do very shortly but
basically we say how do we want to
transform our data so we want to
transform it in a way that's suitable to
this particular kind of model and it

81
00:16:11,040 --> 00:16:18,490
assumes that the photos aside on photos
and that we're going to zoom in up to
ten percent each time we say that we

82
00:16:18,490 --> 00:16:34,360
want to get some data based on paths and
so remember this is this idea that
there's a path called cats and the path
called dogs and they're inside a path
called train and a path called valid
note that you can always overwrite these

83
00:16:34,360 --> 00:16:45,170
with other things so if your things are
in different folders
you could either rename them or you can
see here there's like a train name and a
bowel name you can always pick something

84
00:16:45,270 --> 00:16:56,380
else here also notice there's a test
name so if you want to submit some into
cattle you'll need to fill in the name
the name of the folder where the test
sentence and obviously those those won't
be labeled

85
00:16:59,770 --> 00:17:08,205
so then we create a model from a
pre-training model it's from a resonant
50 model using this data and then we
call fit and remember by default that

86
00:17:08,305 --> 00:17:17,420
has all of the layers but the last few
frozen and again we'll learn a lot more
about what that means

87
00:17:17,420 --> 00:17:23,110
and so that's that's what that does so
that that took two and a half minutes
notice here I didn't say pre-compute

88
00:17:26,119 --> 00:17:35,390
equals true again there's been some
confusion on the forums about like what
that means it's only a is only something
that makes it a little faster for this

89
00:17:35,390 --> 00:17:42,325
first step right so you can always skip
it and if you're at all confused about
it or it's causing you any problems just

90
00:17:42,425 --> 00:17:52,635
leave it off right because it's just a
it's just a shortcut which caches some
of the intermediate steps that don't
have to be recalculating each time and

91
00:17:52,735 --> 00:18:11,180
remember that when we are using pre
computed activations data or
augmentation doesn't work right so even
if you ask for a data augmentation if
you've got pre compute equals true it
doesn't actually do any data
augmentation because it's using the
cached non augmented activations so in

92
00:18:11,180 --> 00:18:14,780
this case to keep this as simple as
possible I have no pre computed anything

93
00:18:14,780 --> 00:18:22,810
going on so I do three cycles of length
one and then I can then unfreeze so it's

94
00:18:23,110 --> 00:18:27,190
now going to train the whole thing

95
00:18:27,590 --> 00:18:39,170
something we haven't seen before and
we'll learn about in the second half is
called B and freeze for now all you need
to know is that if you're using a model
like a bigger deeper model like ResNet

96
00:18:39,170 --> 00:18:49,005
50 or rez next 101 on a data set that's
very very similar to imagenet like these
cat sandbox data set sort of words it's

97
00:18:49,105 --> 00:19:02,495
like sidon photos of standard objects
you know of a similar size to image turn
and money somewhere between 200 and 500
pixels
you should probably add this line when
you unfreeze for those of you that are

98
00:19:02,595 --> 00:19:10,630
more advanced what it's doing is it's
causing the batch normalization moving
averages to not be updated

99
00:19:10,730 --> 00:19:20,409
but in the second half of this course
we're gonna learn all about why we do
that it's something that's not supported
by any other library but it turns out to
be super important anyway so we do one

100
00:19:20,409 --> 00:19:31,789
more
epoch with training the whole network
and then at the end we use test time
augmentation to ensure that we get the

101
00:19:31,889 --> 00:19:37,909
best predictions we can and that gives
us ninety nine point four five percent

102
00:19:38,009 --> 00:19:47,139
so that's that's it right so when you
try a new data set they're basically the
minimum set of steps that you would need

103
00:19:47,139 --> 00:20:02,799
to follow you'll notice this is assuming
I already know what learning wrote to
use so you'd use the learning rate
finder for that it's assuming that I
know that the directory layout and so
forth so that's kind of a minimum set

104
00:20:02,799 --> 00:20:15,499
now one of the things that I wanted to
make sure you had an understanding of
how to do is how to use other libraries
other than fast AI and so I feel like
the best thing to to look at is to look
at care us because care us is a library

105
00:20:15,599 --> 00:20:33,059
just like fast AI sits on top of pi
torch care us sits on top of actually a
whole variety of different backends it
fits mainly people nowadays use it with
tensorflow
there's also an MX net version there's

106
00:20:33,159 --> 00:20:37,450
also a Microsoft CNT K version so what

107
00:20:37,450 --> 00:20:51,024
I've got if you do a git pull you'll see
that there's a something called care us
less than one where I've attempted to
replicate at least parts of lesson one
in care us just to give you a sense of

108
00:20:51,124 --> 00:20:59,510
how that works I'm not going to talk
more about batch two norm freeze now

109
00:21:00,850 --> 00:21:11,950
other than to say if you're using
something which has got a number larger
than 34 at the end
so like ResNet 50 or res next 101 and

110
00:21:11,950 --> 00:21:24,520
you're trading a data set that has that
is very similar to image net so it's
like normal photos of normal sizes where
the thing of interest takes up most of
the frame then you probably should

111
00:21:24,520 --> 00:21:32,989
at the end fries true after unfreeze
if in doubt try trading it with and then
try trading it without more advanced

112
00:21:33,089 --> 00:21:39,910
students will certainly talk about it on
the forums this week and we will be
talking about the details of it in the

113
00:21:39,910 --> 00:21:46,720
second half of the course when we come
back to our CNN in death section in the

114
00:21:50,820 --> 00:22:01,480
second last lesson so with care us again
we import a bunch of stuff and remember

115
00:22:01,480 --> 00:22:12,151
I mentioned that this idea that you've
got a thing called train and a thing
called valid and inside that you've got
a thing called dogs and things called
cats is a standard way of providing
image labelled images so Karis does that

116
00:22:12,251 --> 00:22:22,805
too right so it's going to tell it where
the training set and the validation set
are twice what batch size to used now

117
00:22:22,905 --> 00:22:36,575
you'll notice in Karis we need much much
much more code to do the same thing more
importantly each part of that code has
many many many more things you have to

118
00:22:36,675 --> 00:22:44,650
set and if you set them wrong everything
breaks right so I'll give you a summary
of what they are

119
00:22:44,650 --> 00:22:55,030
so you're basically rather than creating
a single data object in chaos we first
of all have to define something called a
data generator to say kind of generate

120
00:22:55,030 --> 00:23:10,634
the data and so a data generator we
basically have to say what kind of data
augmentation we want to do and we also
we actually have to say what kind of
normalization do we want to do

121
00:23:10,734 --> 00:23:15,619
so we're else with fast AI we just say
whatever ResNet 50 requires just do that

122
00:23:15,719 --> 00:23:21,700
for me please we actually have to kind
of know a little bit about what's
expected of us

123
00:23:21,700 --> 00:23:30,220
generally speaking copying and pasting
cos code from the internet is a good way
to make sure you've got the right the
right stuff to make that work and again

124
00:23:30,220 --> 00:23:43,550
it doesn't have a kind of a standard set
of like here the best data augmentation
parameters to use for photos so you know
I've copied and pasted all of this from
the Kaos documentation so I don't know

125
00:23:43,550 --> 00:23:49,140
if it's I don't think it's the best set
to use it all but it's the set that
they're using in their docks so having

126
00:23:49,240 --> 00:23:56,540
said this is how I want to generate data
so horizontally fit sometimes you know
zoom sometimes sheer sometimes we then

127
00:23:56,540 --> 00:24:03,102
create a generator from that by taking
that data generator and saying I want to
generate images by looking from a

128
00:24:03,202 --> 00:24:17,985
directory we pass in the directory which
is of the same directory structure that
fast AI users and you'll see there's
some overlaps with kind of how fast AI
works here you tell it what size images
you want to create you tell at what
batch size you want in your mini batches

129
00:24:18,085 --> 00:24:31,650
and then there's something you not to
worry about too much but basically if
you're just got two possible outcomes
you would generally say binary here if
you've got multiple possible outcomes
would say categorical yeah so we've only

130
00:24:31,750 --> 00:24:36,525
got cats or dogs
so it's binary so an example of like

131
00:24:36,625 --> 00:24:48,048
where things get a little more complex
is you have to do the same thing for the
validation set so it's up to you to
create a data generator that doesn't
have data augmentation because obviously
for the validation set unless you're

132
00:24:48,148 --> 00:25:00,760
using t/ta that's going to start things
up you also when you train you randomly
reorder the images so that they're
always shown in different orders to make
it more random but with a validation

133
00:25:00,860 --> 00:25:11,270
it's vital that you don't do that
because if you shuffle the validation
set you then can't track how well you're
doing it's in a different order for the

134
00:25:11,270 --> 00:25:18,465
labels
that's a basically these are the kind of
steps you have to do every time with

135
00:25:18,565 --> 00:25:32,750
care us so again the reason I was using
ResNet 50 before is chaos doesn't have
ResNet 34 unfortunately so I just wanted
to compare like with Mike so we're going
to use resident 50 here there isn't the

136
00:25:32,750 --> 00:25:42,290
same idea with chaos of saying like
constructor model that is suitable for
this data set for me so you have to do
it by hand right so the way you do it is

137
00:25:42,290 --> 00:25:50,600
to basically say this is my base model
and then you have to construct on top of
that manually the layers that you want
to add

138
00:25:50,600 --> 00:26:01,370
and so by the end of this course you'll
understand a way it is that these
particular three layers other layers
that we add so having done that in chaos

139
00:26:01,370 --> 00:26:06,070
you basically say okay this is my model
and then again there isn't like a

140
00:26:06,070 --> 00:26:19,515
concept to it like automatically
freezing things or an API for that so
you just have to allow look through the
layers that you want to freeze and call
dot trainable equals false on them in

141
00:26:19,615 --> 00:26:27,110
Karis there's a concept we don't have in
fast AI or play a torch of compiling a
model so basically once your model is
ready to use you have to compile it

142
00:26:28,160 --> 00:26:35,360
passing in what kind of optimizer to use
what kind of loss to look for about
metric so again with fast AI you don't

143
00:26:36,439 --> 00:26:47,324
have to pass this in because we know
what loss is the write loss to use you
can always override it but for a
particular model we give you good
defaults okay so having done all that

144
00:26:47,424 --> 00:26:55,886
rather than calling fit you call
generator passing in those two
generators that you saw earlier the
Train generator in the validation

145
00:26:55,886 --> 00:27:06,681
generator for reasons I don't quite
understand chaos expects you to also
tell it how many batches there are per
epoch so the number of batches is a
quarter the size of the generator

146
00:27:07,081 --> 00:27:13,680
divided by the batch size you can tell
it how many epochs just like in fast AI

147
00:27:15,100 --> 00:27:35,639
you can say how many processes or how
many workers to use for pre-processing
[Music]
unlike fast AI the default in chaos is
basically not to use any so to get good
speed you're going to make sure you
include this and so that's basically

148
00:27:35,739 --> 00:27:43,469
enough to start fine tuning the last
layers so as you can see I got to a

149
00:27:43,569 --> 00:27:53,805
validation accuracy of 95% but as you
can also see something really weird
happened we're after one it was like 49
and then it was 69 and then 95 I don't

150
00:27:53,905 --> 00:28:01,115
know why these are so low that's not
normal
I may have there may be a bug and chaos
they may be a bug in my code I reached

151
00:28:01,215 --> 00:28:07,469
out on Twitter to see if
anybody could figure it out but they
couldn't I guess this is one of the

152
00:28:07,569 --> 00:28:15,509
challenges with using something like
this is one of the reasons I wanted to
use fast AI for this course is it's much
harder to screw things up so I don't

153
00:28:15,609 --> 00:28:18,806
know if I screwed something up or
somebody else did

154
00:28:18,906 --> 00:28:23,270
yes you know this is you've seen the
chance to float back end yeah yeah and

155
00:28:23,500 --> 00:28:50,279
if you want to run this to try it out
yourself you just can just go pip
install tensorflow - GPU Kerris okay
because it's not part of the faster I
environment by default but that should
be all you need to do to get that

156
00:28:50,379 --> 00:29:08,539
working so then there isn't a concept of
like layer groups or differential
learning rates or partial unfreezing or
whatever so you have to decide like I
had to print out all of the layers and
decide manually how many I wanted to
fine-tune so I decided to fine-tune

157
00:29:08,539 --> 00:29:17,210
everything from a layer 140 onwards so
that's why I just looked through like
this after you change that you have to
recompile the model and then after that

158
00:29:17,210 --> 00:29:26,924
I then ran another step and again I
don't know what happened here the
accuracy of the training set stayed
about the same but the validation set
totally fill in the hole and I mean the

159
00:29:27,024 --> 00:29:40,062
main thing to notice even if we put
aside the validation set we're getting I
mean I guess the main thing is there's a
hell of a lot more code here which is
kind of annoying but also the
performance is very different so where

160
00:29:40,162 --> 00:29:49,934
else is here even on the training set
we're getting like 97% after four epochs
that took a total of about eight minutes

161
00:29:50,034 --> 00:30:02,380
you know over here we had 99.5% on the
validation set and it ran a lot faster
so I was like four or five minutes right

162
00:30:02,380 --> 00:30:17,620
so
depending on what you do particularly if
you end up wanting to deploy stuff to
mobile devices at the moment the kind of
PI torch on mobile situation is very

163
00:30:17,620 --> 00:30:24,365
early so you may find yourself wanting
to use tensorflow or you may work for a
company that's kind of settled on

164
00:30:24,465 --> 00:30:37,000
tensorflow so if you need to convert
something like redo something you've
learnt here intensive flow you probably
want to do it with care us but just
recognize you know it's got to take a

165
00:30:37,000 --> 00:30:53,690
bit more work to get there and by
default it's much harder to get I mean I
to get the same state of the out results
you get the faster I you'd have to like
replicate all of the state-of-the-art
algorithms that are in first a nice so
it's hard to get the same level of

166
00:30:54,669 --> 00:31:05,990
results but you can see the basic ideas
are similar okay and it's certainly it's
certainly possible you know like there's
nothing I'm doing in fast AI that like
would be impossible but like you'd have

167
00:31:06,090 --> 00:31:20,389
to implement stochastic gradient percent
with restarts you would have to
implement differential learning rates
you would have to implement batch norm
freezing which you probably don't want
to do I know and well that's not quite

168
00:31:20,489 --> 00:31:29,199
true I think somewhat one person at
least on the forum is attempting to
create a chaos compatible version of or
a tensorflow compatible version of fast
AI which I think I hope will get there I

169
00:31:29,299 --> 00:31:43,270
actually spoke to Google about this a
few weeks ago and they're very
interested in getting faster i ported to
tensorflow so maybe by the time you're
looking at this on the mooc maybe that
will exist I certainly hope so we will

170
00:31:43,270 --> 00:32:05,900
see hey wait
so Karis is Karis intensive flow was
certainly not you know
that difficult to handle and so I don't
think you should worry if you're told
you have to learn them after this course
for some reason even let me take you a
couple of days I'm sure so that's kind

171
00:32:06,000 --> 00:32:19,890
of most of the stuff you would need to
kind of complete this this kind of
assignment from last week which was like
try to do everything you've seen already
but on the dog of reinstated said just

172
00:32:19,890 --> 00:32:26,600
to remind you that kind of last few
minutes of last week's lesson I show you
how to do much of that including like

173
00:32:30,870 --> 00:32:37,885
how I actually explored the data to find
out like what the classes were and how
big the images were and stuff like that

174
00:32:37,985 --> 00:32:41,980
right so if you've forgotten that or
didn't quite follow at all last week
check out the video from last week to

175
00:32:43,080 --> 00:32:53,794
see one thing that we didn't talk about
is how do you actually submit to Carol
so how do you actually get predictions
so I just wanted to show you that last

176
00:32:53,894 --> 00:33:01,500
piece as well and on the wiki thread
this week I've already put a little
image of this to show you these days but

177
00:33:01,500 --> 00:33:12,955
if you go to the kaggle website for
every competition there's a section
called evaluation and they tell you what
it's a bit and so I just copied and
pasted these two lines from from there

178
00:33:13,055 --> 00:33:19,810
and so it says we're expected to submit
a file where the first line contains the

179
00:33:20,410 --> 00:33:31,860
the work the word ID and then a comma
separated list of all of the possible
dog breeds and then every line after
that will contain the idea itself
followed by all the probabilities of all
the different dog breeds so how do you

180
00:33:35,610 --> 00:33:55,240
create that so recognize that inside our
data object there's a dot classes which
has got in alphabetical order all of the
four other classes and then so it's got
all of the different classes and then

181
00:33:55,340 --> 00:34:01,645
inside data dot test data set just yes
you can also see there's all the file
names so and just to remind you dogs and

182
00:34:01,745 --> 00:34:18,444
cats
sorry
cats dog breeds was not provided in the
kind of care our style format where the
dogs and cats from different folders but
instead it was provided as a CSV file of

183
00:34:18,544 --> 00:34:27,859
labels right so when you get a CSV file
of labels you use image classifier data
from CSV rather than image classifier
data from parts there isn't an

184
00:34:31,799 --> 00:34:40,139
equivalent in care us so you'll see like
on the cattle forums people share
scripts for how to convert it to a care
our style folders but in our case we

185
00:34:40,139 --> 00:34:47,574
don't have to we just go image
classifier data from CSV passing in that
CSV file and so the CSV file will you

186
00:34:47,674 --> 00:35:03,740
know has automatically told the data you
know what the masses are and then also
we can see from the folder of test
images what the file names of those are
so with those two pieces of information

187
00:35:03,740 --> 00:35:28,789
we're ready to go so I always think it's
a good idea to use TTA as you saw with
that dogs and cats example just now it
can really improve things particularly
when your model is less good so I can
say learn dot t ta and if you pass in
yeah if you pass in is test equals true

188
00:35:28,889 --> 00:35:35,369
then it's going to give you predictions
on the test set rather than the
validation set okay

189
00:35:35,369 --> 00:35:44,765
and now obviously we can't now get an
accuracy or anything because by
definition we don't know the labels for

190
00:35:44,865 --> 00:35:57,099
the test set right so by default most
high-touch models give you back the log
of the predictions so then we just have
to go X of that to get back out

191
00:35:57,199 --> 00:36:10,529
probabilities so in this case the test
set had ten thousand three hundred and
fifty seven images in it and there are
120 possible breeds all right so we get
back a matrix of of that size and so we

192
00:36:10,529 --> 00:36:17,695
now need to turn that into something
that looks like this
and so the easiest way to do that is

193
00:36:17,795 --> 00:36:27,472
with pandas if you're not familiar with
pandas there's lots of information
online about it or check out the machine
learning course intro to machine
learning that we have where we do lots
of stuff with pandas but basically we

194
00:36:27,572 --> 00:36:41,007
can describe PD data frame and pass in
that matrix and then we can say the
names of the columns are equal to data

195
00:36:41,607 --> 00:36:57,745
duck classes and then finally we can
insert a new column at position 0 called
ID that contains the file names but
you'll notice that the file names
contain five letters at the end I start
we don't want and four letters at the
end we don't want so I just subset in

196
00:36:57,845 --> 00:37:06,550
like so right so at that point I've got
a data frame that looks like this which

197
00:37:06,650 --> 00:37:24,000
is what we want so you can now call a
data frame data so social cues dated DF
not des let's fix it now data frame okay

198
00:37:24,000 --> 00:37:36,680
so you can now call data frame to CSV
and quite often you'll find these files
actually get quite big so it's a good
idea to say compression equals gzip and
that'll zip it up on the server for you

199
00:37:36,780 --> 00:37:45,082
and that's going to create a zipped up
CSV file on the server on wherever
you're running is Jupiter notebook so

200
00:37:45,182 --> 00:37:57,235
you need apps that you now need to get
that back to your computer so you can
upload it or you can use carol CLA so
you can type kgs submit and do it that
way

201
00:37:57,335 --> 00:38:04,170
I generally download it to my computer
so like how often back to this lab
double check it all looks ok so to do

202
00:38:04,170 --> 00:38:27,130
that there's a cool little theme called
file link and if you run file link with
a path on your server it gives you back
a URL which you can click on and it will
download that file from the server onto
your computer so if I click on that now
I can go ahead and save it
and then I can see in my downloads

203
00:38:33,490 --> 00:38:43,025
there it is here's my submission file
they want to open their yeah and as you

204
00:38:43,125 --> 00:38:54,095
can see it's exactly what I asked for
there's my ID and 120 different dog
breeds and then here's my first row
containing the file name and the 120
different probabilities okay so then you

205
00:38:54,195 --> 00:39:00,695
can go ahead and submit that to cattle
through their through their regular form

206
00:39:00,795 --> 00:39:15,160
and so this is also a good way you can
see we've now got a good way of both
grabbing any file off the internet and
getting a to our AWS instance or paper
space or whatever by using the cool
little extension in chrome and we've

207
00:39:17,170 --> 00:39:31,270
also got a way of grabbing stuff off our
server easily those of you that are more
command line oriented you can also use
SCP of course but I kind of like doing
everything through the notebook all

208
00:39:31,270 --> 00:39:42,910
right
one other question I had during the week
was like what if I want to just get a
single a single file that I want to you

209
00:39:42,910 --> 00:39:49,790
know get a prediction for so for example
you know maybe I want to get this first
file from my validation set

210
00:39:49,890 --> 00:39:59,350
so there's its name so you can always
look at a file just by calling image dot
open that just uses regular - imaging

211
00:39:59,350 --> 00:40:16,230
library and so what you can do is
there's actually I'll show you the
shortest version you can just call learn
predict array passing in your your image

212
00:40:16,230 --> 00:40:21,390
okay now the image needs to have been
transformed

213
00:40:21,390 --> 00:40:36,310
so you've seen transform Trent
transforms from model before normally we
just put put it all in one variable but
actually behind the scenes it was
returning to things
it was returning training transforms and
validation transforms so I can actually

214
00:40:36,310 --> 00:40:45,280
split them apart and so here you can see
I'm actually applying example my
training transforms or probably more
likely that would apply validation

215
00:40:45,280 --> 00:40:52,420
transforms that gives me back an array
containing the image the transformed
image which I can then past

216
00:40:55,440 --> 00:41:05,320
everything that gets passed to or
returned from bottles is generally
assumed to be a mini batch right it's
generally assumed to be a bunch of

217
00:41:05,320 --> 00:41:15,250
images so we'll talk more about some
numpy tricks later but basically in this
case we only have one image so we have
to turn that into a mini batch of images

218
00:41:15,250 --> 00:41:21,490
so in other words we need to create a
tensor that basically is not just rows

219
00:41:21,490 --> 00:41:26,230
by columns by channels but it's number
of image by rows by columns by channels

220
00:41:26,230 --> 00:41:31,330
and as one image so it's basically
becomes a 4 dimensional tensor so

221
00:41:31,330 --> 00:41:46,480
there's a cool little trick in numpy
that if you index into an array with
none that basically adds additional unit
access to the start so it turns it from
an image into a mini batch of one images
and so that's why we had to do that so

222
00:41:46,480 --> 00:42:02,080
if you basically find you're trying to
do things with a single image with any
kind of Pi torch or fast AI thing this
is just something you might you might
find it says like expecting four
dimensions only got three it probably

223
00:42:02,080 --> 00:42:15,040
means that or if you get back a return
value from something that has like some
weird first access that's probably why
it's probably giving you like back a
mini batch okay and so we'll learn a lot
more about this but it's just something

224
00:42:15,040 --> 00:42:38,405
to be aware of okay so that's kind of
everything you need to do in practice so
now we're going to kind of get into a
little bit of theory what's actually
going on behind the scenes with these
convolutional neural networks and you

225
00:42:38,505 --> 00:42:54,175
might remember it back in Lesson one we
actually saw our first little bit of
theory which we stole from this
fantastic websites a toaster dot IO

226
00:42:54,275 --> 00:43:03,910
either explained visually and we learnt
that a that a convolution is something
where we basically have a little matrix
in deep learning nearly always three by
three a little matrix that we basically

227
00:43:04,010 --> 00:43:16,790
multiply every element of that matrix by
every element of a three by three
section of an image add them all
together to get the result of that

228
00:43:16,890 --> 00:43:29,065
convolution at one point all right now
let's see how that all gets turned
together to create these these various
layers that we saw in the the Zeiler and

229
00:43:29,165 --> 00:43:39,570
burgers paper and to do that again I'm
going to steal off somebody who's much
smarter than I am
we're going to steal from a guy called
Ottavia good

230
00:43:39,570 --> 00:43:47,155
Ottavia oh good was the guy who created
Word Lens which nowadays is part of
Google Translate if I'm Google Translate

231
00:43:47,255 --> 00:43:58,080
you've ever like done that thing where
you point your camera at something at
something which has any kind of foreign
language on it and in real-time it
overlays it with the translation that

232
00:43:58,080 --> 00:44:06,385
was a views company that built that and
so Tokyo was kind enough to share this
fantastic video
he created he's at Google now and I want

233
00:44:06,485 --> 00:44:21,000
to kind of step you through works I
think it explains really really well
what's going on and then after we look
at the video we're going to see how to
implement the whole a whole sequence of
kintyre set of layers of convolution on
your network in Microsoft Excel

234
00:44:21,500 --> 00:44:27,720
so with you're a visual learner or a
spreadsheet learner hopefully you'll be
able to understand all this okay so

235
00:44:29,880 --> 00:44:41,610
we're going to start with an image and
something that we're going to do later
in the course is we're going to learn to
nice digits so we'll do it like end to
end we'll do the whole thing so this is
pretty similar so we're going to try and

236
00:44:41,610 --> 00:44:51,990
recognize in this case letters so here's
an A which obviously it's actually a
grid of numbers right and so there's the
creative numbers and so what we do is we

237
00:44:51,990 --> 00:44:59,035
take our first convolutional filter so
we're assuming this is all this is
assuming that these are already learned

238
00:44:59,135 --> 00:45:05,010
right and you can see this point it's
got wiped down the right-hand side right
and black down the left so it's like

239
00:45:05,010 --> 00:45:10,350
zero zero zero maybe negative 1 negative
1 negative 1 0 0 0 1 1 1 and so we're
taking each 3x3 part of the image and

240
00:45:13,140 --> 00:45:21,670
multiplying it by that 3x3 matrix not as
a matrix product that an element-wise
product and so you can see what happens

241
00:45:21,770 --> 00:45:30,180
is everywhere where the the white edge
is matching the edge of the a and the
black edge isn't we're getting green

242
00:45:30,180 --> 00:45:36,475
we're getting a positive and everywhere
where it's the opposite we're getting a
negative we're getting a red right and

243
00:45:36,575 --> 00:45:41,920
so that's the first filter creating the
first that the result of the first

244
00:45:42,020 --> 00:45:47,020
kernel right and so here's a new kernel
this one is it's got a white stripe

245
00:45:47,120 --> 00:46:02,610
along the top right so we literally scan
through every 3x3 part of the matrix
multiplying those 3 bits of the a the
neighbors of the a by the 9 bits as a
filter to find out whether it's red or
green and how red or green it is ok and

246
00:46:02,610 --> 00:46:15,570
so this is assuming we had two filters
one was a bottom edge one was a left
edge and you can see here the top edge
not surprisingly it's red here so a
bottom edge was red here and green here
the right edge right here in green here

247
00:46:15,570 --> 00:46:26,520
and then in the next step we add a
non-linearity ok the rectified linear
unit which literally means strongly the
negatives so here the Reds all gone okay

248
00:46:26,520 --> 00:46:30,780
so here's layer 1 the input
here's layup to the result of 2

249
00:46:30,780 --> 00:46:36,180
convolutional filters here's layer 3
which is which is throw away all of the
red stuff and that's called a rectified

250
00:46:38,190 --> 00:46:55,320
linear unit and then layer 4 is
something called a max pull on a layer 4
we replace every 2 by 2 part of this
grid
and we replace it with its maximum mat
so it basically makes it half the size
it's basically the same thing but half

251
00:46:55,420 --> 00:47:05,599
the size and then we can go through and
do exactly the same thing we can have
some new filter three by three filter
that we put through each of the two
results of the previous layer okay and

252
00:47:05,699 --> 00:47:21,190
again we can throw away the red bits
right so get rid of all the negatives so
we just keep the positives that's called
applying a rectified linear unit and
that gets us to our next layer of this

253
00:47:21,190 --> 00:47:30,490
convolutional neural network so you can
see that by you know at this layer back
here it was kind of very interpretive
all it's like we've either got bottom
edges or left edges but then the next

254
00:47:33,010 --> 00:47:42,559
layer was combining the results of
convolutions so it's starting to become
a lot less clear like intuitively what's
happening but it's doing the same thing

255
00:47:42,659 --> 00:47:52,180
and then we do another max pull right so
we replace every 2x2 or 3x3 section with
a single digit so here this 2x2 it's all
black so we replaced it with a black

256
00:47:53,799 --> 00:48:07,865
right and then we go and we take that
and we we compare it to basically a kind
of a template of what we would expect to
see if it was an a it was a B but the
see it was d give it an E and we see how

257
00:48:07,965 --> 00:48:19,456
closely it matches and we can do it in
exactly the same way we can multiply
every one of the values in this four by
eight matrix with every one of the four
by eight in this one and this one and

258
00:48:19,556 --> 00:48:30,614
this one and we add we just add them
together to say like how often does it
match versus how often does it not match
and then that could be converted to give
us a percentage probability that this is

259
00:48:31,114 --> 00:48:38,524
a no so in this case this particular
template matched well with a so notice

260
00:48:38,624 --> 00:48:46,119
we're not doing an each training here
right this is how it would work if we
have a pre trained model all right so

261
00:48:46,119 --> 00:49:00,484
when we download a pre trained imagenet
model off the internet and isn't on an
image without any changing to it this is
what's happening or if we take a model
that you've trained and you're applying
it to some test set or for some new
image this is what it's doing all right

262
00:49:00,584 --> 00:49:17,855
as it's basically taking it through it
buying a convolution to each layer to
each multiple convolutional filters to
each layer and then doing the rectified
linear unit so throw away the negatives
and then do the max pull and then repeat

263
00:49:17,955 --> 00:49:29,395
that a bunch of times and so then we can
do it with a new letter A or letter B or
whatever and keep going through that
process right so as you can see that's

264
00:49:29,495 --> 00:49:40,420
far nice the visualization thing and I
could have created because I'm not at a
vo so thanks to him for sharing this
with us because it's totally awesome

265
00:49:40,420 --> 00:49:49,895
he actually this is not done by hand he
actually wrote a piece of computer
software to actually do these
convolutions this is actually being
actually being done dynamically which is

266
00:49:49,995 --> 00:49:58,060
pretty cool so I'm more of a spreadsheet
guy personally I'm a simple person so
here is the same thing now in

267
00:49:58,060 --> 00:50:08,950
spreadsheet all right and so you'll find
this in the github repo so you can
either get clone the repo to your own
computer open up the spreadsheet or you

268
00:50:08,950 --> 00:50:36,880
can just go to github.com slash / ji and
click on this it's it's inside if you go
to our repo and just go to courses as
usual go to deal 1 as usual you'll see
there's an Excel section there okay and
so he lay all that so you can just
download them by clicking them or you
can clone the whole repo and we're
looking at cognitive example convolution

269
00:50:36,880 --> 00:50:55,290
example all right so you can see I have
here an input right so in this case the
input is the number 7 so I grab this
from a dataset called m-must MN ist
which we'll be looking at in a lot of
detail and I just took one of those

270
00:50:55,290 --> 00:51:09,640
digits at random and I put it into Excel
and so you can see every Hextall is
actually just a number between 9 1 okay
very often actually it'll be a bite
between Norton 255 or sometimes it might

271
00:51:13,060 --> 00:51:25,900
be a float between naught and 1 it
doesn't really matter
by the time it gets to PI torch we're
generally dealing with floats so we if
one of the steps we often will take will
be to convert it to a number between

272
00:51:25,900 --> 00:51:47,299
naught 1 so then you can see I've just
use conditional formatting in Excel to
kind of make the higher numbers more red
so you can clearly see that this is a
red this is a 7 but but it's just a
bunch of numbers that have been imported
into Excel okay so here's our input so

273
00:51:47,399 --> 00:51:55,249
remember what at a via did was he then
applied two filters right with different

274
00:51:55,349 --> 00:52:05,170
shapes so here I've created a filter
which is designed to detect top edges so
this is a 3 by 3 filter okay and I've

275
00:52:05,170 --> 00:52:10,119
got ones along the top zeroes in the
middle minus ones at the bottom right so

276
00:52:10,119 --> 00:52:16,630
let's take a look at an example that's
here right and so if I hit that - you

277
00:52:16,630 --> 00:52:24,112
can see here highlighted this is the 3
by 3 part of the input that this
particular thing is calculating right so

278
00:52:24,212 --> 00:52:44,134
here you can see it's got 1 1 1 are all
being multiplied by 1 and point 1 0 0
are all being multiplied by negative 1
okay so in other words all the positive
bits are getting a lot of positive the
negative bits are getting nearly nothing
at all so we end up with a high number

279
00:52:44,234 --> 00:52:54,670
okay where else on the other side of
this bit of the 7 right you can see how
you know this is basically zeros here or

280
00:52:54,670 --> 00:53:08,859
perhaps more interestingly on the top of
it okay here we've got high numbers at
the top but we've also got high numbers
at the bottom which are negating it ok

281
00:53:08,859 --> 00:53:15,169
so you can see that the only place that
we end up activating is where we're

282
00:53:15,669 --> 00:53:49,070
actually at an edge so in this case this
here this number 3 this is called an
activation ok so when I say an
activation I mean ah
at number a number that is calculated
and it is calculated by taking some
numbers from the input and applying some
kind of linear operation in this case a
convolutional kernel to calculate an

283
00:53:49,170 --> 00:54:03,095
output right you'll notice that other
than going inputs multiplied by kernel
and summing it together
right so here's my some and here's my x

284
00:54:03,195 --> 00:54:32,920
then take that and I go max of 0 comma
that and so that's my rectified linear
unit so it sounds very fancy rectified
linear unit but what they actually mean
is open up Excel and type equals max 0
comma C ok that's all about then you'll
see people in the biz so to say value a
so ral you means rectified linear unit
means max 0 comma thing and I'm not like

285
00:54:32,920 --> 00:54:50,490
simplifying it I really mean it like
when I say like if I'm simplifying I
always say so I'm simplifying but if I'm
not saying I'm simplifying that's the
entirety okay so a rectified linear unit
in its entirety is this and a
convolution in its entirety is is this

286
00:54:50,490 --> 00:55:03,560
okay so a single layer of a
convolutional neural network is being
implemented in its entirety
here in Excel okay and so you can see

287
00:55:03,560 --> 00:55:09,290
what it's done is it's deleted pretty
much the vertical edges and highlighted

288
00:55:09,290 --> 00:55:24,660
the horizontal edges so again this is
assuming that our network is trained and
that at the end of training it a created
a convolutional filter with these
specific line numbers in and so here is

289
00:55:24,760 --> 00:55:31,890
a second convolutional filter it's just
a different line numbers now pi torch

290
00:55:31,990 --> 00:55:54,230
doesn't store them as two separate nine
digit arrays it stores it as a tensor
right remember a tensor just means an
array with more dimensions okay you can
use the word array as well it's the same
thing but in pi torch they always use
the word tensor so I'm going to say

291
00:55:54,230 --> 00:56:03,495
cancer okay so it's just a tensor with
an additional axis which allows us to
stack each of these filters together

292
00:56:03,595 --> 00:56:19,700
right filter and kernel pretty much mean
the same thing yeah right it refers to
one of these three by three matrices or
one of these three by three slices of a
three dimensional tensor so if I take

293
00:56:19,700 --> 00:56:35,295
this one and here I've literally just
copied the formulas in Excel from above
okay and so you can see this one is now
finding a vertebra which as we would
expect

294
00:56:35,395 --> 00:56:42,030
okay so we've now created one layer
right this here is a layer them

295
00:56:42,130 --> 00:56:48,680
specifically we'd say it's a hidden
layer which is it's not an input layer
and it's not an output layer so
everything else is a hidden layer okay

296
00:56:50,810 --> 00:57:05,974
and this particular hidden layer has is
a size two on this dimension right
because it has two
filters right two kernels so what

297
00:57:06,074 --> 00:57:13,729
happens next
well let's do another one okay so as we

298
00:57:13,829 --> 00:57:34,540
kind of go along things can multiply a
little bit in complexity right because
my next filter is going to have to
contain two of these three by threes
because I'm gonna have to say how do I
want to bring Adam I want to write these
three things and at the same time how do

299
00:57:34,540 --> 00:57:53,464
I want to wait the corresponding three
things down here right because in pi
torch this is going to be this whole
thing here is going to be stored as a
multi-dimensional tensor right so you
shouldn't really think of this now as
two 3x3 kernels but one two by three by

300
00:57:53,564 --> 00:58:14,264
three eternal okay so to calculate this
value here I've got the sum product of
all of that plus the sum product of
scroll down all of that okay and so the

301
00:58:14,364 --> 00:58:22,420
top ones are being multiplied by this
part of the kernel and the bottom ones
have been multiplied by this part of the

302
00:58:22,420 --> 00:58:35,530
kernel and so over time you want to
start to get very comfortable with the
idea of these like higher dimensional
linear combinations right like it's it's

303
00:58:35,530 --> 00:58:44,855
harder to draw it on the screen like I
had to put one above the other but
conceptually just stuck it in your mind
like this that's really how you want to

304
00:58:44,955 --> 00:58:50,600
think right and actually Geoffrey Hinton
in his original 2012 neural Nets

305
00:58:50,700 --> 00:58:58,990
Coursera class has a tip which is how
all computer scientists deal with like
very high dimensional spaces which is

306
00:58:58,990 --> 00:59:06,390
that they basically just visualize the
two-dimensional space
and then say like twelve dimensions
really fast and they had lots of tires

307
00:59:06,390 --> 00:59:19,550
so that's it
right we can see two dimensions on the
screen and then you're just going to try
to trust that you can have more
dimensions like the Const
it's just you know there's there's
nothing different about them and so you

308
00:59:19,550 --> 00:59:38,900
can see in Excel you know Excel doesn't
have the ability to handle
three-dimensional tensors so I had to
like say okay take this two-dimensional
dot product add on this two-dimensional
dot product right but if there was some
kind of 3d Excel I could have to stand
that in a single line all right and then

309
00:59:38,900 --> 00:59:44,720
again apply max 0 comma otherwise known
as rectified linear unit otherwise known
as value okay so here is my second layer

310
00:59:48,710 --> 01:00:02,540
and so when people create different
architectures write an architecture
means like how big is your kernel at
layer 1
how many filters are in your kernel at

311
01:00:02,540 --> 01:00:15,250
layer 1 so here I've got a 3 by 3
where's number 1 and a 3 by 3 there's
number 2 so like this architecture I've
created starts off with 2 3 by 3

312
01:00:16,150 --> 01:00:31,895
convolutional kernels and then my second
layer has another two kernels of size 2
by 3 by 3 so there's the first one
and then down here here's a second 2 by
3 by 3 kernel okay and so remember one

313
01:00:31,995 --> 01:00:52,940
of these specific any one of these
numbers is an activation okay so this
activation is being calculated from
these three things here and other 3
things up there and we're using these
this 2 by 3 by 3 kernel okay and so what

314
01:00:52,940 --> 01:00:57,830
tends to happen is people generally give
names to their layers so I say okay

315
01:00:57,830 --> 01:01:10,650
let's call this layer here con 1 and
this layer here and this and this layer
here con - all right so that's you know

316
01:01:10,750 --> 01:01:21,990
but generally you'll just see that like
when you print out a summary of a
network every layer will have some kind
of name okay and so then what happens

317
01:01:22,090 --> 01:01:36,520
next well part of the architecture is
like do you have some max pooling where
bounces up Matt spalling happen so in
this architecture we're inventing we're
going to next step is do max fully okay

318
01:01:36,520 --> 01:01:43,569
Matt spooling is a little hard to kind
of show in Excel but we've got it so max

319
01:01:43,569 --> 01:01:53,219
pooling if I do a two by two max pooling
it's going to have the resolution both
height and width so you can see here

320
01:01:53,319 --> 01:01:59,909
that I've replaced these four numbers
with the maximum of those four numbers

321
01:02:00,009 --> 01:02:06,780
right and so because I'm having the
resolution it only makes sense to
actually have something every two cells

322
01:02:06,780 --> 01:02:19,030
okay so you can see here the way I've
got kind of the same looking shape as I
had back here okay but it's now half the
resolution so for placed every two by

323
01:02:19,030 --> 01:02:24,160
two with its max and you'll notice like
it's not every possible two by two I
skip over from here so this is like

324
01:02:26,319 --> 01:02:38,050
starting at beat Hugh and then the next
one starts at BS right so they're like
non-overlapping that's why it's
decreasing the resolution okay

325
01:02:38,050 --> 01:02:47,525
so anybody who's comfortable with
spreadsheets you know you can open this
and have a look and so after our max

326
01:02:47,625 --> 01:02:55,084
pooling there's a number of different
things we could do next and I'm going to

327
01:02:55,184 --> 01:03:00,454
show you a kind of classic old style
approach nowadays in fact what generally
happens nowadays is we do a max pool

328
01:03:00,554 --> 01:03:07,490
where we kind of like max across the
entire size right but on older
architectures and also on all the

329
01:03:07,590 --> 01:03:14,810
structured data stuff we do we actually
do something called a fully connected

330
01:03:14,910 --> 01:03:43,094
layer and so here's a fully connected
layer I'm going to take every single one
of these activations and I've got to
give every single one of them or weight
right and so then I'm going to take over
here here is the sum product of every
one of the activations by every one of
the weights for both of the
two levels of my three-dimensional

331
01:03:43,194 --> 01:03:49,490
tensor right and so this is called a
fully connected layer notice it's
different to a convolution I'm not going
through a few at a time right but I'm

332
01:03:51,980 --> 01:04:03,290
creating a really big weight matrix
right so rather than having a couple of
little 3x3 kernels my weight matrix is
now as big as the entire input and so as

333
01:04:03,290 --> 01:04:17,744
you can imagine architectures that make
heavy use of fully convolutional layers
can have a lot of weights which means
they can have trouble with overfitting
and they can also be slow and so you're

334
01:04:17,844 --> 01:04:28,465
going to see a lot an architecture
called vgg because it was the first kind
of successful deeper architecture it has
up to 19 layers and vgg actually

335
01:04:28,565 --> 01:04:42,930
contains a fully connected layer with
4096 weights connected to a hidden layer
with 4,000 sorry 4096 activations
connected to a hidden layer with 4096

336
01:04:43,030 --> 01:04:53,180
activations so you've got like 4096 by
4096 x remember or apply it by the
number of kind of kernels that we've

337
01:04:53,180 --> 01:05:09,380
calculated so in vgg there's this I
think it's like 300 million weights of
which something like 250 million of them
are in these fully connected layers so

338
01:05:09,380 --> 01:05:13,700
we'll learn later on in the course about
how we can kind of avoid using these big
fully connected layers and behind the

339
01:05:15,470 --> 01:05:23,004
scenes all the stuff that you've seen us
using like ResNet and res next none of
them use very large fully connected

340
01:05:23,104 --> 01:05:40,280
layers you know you had a question
sorry yeah come on um so could you tell
us more about for example if we had like
three channels for the input what would
be the shape yeah these filters right so

341
01:05:40,280 --> 01:05:52,160
that's a great question so if we have 3
channels of input it would look exactly
like conv one right cons one kind of has
two channels right and so you can see

342
01:05:52,160 --> 01:06:00,164
with cons one we had two channels so
therefore our filters had to have like
two channels per filter and so you could

343
01:06:00,264 --> 01:06:11,749
like imagine that this input didn't
exist you know and actually this was the
airport alright so when you have a
multi-channel input it just means that
your filters look like this and so

344
01:06:11,749 --> 01:06:17,779
images often full color they have three
red green and blue sometimes they also

345
01:06:17,779 --> 01:06:23,059
have an alpha Channel so however many
you have that's how many inputs you need

346
01:06:23,059 --> 01:06:30,764
and so something which I know Jeanette
was playing with recently was like using
a full color image net model in medical

347
01:06:30,864 --> 01:06:35,124
imaging for something called bone age
calculations which has a single channel

348
01:06:35,224 --> 01:06:51,989
and so what she did was basically take
the the input the the single channel
input and make three copies of it so you
end up with basically like one two three
versions of the same thing which is like

349
01:06:52,089 --> 01:06:59,630
it's kind of a small idea like it's kind
of redundant information that we don't
quite want but it does mean that then if

350
01:06:59,630 --> 01:07:08,640
you had a something that expected a
three channel convolutional filter you
can use it right and so at the moment

351
01:07:08,740 --> 01:07:17,660
there's a cable competition for iceberg
detection using a some funky satellite
specific data format that has two

352
01:07:17,660 --> 01:07:30,029
channels so here's how you could do that
you could either copy one of those two
channels into the third channel or I
think what people in Carroll are doing
is to take the average of the two again

353
01:07:30,129 --> 01:07:36,794
it's not ideal but it's a way that you
can use pre-trained networks yeah I've

354
01:07:36,894 --> 01:07:48,410
done a lot of fiddling around like that
you can also actually I've actually done
things where I wanted to use a three
channel image net Network on four
channel data

355
01:07:48,410 --> 01:07:53,029
I had a satellite data where the fourth
channel was near-infrared and so

356
01:07:53,029 --> 01:08:08,545
basically I added an extra kind of level
to my convolutional kernels that were
all zeros and so basically like started
off by ignoring the near-infrared band
and

357
01:08:08,645 --> 01:08:21,500
so what happens it basically and you'll
see this next week is that rather than
having these like carefully trained
filters when you're actually training
something from scratch we're actually
going to start with random numbers

358
01:08:21,600 --> 01:08:33,070
that's actually what we do we actually
start with random numbers and then we
use this thing called stochastic
gradient descent which we've kind of
seen conceptually
to slightly improve those random numbers
to make them less random and we
basically do that again and again and

359
01:08:35,350 --> 01:08:44,985
again okay great
let's take a seven minute break and
we'll come back at 7:50 all right so

360
01:08:50,310 --> 01:09:04,685
so we've got as far as doing a fully
connected layer right so we had our the
results of our max pooling layer got fed
to a fully connected layer and he might

361
01:09:04,785 --> 01:09:11,770
notice those of you that remember your
linear algebra the fully connected layer
is actually doing a classic traditional

362
01:09:11,770 --> 01:09:23,970
matrix product okay so it's basically
just going through each pair in turn
multiplying them together and then
adding them up to do a matrix product

363
01:09:23,970 --> 01:09:47,505
now in practice if we want to calculate
which one of the ten digits we're
looking at their single number we've
calculated isn't enough we would
actually calculate ten numbers so what

364
01:09:47,605 --> 01:10:06,860
we will have is rather than just having
one set of fully connected weights like
this and I say set because remember
there's like a whole 3d kind of tensor
of them we would actually need ten of
those right so you can see that these

365
01:10:06,960 --> 01:10:21,430
tensors start to get a little bit high
dimensional right and so this is where
my patients we're doing it next cell ran
out but imagine that I had done this ten
times I could now have ten different
numbers or being calculated
yeah using exactly the same process

366
01:10:21,530 --> 01:10:40,190
right we'll just be ten of these fully
connected to by m-by-n erased basically
and so then we would have ten numbers

367
01:10:40,190 --> 01:10:52,314
being spat out so what happens next so
next up we can open up a different Excel
worksheet entropy example dot XLS that's

368
01:10:52,414 --> 01:10:59,234
got two different worksheets one of them
is called soft mass and what happens

369
01:10:59,334 --> 01:11:10,820
here sorry I've changed domains rather
than predicting whether it's the number
from one not to nine I'm going to
predict whether something is a cat a dog
a plane of Fisher Building okay so out

370
01:11:10,820 --> 01:11:27,675
of our that fully connected layer we've
got this case we'd have five numbers and
notice at this point there's no rail you
okay in the last layer there's no rail
you okay so I can have negatives so I

371
01:11:27,775 --> 01:11:42,359
want to turn these five numbers H into a
probability I want to turn it into a
probability from naught to one that it's
a cat
that's a dog there's a plane that it's a
fish that it's a building and I want

372
01:11:42,459 --> 01:11:55,969
those probabilities to have a couple of
characteristics first is that each of
them should be between zero and one and
the second is that this state together
should add up to one right it's
definitely one of these five things okay

373
01:11:55,969 --> 01:12:01,400
so to do that we use a different kind of
activation function what's an activation

374
01:12:01,400 --> 01:12:16,340
function an activation function is a
function that is applied to activations
so for example max 0 comma something is
a function that I applied to an

375
01:12:16,340 --> 01:12:23,630
activation so an activation function
always takes in one number and spits out

376
01:12:23,630 --> 01:12:36,115
one number so max of 0 comma X takes in
a number X and spits out some different
number value of s
that's all an activation function is and

377
01:12:36,215 --> 01:12:44,230
if you remember back to that PowerPoint
we saw in Lesson one

378
01:12:45,590 --> 01:12:55,230
each of our layers was just a linear
function and then after every layer we
said we needed some non-linearity act as

379
01:12:58,619 --> 01:13:18,210
if you stack a bunch of linear layers
together right then all you end up with
is a linear layer okay
so somebody's talking can can you not a
slow just acting thank you
if you stack a number of linear
functions together you just end up with

380
01:13:18,210 --> 01:13:25,609
a linear function and nobody does any
cool deep learning with displaying your
functions right but remember we also

381
01:13:25,709 --> 01:13:34,769
learnt that by stacking linear functions
with between each one a non-linearity we
could create like arbitrarily complex

382
01:13:34,769 --> 01:13:47,119
shapes and so the non-linearity that
we're using after every hidden layer is
a rally rectified linear unit a
non-linearity is an activation function
an activation function is a

383
01:13:47,219 --> 01:13:57,444
non-linearity in with in deep way
obviously there's lots of other
nonlinearities and in the world but in
deep learning this is what we mean so an

384
01:13:57,544 --> 01:14:06,239
activation function is any function that
takes some activation in as a single
number and spits out some new activation
like max of 0 comma so I'm now going to

385
01:14:06,339 --> 01:14:15,744
tell you about a different activation
function it's slightly more complicated
than value but not too much it's called

386
01:14:15,844 --> 01:14:22,029
soft max soft max only ever occurs in
the final layer at the very end and the

387
01:14:22,129 --> 01:14:35,729
reason why is that soft max always spits
out numbers as an activation function
that always spits out a number between
Norton 1 and it always spits out a bunch
of numbers that add to 1 so a soft max

388
01:14:35,729 --> 01:14:53,489
gives us what we want right in theory
this isn't strictly necessary right like
we could ask our neural net to learn a
set of kernels which have you know which
which give probabilities that line up as
closely as possible with what we want

389
01:14:54,960 --> 01:15:10,170
but in general with deep learning if you
can construct your architecture so that
the desired characteristics are as easy
to express as possible you'll end up
with better models like they'll learn
more quickly with less parameters so in

390
01:15:10,170 --> 01:15:29,610
this case we know that our probabilities
should end up being between 9 1 we know
that they should end up adding to 1 so
if we construct an activation function
which always has those features then
we're going to make our neural network
do a better job
it's gonna make it easier for it it
doesn't have to learn to do those things

391
01:15:29,610 --> 01:15:51,420
because it all happen automatically okay
so in order to make this work we first
of all have to get rid of all of the
negatives right like we can't have
negative probabilities so to make things
not being negative one way we could do
it is just go into the pair of right so
here you can see my first step is to go

392
01:15:51,420 --> 01:16:03,450
X of the previous one right and I think
I've mentioned this before but of all
the math that you just need to be super
familiar with to do deep learning the

393
01:16:03,450 --> 01:16:14,050
one you really need is logarithms and
asks write all of deep learning and all
of machine learning they appear all the

394
01:16:14,150 --> 01:16:33,090
time right so for example you absolutely
need to know that log of x times y
equals log of X plus log of Y all right

395
01:16:33,090 --> 01:16:43,135
and like not just know that that's a
formula that exists but have a sense of
like what does that mean why is that
interesting oh I can turn
multiplications into additions that
could be really handy right and

396
01:16:43,235 --> 01:17:00,385
therefore log of x over y equals log of
X minus log of Y again that's going to
come in pretty handy you know rather
than dividing I can just subtract things

397
01:17:00,485 --> 01:17:17,860
right and also remember that if I've got
log of x equals y then that means a to
the y equals x in other words log log
and E to the for the inverse of each

398
01:17:17,960 --> 01:17:28,345
other okay again you just you need to
really really understand these things
and like so if you if you haven't spent
much time with logs and X for a while
try plotting them in Excel or a notebook

399
01:17:28,445 --> 01:17:38,260
have a sense of what shape they are how
they combine together just make sure
you're really comfortable with them so

400
01:17:38,260 --> 01:17:49,450
we're using it here right we're using it
here so one of the things that we know
is a to the power of something is
positive okay so that's great

401
01:17:49,450 --> 01:18:03,965
the other thing you'll notice about e to
the power of something is because it's a
power numbers that are slightly bigger
than other numbers like four is a little
bit bigger than 2.8 when you go e to the
power of it really accentuates that
difference okay

402
01:18:04,065 --> 01:18:07,850
so we're going to take advantage of both
of these features for the purpose of

403
01:18:07,950 --> 01:18:15,850
deep learning okay so we take our the
results of this fully connected layer we
go e to the power of for each of them

404
01:18:15,850 --> 01:18:28,120
and then we're gonna yeah and then we're
going to add them up okay so here is the
sum of e to the power of so then here

405
01:18:32,340 --> 01:18:40,795
we're going to take e to the power of
divided by the sum of e to the power of
so if you take all of these things

406
01:18:40,895 --> 01:18:47,610
divided by their sum then by definition
all of those things must add up to 1 and

407
01:18:47,710 --> 01:19:00,815
furthermore since we're dividing by
their sum they must always vary between
0 and 1 because they were always
positive alright and that's it so that's

408
01:19:00,915 --> 01:19:04,840
what softmax is ok so I've got this kind

409
01:19:04,840 --> 01:19:11,050
of doing random numbers each time right
and so you can see like as I as I look

410
01:19:11,050 --> 01:19:18,574
through my softmax generally has quite a
few things that are so close to zero
that they round down to zero and you
know maybe one thing that's nearly 1

411
01:19:18,674 --> 01:19:30,710
right and the reason for that is what we
just talked about that is with the X
just having one number a bit bigger than
the others tends to like push it out

412
01:19:30,810 --> 01:19:47,140
further right so even though my inputs
here around on numbers between negative
5 and 5 right my outputs from the
softmax don't really look that random at
all in the sense that they tend to have
one big number and a bunch of small

413
01:19:47,140 --> 01:20:03,755
numbers and now that's what we want
right we want to say like in terms of
like is this a cat a dog a plane a fish
or a building we really want it to say
like it's it's that you know it's it's a
dog or it's a plane not like I don't

414
01:20:03,855 --> 01:20:18,740
know okay so softmax has lots of these
cool properties right it's going to
return a probability that adds up to 1
and it's going to tend to want to pick
one thing particularly strongly okay so

415
01:20:18,840 --> 01:20:22,600
that's soft mess your net could you pass

416
01:20:22,600 --> 01:20:34,780
actually bust me up we how would we do
something that as let's say you have any
imaging you want to count in categorize
I was like cat and the dog or like has
multiple things but what kind of

417
01:20:37,600 --> 01:20:43,230
function will we try to use so happens
we're going to do that right now so

418
01:20:45,419 --> 01:20:53,962
so hope you think about why we might
want to do that and so runways where you
might want to do that is to do
multi-label classification so we're
looking now at listen to image models

419
01:20:54,062 --> 01:21:04,254
and specifically we're going to take a
look at the planet competition satellite
imaging competition now the satellite

420
01:21:04,354 --> 01:21:10,659
imaging competition has some
similarities to stuff we've seen before

421
01:21:10,759 --> 01:21:19,769
right so before we've seen cat versus
dog and these images are a cat or a dog
they're not Maya they're not both right

422
01:21:19,769 --> 01:21:34,260
but the satellite imaging competition
has stayed as images that look like this
and in fact every single one of the
images is classified by whether there's
four kinds of weather
one of which is haze and another of

423
01:21:34,260 --> 01:21:50,999
which is clear in addition to which
there is a list of features that may be
present including agriculture which is
like some some cleared area used for
agriculture primary which means primary
rainforest and water which means a river

424
01:21:50,999 --> 01:21:59,709
or a creek so here is a clear day
satellite image showing some agriculture
some primary rainforest and some water

425
01:21:59,809 --> 01:22:06,145
features and here's one which is in haze
and is entirely primary rainforest so in

426
01:22:06,245 --> 01:22:12,714
this case we're going to want to be able
to show we're going to predict multiple

427
01:22:12,814 --> 01:22:18,669
things and so softmax wouldn't be good
because softmax doesn't like predicting
multiple things and like I would

428
01:22:18,769 --> 01:22:28,079
definitely recommend anthropomorphizing
your activation functions right they
have personalities okay and the

429
01:22:28,079 --> 01:22:36,939
personality of the softmax is it wants
to pick a thing and people forget this
all the time I've seen many people even

430
01:22:37,039 --> 01:22:47,859
well-regarded researchers in famous
academic papers using like soft maps for
multi-label classification it happens
all the time

431
01:22:47,959 --> 01:22:55,249
right and it's kind of ridiculous
because they're not understanding the
personality of their activation function

432
01:22:55,249 --> 01:23:05,590
so for multi
classification where each sample can
belong to one or more classes we have to
change a few things but here's the good

433
01:23:05,590 --> 01:23:10,330
news
in fast AI we don't have to change
anything right so fast AI will look at

434
01:23:10,430 --> 01:23:21,040
the labels in the CSV and if there is
more than one label ever for any item it

435
01:23:21,880 --> 01:23:25,600
will automatically switch into like
multi-label mode so I'm going to show

436
01:23:25,600 --> 01:23:32,425
you how it works behind the scenes but
the good news is you don't actually have
to care it happens anywhere so if you

437
01:23:32,525 --> 01:23:45,710
have multi label images multi label
objects you obviously can't use the
classic Kerris style approach where

438
01:23:45,810 --> 01:24:04,050
things are in folders because something
can't conveniently be in multiple
folders at the same time right so that's
why we you basically have to use the
from CSV approach right so if we look at

439
01:24:08,155 --> 01:24:14,240
an example actually I'll show you I tend
to take you through it right so we can

440
01:24:14,340 --> 01:24:18,965
say okay this is the CSV file containing
our labels this looks exactly the same

441
01:24:19,065 --> 01:24:24,365
as I did before but rather than side on
its top down alright and top down I've

442
01:24:24,465 --> 01:24:32,420
mentioned before that can do our
vertical flips it actually does more
than that there's actually eight
possible symmetries for a square which

443
01:24:32,520 --> 01:24:37,355
is it can be rotated through 90 180 270
or 0 degrees and for each of those it

444
01:24:37,455 --> 01:24:47,795
can be flipped and if you think about it
for awhile you'll realize that that's a
complete enumeration of everything that
you can do in terms of symmetries to a

445
01:24:47,895 --> 01:24:57,070
square so they're called it's called the
dihedral group of eight so if you see in
the code there's actually a transform or
dihedral that's why it's called that so

446
01:24:57,170 --> 01:25:11,130
this transforms will basically do the
full set of eight symmetric dihedral
rotations and flips plus everything
which we can do to dogs and cats

447
01:25:11,130 --> 01:25:19,765
you know small clinical rotations a
little bit of zooming a little bit of
contrast and brightness adjustment so

448
01:25:19,865 --> 01:25:28,440
these images are a size 256 by 256 so I
just create a little function here to
let me quickly grab you know data loader
of any size so here's a 256 by 256 once

449
01:25:32,429 --> 01:25:45,745
you've got a data object inside it we've
already seen that there's things called
Val D s test D s train D s there are
things that you can just index into and
grab a particular image so you just use

450
01:25:45,845 --> 01:25:53,310
square brackets 0 you'll also see that
all of those things have a DL that's a
data loader so des is data set DL is

451
01:25:53,310 --> 01:26:01,312
data motor these are concepts from PI
watch so if you Google PI torch data set
or pipe watch data loader you can
basically see what it means but the

452
01:26:01,412 --> 01:26:10,614
basic idea is a data set gives you a
single image or a single object back a
data loader gives you back a mini batch

453
01:26:10,714 --> 01:26:25,560
and specifically it gives you back a
transformed mini - so that's why when we
create our data object we can pass in
num workers and transforms it's like how
many processes do you want to use what

454
01:26:25,560 --> 01:26:37,260
transforms do you want and so with with
a data loader you can't ask for an
individual image you can only get back
at a mini batch and you can't get back a
particular mini batch you can only get

455
01:26:37,260 --> 01:26:49,884
back the next mini - so something
reverses look through grabbing a mini
batch at a time and so in Python the
thing that does that is called a
generator right or an iterator this
slightly different versions are the same

456
01:26:49,984 --> 01:27:02,550
thing so to turn a data loader into an
iterator you use the standard Python
function cordetta
that's a Python function just a regular
part of the Python basic language that

457
01:27:02,550 --> 01:27:18,800
returns you an iterator and an iterator
is something that takes you can pass the
static give pass it to the standard
Python function or statement next and
that just says give me another batch
from this iterator

458
01:27:18,800 --> 01:27:28,109
so we're basically this is one of the
things I really like about PI torch is
it really leverages
modern pythons kind of stuff you know in

459
01:27:28,209 --> 01:27:34,859
in tensorflow they invent their whole
new world earth ways of doing things and

460
01:27:34,959 --> 01:27:43,262
so it's kind of more in a sense it's
more like cross-platform but in another
sense like it's not a good fit to any

461
01:27:43,362 --> 01:27:53,960
platform so it's nice if you if you know
Python well PI torch comes very
naturally if you don't know Python well
PI torches are good reason to learn

462
01:27:53,960 --> 01:28:05,690
Python well a PI torch your module
neural network module is a standard
Python bus for example so any work you
put into learning Python better will pay
off with paid watch so here I am using

463
01:28:08,090 --> 01:28:15,619
standard Python iterators and next to
grab my next mini batch from the
validation sets data loader and that's

464
01:28:17,780 --> 01:28:20,960
going to return two things it's going to
return the images in the mini batch and

465
01:28:20,960 --> 01:28:25,360
the labels of the mini - so standard
Python approach I can pull them apart

466
01:28:25,360 --> 01:28:47,840
like so and so here is one mini batch of
labels and so not surprisingly since I
said that my batch size let's go ahead
and find it
Oh actually it's the batch size by
default is 64 so I didn't pass in a

467
01:28:47,840 --> 01:28:54,854
batch size and so just remember shift
tab to see like what are the things you
can pass and what are the defaults so by

468
01:28:54,954 --> 01:29:07,040
default my batch size is 64 so I've got
that something of size 64 by 17 so there
are 17 of the possible classes right so

469
01:29:07,040 --> 01:29:21,559
let's take a look at the zeroth set of
labels so the zeroth images labels so I
can zip again standard Python things it
takes two lists and combines it so you

470
01:29:21,559 --> 01:29:31,190
get the zero theme from the first list
as you're asking for the second list and
the first thing for the first first this
first thing from the second list and so
forth so I can zip them together and

471
01:29:31,190 --> 01:29:36,790
that way I can find out for the zeroth
image and the validation set is

472
01:29:36,790 --> 01:29:44,841
agriculture
it's clear its primary rainforest its
slash-and-burn its water okay so as you

473
01:29:44,941 --> 01:29:57,099
can see here this is a MOLLE label you
see here's a way to do multi-label
classification so by the same token

474
01:29:57,199 --> 01:30:04,105
right if we go back to our single label
classification it's a cat dog playing

475
01:30:04,205 --> 01:30:15,684
official building behind the scenes we
haven't actually looked at it but behind
the scenes fast AI imply torch are
turning our labels into something called
one hot encoded labels and so if it was

476
01:30:15,784 --> 01:30:29,699
actually a dog than the actual values
would be like that right so these are
like the actuals okay so do you remember

477
01:30:29,699 --> 01:30:44,489
at the very end of at AV o--'s video he
showed how like the template had to
match to one of the like five ABCDE
templates and so what it's actually
doing is it's comparing when I said it's
basically doing a dot product it's

478
01:30:44,489 --> 01:30:53,440
actually a fully connected layer at the
end right that calculates an output
activation that goes through a soft Max

479
01:30:53,540 --> 01:31:03,540
and then the soft max is compared to the
one hot encoded label right so if it was
a dog there would be a one here and then

480
01:31:03,540 --> 01:31:12,179
we take the difference between the
actuals and the softmax activation is to
say and add those add up those
differences to say how much error is

481
01:31:12,179 --> 01:31:21,320
there essentially we're skipping over
something called a loss function that
we'll learn about next week but
essentially we're basically doing that

482
01:31:21,680 --> 01:31:32,340
now if it's one hot encoded like there's
only one thing which have a 1 in it then
actually storing it as 0 1 0 0 0 is

483
01:31:32,440 --> 01:31:52,515
terribly inefficient right like we could
basically say what are the index of each
of these things right so we can say it's
like 0 1 2 3 4 like so right and so
rather than storing it is 0 1 0 0 0
we actually just store the index value

484
01:31:52,615 --> 01:32:08,215
right so if you look at the the Y values
for the cats and dogs competition or the
dog breeds competition you won't
actually see a big lists of ones and
zeros like this you'll see a single
integer right which is like what what

485
01:32:08,315 --> 01:32:20,460
class index is it right and internally
inside pipe arch it will actually turn
that into a one hot encoded vector but
like you will literally never see it

486
01:32:20,560 --> 01:32:32,055
okay and and pi torch has different loss
functions where you basically say this
thing's won this thing is one hot
encoder door this thing is not and it
uses different bus functions

487
01:32:32,155 --> 01:32:40,040
that's all hidden by the faster I
library right so like you don't have to
worry about it but is but the the cool

488
01:32:40,040 --> 01:32:51,080
thing to realize is that this approach
for multi-label encoding with these ones
and zeros behind the scenes the exact
same thing happens for single level

489
01:32:51,080 --> 01:33:03,420
classification does it make sense to
change the beginners of the sigmoid of
the softmax function by changing the

490
01:33:03,520 --> 01:33:33,110
base no because when you change the more
math log base a of B equals log B over
log a so changing the base is just a
linear scaling and linear scaling is
something which the neural net can
with very easily

491
01:33:35,450 --> 01:34:08,670
good question okay so here is that image
right here is the image with
slash-and-burn water etc etc one of the
things to notice here is like when I
first displayed this image it was so
washed out I really couldn't see it
right but remember images now you know
we know images are just matrices of
numbers and so you can see here I just
said times 1.4 just to make it more

492
01:34:08,670 --> 01:34:19,710
visible right so like now that you kind
of it's the kind of thing I want you to
get familiar with is the idea that this
stuff you're dealing with
they're just matrices of numbers and you
can fiddle around with them so if you're

493
01:34:19,710 --> 01:34:25,360
looking at something like guys a bit
washed out you can just multiply it by
something to brighten it up a bit okay

494
01:34:25,460 --> 01:34:37,105
so here we can see I guess this is the
slash-and-burn here's the river that's
the water here's the primary rainforest
maybe that's the agriculture so forth

495
01:34:37,205 --> 01:34:49,210
okay so so you know with all that
background how do we actually use this
exactly the same way as everything we've
done before right so you know size and

496
01:34:49,310 --> 01:35:10,900
and the interesting thing about playing
around with this planet competition is
that these images are not at all like
image there and I would guess that the
vast majority is of stuff that the vast
majority of you do involving
convolutional neural Nets won't actually
be anything like image net you know

497
01:35:11,000 --> 01:35:25,650
it'll be it'll be medical imaging it'll
be like classifying different kinds of
steel tube or figuring out whether a
world you know is going to break or not
or or looking at satellite images or you

498
01:35:25,650 --> 01:35:34,880
know whatever right so it's it's good to
experiment with stuff like this planet
competition to get a sense of kind of

499
01:35:34,980 --> 01:35:44,910
what you want to do and so you'll see
here I start out by resizing my data to
64 by 64 it starts out at 256 by 256

500
01:35:44,910 --> 01:35:57,160
right now
I wouldn't want to do this for the cats
and dogs competition because it cats end
on competition we start with a pre
trained imagenet Network it's it's
nearly isn't it starts off nearly

501
01:35:57,260 --> 01:36:09,670
perfect right so if we resized
everything to 64 by 64 and then
retrained the whole set regular it we'd
basically destroy the weights that are
already pre trained to be very good

502
01:36:09,770 --> 01:36:19,115
remember imagenet most imagenet models
are trained at either 224 by 224 or
$2.99 by 299 all right so if we like
retrain them at 64 by 64 we're going to

503
01:36:19,215 --> 01:36:28,540
we're going to kill it on the other hand
there's nothing in image net that looks
anything like this you know there's no

504
01:36:28,640 --> 01:36:42,115
satellite images so the only useful bits
of the image net Network for us are kind
of layers like this one you know finding
edges and gradients and this one you

505
01:36:42,215 --> 01:36:51,725
know finding kind of textures and
repeating patterns and maybe these ones
are kind of finding more complex
textures but that's probably about it

506
01:36:51,825 --> 01:37:04,920
right so so in other words you know
starting out by training very small
images works pretty well when you're
using stuff like satellites so in this

507
01:37:04,920 --> 01:37:17,280
case I started right back at 64 by 64
grab some data built my model found out
what learning rate to use and
interestingly it turned out to be quite

508
01:37:17,280 --> 01:37:30,390
high it seems that because like it's so
unlike imagenet I needed to do quite a
bit more fitting with just that last
layer before it started to flatten out

509
01:37:30,390 --> 01:37:38,090
then I unfreeze dit and again this is
the difference to image net like

510
01:37:38,090 --> 01:37:52,410
datasets is my learning rate in the
initial layer i set 2/9 the middle
layers I said 2/3 where else for stuff
like it's like image net I had a
multiple of 10 each of those you know

511
01:37:52,410 --> 01:38:04,620
again the idea being that that earlier
layers probably and not as close to what
they need to be compared to the
like dances again

512
01:38:04,620 --> 01:38:14,820
unfreeze train for a while and you can
kind of see here
you know there's cycle one there's cycle
- there's cycle three and then I kind of

513
01:38:14,820 --> 01:38:25,950
increased double the size with my images
fit for a while and freeze fit for a
while double the size of the images
again fit for a while I'm freeze for a

514
01:38:25,950 --> 01:38:30,090
while and then add TTA and so as I
mentioned last time we looked at this

515
01:38:30,090 --> 01:38:35,300
this process ends up you know getting us
about 30th place in this competition

516
01:38:35,300 --> 01:38:46,595
which is really cool because people you
know a lot of very very smart people
just a few months ago worked very very
hard on this competition a couple of

517
01:38:46,695 --> 01:38:55,130
things people have asked about one is
what is this data dot resize do so a

518
01:38:55,230 --> 01:39:12,810
couple of different pieces here the
first is that when we say back here what
transforms do we apply and here's our
transforms we actually pass in a size
right so one of the things that that one

519
01:39:12,810 --> 01:39:20,300
of the things that data loaded does is
to resize the images like on-demand
every time it sees them

520
01:39:21,510 --> 01:39:35,099
it's got nothing to do with that dot
resize method right so this is this is
the thing that happens at the end like
whatever's passed in before it hits out
that before our data loader spits it out
it's going to resize it to this size if

521
01:39:35,099 --> 01:39:51,300
the initial input is like a thousand by
a thousand reading that JPEG and
resizing it to 64 by 64 turns out to
actually take more time than training
the content that's for each batch all

522
01:39:51,300 --> 01:40:14,070
right so basically all resize does is it
says hey I'm not going to be using any
images bigger than size times 1.3 so
just grow through once and create new
JPEGs of this size right and they're
rectangular right so new JPEGs where the
smallest edges of this size and again

523
01:40:14,070 --> 01:40:25,240
it's like you never have to do this
there's no reason to ever use it if you
don't want to it's just a speed-up okay
but if you've got really big images
coming in it saves you a lot of time and

524
01:40:25,340 --> 01:40:37,641
you'll often see on like Carol kernels
or forum posts or whatever people will
have like bash script stuff like that -
like loop through and resize images to
save time you never have to do that

525
01:40:37,741 --> 01:40:53,880
right just you can just say dot resize
and it'll just create you know once-off
it'll go through and create that if it's
already there and it'll use the
criticized ones for you okay so it's
just it's just a speed up convenience

526
01:40:53,880 --> 01:41:19,170
function no more okay so for those of
you that are kind of past dog breeds I
would be looking at planet next you know
like try it like play around with with
trying to get a sense of like how can
you get this as an accurate model one

527
01:41:19,170 --> 01:41:32,570
thing to mention and I'm not really
going to go into it in details there's
nothing to do with deep learning
particularly is that I'm using a
different metric I didn't use metrics
equals accuracy but I said metrics
equals f2

528
01:41:33,980 --> 01:41:43,195
remember from last week that confusion
matrix that like two by two you know
correct incorrect for each of dogs and

529
01:41:43,295 --> 01:41:59,390
cats there's a lot of different ways you
could turn that confusion matrix into a
score you know do you care more about
false negatives or do you care more
about false positives and how do you
weight them and how do you combine them
together right there's a basic there's

530
01:41:59,390 --> 01:42:07,905
basically a function called F beta where
the beta says how much do you weight
false negatives versus false positives

531
01:42:08,005 --> 01:42:25,184
and so f 2 is f beta with beta equals 2
and it's basically as particular way of
weighting false negatives and false
positives and the reason we use it is
because cattle told us that planet who
are running this competition wanted to
use this particular F beta metric the

532
01:42:25,284 --> 01:42:33,110
important thing for you to know is that
you can create custom metrics so in this
case you can see here it says from

533
01:42:33,110 --> 01:42:47,270
Planet import f2 and really I've got
this here so that you can see how to do
it right so if you look inside courses
DL 1 you can see there's something

534
01:42:47,270 --> 01:43:13,400
called planet py right and so if I look
at planet py you'll see there's a
function there called
f2 right and so f2 simply calls F beta
score from psychic or side PI and patent
where it came from and does a couple
little tweets that are particularly

535
01:43:13,400 --> 01:43:25,970
important but the important thing is
like you can write any metric you like
right as long as it takes in set of
predictions and a set of targets and

536
01:43:25,970 --> 01:43:30,530
they're both going to be numpy arrays
one dimensional non pyros and then you
return back a number okay and so as long

537
01:43:33,380 --> 01:43:44,290
as you put a function that takes two
vectors and returns at number you can
call it as a metric and so then when we
said

538
01:43:46,710 --> 01:44:00,580
see here learn metrics equals and then
past in that array which just contains a
single function f2 then it's just going
to be printed out after every for you

539
01:44:00,580 --> 01:44:22,445
okay so in general like the the faster I
library everything is customizable so
kind of the idea is that everything is
everything is kind of gives you what you
might want by default but also
everything can be changed as well yes

540
01:44:22,545 --> 01:44:42,560
you know um we have a little confusion
about the difference between multi-label
and a single label uh-huh
the vanish as an example in which
compared like similarly the example they
just show us ah activation function yeah

541
01:44:42,660 --> 01:44:47,080
so so I'm so sorry I said I'd do that

542
01:44:47,080 --> 01:44:58,390
then I didn't so the activation the
output activation function for a single
label classification is softmax but all
the reasons that we talked today but if

543
01:44:58,390 --> 01:45:14,980
we were trying to predict something that
was like 0 0 1 1 0 then softmax would be
a terrible choice because it's very hard
to come up with something where both of
these are high in fact it's impossible
because they have to add up to 1 so the
closest they could be would be point 5

544
01:45:15,080 --> 01:45:22,690
so for multi-label classification our
activation function is called sigmoid ok

545
01:45:23,650 --> 01:45:37,490
and again the faster library does this
automatically for you if it notices you
have a multi label problem and it does
that by checking your data tip to see if
anything has more than one label applied

546
01:45:37,590 --> 01:46:00,094
to it and so sigmoid is a function which
is equal to it's basically the same
thing except rather than we never add up
all of these X but instead we just take
this X when we say it's just equal to it
divided by one plus

547
01:46:00,194 --> 01:46:15,439
it and so the nice thing about that is
that now like multiple things can be
high at once right and so generally then

548
01:46:15,439 --> 01:46:26,300
if something is less than zero its
sigmoid is going to be less than 0.5 if
it's greater than zero is signal it's
going to be greater than 0.5 and so the

549
01:46:26,300 --> 01:46:49,604
important thing to know about a sigmoid
function is that its shape is
something which asymptotes at the top to
one and asymptotes drew
asymptotes at the bottom to zero and so

550
01:46:49,704 --> 01:47:07,144
therefore it's a good thing to model a
probability with anybody who has done
any logistic regression will be familiar
with this is what we do in logistic
regression so it kind of appears
everywhere in machine learning and

551
01:47:07,244 --> 01:47:19,869
you'll see that kind of a sigmoid and a
softmax they're very close to each other
conceptually but this is what we want is
our activation function for multi-label
and this is what we want the single

552
01:47:19,869 --> 01:47:22,659
label and again first AI does it all for
you

553
01:47:22,659 --> 01:47:28,309
there was a question over here yes I

554
01:47:28,409 --> 01:47:46,904
have a question about the initial
training that you do if I understand
correctly you have we have frozen the
the premium model and you only need
initially try to train the latest

555
01:47:47,004 --> 01:48:02,904
playwright right but from the other hand
we said that only the initial layer so
let's last probably the first layer is
like important to us and the other two
are more like features that are you must
not related and we then apply in this

556
01:48:03,004 --> 01:48:16,210
case what that they the lie is a very
important but the pre-trained weights in
them aren't so it's the later layers
that we really want to train the most so

557
01:48:16,210 --> 01:48:23,769
earlier layers likely to be like already
closer to what we want okay so you

558
01:48:23,769 --> 01:48:46,589
started with the latest one and then you
go right so if you go back to our quick
dogs and cats right when we create a
model from pre train from a pre train
model it returns something where all of
the convolutional layers are frozen and
some randomly set fully connected layers
we add to the end our unfrozen and so

559
01:48:46,689 --> 01:48:58,544
when we go fit at first it just trains
the randomly set a randomly initialized
fully connected letters right
and if something is like really close to

560
01:48:58,644 --> 01:49:12,555
imagenet that's often all we need
but because the early early layers are
already good at finding edges gradients
repeating patterns for ears and dogs

561
01:49:12,655 --> 01:49:25,565
heads you know so then when we unfreeze
we set the learning rates for the early
layers to be really low because we don't
want to change the mesh for us the later

562
01:49:25,665 --> 01:49:31,430
ones we set them to be higher where else
for satellite data right this is no
longer true you know the early layers

563
01:49:31,530 --> 01:49:40,025
are still like better than the later
layers but we still probably need to
change them quite a bit so that's right

564
01:49:40,125 --> 01:49:52,150
this learning rate is nine times smaller
than the final learning rate rather than
a thousand times smaller the final loan

565
01:49:52,150 --> 01:49:56,134
rate okay you play with with the weights

566
01:49:56,234 --> 01:50:05,789
of the layers yeah normally most of the
stuff you see online if they talk about
this at all they'll talk about
unfreezing different subsets of layers

567
01:50:07,300 --> 01:50:18,250
and indeed we do unfreeze our randomly
generated runs but what I found is
although the first layer library you can
type learn dot freeze too and just

568
01:50:18,250 --> 01:50:28,179
freeze a subset of layers this approach
of using differential learning rates
seems to be like more flexible to the
point that I never find myself I'm

569
01:50:28,179 --> 01:50:40,270
freezing subsets of layers that I would
expect you to start with that with a
different cell the different learning
rates rather than trying to learn the

570
01:50:40,270 --> 01:50:53,890
last layer so the reason okay so you
could skip this training just the last
layers and just go straight to
differential learning rates but you
probably don't want to and the reason

571
01:50:53,890 --> 01:51:03,050
you probably don't want to is that
there's a difference the convolutional
layers all contain pre-trained weights
so they're like they're not random for

572
01:51:03,150 --> 01:51:11,170
things that are close to imagenet
they're actually really good for things
that are not close to imagenet they're
better than that

573
01:51:11,270 --> 01:51:24,050
all of our fully connected layers
however are totally random so therefore
you would always want to make the fully
connected weights better than random by
training them a bit first because

574
01:51:24,050 --> 01:51:37,256
otherwise if you go straight to unfreeze
then you're actually going to be like
fiddling around of those early early can
early layer weights when the later ones
are still random that's probably not
what you want I think that's another

575
01:51:37,356 --> 01:51:54,349
question here any possible so when we
unfreeze what are the things we're
trying to change there will it change
the Colonel's themselves that that's

576
01:51:54,349 --> 01:52:26,985
always what SGD does yeah so the only
thing what training means is setting
these numbers right and these numbers
and these numbers the weights so the
weights are the weights of the fully
connected layers and the weights in
those kernels and the convolutions so
that's what training means it's and

577
01:52:27,085 --> 01:52:32,190
we'll learn about how to do it with SGD
but training literally is setting those

578
01:52:32,290 --> 01:52:45,239
numbers these numbers on the other hand
are activations they're calculated
they're calculated from the weights and
the previous layers activations or

579
01:52:45,339 --> 01:53:05,966
amounts of questions so you can lift it
up higher and speak badly so in your
example of a cheerleader set of that
English example so you start with very
small size existed for yeah so does it
literally mean you know the model takes
a small area from the entire image that
is 64 bytes so how do we get that 64 by

580
01:53:06,066 --> 01:53:13,489
64 depends on the transforms by default
our transform takes the smallest edge

581
01:53:16,419 --> 01:53:30,004
and recites the whole thing out
samples it so the smallest edge is
societics t4 and then it takes a Center
crop of that okay

582
01:53:30,104 --> 01:53:36,769
although when we're using data
augmentation it actually takes a

583
01:53:36,869 --> 01:53:50,170
randomly chosen prop ie the case where
the image ties to multiple objects don't
in this case like would it be possible
that you would just lose the other
things that they try to predict yeah

584
01:53:50,170 --> 01:53:59,289
which is why data augmentation is
important so by by and particularly
their test time augmentation is going to
be particularly important because you

585
01:53:59,289 --> 01:54:13,129
would you wouldn't want to you know that
there may be a artisanal mine out in the
corner which if you take a center crop
you you don't see so data augmentation
becomes very important yeah so when we

586
01:54:13,229 --> 01:54:22,095
talk on their tributaries are he
receiver up to that's not really what a
model choice Delton that's a great point

587
01:54:22,195 --> 01:54:32,740
that's not the loss function yeah right
the loss function is something we'll be
learning about next week and it uses
cross entropy or otherwise known as like

588
01:54:32,840 --> 01:54:43,114
negative log likelihood the metric is
just this thing that's printed so we can
see what's going on just next to that so

589
01:54:43,214 --> 01:55:04,030
in the context of my deep pass modeling
cannot change data does it trading it
also have to be multiplied so can I
train on just like images of pure cats
and dogs and expect it at prediction
time to predict if I give it a picture
of both having cat eye on it over I've

590
01:55:04,030 --> 01:55:14,949
never tried that and I've never seen an
example of something that needed it
I guess conceptually there's no reason
it wouldn't work but it's kind of out

591
01:55:14,949 --> 01:55:20,131
there and you still use a sigmoid you
would have to make sure you're using a
sigmoid loss function so in this case

592
01:55:20,231 --> 01:55:31,230
faster eyes default would not work
because by default first day I would say
your training data knitter has both a
cat and the dog so you would have to
override the loss function

593
01:55:35,349 --> 01:55:43,589
when you use the differential learning
rates those three learning rates do they
just kind of spread evenly across the

594
01:55:43,689 --> 01:55:51,836
layers yeah we'll talk more about this
later in the course but I mean the
faster I library there's a concept of
layer groups so in something like a

595
01:55:51,936 --> 01:56:03,679
resonant 50 you know there's hundreds of
layers and I think it you don't want to
write down hundreds of learning rates so
I've basically decided for you how to

596
01:56:03,679 --> 01:56:12,524
split them and the the last one always
refers just to the fully connected
layers that we've randomly initialized

597
01:56:12,624 --> 01:56:17,249
and edit to the end and then these ones
are split generally about halfway

598
01:56:17,349 --> 01:56:26,630
through basically I've tried to make it
so that these you know these ones are
kind of the ones which you hardly want
to change at all and these are the ones
you might want to change a little bit

599
01:56:26,630 --> 01:56:36,314
and I don't think we're covered in the
course but if you're interested we can
talk about in the forum there are ways
you can override this behavior to define
your own layer groups if you want to and

600
01:56:36,414 --> 01:56:43,999
is there any way to visualize the model
easily or like don't dump the layers of
the model yeah absolutely

601
01:56:43,999 --> 01:57:02,479
you can let's make sure we've got one
here okay so if you just type learn it
doesn't tell you much at all but what
you can do is go learn summary and that

602
01:57:02,479 --> 01:57:14,419
spits out basically everything there's
all the letters and so you can see in
this case these are the names I
mentioned how they look up names right

603
01:57:14,419 --> 01:57:30,135
so the first layer is called con 2 d 1
and it's going to take as input this is
useful to actually look at it's taking
64 by 64 images which is what we told it
we're going to transform things to this

604
01:57:30,235 --> 01:57:38,449
is three channels high torch like most
things have channels at the end would
say 64 by 64 by 3 ply torch music to the
front so it's 3 by 64 by 64 that's

605
01:57:41,869 --> 01:57:53,530
because it turns out that some of the
GPU computations run faster when it
in that order okay but that happens all
behind-the-scenes automatic plays a part
of that transformation stuff that's kind

606
01:57:53,630 --> 01:58:10,305
of all done automatically is to do that
minus one means however however big the
batch size is in care us they use the
number they use a special number none in
pile types that used minus one so this

607
01:58:10,405 --> 01:58:23,660
is a four dimensional mini batch the
number of elements in the amount of
images in the mini batch is dynamic you
can change that the number of channels
is three number which is a 64 by 64 okay

608
01:58:23,660 --> 01:58:35,240
and so then you can basically see that
this particular convolutional kernel
apparently has 64 kernels in it and it's
also having we haven't talked about this

609
01:58:35,240 --> 01:58:43,485
but convolutions can have something
called a stride that is like Matt pullin
it changes the size so it's returning a

610
01:58:43,585 --> 01:58:47,870
32 by 32 564 kernel tenza and so on and

611
01:58:47,870 --> 01:58:57,375
so forth so that's summary and we'll
learn all about that's doing in detail
on in the second half of the course one

612
01:58:57,475 --> 01:59:18,260
where I clicked in my own data set and I
try to use the in as a really small data
set these currencies from Google Images
and I tried to do a learning rate find
and then the plot and it just it gave me
some numbers which I didn't understand
and the learning rate font yeah and then
the plot was empty so yeah I mean let's

613
01:59:18,260 --> 01:59:24,630
let's talk about that on the forum but
basically the learning rate finder is
going to go through a mini batch at a

614
01:59:24,730 --> 01:59:35,530
time if you've got a tiny data set
there's just not enough mini batches so
the trick is to make your mini bit make
your batch size really small like try
making it like four

615
01:59:38,250 --> 01:59:48,160
okay they were great questions it's not
nothing online to add you know they were
great questions we've got a little bit
past where I hope to but let's let's

616
01:59:48,160 --> 01:59:57,545
quickly talk about structured data so we
can start thinking about it for next
week so this is really weird right to me

617
01:59:57,645 --> 02:00:06,350
there's basically two types of data set
we use in machine learning there's a

618
02:00:06,450 --> 02:00:13,400
type of data like audio images natural
language text where all of the all of

619
02:00:13,500 --> 02:00:30,130
the things inside an object like all of
the pixels inside an image are all the
same kind of thing they're all pixels or
they're all apertures of a waveform or
they're all words I call this kind of

620
02:00:30,130 --> 02:00:37,199
data unstructured and then there's data
sets like a profit and loss statement or
the information about a Facebook user

621
02:00:39,960 --> 02:00:53,072
where each column is like structurally
quite different you know one thing is
representing like how many page views
last month another one is their sex
another one is what zip code they're in
and I call this structure there that

622
02:00:53,172 --> 02:01:05,489
particular terminology is not unusual
like lots of people use that terminology
but lots of people don't
there's no particularly agreed-upon

623
02:01:05,489 --> 02:01:17,727
terminology so when I say structured
data I'm referring to kind of columnar
data as you might find in a database or
a spreadsheet where different columns
represent different kinds of things and

624
02:01:17,827 --> 02:01:26,469
each row represents an observation and
so structured data is probably what most
of you are analyzing most of the time

625
02:01:26,569 --> 02:01:37,220
funnily enough you know academics in the
deep learning world don't really give a
about structured data because it's

626
02:01:37,320 --> 02:01:46,179
pretty hard to get published in fancy
conference proceedings if you're like if
you've got a better logistics model you

627
02:01:46,179 --> 02:01:58,544
know it's the thing that makes the world
goes round it's a thing that makes
everybody you know
and efficiency and make stuff work but
it's largely ignored sadly so we're not

628
02:01:58,644 --> 02:02:03,479
going to ignore it because we're a
practical deep learning and cackled

629
02:02:03,579 --> 02:02:10,070
doesn't ignore it either because people
put prize money up on Cagle to solve
real-world problems so there are some

630
02:02:10,070 --> 02:02:21,130
great capable competitions we can look
at there's one running right now which
is the grocery sales forecasting
competition for Ecuador's largest chain

631
02:02:21,399 --> 02:02:29,164
it's always a little I've got to be a
little careful about how much I show you
about currently running competitions
because I don't want to you know help

632
02:02:29,264 --> 02:02:40,099
you cheat but it so happens there was a
competition a year or two ago for one of
Germany's magistrate chains which is
almost identical so I'm going to show

633
02:02:40,099 --> 02:02:53,780
you how to do that so that was called
the Rossman stores data and so I would
suggest you know first of all try
practicing what we're learning on

634
02:02:53,780 --> 02:02:58,579
Russman right but then see if you can
get it working on on grocery because

635
02:02:58,579 --> 02:03:10,130
currently on the leaderboard no one
seems to basically know what they're
doing in the groceries competition if
you look at the leaderboard the let's

636
02:03:10,130 --> 02:03:19,849
see here yeah these ones around 5 to 9 v
3o are people that are literally finding
like group averages and submitting those
I know because they're kernels that

637
02:03:19,849 --> 02:03:24,369
they're using so you know the basically
the people around 20th place are not
actually doing any machine learning so

638
02:03:26,759 --> 02:03:32,239
yeah let's see if we can improve things
so you'll see there's a less than 3

639
02:03:33,339 --> 02:03:40,189
rossmann notebook sure you get pull ok
in fact you know just reminder you know

640
02:03:40,189 --> 02:03:48,089
before you start working get pull in
you're faster a repo and from time to
time Condor and update for you guys

641
02:03:48,889 --> 02:03:55,259
during the in-person course the Condor
and update you should do it more often
because we're kind of changing things a

642
02:03:55,359 --> 02:04:00,839
little bit um folks in the MOOC you know
more like once a month should be fine

643
02:04:00,939 --> 02:04:06,735
so anyway I just just changed this a
little bit so make sure you get Paul to

644
02:04:06,835 --> 02:04:14,600
get lesson 3 Rossman and there's a
couple of new libraries here one is fast
AI dot structure faster guided

645
02:04:14,600 --> 02:04:25,000
structured contain stuff which is
actually not at all high torch specific
and we actually use that in the machine
learning course as well for doing random
forests with no tie torch at all I

646
02:04:25,000 --> 02:04:33,105
mentioned that because you can use that
particular library without any of the
other parts of fast AI so that can be

647
02:04:33,205 --> 02:04:45,315
handy and then we're also going to use
faster column data which is basically
some stuff that allows us to do fast a
type a torch stuff with columnar

648
02:04:45,415 --> 02:05:03,690
structured data for structured data we
need to use pandas a lot anybody who's
used our data frames will be very
familiar with pandas pandas is basically
an attempt to kind of replicate data
friends in Python you know and a bit

649
02:05:03,790 --> 02:05:21,420
more if you're not entirely familiar
with pandas there's a great book
[Music]
which I think I might have mentioned
before - for data analysis by Wes

650
02:05:21,520 --> 02:05:27,719
McKinney there's a new addition that
just came out a couple of weeks ago

651
02:05:27,819 --> 02:05:33,929
obviously being by the pandas author its
coverage of pandas is excellent but it

652
02:05:34,029 --> 02:05:40,515
also covers numpy scipy matplotlib
scikit-learn - and Jupiter really well

653
02:05:41,015 --> 02:05:59,030
okay and so I'm kind of going to assume
that you know your way around these
libraries to some extent also there was
the workshop we did before they started
and there's a video of that online where
we kind of have a brief mention of all

654
02:05:59,030 --> 02:06:08,610
of those tools structured data is
generally shared as CSV files it was no
different in this competition as you'll

655
02:06:08,710 --> 02:06:19,579
see there's a hyperlink to the rustman
data set here right now if you look at
the bottom of my screen you'll see this
goes to file start faster day.i because
this doesn't require any login or

656
02:06:19,579 --> 02:06:44,150
anything to grab this data set it's as
simple as right clicking copy link
address head over to wherever you want
it and just type W get and the URL okay
so that's because you know it's it's not
behind a login or anything so you can

657
02:06:44,150 --> 02:06:52,679
grab the grab it from there and you can
always read a CSV file with just pandas
don't read CSV now in this particular

658
02:06:52,779 --> 02:07:08,780
case there's a lot of pre-processing
that we do and what I've actually done
here is I've I've actually stolen the
entire pipeline from the third place
winner roster okay so they made all

659
02:07:08,780 --> 02:07:16,880
their data they're really great you know
they better get hub available with
everything that we need and I've ported
it all across and simplified it and

660
02:07:16,880 --> 02:07:28,460
tried to make it pretty easy to
understand this course is about deep
learning not about data processing so
I'm not going to go through it but we

661
02:07:28,460 --> 02:07:34,469
will be going through it in the machine
learning course
in some detail because feature
engineering is really important so if

662
02:07:34,469 --> 02:07:41,520
you're interested you know check out the
machine learning course for that I will

663
02:07:41,520 --> 02:07:48,780
however show you kind of what it looks
like so once we read the CSV Xin you can
see basically what's there so the key

664
02:07:50,909 --> 02:08:26,510
one is for a particular store we have
the
we have the date and we have the sales
for that particular store we know
whether that thing is on promo or not we
know the number of customers at that
particular store had we know whether
that date was a school holiday

665
02:08:29,940 --> 02:08:38,320
we also know what kind of store it is so
this is pretty common right you'll often

666
02:08:38,320 --> 02:08:49,535
get datasets where there's some column
with like just some kind of code we
don't really know what the code means
and most of the time I find it doesn't
matter what it means like normally you

667
02:08:49,635 --> 02:08:59,740
get given a data dictionary when you
start on a project and obviously if
you're working on an internal project
you can ask the people at your company
what does this column mean I kind of

668
02:08:59,740 --> 02:09:03,400
stay away from learning too much about
it I prefer to like to see what the data

669
02:09:03,400 --> 02:09:25,129
says first there's something about what
kind of product are we selling in this
particular row
and then there's information about like
how far away is the nearest competitor
how long have they been open for how

670
02:09:25,229 --> 02:09:37,910
long is the promo being on for for each
store we can find out what state it's in
for each state we can find at the name
of the state this is in Germany and

671
02:09:38,010 --> 02:09:46,400
interestingly they were allowed to
download any data external data they
wanted in this competition just very
common as long as you share it with

672
02:09:46,500 --> 02:09:59,430
everybody else and so some folks tried
downloading data from Google Trends I'm
not sure exactly what it was that they
would check in the trend off but we have
this information from Google Trends

673
02:09:59,430 --> 02:10:05,760
somebody downloaded the weather for
every day in Germany every state

674
02:10:09,250 --> 02:10:22,520
and yeah that's about it right so you
can get a data frame summary with pandas
which kind of lets you see how many

675
02:10:22,620 --> 02:10:27,230
observations and means and standard
deviations again I don't do a hell of a

676
02:10:27,330 --> 02:10:32,780
lot with that early on but it's nice to
note there so what we do you know this

677
02:10:32,880 --> 02:10:38,920
is called a relational data set a
relational data set is one where there's
quite a few tables we have to join

678
02:10:38,920 --> 02:10:45,020
together it's very easy to do that in
pandas there's a thing called merge so
great little function to do that and so

679
02:10:45,120 --> 02:10:54,405
I just started joining everything
together
joining the weather or the Google Trends
stores yeah that's about everything I

680
02:10:54,505 --> 02:11:05,915
guess you'll see there's one thing that
I'm using from the FASTA a library which
is called add date part we talked about
this a lot in the machine learning

681
02:11:06,015 --> 02:11:18,460
course but basically this is going to
take a date and pull out of a bunch of
columns day of week is at the start of a
quarter month of year so on and so forth
and add them all in to the dataset okay

682
02:11:18,460 --> 02:11:29,950
so this is all standard pre-processing
all right so we join everything together
we fiddle around with some of the dates
a little bit some of them are in month
in year format we turn it into date

683
02:11:29,950 --> 02:11:48,605
format we spend a lot of time trying to
take information about for example
holidays and add a column for like how
long until the next holiday how long has
it been since the last holiday ditto for
promos so on and so forth okay so we do

684
02:11:48,705 --> 02:12:16,420
all that and at the very end we
basically save a big structured data
file that contains all that stuff
something that those of you that use
pandas may not be aware of is that
there's a very cool new format called
feather which you can save a panda's
data frame into this feather format it's
kind of pretty much takes it as it sits
in RAM and dumps it to the disk and so
it's like really really really fast the

685
02:12:16,420 --> 02:12:23,540
reason that you need to know this is
because the ecuadorian grocery
competition is on now has
350 million records so you will care

686
02:12:23,640 --> 02:12:34,925
about how long things take a talk I
believe about six seconds for me to save
three hundred and fifty million records
to feather format so that's pretty cool

687
02:12:35,025 --> 02:12:45,400
so at the end of all that I'd save it as
a feather format and for the rest of
this discussion I'm just going to take
it as given that we've got this nicely
pre-processed feature engineered file

688
02:12:45,400 --> 02:13:05,495
and I can just go read better okay but
for you to play along at home you will
have to run those previous cells oh
except the see these ones have commented
out you don't have to run those because
the file that you download from files
doc bastard AI has already done that for

689
02:13:05,595 --> 02:13:20,252
you okay all right so we basically have
all these columns so it basically is
going to tell us you know how many of
this thing was sold on this date at this

690
02:13:20,352 --> 02:13:36,160
store and so the goal of this
competition is to find out how many
things will be sold for each store for
each type of thing in the future okay

691
02:13:36,160 --> 02:13:48,640
and so that's basically what we're going
to be trying to do and so here's an
example of what some of the data looks
like and so next week we're going to see

692
02:13:48,640 --> 02:13:55,340
how to go through these steps but
basically what we're going to learn is
we're going to learn to split the
columns into two types some columns were

693
02:13:55,440 --> 02:14:18,005
going to treat as categorical which is
to say store ID one and store ID - I'm
not numerically related to each other
they're categories right we're going to
treat day of week like that - Monday and
Tuesday day zero and day one not
numerically related to each other where

694
02:14:18,105 --> 02:14:26,170
else distance in kilometers to the
nearest competitor that's a number that
we're going to treat numerically right

695
02:14:26,170 --> 02:14:31,854
so in other words the categorical
variables we basically are going to one
how to encode them you can think of it
as one hot encoding on where

696
02:14:31,954 --> 02:14:39,674
the continuous variables we're going to
be feeding into fully connected layers

697
02:14:39,774 --> 02:14:55,994
just as is okay so what we'll be doing
is we'll be basically creating a
validation set and you'll see like a lot
of these are start to look familiar this
is the same function we used on planet
and dog breeds to create a validation

698
02:14:56,094 --> 02:15:09,109
set there's some stuff that you haven't
seen before
where we're going to basically rather
than saying image data dot from CSV
we're going to say columnar data from

699
02:15:09,109 --> 02:15:13,780
data frame right so you can see like the
basic API concepts will be the same but
they're a little different right but

700
02:15:17,689 --> 02:15:23,659
just like before we're going to get a
learner and we're going to go lr find to
find our best learning rate and then

701
02:15:26,359 --> 02:15:34,459
we're going to go dot fit with a metric
with a cycle length okay so the basic
sequence who's going to end up looking

702
02:15:34,559 --> 02:15:39,079
hopefully very familiar okay so we're
out of time

703
02:15:40,339 --> 02:15:48,791
so what I suggest you do this week is
like try to enter as many capital image
competitions as possible like like try

704
02:15:48,891 --> 02:16:08,399
to really get this feel for like cycling
learning rates plotting things you know
that that post I showed you at the start
of class today that kind of took you
through lesson one like really go

705
02:16:08,499 --> 02:16:19,879
through that on as many image datasets
as you can you just feel really
comfortable with it right because you
want to get to the point where next week
when we start talking about structured

706
02:16:19,879 --> 02:16:28,869
data that this idea of like how learners
kind of work and data works and data
loaders and data sets and looking at
pictures should be really you know

707
02:16:28,869 --> 02:16:34,119
intuitive all right good luck see you
next week

