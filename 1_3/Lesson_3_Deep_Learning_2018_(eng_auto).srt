1
00:00:00,149 --> 00:00:03,600
welcome back everybody
[Music]

2
00:00:03,600 --> 00:00:09,599
I'm sure you've noticed but there's been
a lot of cool activity on the forum this

3
00:00:09,599 --> 00:00:12,780
week and one of the things that's been
really great to see is that a lot of you
have started creating really helpful

4
00:00:15,630 --> 00:00:20,010
materials both for your classmates to
better understand stuff and also for you

5
00:00:20,010 --> 00:00:25,920
to better understand stuff by trying to
teach what you've learned I just wanted

6
00:00:25,920 --> 00:00:32,340
to highlight a few i've actually posted
to the wiki thread of a few of these but

7
00:00:32,340 --> 00:00:38,579
there's there's lots more Russian has
posted a whole bunch of nice

8
00:00:38,579 --> 00:00:42,480
introductory tutorials so for example if
you're having any trouble getting
connected with AWS she's got a whole

9
00:00:46,079 --> 00:00:51,629
step-by-step how to go about logging in
and getting everything working which i

10
00:00:51,629 --> 00:00:57,750
think is a really terrific thing and so
it's a kind of thing that if you are

11
00:00:57,750 --> 00:01:01,289
writing some notes for yourself to
remind you how to do it

12
00:01:01,289 --> 00:01:05,460
you may as well post them for others to
do it as well and by using a markdown
file like this and it's actually good

13
00:01:07,439 --> 00:01:10,549
practice if you haven't used github
before if you put it up on github

14
00:01:10,549 --> 00:01:17,130
everybody can now use it or of course
you can just put it in the forum so more
advanced a thing that Reshma wrote up

15
00:01:20,009 --> 00:01:25,140
about is she noticed that I like using
Tmax which is a handy little thing which

16
00:01:25,140 --> 00:01:33,720
lets me lets me basically have a window
I'll show you so as soon as I log into

17
00:01:33,720 --> 00:01:39,869
my computer if I run Tmax you'll see
that all of my windows pop straight up

18
00:01:39,869 --> 00:01:44,250
basically and I can like continue
running stuff in the background and I

19
00:01:44,250 --> 00:01:48,210
can like I've got them over here and I
can kind of zoom into it or I can move

20
00:01:48,210 --> 00:01:53,579
over to the top which is here so Jupiter
colonel running and so forth so if that

21
00:01:53,579 --> 00:01:57,479
sounds interesting
Reshma has a tutorial here on how you

22
00:01:57,479 --> 00:02:03,060
can use two maps and it's actually got a
whole bunch of stuff in her github so

23
00:02:03,060 --> 00:02:10,860
that's that's really cool I built among
has written a very nice kind of summary

24
00:02:10,860 --> 00:02:17,319
basically of our last lesson
which kind of covers what are the key
things we did and why did we do them so

25
00:02:19,120 --> 00:02:23,110
if you are a kind of
wondering like how does it fit together

26
00:02:23,110 --> 00:02:28,810
I think this is a really helpful summary
like what if those couple of hours look

27
00:02:28,810 --> 00:02:38,470
like if we summarize it all into a page
or two I also really like Pavel has dad

28
00:02:38,470 --> 00:02:45,069
kind of done a deep dive on the learning
rate finder which is a topic that a lot

29
00:02:45,069 --> 00:02:49,330
of you have been interested in learning
more about particularly those of you who

30
00:02:49,330 --> 00:02:52,720
have done deep learning before I
realized that this is like a solution to

31
00:02:52,720 --> 00:02:55,989
a problem that you've been having for a
long time and haven't seen before and so

32
00:02:55,989 --> 00:02:59,500
it's kind of something which hasn't
really been blogged about before so this

33
00:02:59,500 --> 00:03:04,079
is the first I've seen it's logged about
so when I put this on Twitter a link to

34
00:03:04,079 --> 00:03:08,739
pebbles post it's been shared you know
hundreds of times it's been really

35
00:03:08,739 --> 00:03:13,560
really popular and viewed many thousands
of times so that's some great content

36
00:03:13,560 --> 00:03:18,579
radec has posted lots of cool stuff I
really like this practitioners guide to

37
00:03:18,579 --> 00:03:23,079
apply torch which again this is more for
more advanced students but it's like

38
00:03:23,079 --> 00:03:28,650
digging into people who have never used
hi torch before but know a bit about

39
00:03:28,650 --> 00:03:32,260
numerical programming in general and
it's a quick introduction to how high
torch is different and then there's been

40
00:03:35,560 --> 00:03:38,709
some interesting little bits of research
like what's the relationship between

41
00:03:38,709 --> 00:03:42,639
learning rate and batch sites so one of
the students actually asked me this

42
00:03:42,639 --> 00:03:46,030
before class and I said oh well one of
the other students has written an

43
00:03:46,030 --> 00:03:51,489
analysis of exactly that so what he's
done is basically looked through and

44
00:03:51,489 --> 00:03:54,010
tried different batch sizes and
different learning rates and tried to

45
00:03:54,010 --> 00:03:58,510
see how they seemed to relate together
and these are all like cool experiments

46
00:03:58,510 --> 00:04:04,060
which you know you can try yourself
I predict again he's written something

47
00:04:04,060 --> 00:04:10,810
again a kind of a research into this
question I made a claim that the the

48
00:04:10,810 --> 00:04:14,769
stochastic gradient descent with
restarts finds more generalizable parts

49
00:04:14,769 --> 00:04:18,340
of the function surface because they're
kind of flatter and he's been trying to
figure out is there a way to measure

50
00:04:19,599 --> 00:04:23,770
that more directly not quite successful
yet but a really interesting piece of

51
00:04:23,770 --> 00:04:32,999
research got some introductions to
convolutional neural networks and then

52
00:04:32,999 --> 00:04:36,389
something that we'll be learning about
towards the end of this course but I'm

53
00:04:36,389 --> 00:04:40,499
sure you've noticed we're using
something called ResNet and a nonce aha

54
00:04:40,499 --> 00:04:45,569
actually posted a pretty impressive
analysis of like watts arrest net and

55
00:04:45,569 --> 00:04:49,169
why is it interesting and this one's
actually being very already shared very
widely around the internet I've seen

56
00:04:50,639 --> 00:04:57,059
also so some more advanced students who
are interested in jumping ahead can look

57
00:04:57,059 --> 00:05:04,349
at that and uphill Tamang also has done
something similar so lots of yeah lots
of stuff going on on the forums

58
00:05:06,059 --> 00:05:11,069
I'm sure you've also noticed we have a
beginner forum now specifically for you

59
00:05:11,069 --> 00:05:17,479
know asking questions which you know it
is always the case that there are no
dumb questions but when there's lots of

60
00:05:19,860 --> 00:05:23,369
people around you talking about advanced
topics it might not feel that way so

61
00:05:23,369 --> 00:05:29,819
hopefully the beginners forum is just a
less intimidating space and if there are

62
00:05:29,819 --> 00:05:34,079
more advanced student who can help
answer those questions please do but

63
00:05:34,079 --> 00:05:37,800
remember when you do answer those
questions try to answer in a way that's

64
00:05:37,800 --> 00:05:41,429
friendly to people that maybe you know
have no more than a year of programming
experience you haven't done any machine

65
00:05:42,989 --> 00:05:52,979
learning before so you know I hope other
people in the class feel like you can

66
00:05:52,979 --> 00:05:56,159
contribute as well and just remember all
of the people we just looked at or many

67
00:05:56,159 --> 00:06:00,959
of them I believe have never hosted
anything to the internet before right I

68
00:06:00,959 --> 00:06:04,769
mean you don't have to be a particular
kind of person to be allowed to block

69
00:06:04,769 --> 00:06:11,399
something you can just jot down your
notes throw it up there and one handy

70
00:06:11,399 --> 00:06:15,779
thing is if you just put it on the forum
and you're not quite sure of some of the

71
00:06:15,779 --> 00:06:20,399
details then then you know you have an
opportunity to get feedback and say like

72
00:06:20,399 --> 00:06:23,579
oh well that's not quite how that works
you know actually it works this way

73
00:06:23,579 --> 00:06:26,819
instead or oh that's a really
interesting insight have you thought

74
00:06:26,819 --> 00:06:32,159
about taking this further and so forth
so what we've done so far is a kind of

75
00:06:32,159 --> 00:06:38,969
an injury an introduction as a just as a
practitioner to convolutional neural

76
00:06:38,969 --> 00:06:43,389
networks for images and we haven't
really talked much at all about

77
00:06:43,389 --> 00:06:48,280
the theory or why they work or the math
of them but on the other hand what we
have done is seen how to build a model

78
00:06:52,659 --> 00:06:59,319
which actually works exceptionally well
compact world-class level models and

79
00:06:59,319 --> 00:07:06,669
we'll kind of review a little bit of
that today and then also today we're
going to dig in a little quite a lot

80
00:07:08,289 --> 00:07:12,280
more actually into the underlying theory
of like what is a what is a CNN what's a

81
00:07:12,280 --> 00:07:16,000
convolution how does this work and then
we're going to kind of go through this

82
00:07:16,000 --> 00:07:21,039
this cycle where we're going to dig
we're going to do a little intro into a

83
00:07:21,039 --> 00:07:25,930
whole bunch of application areas using
neural nets for structured data so kind

84
00:07:25,930 --> 00:07:31,449
of like logistics or forecasting or you
know financial data or that kind of

85
00:07:31,449 --> 00:07:36,669
thing and then looking at language
applications and LP applications using
recurrent neural Nets and then

86
00:07:39,210 --> 00:07:45,550
collaborative filtering for
recommendations and systems and so these

87
00:07:45,550 --> 00:07:50,199
will all be like similar to what we've
done for cnn's for images would be like

88
00:07:50,199 --> 00:07:53,020
here's how you can get a
state-of-the-art result without digging
into the theory but but knowing how to

89
00:07:55,150 --> 00:08:00,400
actually make a work and then we're kind
of going to go back through those almost

90
00:08:00,400 --> 00:08:03,669
in reverse order so then we're going to
dig right into collaborative filtering

91
00:08:03,669 --> 00:08:08,409
in a lot of detail and see how how to
write the code underneath and how the

92
00:08:08,409 --> 00:08:11,589
math works underneath and then we're
going to do the same thing for the

93
00:08:11,589 --> 00:08:15,400
structured data analysis we're going to
do the same thing for comp nets for

94
00:08:15,400 --> 00:08:20,620
images and finally an in depth deep dive
into apparent neural networks so that's

95
00:08:20,620 --> 00:08:29,020
kind of where we're okay so let's start
by doing a little bit of a review and I

96
00:08:29,020 --> 00:08:35,050
want to also provide a bit more detail
on some on some steps that we only

97
00:08:35,049 --> 00:08:39,208
briefly slipped over so I want to make
sure that we're all able to complete

98
00:08:39,208 --> 00:08:45,310
kind of last week's assignment which was
that the dog breeze I mean to basically

99
00:08:45,310 --> 00:08:48,790
apply what you've learned with another
data set and I thought the easiest one

100
00:08:48,790 --> 00:08:52,089
to do with me the dog breeds cattle
competition and so I want to make sure

101
00:08:52,089 --> 00:08:55,889
everybody has everything you need to do
this right now so and the

102
00:08:55,889 --> 00:09:01,230
first thing is to make sure that you
know how to download data and so there's

103
00:09:01,230 --> 00:09:04,799
there's two main places at the moment
we're kind of downloading data from one

104
00:09:04,799 --> 00:09:11,339
is from cattle and the other is from
like anywhere else and so I'll first of

105
00:09:11,339 --> 00:09:18,569
all do the the casual version so to
download from cattle we use something
called cattle CLI which is gear and to

106
00:09:24,809 --> 00:09:34,980
install what I think it's already in the
system will shake yeah so it should

107
00:09:34,980 --> 00:09:41,040
already be in your environment but to
make sure one thing that happens is

108
00:09:41,040 --> 00:09:44,369
because this is downloading from the
cattle website through experience rating

109
00:09:44,369 --> 00:09:48,149
every time cap will change us the
website it breaks so anytime you try to

110
00:09:48,149 --> 00:09:53,279
use it and if the cattles websites
changed recent when you'll need to make

111
00:09:53,279 --> 00:10:00,379
sure you get the most recent version so
you can always go to pip install cable -

112
00:10:00,379 --> 00:10:07,829
CL I - - upgrade and so that'll just
make sure that you've got the latest
version of of it and everything that it

113
00:10:10,290 --> 00:10:16,679
depends on okay and so then having done
that you can follow the instructions

114
00:10:16,679 --> 00:10:21,720
actually I think Reshma was kind enough
to they go there's a cable CLI you know

115
00:10:21,720 --> 00:10:29,910
like everything you need to know can be
under Reshma's github so basically to do
that at the next step you go kg download

116
00:10:35,089 --> 00:10:41,100
and then you provide your username with
- you you provide your password with - P

117
00:10:41,100 --> 00:10:46,019
and then - see it the competition name
and a lot of people in the forum is

118
00:10:46,019 --> 00:10:49,529
being confused about what to enter here
and so the key thing to note is that

119
00:10:49,529 --> 00:10:55,559
when you're at a capital competition
after the /c there's a specific name

120
00:10:55,559 --> 00:11:02,100
planet - understanding - etcetera right
that's the name you need okay the other

121
00:11:02,100 --> 00:11:06,809
thing you'll need to make sure is that
you've on your own computer have

122
00:11:06,809 --> 00:11:09,670
attempted to click download at least
once because when you do

123
00:11:09,670 --> 00:11:15,730
ask you to accept the rules if you've
forgotten to do that kg download will

124
00:11:15,730 --> 00:11:18,850
give you a hint it'll say oh it looks
like you might have forgotten the rules

125
00:11:18,850 --> 00:11:24,010
if you log into cattle with like a
Google account like anything other than

126
00:11:24,010 --> 00:11:28,720
a username password this won't work so
you'll need to click forgot password on

127
00:11:28,720 --> 00:11:33,550
Kaggle and get them to send you a normal
password so that's the cattle version
right and so when you do that you end up

128
00:11:36,220 --> 00:11:41,430
with a whole folder created for you with
all of that competition and data in it
so a couple of reasons you might want to

129
00:11:44,080 --> 00:11:48,220
not use that the first years that you're
using a data set that's not on cattle

130
00:11:48,220 --> 00:11:52,600
the second is that you don't want all of
the data sets in a cattle competition

131
00:11:52,600 --> 00:11:56,800
for example the planet competition that
we've been looking at a little bit we'll
look at again today has data in two

132
00:12:00,010 --> 00:12:06,370
formats TIFF and JPEG the TIFF is 19
gigabytes and the JPEG is 600 megabytes

133
00:12:06,370 --> 00:12:11,890
so you probably don't want to download
both so I'll show you a really cool kit

134
00:12:11,890 --> 00:12:14,920
which actually somebody on the forum
taught me I think was one of the young

135
00:12:14,920 --> 00:12:24,160
MSN students here at USF there's a
Chrome extension cord curl W get so you

136
00:12:24,160 --> 00:12:29,590
can just search for a curl W get and
then you install it by just clicking on

137
00:12:29,590 --> 00:12:35,470
installed and having an extension before
and then from now on every time you try

138
00:12:35,470 --> 00:12:39,840
to download something so I'll try and
download this file

139
00:12:40,480 --> 00:12:45,220
and I'll just go ahead and cancel it
right and now you see this little yellow

140
00:12:45,220 --> 00:12:51,310
button that's added up here there's a
whole command here right so I can copy
that and paste it into my window and hit

141
00:12:59,949 --> 00:13:06,040
go and it's there cuz okay so what that
does is like all of your cookies and

142
00:13:06,040 --> 00:13:10,449
headers and everything else needed to
download that file is like say so this

143
00:13:10,449 --> 00:13:16,029
is not just useful for downloading data
it's also useful if you like trying to

144
00:13:16,029 --> 00:13:19,420
download some I don't know TV show or
something anything where you're it's

145
00:13:19,420 --> 00:13:25,120
hidden behind a login or something you
can you can grab it and actually that is

146
00:13:25,120 --> 00:13:28,240
very useful for data science because
quite often we want to analyze things

147
00:13:28,240 --> 00:13:33,610
like videos on our on our consoles so
this is a good trick alright so there's

148
00:13:33,610 --> 00:13:44,019
two ways to get the data so then having
got the data you then need to build your

149
00:13:44,019 --> 00:13:49,209
model right so what I tend to do like
you'll notice that I tend to assume that
the data is in a directory called data

150
00:13:51,880 --> 00:13:57,670
that's a subdirectory of wherever your
notebook is right now you don't

151
00:13:57,670 --> 00:14:01,600
necessarily actually want to put your
data there you might want to put it

152
00:14:01,600 --> 00:14:04,990
directly in your home directory or you
might wanna put it on another drive or

153
00:14:04,990 --> 00:14:11,620
whatever so what I do is if you look
inside my courses do one folder you'll

154
00:14:11,620 --> 00:14:17,949
see that data is actually a symbolic
link to a different drive alright so you

155
00:14:17,949 --> 00:14:22,269
can put it anywhere you like and then
you can just add a symbolic link or you

156
00:14:22,269 --> 00:14:26,050
can just put it there directly it's up
to you if you haven't used symlinks

157
00:14:26,050 --> 00:14:32,019
before they're like aliases or shortcuts
on the mac or windows very handy and

158
00:14:32,019 --> 00:14:35,829
there's some threads on the forum about
how to use them if you want help with

159
00:14:35,829 --> 00:14:41,290
that that for example is also how we
actually have the fast AI modules

160
00:14:41,290 --> 00:14:45,490
available from the same place as our
notebooks it's just a symlink

161
00:14:45,490 --> 00:14:51,790
to where they come from anytime you want
to see like where things actually point

162
00:14:51,790 --> 00:14:57,490
to in Linux you can just use the
- L flag - listing a directory and

163
00:14:57,490 --> 00:15:01,450
that'll show you where the symlinks
exist still lost I'll show you which

164
00:15:01,450 --> 00:15:12,340
scenes the directories so forth okay so
one thing which may be a little unclear

165
00:15:12,340 --> 00:15:17,860
based on what we've done so far is like
how little code you actually need to do

166
00:15:17,860 --> 00:15:22,870
this end to it so what I've got here is
is in a single window is an entire

167
00:15:22,870 --> 00:15:27,220
end-to-end process to get a
state-of-the-art result put cats versus

168
00:15:27,220 --> 00:15:31,810
dogs all right I've only step I've
skipped is the bit where we've
downloaded it from title and then where

169
00:15:33,970 --> 00:15:43,180
we unzip it all right so these are
literally all the steps and so we import
our libraries and actually if you import

170
00:15:45,730 --> 00:15:51,130
this one Kampf loner that basically
imports everything else so that's that

171
00:15:51,130 --> 00:15:55,630
we need to tell at the path of where
things are the size that we want the

172
00:15:55,630 --> 00:16:00,190
batch size that we want right so then
and we're going to learn a lot more

173
00:16:00,190 --> 00:16:04,000
about what these do very shortly but
basically we say how do we want to

174
00:16:04,000 --> 00:16:08,530
transform our data so we want to
transform it in a way that's suitable to

175
00:16:08,530 --> 00:16:13,450
this particular kind of model and it
assumes that the photos aside on photos

176
00:16:13,450 --> 00:16:18,490
and that we're going to zoom in up to
ten percent each time we say that we

177
00:16:18,490 --> 00:16:23,020
want to get some data based on paths and
so remember this is this idea that

178
00:16:23,020 --> 00:16:26,860
there's a path called cats and the path
called dogs and they're inside a path

179
00:16:26,860 --> 00:16:34,360
called train and a path called valid
note that you can always overwrite these

180
00:16:34,360 --> 00:16:37,360
with other things so if your things are
in different folders

181
00:16:37,360 --> 00:16:42,160
you could either rename them or you can
see here there's like a train name and a

182
00:16:42,160 --> 00:16:48,280
bowel name you can always pick something
else here also notice there's a test

183
00:16:48,280 --> 00:16:52,839
name so if you want to submit some into
cattle you'll need to fill in the name

184
00:16:52,839 --> 00:16:56,380
the name of the folder where the test
sentence and obviously those those won't
be labeled

185
00:16:59,770 --> 00:17:04,910
so then we create a model from a
pre-training model it's from a resonant

186
00:17:04,910 --> 00:17:11,600
50 model using this data and then we
call fit and remember by default that
has all of the layers but the last few

187
00:17:13,640 --> 00:17:17,420
frozen and again we'll learn a lot more
about what that means

188
00:17:17,420 --> 00:17:23,110
and so that's that's what that does so
that that took two and a half minutes
notice here I didn't say pre-compute

189
00:17:26,119 --> 00:17:29,510
equals true again there's been some
confusion on the forums about like what

190
00:17:29,510 --> 00:17:35,390
that means it's only a is only something
that makes it a little faster for this

191
00:17:35,390 --> 00:17:39,710
first step right so you can always skip
it and if you're at all confused about

192
00:17:39,710 --> 00:17:45,040
it or it's causing you any problems just
leave it off right because it's just a

193
00:17:45,040 --> 00:17:50,660
it's just a shortcut which caches some
of the intermediate steps that don't

194
00:17:50,660 --> 00:17:54,710
have to be recalculating each time and
remember that when we are using pre

195
00:17:54,710 --> 00:17:59,390
computed activations data or
augmentation doesn't work right so even

196
00:17:59,390 --> 00:18:04,070
if you ask for a data augmentation if
you've got pre compute equals true it
doesn't actually do any data

197
00:18:05,090 --> 00:18:11,180
augmentation because it's using the
cached non augmented activations so in

198
00:18:11,180 --> 00:18:14,780
this case to keep this as simple as
possible I have no pre computed anything

199
00:18:14,780 --> 00:18:24,110
going on so I do three cycles of length
one and then I can then unfreeze so it's

200
00:18:24,110 --> 00:18:27,770
now going to train the whole thing
something we haven't seen before and

201
00:18:27,770 --> 00:18:33,260
we'll learn about in the second half is
called B and freeze for now all you need

202
00:18:33,260 --> 00:18:39,170
to know is that if you're using a model
like a bigger deeper model like ResNet

203
00:18:39,170 --> 00:18:46,250
50 or rez next 101 on a data set that's
very very similar to imagenet like these

204
00:18:46,250 --> 00:18:51,860
cat sandbox data set sort of words it's
like sidon photos of standard objects

205
00:18:51,860 --> 00:18:56,720
you know of a similar size to image turn
and money somewhere between 200 and 500

206
00:18:56,720 --> 00:19:01,400
pixels
you should probably add this line when

207
00:19:01,400 --> 00:19:06,080
you unfreeze for those of you that are
more advanced what it's doing is it's
causing the batch normalization moving

208
00:19:09,290 --> 00:19:12,070
averages to not be updated
but in the second half of this course

209
00:19:12,070 --> 00:19:15,610
we're gonna learn all about why we do
that it's something that's not supported

210
00:19:15,610 --> 00:19:20,409
by any other library but it turns out to
be super important anyway so we do one

211
00:19:20,409 --> 00:19:26,039
more
epoch with training the whole network
and then at the end we use test time

212
00:19:28,809 --> 00:19:34,870
augmentation to ensure that we get the
best predictions we can and that gives

213
00:19:34,870 --> 00:19:41,049
us ninety nine point four five percent
so that's that's it right so when you

214
00:19:41,049 --> 00:19:47,139
try a new data set they're basically the
minimum set of steps that you would need

215
00:19:47,139 --> 00:19:51,820
to follow you'll notice this is assuming
I already know what learning wrote to

216
00:19:51,820 --> 00:19:56,019
use so you'd use the learning rate
finder for that it's assuming that I

217
00:19:56,019 --> 00:20:02,799
know that the directory layout and so
forth so that's kind of a minimum set

218
00:20:02,799 --> 00:20:07,210
now one of the things that I wanted to
make sure you had an understanding of

219
00:20:07,210 --> 00:20:12,999
how to do is how to use other libraries
other than fast AI and so I feel like

220
00:20:12,999 --> 00:20:18,100
the best thing to to look at is to look
at care us because care us is a library
just like fast AI sits on top of pi

221
00:20:20,590 --> 00:20:25,450
torch care us sits on top of actually a
whole variety of different backends it

222
00:20:25,450 --> 00:20:28,769
fits mainly people nowadays use it with
tensorflow

223
00:20:28,769 --> 00:20:37,450
there's also an MX net version there's
also a Microsoft CNT K version so what

224
00:20:37,450 --> 00:20:43,480
I've got if you do a git pull you'll see
that there's a something called care us

225
00:20:43,480 --> 00:20:48,429
less than one where I've attempted to
replicate at least parts of lesson one

226
00:20:48,429 --> 00:20:59,110
in care us just to give you a sense of
how that works I'm not going to talk
more about batch two norm freeze now

227
00:21:00,850 --> 00:21:07,659
other than to say if you're using
something which has got a number larger

228
00:21:07,659 --> 00:21:11,950
than 34 at the end
so like ResNet 50 or res next 101 and

229
00:21:11,950 --> 00:21:18,129
you're trading a data set that has that
is very similar to image net so it's
like normal photos of normal sizes where

230
00:21:20,529 --> 00:21:24,520
the thing of interest takes up most of
the frame then you probably should

231
00:21:24,520 --> 00:21:30,400
at the end fries true after unfreeze
if in doubt try trading it with and then

232
00:21:30,400 --> 00:21:35,679
try trading it without more advanced
students will certainly talk about it on

233
00:21:35,679 --> 00:21:39,910
the forums this week and we will be
talking about the details of it in the

234
00:21:39,910 --> 00:21:46,720
second half of the course when we come
back to our CNN in death section in the

235
00:21:46,720 --> 00:22:01,480
second last lesson so with care us again
we import a bunch of stuff and remember

236
00:22:01,480 --> 00:22:04,630
I mentioned that this idea that you've
got a thing called train and a thing

237
00:22:04,630 --> 00:22:07,540
called valid and inside that you've got
a thing called dogs and things called
cats is a standard way of providing

238
00:22:10,679 --> 00:22:16,870
image labelled images so Karis does that
too right so it's going to tell it where
the training set and the validation set

239
00:22:18,610 --> 00:22:27,100
are twice what batch size to used now
you'll notice in Karis we need much much

240
00:22:27,100 --> 00:22:34,600
much more code to do the same thing more
importantly each part of that code has

241
00:22:34,600 --> 00:22:38,650
many many many more things you have to
set and if you set them wrong everything

242
00:22:38,650 --> 00:22:44,650
breaks right so I'll give you a summary
of what they are

243
00:22:44,650 --> 00:22:51,250
so you're basically rather than creating
a single data object in chaos we first

244
00:22:51,250 --> 00:22:55,030
of all have to define something called a
data generator to say kind of generate

245
00:22:55,030 --> 00:23:00,010
the data and so a data generator we
basically have to say what kind of data

246
00:23:00,010 --> 00:23:08,190
augmentation we want to do and we also
we actually have to say what kind of

247
00:23:08,190 --> 00:23:13,179
normalization do we want to do
so we're else with fast AI we just say

248
00:23:13,179 --> 00:23:18,160
whatever ResNet 50 requires just do that
for me please we actually have to kind

249
00:23:18,160 --> 00:23:21,700
of know a little bit about what's
expected of us

250
00:23:21,700 --> 00:23:25,300
generally speaking copying and pasting
cos code from the internet is a good way

251
00:23:25,300 --> 00:23:30,220
to make sure you've got the right the
right stuff to make that work and again

252
00:23:30,220 --> 00:23:34,600
it doesn't have a kind of a standard set
of like here the best data augmentation
parameters to use for photos so you know

253
00:23:37,280 --> 00:23:43,550
I've copied and pasted all of this from
the Kaos documentation so I don't know

254
00:23:43,550 --> 00:23:46,910
if it's I don't think it's the best set
to use it all but it's the set that

255
00:23:46,910 --> 00:23:51,470
they're using in their docks so having
said this is how I want to generate data

256
00:23:51,470 --> 00:23:56,540
so horizontally fit sometimes you know
zoom sometimes sheer sometimes we then

257
00:23:56,540 --> 00:24:01,880
create a generator from that by taking
that data generator and saying I want to

258
00:24:01,880 --> 00:24:07,070
generate images by looking from a
directory we pass in the directory which
is of the same directory structure that

259
00:24:09,440 --> 00:24:14,090
fast AI users and you'll see there's
some overlaps with kind of how fast AI
works here you tell it what size images

260
00:24:16,220 --> 00:24:19,850
you want to create you tell at what
batch size you want in your mini batches
and then there's something you not to

261
00:24:22,640 --> 00:24:26,390
worry about too much but basically if
you're just got two possible outcomes

262
00:24:26,390 --> 00:24:29,960
you would generally say binary here if
you've got multiple possible outcomes

263
00:24:29,960 --> 00:24:33,440
would say categorical yeah so we've only
got cats or dogs

264
00:24:33,440 --> 00:24:39,710
so it's binary so an example of like
where things get a little more complex

265
00:24:39,710 --> 00:24:43,580
is you have to do the same thing for the
validation set so it's up to you to

266
00:24:43,580 --> 00:24:47,570
create a data generator that doesn't
have data augmentation because obviously

267
00:24:47,570 --> 00:24:51,890
for the validation set unless you're
using t/ta that's going to start things

268
00:24:51,890 --> 00:24:59,090
up you also when you train you randomly
reorder the images so that they're
always shown in different orders to make

269
00:25:01,190 --> 00:25:06,080
it more random but with a validation
it's vital that you don't do that
because if you shuffle the validation

270
00:25:07,520 --> 00:25:11,270
set you then can't track how well you're
doing it's in a different order for the

271
00:25:11,270 --> 00:25:14,870
labels
that's a basically these are the kind of

272
00:25:14,870 --> 00:25:22,160
steps you have to do every time with
care us so again the reason I was using

273
00:25:22,160 --> 00:25:26,960
ResNet 50 before is chaos doesn't have
ResNet 34 unfortunately so I just wanted

274
00:25:26,960 --> 00:25:32,750
to compare like with Mike so we're going
to use resident 50 here there isn't the

275
00:25:32,750 --> 00:25:37,520
same idea with chaos of saying like
constructor model that is suitable for

276
00:25:37,520 --> 00:25:42,290
this data set for me so you have to do
it by hand right so the way you do it is

277
00:25:42,290 --> 00:25:47,330
to basically say this is my base model
and then you have to construct on top of

278
00:25:47,330 --> 00:25:50,600
that manually the layers that you want
to add

279
00:25:50,600 --> 00:25:53,809
and so by the end of this course you'll
understand a way it is that these

280
00:25:53,809 --> 00:26:01,370
particular three layers other layers
that we add so having done that in chaos

281
00:26:01,370 --> 00:26:06,070
you basically say okay this is my model
and then again there isn't like a

282
00:26:06,070 --> 00:26:10,880
concept to it like automatically
freezing things or an API for that so

283
00:26:10,880 --> 00:26:16,190
you just have to allow look through the
layers that you want to freeze and call

284
00:26:16,190 --> 00:26:22,940
dot trainable equals false on them in
Karis there's a concept we don't have in

285
00:26:22,940 --> 00:26:27,110
fast AI or play a torch of compiling a
model so basically once your model is
ready to use you have to compile it

286
00:26:28,960 --> 00:26:33,860
passing in what kind of optimizer to use
what kind of loss to look for about
metric so again with fast AI you don't

287
00:26:37,039 --> 00:26:41,720
have to pass this in because we know
what loss is the write loss to use you

288
00:26:41,720 --> 00:26:44,570
can always override it but for a
particular model we give you good

289
00:26:44,570 --> 00:26:50,179
defaults okay so having done all that
rather than calling fit you call

290
00:26:50,179 --> 00:26:54,679
generator passing in those two
generators that you saw earlier the

291
00:26:54,679 --> 00:26:59,809
Train generator in the validation
generator for reasons I don't quite
understand chaos expects you to also

292
00:27:01,820 --> 00:27:06,409
tell it how many batches there are per
epoch so the number of batches is a

293
00:27:06,409 --> 00:27:11,780
quarter the size of the generator
divided by the batch size you can tell
it how many epochs just like in fast AI

294
00:27:16,100 --> 00:27:21,840
you can say how many processes or how
many workers to use for pre-processing

295
00:27:21,840 --> 00:27:26,330
[Music]
unlike fast AI the default in chaos is

296
00:27:26,330 --> 00:27:32,090
basically not to use any so to get good
speed you're going to make sure you

297
00:27:32,090 --> 00:27:39,289
include this and so that's basically
enough to start fine tuning the last

298
00:27:39,289 --> 00:27:47,750
layers so as you can see I got to a
validation accuracy of 95% but as you

299
00:27:47,750 --> 00:27:50,960
can also see something really weird
happened we're after one it was like 49

300
00:27:50,960 --> 00:27:56,750
and then it was 69 and then 95 I don't
know why these are so low that's not

301
00:27:56,750 --> 00:28:00,140
normal
I may have there may be a bug and chaos

302
00:28:00,140 --> 00:28:04,340
they may be a bug in my code I reached
out on Twitter to see if
anybody could figure it out but they

303
00:28:05,630 --> 00:28:09,409
couldn't I guess this is one of the
challenges with using something like

304
00:28:09,409 --> 00:28:13,700
this is one of the reasons I wanted to
use fast AI for this course is it's much

305
00:28:13,700 --> 00:28:17,419
harder to screw things up so I don't
know if I screwed something up or

306
00:28:17,419 --> 00:28:23,270
somebody else did
yes you know this is you've seen the
chance to float back end yeah yeah and

307
00:28:25,400 --> 00:28:31,850
if you want to run this to try it out
yourself you just can just go pip
install tensorflow - GPU Kerris okay

308
00:28:39,020 --> 00:28:45,049
because it's not part of the faster I
environment by default but that should

309
00:28:45,049 --> 00:28:55,610
be all you need to do to get that
working so then there isn't a concept of

310
00:28:55,610 --> 00:29:00,230
like layer groups or differential
learning rates or partial unfreezing or

311
00:29:00,230 --> 00:29:03,470
whatever so you have to decide like I
had to print out all of the layers and

312
00:29:03,470 --> 00:29:08,539
decide manually how many I wanted to
fine-tune so I decided to fine-tune

313
00:29:08,539 --> 00:29:11,809
everything from a layer 140 onwards so
that's why I just looked through like

314
00:29:11,809 --> 00:29:17,210
this after you change that you have to
recompile the model and then after that

315
00:29:17,210 --> 00:29:20,750
I then ran another step and again I
don't know what happened here the

316
00:29:20,750 --> 00:29:24,169
accuracy of the training set stayed
about the same but the validation set

317
00:29:24,169 --> 00:29:29,779
totally fill in the hole and I mean the
main thing to notice even if we put

318
00:29:29,779 --> 00:29:35,299
aside the validation set we're getting I
mean I guess the main thing is there's a

319
00:29:35,299 --> 00:29:39,230
hell of a lot more code here which is
kind of annoying but also the

320
00:29:39,230 --> 00:29:42,860
performance is very different so where
else is here even on the training set
we're getting like 97% after four epochs

321
00:29:46,070 --> 00:29:53,899
that took a total of about eight minutes
you know over here we had 99.5% on the

322
00:29:53,899 --> 00:30:02,380
validation set and it ran a lot faster
so I was like four or five minutes right

323
00:30:02,380 --> 00:30:05,380
so

324
00:30:06,870 --> 00:30:10,809
depending on what you do particularly if
you end up wanting to deploy stuff to

325
00:30:10,809 --> 00:30:17,620
mobile devices at the moment the kind of
PI torch on mobile situation is very

326
00:30:17,620 --> 00:30:21,760
early so you may find yourself wanting
to use tensorflow or you may work for a

327
00:30:21,760 --> 00:30:27,070
company that's kind of settled on
tensorflow so if you need to convert

328
00:30:27,070 --> 00:30:31,929
something like redo something you've
learnt here intensive flow you probably

329
00:30:31,929 --> 00:30:37,000
want to do it with care us but just
recognize you know it's got to take a

330
00:30:37,000 --> 00:30:43,000
bit more work to get there and by
default it's much harder to get I mean I

331
00:30:43,000 --> 00:30:46,929
to get the same state of the out results
you get the faster I you'd have to like

332
00:30:46,929 --> 00:30:52,090
replicate all of the state-of-the-art
algorithms that are in first a nice so
it's hard to get the same level of

333
00:30:54,669 --> 00:31:01,600
results but you can see the basic ideas
are similar okay and it's certainly it's
certainly possible you know like there's

334
00:31:03,970 --> 00:31:08,110
nothing I'm doing in fast AI that like
would be impossible but like you'd have
to implement stochastic gradient percent

335
00:31:09,910 --> 00:31:14,169
with restarts you would have to
implement differential learning rates

336
00:31:14,169 --> 00:31:18,700
you would have to implement batch norm
freezing which you probably don't want

337
00:31:18,700 --> 00:31:22,179
to do I know and well that's not quite
true I think somewhat one person at

338
00:31:22,179 --> 00:31:26,559
least on the forum is attempting to
create a chaos compatible version of or
a tensorflow compatible version of fast

339
00:31:28,179 --> 00:31:32,559
AI which I think I hope will get there I
actually spoke to Google about this a
few weeks ago and they're very

340
00:31:33,820 --> 00:31:38,590
interested in getting faster i ported to
tensorflow so maybe by the time you're

341
00:31:38,590 --> 00:31:43,270
looking at this on the mooc maybe that
will exist I certainly hope so we will

342
00:31:43,270 --> 00:31:49,390
see hey wait
so Karis is Karis intensive flow was

343
00:31:49,390 --> 00:31:55,590
certainly not you know
that difficult to handle and so I don't
think you should worry if you're told

344
00:31:56,820 --> 00:32:00,780
you have to learn them after this course
for some reason even let me take you a

345
00:32:00,780 --> 00:32:11,120
couple of days I'm sure so that's kind
of most of the stuff you would need to

346
00:32:11,120 --> 00:32:15,030
kind of complete this this kind of
assignment from last week which was like

347
00:32:15,030 --> 00:32:19,890
try to do everything you've seen already
but on the dog of reinstated said just

348
00:32:19,890 --> 00:32:26,600
to remind you that kind of last few
minutes of last week's lesson I show you
how to do much of that including like

349
00:32:30,870 --> 00:32:35,910
how I actually explored the data to find
out like what the classes were and how

350
00:32:35,910 --> 00:32:39,960
big the images were and stuff like that
right so if you've forgotten that or

351
00:32:39,960 --> 00:32:44,100
didn't quite follow at all last week
check out the video from last week to
see one thing that we didn't talk about

352
00:32:47,040 --> 00:32:51,260
is how do you actually submit to Carol
so how do you actually get predictions

353
00:32:51,260 --> 00:32:56,429
so I just wanted to show you that last
piece as well and on the wiki thread

354
00:32:56,429 --> 00:33:01,500
this week I've already put a little
image of this to show you these days but

355
00:33:01,500 --> 00:33:06,210
if you go to the kaggle website for
every competition there's a section

356
00:33:06,210 --> 00:33:10,230
called evaluation and they tell you what
it's a bit and so I just copied and

357
00:33:10,230 --> 00:33:15,780
pasted these two lines from from there
and so it says we're expected to submit

358
00:33:15,780 --> 00:33:22,140
a file where the first line contains the
the work the word ID and then a comma

359
00:33:22,140 --> 00:33:26,190
separated list of all of the possible
dog breeds and then every line after

360
00:33:26,190 --> 00:33:31,860
that will contain the idea itself
followed by all the probabilities of all
the different dog breeds so how do you

361
00:33:35,610 --> 00:33:42,960
create that so recognize that inside our
data object there's a dot classes which

362
00:33:42,960 --> 00:33:52,200
has got in alphabetical order all of the
four other classes and then so it's got

363
00:33:52,200 --> 00:33:58,380
all of the different classes and then
inside data dot test data set just yes

364
00:33:58,380 --> 00:34:05,010
you can also see there's all the file
names so and just to remind you dogs and
cats

365
00:34:05,490 --> 00:34:11,579
sorry
cats dog breeds was not provided in the

366
00:34:11,579 --> 00:34:15,929
kind of care our style format where the
dogs and cats from different folders but

367
00:34:15,929 --> 00:34:21,059
instead it was provided as a CSV file of
labels right so when you get a CSV file

368
00:34:21,059 --> 00:34:27,859
of labels you use image classifier data
from CSV rather than image classifier
data from parts there isn't an

369
00:34:31,799 --> 00:34:36,149
equivalent in care us so you'll see like
on the cattle forums people share

370
00:34:36,149 --> 00:34:40,139
scripts for how to convert it to a care
our style folders but in our case we

371
00:34:40,139 --> 00:34:44,039
don't have to we just go image
classifier data from CSV passing in that

372
00:34:44,039 --> 00:34:51,210
CSV file and so the CSV file will you
know has automatically told the data you

373
00:34:51,210 --> 00:34:57,750
know what the masses are and then also
we can see from the folder of test

374
00:34:57,750 --> 00:35:03,740
images what the file names of those are
so with those two pieces of information

375
00:35:03,740 --> 00:35:09,660
we're ready to go so I always think it's
a good idea to use TTA as you saw with

376
00:35:09,660 --> 00:35:13,500
that dogs and cats example just now it
can really improve things particularly

377
00:35:13,500 --> 00:35:22,130
when your model is less good so I can
say learn dot t ta and if you pass in

378
00:35:25,549 --> 00:35:32,130
yeah if you pass in is test equals true
then it's going to give you predictions

379
00:35:32,130 --> 00:35:35,369
on the test set rather than the
validation set okay

380
00:35:35,369 --> 00:35:40,980
and now obviously we can't now get an
accuracy or anything because by

381
00:35:40,980 --> 00:35:48,650
definition we don't know the labels for
the test set right so by default most

382
00:35:48,650 --> 00:35:54,539
high-touch models give you back the log
of the predictions so then we just have

383
00:35:54,539 --> 00:35:59,760
to go X of that to get back out
probabilities so in this case the test

384
00:35:59,760 --> 00:36:03,809
set had ten thousand three hundred and
fifty seven images in it and there are

385
00:36:03,809 --> 00:36:10,529
120 possible breeds all right so we get
back a matrix of of that size and so we

386
00:36:10,529 --> 00:36:16,020
now need to turn that into something
that looks like this

387
00:36:16,020 --> 00:36:19,470
and so the easiest way to do that is
with pandas if you're not familiar with

388
00:36:19,470 --> 00:36:23,340
pandas there's lots of information
online about it or check out the machine

389
00:36:23,340 --> 00:36:26,400
learning course intro to machine
learning that we have where we do lots

390
00:36:26,400 --> 00:36:30,990
of stuff with pandas but basically we
can describe PD data frame and pass in
that matrix and then we can say the

391
00:36:33,630 --> 00:36:38,640
names of the columns are equal to data
duck classes and then finally we can
insert a new column at position 0 called

392
00:36:41,670 --> 00:36:47,300
ID that contains the file names but
you'll notice that the file names

393
00:36:47,300 --> 00:36:52,800
contain five letters at the end I start
we don't want and four letters at the

394
00:36:52,800 --> 00:37:02,790
end we don't want so I just subset in
like so right so at that point I've got

395
00:37:02,790 --> 00:37:10,410
a data frame that looks like this which
is what we want so you can now call a

396
00:37:10,410 --> 00:37:24,000
data frame data so social cues dated DF
not des let's fix it now data frame okay

397
00:37:24,000 --> 00:37:30,900
so you can now call data frame to CSV
and quite often you'll find these files

398
00:37:30,900 --> 00:37:35,280
actually get quite big so it's a good
idea to say compression equals gzip and

399
00:37:35,280 --> 00:37:41,180
that'll zip it up on the server for you
and that's going to create a zipped up
CSV file on the server on wherever

400
00:37:44,280 --> 00:37:47,790
you're running is Jupiter notebook so
you need apps that you now need to get
that back to your computer so you can

401
00:37:49,350 --> 00:37:55,620
upload it or you can use carol CLA so
you can type kgs submit and do it that

402
00:37:55,620 --> 00:37:58,950
way
I generally download it to my computer

403
00:37:58,950 --> 00:38:04,170
so like how often back to this lab
double check it all looks ok so to do

404
00:38:04,170 --> 00:38:09,300
that there's a cool little theme called
file link and if you run file link with

405
00:38:09,300 --> 00:38:14,970
a path on your server it gives you back
a URL which you can click on and it will
download that file from the server onto

406
00:38:18,390 --> 00:38:25,290
your computer so if I click on that now
I can go ahead and save it

407
00:38:25,290 --> 00:38:31,430
and then I can see in my downloads

408
00:38:33,490 --> 00:38:38,590
there it is here's my submission file

409
00:38:40,330 --> 00:38:45,820
they want to open their yeah and as you
can see it's exactly what I asked for

410
00:38:45,820 --> 00:38:51,820
there's my ID and 120 different dog
breeds and then here's my first row

411
00:38:51,820 --> 00:38:56,470
containing the file name and the 120
different probabilities okay so then you
can go ahead and submit that to cattle

412
00:38:58,120 --> 00:39:03,370
through their through their regular form
and so this is also a good way you can

413
00:39:03,370 --> 00:39:08,380
see we've now got a good way of both
grabbing any file off the internet and

414
00:39:08,380 --> 00:39:15,160
getting a to our AWS instance or paper
space or whatever by using the cool
little extension in chrome and we've

415
00:39:17,170 --> 00:39:22,200
also got a way of grabbing stuff off our
server easily those of you that are more
command line oriented you can also use

416
00:39:24,940 --> 00:39:31,270
SCP of course but I kind of like doing
everything through the notebook all

417
00:39:31,270 --> 00:39:34,960
right
one other question I had during the week

418
00:39:34,960 --> 00:39:42,910
was like what if I want to just get a
single a single file that I want to you

419
00:39:42,910 --> 00:39:47,290
know get a prediction for so for example
you know maybe I want to get this first

420
00:39:47,290 --> 00:39:52,390
file from my validation set
so there's its name so you can always

421
00:39:52,390 --> 00:39:59,350
look at a file just by calling image dot
open that just uses regular - imaging

422
00:39:59,350 --> 00:40:05,800
library and so what you can do is
there's actually I'll show you the

423
00:40:05,800 --> 00:40:16,230
shortest version you can just call learn
predict array passing in your your image

424
00:40:16,230 --> 00:40:21,390
okay now the image needs to have been
transformed

425
00:40:21,390 --> 00:40:27,760
so you've seen transform Trent
transforms from model before normally we

426
00:40:27,760 --> 00:40:31,270
just put put it all in one variable but
actually behind the scenes it was
returning to things

427
00:40:32,500 --> 00:40:36,310
it was returning training transforms and
validation transforms so I can actually

428
00:40:36,310 --> 00:40:41,110
split them apart and so here you can see
I'm actually applying example my

429
00:40:41,110 --> 00:40:45,280
training transforms or probably more
likely that would apply validation

430
00:40:45,280 --> 00:40:51,220
transforms that gives me back an array
containing the image the transformed
image which I can then past

431
00:40:55,440 --> 00:41:01,570
everything that gets passed to or
returned from bottles is generally

432
00:41:01,570 --> 00:41:05,320
assumed to be a mini batch right it's
generally assumed to be a bunch of

433
00:41:05,320 --> 00:41:10,690
images so we'll talk more about some
numpy tricks later but basically in this

434
00:41:10,690 --> 00:41:15,250
case we only have one image so we have
to turn that into a mini batch of images

435
00:41:15,250 --> 00:41:21,490
so in other words we need to create a
tensor that basically is not just rows

436
00:41:21,490 --> 00:41:26,230
by columns by channels but it's number
of image by rows by columns by channels

437
00:41:26,230 --> 00:41:31,330
and as one image so it's basically
becomes a 4 dimensional tensor so

438
00:41:31,330 --> 00:41:36,610
there's a cool little trick in numpy
that if you index into an array with

439
00:41:36,610 --> 00:41:41,350
none that basically adds additional unit
access to the start so it turns it from

440
00:41:41,350 --> 00:41:46,480
an image into a mini batch of one images
and so that's why we had to do that so

441
00:41:46,480 --> 00:41:52,630
if you basically find you're trying to
do things with a single image with any

442
00:41:52,630 --> 00:41:56,980
kind of Pi torch or fast AI thing this
is just something you might you might

443
00:41:56,980 --> 00:42:02,080
find it says like expecting four
dimensions only got three it probably

444
00:42:02,080 --> 00:42:07,330
means that or if you get back a return
value from something that has like some

445
00:42:07,330 --> 00:42:11,350
weird first access that's probably why
it's probably giving you like back a

446
00:42:11,350 --> 00:42:15,040
mini batch okay and so we'll learn a lot
more about this but it's just something

447
00:42:15,040 --> 00:42:22,050
to be aware of okay so that's kind of

448
00:42:22,080 --> 00:42:29,580
everything you need to do in practice so
now we're going to kind of get into a

449
00:42:29,580 --> 00:42:34,500
little bit of theory what's actually
going on behind the scenes with these

450
00:42:34,500 --> 00:42:42,410
convolutional neural networks and you
might remember it back in Lesson one we

451
00:42:43,850 --> 00:42:51,720
actually saw our first little bit of
theory which we stole from this

452
00:42:51,720 --> 00:42:56,730
fantastic websites a toaster dot IO
either explained visually and we learnt

453
00:42:56,730 --> 00:43:01,320
that a that a convolution is something
where we basically have a little matrix

454
00:43:01,320 --> 00:43:06,600
in deep learning nearly always three by
three a little matrix that we basically
multiply every element of that matrix by

455
00:43:09,030 --> 00:43:14,010
every element of a three by three
section of an image add them all

456
00:43:14,010 --> 00:43:19,670
together to get the result of that
convolution at one point all right now

457
00:43:19,670 --> 00:43:26,370
let's see how that all gets turned
together to create these these various

458
00:43:26,370 --> 00:43:31,860
layers that we saw in the the Zeiler and
burgers paper and to do that again I'm

459
00:43:31,860 --> 00:43:35,010
going to steal off somebody who's much
smarter than I am

460
00:43:35,010 --> 00:43:39,570
we're going to steal from a guy called
Ottavia good

461
00:43:39,570 --> 00:43:45,540
Ottavia oh good was the guy who created
Word Lens which nowadays is part of

462
00:43:45,540 --> 00:43:48,870
Google Translate if I'm Google Translate
you've ever like done that thing where

463
00:43:48,870 --> 00:43:54,600
you point your camera at something at
something which has any kind of foreign

464
00:43:54,600 --> 00:43:58,080
language on it and in real-time it
overlays it with the translation that

465
00:43:58,080 --> 00:44:03,630
was a views company that built that and
so Tokyo was kind enough to share this

466
00:44:03,630 --> 00:44:09,240
fantastic video
he created he's at Google now and I want
to kind of step you through works I

467
00:44:10,350 --> 00:44:14,310
think it explains really really well
what's going on and then after we look

468
00:44:14,310 --> 00:44:18,710
at the video we're going to see how to
implement the whole a whole sequence of

469
00:44:18,710 --> 00:44:23,400
kintyre set of layers of convolution on
your network in Microsoft Excel

470
00:44:23,400 --> 00:44:27,720
so with you're a visual learner or a
spreadsheet learner hopefully you'll be
able to understand all this okay so

471
00:44:29,880 --> 00:44:33,540
we're going to start with an image and
something that we're going to do later

472
00:44:33,540 --> 00:44:36,990
in the course is we're going to learn to
nice digits so we'll do it like end to

473
00:44:36,990 --> 00:44:41,610
end we'll do the whole thing so this is
pretty similar so we're going to try and

474
00:44:41,610 --> 00:44:46,290
recognize in this case letters so here's
an A which obviously it's actually a

475
00:44:46,290 --> 00:44:51,990
grid of numbers right and so there's the
creative numbers and so what we do is we

476
00:44:51,990 --> 00:44:57,150
take our first convolutional filter so
we're assuming this is all this is

477
00:44:57,150 --> 00:45:01,020
assuming that these are already learned
right and you can see this point it's

478
00:45:01,020 --> 00:45:05,010
got wiped down the right-hand side right
and black down the left so it's like

479
00:45:05,010 --> 00:45:10,350
zero zero zero maybe negative 1 negative
1 negative 1 0 0 0 1 1 1 and so we're
taking each 3x3 part of the image and

480
00:45:13,140 --> 00:45:18,990
multiplying it by that 3x3 matrix not as
a matrix product that an element-wise

481
00:45:18,990 --> 00:45:24,450
product and so you can see what happens
is everywhere where the the white edge

482
00:45:24,450 --> 00:45:30,180
is matching the edge of the a and the
black edge isn't we're getting green

483
00:45:30,180 --> 00:45:33,870
we're getting a positive and everywhere
where it's the opposite we're getting a

484
00:45:33,870 --> 00:45:39,180
negative we're getting a red right and
so that's the first filter creating the

485
00:45:39,180 --> 00:45:44,760
first that the result of the first
kernel right and so here's a new kernel

486
00:45:44,760 --> 00:45:49,380
this one is it's got a white stripe
along the top right so we literally scan

487
00:45:49,380 --> 00:45:55,890
through every 3x3 part of the matrix
multiplying those 3 bits of the a the
neighbors of the a by the 9 bits as a

488
00:45:57,450 --> 00:46:02,610
filter to find out whether it's red or
green and how red or green it is ok and

489
00:46:02,610 --> 00:46:07,080
so this is assuming we had two filters
one was a bottom edge one was a left

490
00:46:07,080 --> 00:46:10,800
edge and you can see here the top edge
not surprisingly it's red here so a

491
00:46:10,800 --> 00:46:15,570
bottom edge was red here and green here
the right edge right here in green here

492
00:46:15,570 --> 00:46:20,460
and then in the next step we add a
non-linearity ok the rectified linear

493
00:46:20,460 --> 00:46:26,520
unit which literally means strongly the
negatives so here the Reds all gone okay

494
00:46:26,520 --> 00:46:30,780
so here's layer 1 the input
here's layup to the result of 2

495
00:46:30,780 --> 00:46:36,180
convolutional filters here's layer 3
which is which is throw away all of the
red stuff and that's called a rectified

496
00:46:38,190 --> 00:46:43,890
linear unit and then layer 4 is
something called a max pull on a layer 4

497
00:46:43,890 --> 00:46:48,700
we replace every 2 by 2 part of this
grid

498
00:46:48,700 --> 00:46:53,040
and we replace it with its maximum mat
so it basically makes it half the size

499
00:46:53,040 --> 00:46:57,700
it's basically the same thing but half
the size and then we can go through and

500
00:46:57,700 --> 00:47:02,619
do exactly the same thing we can have
some new filter three by three filter

501
00:47:02,619 --> 00:47:08,680
that we put through each of the two
results of the previous layer okay and
again we can throw away the red bits

502
00:47:10,510 --> 00:47:14,349
right so get rid of all the negatives so
we just keep the positives that's called

503
00:47:14,349 --> 00:47:21,190
applying a rectified linear unit and
that gets us to our next layer of this

504
00:47:21,190 --> 00:47:26,530
convolutional neural network so you can
see that by you know at this layer back

505
00:47:26,530 --> 00:47:30,490
here it was kind of very interpretive
all it's like we've either got bottom
edges or left edges but then the next

506
00:47:33,010 --> 00:47:37,660
layer was combining the results of
convolutions so it's starting to become
a lot less clear like intuitively what's

507
00:47:40,089 --> 00:47:45,130
happening but it's doing the same thing
and then we do another max pull right so

508
00:47:45,130 --> 00:47:52,180
we replace every 2x2 or 3x3 section with
a single digit so here this 2x2 it's all
black so we replaced it with a black

509
00:47:53,799 --> 00:48:00,430
right and then we go and we take that
and we we compare it to basically a kind

510
00:48:00,430 --> 00:48:05,140
of a template of what we would expect to
see if it was an a it was a B but the

511
00:48:05,140 --> 00:48:10,690
see it was d give it an E and we see how
closely it matches and we can do it in

512
00:48:10,690 --> 00:48:15,700
exactly the same way we can multiply
every one of the values in this four by
eight matrix with every one of the four

513
00:48:18,579 --> 00:48:22,390
by eight in this one and this one and
this one and we add we just add them
together to say like how often does it

514
00:48:24,579 --> 00:48:29,799
match versus how often does it not match
and then that could be converted to give

515
00:48:29,799 --> 00:48:35,530
us a percentage probability that this is
a no so in this case this particular

516
00:48:35,530 --> 00:48:41,619
template matched well with a so notice
we're not doing an each training here

517
00:48:41,619 --> 00:48:46,119
right this is how it would work if we
have a pre trained model all right so

518
00:48:46,119 --> 00:48:50,170
when we download a pre trained imagenet
model off the internet and isn't on an

519
00:48:50,170 --> 00:48:54,460
image without any changing to it this is
what's happening or if we take a model

520
00:48:54,460 --> 00:48:58,569
that you've trained and you're applying
it to some test set or for some new

521
00:48:58,569 --> 00:49:02,500
image this is what it's doing all right
as it's basically taking it through it

522
00:49:02,500 --> 00:49:08,710
buying a convolution to each layer to
each multiple convolutional filters to

523
00:49:08,710 --> 00:49:14,680
each layer and then doing the rectified
linear unit so throw away the negatives

524
00:49:14,680 --> 00:49:21,130
and then do the max pull and then repeat
that a bunch of times and so then we can

525
00:49:21,130 --> 00:49:28,030
do it with a new letter A or letter B or
whatever and keep going through that

526
00:49:28,030 --> 00:49:33,790
process right so as you can see that's
far nice the visualization thing and I
could have created because I'm not at a

527
00:49:35,200 --> 00:49:40,420
vo so thanks to him for sharing this
with us because it's totally awesome

528
00:49:40,420 --> 00:49:43,480
he actually this is not done by hand he
actually wrote a piece of computer

529
00:49:43,480 --> 00:49:47,410
software to actually do these
convolutions this is actually being

530
00:49:47,410 --> 00:49:52,480
actually being done dynamically which is
pretty cool so I'm more of a spreadsheet

531
00:49:52,480 --> 00:49:58,060
guy personally I'm a simple person so
here is the same thing now in

532
00:49:58,060 --> 00:50:03,490
spreadsheet all right and so you'll find
this in the github repo so you can

533
00:50:03,490 --> 00:50:08,950
either get clone the repo to your own
computer open up the spreadsheet or you

534
00:50:08,950 --> 00:50:21,070
can just go to github.com slash / ji and
click on this it's it's inside if you go

535
00:50:21,070 --> 00:50:26,890
to our repo and just go to courses as
usual go to deal 1 as usual you'll see
there's an Excel section there okay and

536
00:50:29,170 --> 00:50:32,230
so he lay all that so you can just
download them by clicking them or you

537
00:50:32,230 --> 00:50:36,880
can clone the whole repo and we're
looking at cognitive example convolution

538
00:50:36,880 --> 00:50:44,710
example all right so you can see I have
here an input right so in this case the

539
00:50:44,710 --> 00:50:51,310
input is the number 7 so I grab this
from a dataset called m-must MN ist

540
00:50:51,310 --> 00:50:55,290
which we'll be looking at in a lot of
detail and I just took one of those

541
00:50:55,290 --> 00:51:01,360
digits at random and I put it into Excel
and so you can see every Hextall is

542
00:51:01,360 --> 00:51:09,640
actually just a number between 9 1 okay
very often actually it'll be a bite
between Norton 255 or sometimes it might

543
00:51:13,060 --> 00:51:16,089
be a float between naught and 1 it
doesn't really matter

544
00:51:16,089 --> 00:51:22,449
by the time it gets to PI torch we're
generally dealing with floats so we if

545
00:51:22,449 --> 00:51:25,900
one of the steps we often will take will
be to convert it to a number between

546
00:51:25,900 --> 00:51:32,170
naught 1 so then you can see I've just
use conditional formatting in Excel to

547
00:51:32,170 --> 00:51:36,489
kind of make the higher numbers more red
so you can clearly see that this is a

548
00:51:36,489 --> 00:51:42,400
red this is a 7 but but it's just a
bunch of numbers that have been imported

549
00:51:42,400 --> 00:51:52,299
into Excel okay so here's our input so
remember what at a via did was he then

550
00:51:52,299 --> 00:51:58,299
applied two filters right with different
shapes so here I've created a filter

551
00:51:58,299 --> 00:52:05,170
which is designed to detect top edges so
this is a 3 by 3 filter okay and I've

552
00:52:05,170 --> 00:52:10,119
got ones along the top zeroes in the
middle minus ones at the bottom right so

553
00:52:10,119 --> 00:52:16,630
let's take a look at an example that's
here right and so if I hit that - you

554
00:52:16,630 --> 00:52:22,029
can see here highlighted this is the 3
by 3 part of the input that this

555
00:52:22,029 --> 00:52:28,239
particular thing is calculating right so
here you can see it's got 1 1 1 are all

556
00:52:28,239 --> 00:52:35,249
being multiplied by 1 and point 1 0 0
are all being multiplied by negative 1
okay so in other words all the positive

557
00:52:37,989 --> 00:52:41,319
bits are getting a lot of positive the
negative bits are getting nearly nothing

558
00:52:41,319 --> 00:52:47,049
at all so we end up with a high number
okay where else on the other side of

559
00:52:47,049 --> 00:52:54,670
this bit of the 7 right you can see how
you know this is basically zeros here or

560
00:52:54,670 --> 00:53:04,420
perhaps more interestingly on the top of
it okay here we've got high numbers at

561
00:53:04,420 --> 00:53:08,859
the top but we've also got high numbers
at the bottom which are negating it ok

562
00:53:08,859 --> 00:53:14,769
so you can see that the only place that
we end up activating is where we're

563
00:53:14,769 --> 00:53:22,359
actually at an edge so in this case this
here this number 3 this is called an

564
00:53:22,359 --> 00:53:28,990
activation ok so when I say an
activation I mean ah

565
00:53:28,990 --> 00:53:38,500
at number a number that is calculated
and it is calculated by taking some

566
00:53:38,500 --> 00:53:46,000
numbers from the input and applying some
kind of linear operation in this case a

567
00:53:46,000 --> 00:53:52,240
convolutional kernel to calculate an
output right you'll notice that other

568
00:53:52,240 --> 00:53:59,230
than going inputs multiplied by kernel
and summing it together

569
00:53:59,230 --> 00:54:07,060
right so here's my some and here's my x
then take that and I go max of 0 comma

570
00:54:07,060 --> 00:54:13,750
that and so that's my rectified linear
unit so it sounds very fancy rectified

571
00:54:13,750 --> 00:54:17,710
linear unit but what they actually mean
is open up Excel and type equals max 0

572
00:54:17,710 --> 00:54:25,480
comma C ok that's all about then you'll
see people in the biz so to say value a

573
00:54:25,480 --> 00:54:32,920
so ral you means rectified linear unit
means max 0 comma thing and I'm not like

574
00:54:32,920 --> 00:54:36,610
simplifying it I really mean it like
when I say like if I'm simplifying I

575
00:54:36,610 --> 00:54:40,930
always say so I'm simplifying but if I'm
not saying I'm simplifying that's the
entirety okay so a rectified linear unit

576
00:54:42,970 --> 00:54:50,490
in its entirety is this and a
convolution in its entirety is is this

577
00:54:50,490 --> 00:54:57,370
okay so a single layer of a
convolutional neural network is being

578
00:54:57,370 --> 00:55:03,560
implemented in its entirety
here in Excel okay and so you can see

579
00:55:03,560 --> 00:55:09,290
what it's done is it's deleted pretty
much the vertical edges and highlighted

580
00:55:09,290 --> 00:55:15,920
the horizontal edges so again this is
assuming that our network is trained and

581
00:55:15,920 --> 00:55:20,750
that at the end of training it a created
a convolutional filter with these

582
00:55:20,750 --> 00:55:28,670
specific line numbers in and so here is
a second convolutional filter it's just

583
00:55:28,670 --> 00:55:35,210
a different line numbers now pi torch
doesn't store them as two separate nine

584
00:55:35,210 --> 00:55:41,540
digit arrays it stores it as a tensor
right remember a tensor just means an

585
00:55:41,540 --> 00:55:50,060
array with more dimensions okay you can
use the word array as well it's the same

586
00:55:50,060 --> 00:55:54,230
thing but in pi torch they always use
the word tensor so I'm going to say

587
00:55:54,230 --> 00:55:59,870
cancer okay so it's just a tensor with
an additional axis which allows us to

588
00:55:59,870 --> 00:56:07,220
stack each of these filters together
right filter and kernel pretty much mean

589
00:56:07,220 --> 00:56:13,490
the same thing yeah right it refers to
one of these three by three matrices or

590
00:56:13,490 --> 00:56:19,700
one of these three by three slices of a
three dimensional tensor so if I take

591
00:56:19,700 --> 00:56:23,990
this one and here I've literally just
copied the formulas in Excel from above

592
00:56:23,990 --> 00:56:30,680
okay and so you can see this one is now
finding a vertebra which as we would

593
00:56:30,680 --> 00:56:40,010
expect
okay so we've now created one layer

594
00:56:40,010 --> 00:56:44,150
right this here is a layer them
specifically we'd say it's a hidden

595
00:56:44,150 --> 00:56:48,680
layer which is it's not an input layer
and it's not an output layer so
everything else is a hidden layer okay

596
00:56:50,810 --> 00:56:59,210
and this particular hidden layer has is
a size two on this dimension right

597
00:56:59,210 --> 00:57:02,080
because it has two

598
00:57:02,200 --> 00:57:09,849
filters right two kernels so what
happens next

599
00:57:09,849 --> 00:57:17,710
well let's do another one okay so as we
kind of go along things can multiply a

600
00:57:17,710 --> 00:57:23,589
little bit in complexity right because
my next filter is going to have to

601
00:57:23,589 --> 00:57:28,930
contain two of these three by threes
because I'm gonna have to say how do I

602
00:57:28,930 --> 00:57:34,540
want to bring Adam I want to write these
three things and at the same time how do

603
00:57:34,540 --> 00:57:38,589
I want to wait the corresponding three
things down here right because in pi

604
00:57:38,589 --> 00:57:44,109
torch this is going to be this whole
thing here is going to be stored as a

605
00:57:44,109 --> 00:57:48,430
multi-dimensional tensor right so you
shouldn't really think of this now as

606
00:57:48,430 --> 00:57:58,599
two 3x3 kernels but one two by three by
three eternal okay so to calculate this

607
00:57:58,599 --> 00:58:09,510
value here I've got the sum product of
all of that plus the sum product of

608
00:58:09,510 --> 00:58:19,119
scroll down all of that okay and so the
top ones are being multiplied by this

609
00:58:19,119 --> 00:58:22,420
part of the kernel and the bottom ones
have been multiplied by this part of the

610
00:58:22,420 --> 00:58:28,000
kernel and so over time you want to
start to get very comfortable with the

611
00:58:28,000 --> 00:58:35,530
idea of these like higher dimensional
linear combinations right like it's it's

612
00:58:35,530 --> 00:58:40,720
harder to draw it on the screen like I
had to put one above the other but
conceptually just stuck it in your mind

613
00:58:42,670 --> 00:58:47,140
like this that's really how you want to
think right and actually Geoffrey Hinton

614
00:58:47,140 --> 00:58:54,160
in his original 2012 neural Nets
Coursera class has a tip which is how

615
00:58:54,160 --> 00:58:58,990
all computer scientists deal with like
very high dimensional spaces which is

616
00:58:58,990 --> 00:59:01,930
that they basically just visualize the
two-dimensional space

617
00:59:01,930 --> 00:59:06,390
and then say like twelve dimensions
really fast and they had lots of tires

618
00:59:06,390 --> 00:59:09,760
so that's it
right we can see two dimensions on the

619
00:59:09,760 --> 00:59:14,470
screen and then you're just going to try
to trust that you can have more
dimensions like the Const

620
00:59:15,800 --> 00:59:19,550
it's just you know there's there's
nothing different about them and so you

621
00:59:19,550 --> 00:59:23,090
can see in Excel you know Excel doesn't
have the ability to handle

622
00:59:23,090 --> 00:59:27,730
three-dimensional tensors so I had to
like say okay take this two-dimensional

623
00:59:27,730 --> 00:59:33,170
dot product add on this two-dimensional
dot product right but if there was some

624
00:59:33,170 --> 00:59:38,900
kind of 3d Excel I could have to stand
that in a single line all right and then

625
00:59:38,900 --> 00:59:44,720
again apply max 0 comma otherwise known
as rectified linear unit otherwise known
as value okay so here is my second layer

626
00:59:48,710 --> 00:59:54,740
and so when people create different
architectures write an architecture
means like how big is your kernel at

627
00:59:59,300 --> 01:00:02,540
layer 1
how many filters are in your kernel at

628
01:00:02,540 --> 01:00:08,510
layer 1 so here I've got a 3 by 3
where's number 1 and a 3 by 3 there's

629
01:00:08,510 --> 01:00:15,250
number 2 so like this architecture I've
created starts off with 2 3 by 3

630
01:00:15,250 --> 01:00:25,100
convolutional kernels and then my second
layer has another two kernels of size 2

631
01:00:25,100 --> 01:00:30,650
by 3 by 3 so there's the first one
and then down here here's a second 2 by

632
01:00:30,650 --> 01:00:35,930
3 by 3 kernel okay and so remember one
of these specific any one of these
numbers is an activation okay so this

633
01:00:40,640 --> 01:00:45,680
activation is being calculated from
these three things here and other 3

634
01:00:45,680 --> 01:00:52,940
things up there and we're using these
this 2 by 3 by 3 kernel okay and so what

635
01:00:52,940 --> 01:00:57,830
tends to happen is people generally give
names to their layers so I say okay

636
01:00:57,830 --> 01:01:07,670
let's call this layer here con 1 and
this layer here and this and this layer

637
01:01:07,670 --> 01:01:13,730
here con - all right so that's you know
but generally you'll just see that like

638
01:01:13,730 --> 01:01:17,930
when you print out a summary of a
network every layer will have some kind

639
01:01:17,930 --> 01:01:26,150
of name okay and so then what happens
next well part of the architecture is

640
01:01:26,150 --> 01:01:30,400
like do you have some max pooling where
bounces up Matt spalling happen so in

641
01:01:30,400 --> 01:01:36,520
this architecture we're inventing we're
going to next step is do max fully okay

642
01:01:36,520 --> 01:01:43,569
Matt spooling is a little hard to kind
of show in Excel but we've got it so max

643
01:01:43,569 --> 01:01:49,180
pooling if I do a two by two max pooling
it's going to have the resolution both

644
01:01:49,180 --> 01:01:57,359
height and width so you can see here
that I've replaced these four numbers

645
01:01:57,359 --> 01:02:02,560
with the maximum of those four numbers
right and so because I'm having the

646
01:02:02,560 --> 01:02:06,780
resolution it only makes sense to
actually have something every two cells

647
01:02:06,780 --> 01:02:13,690
okay so you can see here the way I've
got kind of the same looking shape as I

648
01:02:13,690 --> 01:02:19,030
had back here okay but it's now half the
resolution so for placed every two by

649
01:02:19,030 --> 01:02:24,160
two with its max and you'll notice like
it's not every possible two by two I
skip over from here so this is like

650
01:02:26,319 --> 01:02:33,460
starting at beat Hugh and then the next
one starts at BS right so they're like

651
01:02:33,460 --> 01:02:38,050
non-overlapping that's why it's
decreasing the resolution okay

652
01:02:38,050 --> 01:02:43,000
so anybody who's comfortable with
spreadsheets you know you can open this

653
01:02:43,000 --> 01:02:52,150
and have a look and so after our max
pooling there's a number of different

654
01:02:52,150 --> 01:02:58,119
things we could do next and I'm going to
show you a kind of classic old style

655
01:02:58,119 --> 01:03:02,890
approach nowadays in fact what generally
happens nowadays is we do a max pool
where we kind of like max across the

656
01:03:04,900 --> 01:03:10,180
entire size right but on older
architectures and also on all the
structured data stuff we do we actually

657
01:03:12,790 --> 01:03:16,930
do something called a fully connected
layer and so here's a fully connected

658
01:03:16,930 --> 01:03:22,300
layer I'm going to take every single one
of these activations and I've got to

659
01:03:22,300 --> 01:03:28,150
give every single one of them or weight
right and so then I'm going to take over

660
01:03:28,150 --> 01:03:35,650
here here is the sum product of every
one of the activations by every one of

661
01:03:35,650 --> 01:03:40,440
the weights for both of the

662
01:03:40,670 --> 01:03:45,619
two levels of my three-dimensional
tensor right and so this is called a

663
01:03:45,619 --> 01:03:49,490
fully connected layer notice it's
different to a convolution I'm not going
through a few at a time right but I'm

664
01:03:51,980 --> 01:03:57,079
creating a really big weight matrix
right so rather than having a couple of

665
01:03:57,079 --> 01:04:03,290
little 3x3 kernels my weight matrix is
now as big as the entire input and so as

666
01:04:03,290 --> 01:04:09,920
you can imagine architectures that make
heavy use of fully convolutional layers

667
01:04:09,920 --> 01:04:15,049
can have a lot of weights which means
they can have trouble with overfitting

668
01:04:15,049 --> 01:04:20,540
and they can also be slow and so you're
going to see a lot an architecture

669
01:04:20,540 --> 01:04:25,280
called vgg because it was the first kind
of successful deeper architecture it has

670
01:04:25,280 --> 01:04:31,750
up to 19 layers and vgg actually
contains a fully connected layer with

671
01:04:31,750 --> 01:04:39,710
4096 weights connected to a hidden layer
with 4,000 sorry 4096 activations

672
01:04:39,710 --> 01:04:46,250
connected to a hidden layer with 4096
activations so you've got like 4096 by

673
01:04:46,250 --> 01:04:53,180
4096 x remember or apply it by the
number of kind of kernels that we've

674
01:04:53,180 --> 01:05:03,319
calculated so in vgg there's this I
think it's like 300 million weights of

675
01:05:03,319 --> 01:05:09,380
which something like 250 million of them
are in these fully connected layers so

676
01:05:09,380 --> 01:05:13,700
we'll learn later on in the course about
how we can kind of avoid using these big
fully connected layers and behind the

677
01:05:15,470 --> 01:05:20,059
scenes all the stuff that you've seen us
using like ResNet and res next none of

678
01:05:20,059 --> 01:05:26,049
them use very large fully connected
layers you know you had a question

679
01:05:26,049 --> 01:05:32,210
sorry yeah come on um so could you tell
us more about for example if we had like

680
01:05:32,210 --> 01:05:40,280
three channels for the input what would
be the shape yeah these filters right so

681
01:05:40,280 --> 01:05:44,960
that's a great question so if we have 3
channels of input it would look exactly

682
01:05:44,960 --> 01:05:52,160
like conv one right cons one kind of has
two channels right and so you can see

683
01:05:52,160 --> 01:05:57,440
with cons one we had two channels so
therefore our filters had to have like

684
01:05:57,440 --> 01:06:02,989
two channels per filter and so you could
like imagine that this input didn't

685
01:06:02,989 --> 01:06:06,950
exist you know and actually this was the
airport alright so when you have a

686
01:06:06,950 --> 01:06:11,749
multi-channel input it just means that
your filters look like this and so

687
01:06:11,749 --> 01:06:17,779
images often full color they have three
red green and blue sometimes they also

688
01:06:17,779 --> 01:06:23,059
have an alpha Channel so however many
you have that's how many inputs you need

689
01:06:23,059 --> 01:06:27,529
and so something which I know Jeanette
was playing with recently was like using

690
01:06:27,529 --> 01:06:34,099
a full color image net model in medical
imaging for something called bone age

691
01:06:34,099 --> 01:06:38,499
calculations which has a single channel
and so what she did was basically take
the the input the the single channel

692
01:06:42,559 --> 01:06:48,619
input and make three copies of it so you
end up with basically like one two three

693
01:06:48,619 --> 01:06:55,460
versions of the same thing which is like
it's kind of a small idea like it's kind

694
01:06:55,460 --> 01:06:59,630
of redundant information that we don't
quite want but it does mean that then if

695
01:06:59,630 --> 01:07:05,960
you had a something that expected a
three channel convolutional filter you

696
01:07:05,960 --> 01:07:11,420
can use it right and so at the moment
there's a cable competition for iceberg

697
01:07:11,420 --> 01:07:17,660
detection using a some funky satellite
specific data format that has two

698
01:07:17,660 --> 01:07:23,719
channels so here's how you could do that
you could either copy one of those two

699
01:07:23,719 --> 01:07:27,380
channels into the third channel or I
think what people in Carroll are doing

700
01:07:27,380 --> 01:07:32,779
is to take the average of the two again
it's not ideal but it's a way that you

701
01:07:32,779 --> 01:07:40,910
can use pre-trained networks yeah I've
done a lot of fiddling around like that

702
01:07:40,910 --> 01:07:45,440
you can also actually I've actually done
things where I wanted to use a three

703
01:07:45,440 --> 01:07:48,410
channel image net Network on four
channel data

704
01:07:48,410 --> 01:07:53,029
I had a satellite data where the fourth
channel was near-infrared and so

705
01:07:53,029 --> 01:08:02,239
basically I added an extra kind of level
to my convolutional kernels that were

706
01:08:02,239 --> 01:08:06,890
all zeros and so basically like started
off by ignoring the near-infrared band

707
01:08:06,890 --> 01:08:10,300
and
so what happens it basically and you'll

708
01:08:10,300 --> 01:08:15,940
see this next week is that rather than
having these like carefully trained

709
01:08:15,940 --> 01:08:19,870
filters when you're actually training
something from scratch we're actually

710
01:08:19,870 --> 01:08:23,230
going to start with random numbers
that's actually what we do we actually

711
01:08:23,229 --> 01:08:26,469
start with random numbers and then we
use this thing called stochastic

712
01:08:26,470 --> 01:08:29,050
gradient descent which we've kind of
seen conceptually

713
01:08:29,050 --> 01:08:33,070
to slightly improve those random numbers
to make them less random and we
basically do that again and again and

714
01:08:35,350 --> 01:08:39,760
again okay great
let's take a seven minute break and

715
01:08:39,760 --> 01:08:50,310
we'll come back at 7:50 all right so
what happens next

716
01:08:50,310 --> 01:09:00,339
so we've got as far as doing a fully
connected layer right so we had our the
results of our max pooling layer got fed

717
01:09:02,290 --> 01:09:07,180
to a fully connected layer and he might
notice those of you that remember your

718
01:09:07,180 --> 01:09:11,770
linear algebra the fully connected layer
is actually doing a classic traditional

719
01:09:11,770 --> 01:09:17,920
matrix product okay so it's basically
just going through each pair in turn

720
01:09:17,920 --> 01:09:23,970
multiplying them together and then
adding them up to do a matrix product

721
01:09:23,970 --> 01:09:36,460
now in practice if we want to calculate
which one of the ten digits we're

722
01:09:36,460 --> 01:09:44,740
looking at their single number we've
calculated isn't enough we would

723
01:09:44,740 --> 01:09:50,370
actually calculate ten numbers so what
we will have is rather than just having

724
01:09:50,370 --> 01:09:57,130
one set of fully connected weights like
this and I say set because remember

725
01:09:57,130 --> 01:10:04,030
there's like a whole 3d kind of tensor
of them we would actually need ten of

726
01:10:04,030 --> 01:10:09,790
those right so you can see that these
tensors start to get a little bit high

727
01:10:09,790 --> 01:10:14,680
dimensional right and so this is where
my patients we're doing it next cell ran

728
01:10:14,680 --> 01:10:19,810
out but imagine that I had done this ten
times I could now have ten different

729
01:10:19,810 --> 01:10:23,150
numbers or being calculated
yeah using exactly the same process
right we'll just be ten of these fully

730
01:10:27,110 --> 01:10:40,190
connected to by m-by-n erased basically
and so then we would have ten numbers

731
01:10:40,190 --> 01:10:48,190
being spat out so what happens next so
next up we can open up a different Excel

732
01:10:48,190 --> 01:10:56,539
worksheet entropy example dot XLS that's
got two different worksheets one of them

733
01:10:56,539 --> 01:11:02,030
is called soft mass and what happens
here sorry I've changed domains rather

734
01:11:02,030 --> 01:11:06,199
than predicting whether it's the number
from one not to nine I'm going to

735
01:11:06,199 --> 01:11:10,820
predict whether something is a cat a dog
a plane of Fisher Building okay so out

736
01:11:10,820 --> 01:11:16,989
of our that fully connected layer we've
got this case we'd have five numbers and

737
01:11:16,989 --> 01:11:23,150
notice at this point there's no rail you
okay in the last layer there's no rail

738
01:11:23,150 --> 01:11:32,300
you okay so I can have negatives so I
want to turn these five numbers H into a

739
01:11:32,300 --> 01:11:37,039
probability I want to turn it into a
probability from naught to one that it's

740
01:11:37,039 --> 01:11:40,340
a cat
that's a dog there's a plane that it's a

741
01:11:40,340 --> 01:11:44,479
fish that it's a building and I want
those probabilities to have a couple of

742
01:11:44,479 --> 01:11:48,249
characteristics first is that each of
them should be between zero and one and
the second is that this state together

743
01:11:50,510 --> 01:11:55,969
should add up to one right it's
definitely one of these five things okay

744
01:11:55,969 --> 01:12:01,400
so to do that we use a different kind of
activation function what's an activation

745
01:12:01,400 --> 01:12:07,449
function an activation function is a
function that is applied to activations

746
01:12:07,449 --> 01:12:16,340
so for example max 0 comma something is
a function that I applied to an

747
01:12:16,340 --> 01:12:23,630
activation so an activation function
always takes in one number and spits out

748
01:12:23,630 --> 01:12:29,780
one number so max of 0 comma X takes in
a number X and spits out some different

749
01:12:29,780 --> 01:12:32,920
number value of s

750
01:12:33,070 --> 01:12:39,260
that's all an activation function is and
if you remember back to that PowerPoint

751
01:12:39,260 --> 01:12:44,230
we saw in Lesson one

752
01:12:45,590 --> 01:12:55,230
each of our layers was just a linear
function and then after every layer we
said we needed some non-linearity act as

753
01:12:58,619 --> 01:13:04,500
if you stack a bunch of linear layers
together right then all you end up with

754
01:13:04,500 --> 01:13:09,840
is a linear layer okay
so somebody's talking can can you not a

755
01:13:09,840 --> 01:13:13,340
slow just acting thank you

756
01:13:13,829 --> 01:13:18,210
if you stack a number of linear
functions together you just end up with

757
01:13:18,210 --> 01:13:22,139
a linear function and nobody does any
cool deep learning with displaying your

758
01:13:22,139 --> 01:13:29,179
functions right but remember we also
learnt that by stacking linear functions

759
01:13:29,179 --> 01:13:34,769
with between each one a non-linearity we
could create like arbitrarily complex

760
01:13:34,769 --> 01:13:39,599
shapes and so the non-linearity that
we're using after every hidden layer is

761
01:13:39,599 --> 01:13:45,570
a rally rectified linear unit a
non-linearity is an activation function

762
01:13:45,570 --> 01:13:51,269
an activation function is a
non-linearity in with in deep way

763
01:13:51,269 --> 01:13:54,749
obviously there's lots of other
nonlinearities and in the world but in

764
01:13:54,749 --> 01:14:00,239
deep learning this is what we mean so an
activation function is any function that

765
01:14:00,239 --> 01:14:04,979
takes some activation in as a single
number and spits out some new activation

766
01:14:04,979 --> 01:14:10,320
like max of 0 comma so I'm now going to
tell you about a different activation
function it's slightly more complicated

767
01:14:12,300 --> 01:14:19,289
than value but not too much it's called
soft max soft max only ever occurs in

768
01:14:19,289 --> 01:14:24,869
the final layer at the very end and the
reason why is that soft max always spits

769
01:14:24,869 --> 01:14:29,550
out numbers as an activation function
that always spits out a number between

770
01:14:29,550 --> 01:14:35,729
Norton 1 and it always spits out a bunch
of numbers that add to 1 so a soft max

771
01:14:35,729 --> 01:14:43,289
gives us what we want right in theory
this isn't strictly necessary right like
we could ask our neural net to learn a

772
01:14:46,139 --> 01:14:53,489
set of kernels which have you know which
which give probabilities that line up as
closely as possible with what we want

773
01:14:54,960 --> 01:14:59,940
but in general with deep learning if you
can construct your architecture so that

774
01:14:59,940 --> 01:15:06,090
the desired characteristics are as easy
to express as possible you'll end up

775
01:15:06,090 --> 01:15:10,170
with better models like they'll learn
more quickly with less parameters so in

776
01:15:10,170 --> 01:15:16,199
this case we know that our probabilities
should end up being between 9 1 we know
that they should end up adding to 1 so

777
01:15:18,630 --> 01:15:23,280
if we construct an activation function
which always has those features then

778
01:15:23,280 --> 01:15:26,670
we're going to make our neural network
do a better job

779
01:15:26,670 --> 01:15:29,610
it's gonna make it easier for it it
doesn't have to learn to do those things

780
01:15:29,610 --> 01:15:37,470
because it all happen automatically okay
so in order to make this work we first

781
01:15:37,470 --> 01:15:41,760
of all have to get rid of all of the
negatives right like we can't have

782
01:15:41,760 --> 01:15:46,500
negative probabilities so to make things
not being negative one way we could do

783
01:15:46,500 --> 01:15:51,420
it is just go into the pair of right so
here you can see my first step is to go

784
01:15:51,420 --> 01:15:58,650
X of the previous one right and I think
I've mentioned this before but of all

785
01:15:58,650 --> 01:16:03,450
the math that you just need to be super
familiar with to do deep learning the

786
01:16:03,450 --> 01:16:08,670
one you really need is logarithms and
asks write all of deep learning and all

787
01:16:08,670 --> 01:16:19,530
of machine learning they appear all the
time right so for example you absolutely

788
01:16:19,530 --> 01:16:33,090
need to know that log of x times y
equals log of X plus log of Y all right

789
01:16:33,090 --> 01:16:37,260
and like not just know that that's a
formula that exists but have a sense of

790
01:16:37,260 --> 01:16:41,010
like what does that mean why is that
interesting oh I can turn

791
01:16:41,010 --> 01:16:45,360
multiplications into additions that
could be really handy right and
therefore log of x over y equals log of

792
01:16:50,820 --> 01:16:57,480
X minus log of Y again that's going to
come in pretty handy you know rather

793
01:16:57,480 --> 01:17:03,390
than dividing I can just subtract things
right and also remember that if I've got

794
01:17:03,390 --> 01:17:14,370
log of x equals y then that means a to
the y equals x in other words log log

795
01:17:14,370 --> 01:17:21,450
and E to the for the inverse of each
other okay again you just you need to

796
01:17:21,450 --> 01:17:25,140
really really understand these things
and like so if you if you haven't spent

797
01:17:25,140 --> 01:17:31,650
much time with logs and X for a while
try plotting them in Excel or a notebook
have a sense of what shape they are how

798
01:17:33,780 --> 01:17:38,260
they combine together just make sure
you're really comfortable with them so

799
01:17:38,260 --> 01:17:44,590
we're using it here right we're using it
here so one of the things that we know

800
01:17:44,590 --> 01:17:49,450
is a to the power of something is
positive okay so that's great

801
01:17:49,450 --> 01:17:52,630
the other thing you'll notice about e to
the power of something is because it's a

802
01:17:52,630 --> 01:17:58,210
power numbers that are slightly bigger
than other numbers like four is a little

803
01:17:58,210 --> 01:18:02,710
bit bigger than 2.8 when you go e to the
power of it really accentuates that

804
01:18:02,710 --> 01:18:05,320
difference okay
so we're going to take advantage of both

805
01:18:05,320 --> 01:18:10,480
of these features for the purpose of
deep learning okay so we take our the

806
01:18:10,480 --> 01:18:15,850
results of this fully connected layer we
go e to the power of for each of them

807
01:18:15,850 --> 01:18:28,120
and then we're gonna yeah and then we're
going to add them up okay so here is the
sum of e to the power of so then here

808
01:18:32,340 --> 01:18:37,830
we're going to take e to the power of
divided by the sum of e to the power of

809
01:18:37,830 --> 01:18:43,860
so if you take all of these things
divided by their sum then by definition

810
01:18:43,860 --> 01:18:51,460
all of those things must add up to 1 and
furthermore since we're dividing by

811
01:18:51,460 --> 01:18:56,890
their sum they must always vary between
0 and 1 because they were always

812
01:18:56,890 --> 01:19:04,840
positive alright and that's it so that's
what softmax is ok so I've got this kind

813
01:19:04,840 --> 01:19:11,050
of doing random numbers each time right
and so you can see like as I as I look

814
01:19:11,050 --> 01:19:16,510
through my softmax generally has quite a
few things that are so close to zero

815
01:19:16,510 --> 01:19:20,739
that they round down to zero and you
know maybe one thing that's nearly 1
right and the reason for that is what we

816
01:19:23,110 --> 01:19:28,420
just talked about that is with the X
just having one number a bit bigger than

817
01:19:28,420 --> 01:19:33,100
the others tends to like push it out
further right so even though my inputs

818
01:19:33,100 --> 01:19:39,190
here around on numbers between negative
5 and 5 right my outputs from the
softmax don't really look that random at

819
01:19:41,140 --> 01:19:47,140
all in the sense that they tend to have
one big number and a bunch of small

820
01:19:47,140 --> 01:19:53,680
numbers and now that's what we want
right we want to say like in terms of

821
01:19:53,680 --> 01:19:57,700
like is this a cat a dog a plane a fish
or a building we really want it to say
like it's it's that you know it's it's a

822
01:20:00,100 --> 01:20:07,510
dog or it's a plane not like I don't
know okay so softmax has lots of these

823
01:20:07,510 --> 01:20:12,610
cool properties right it's going to
return a probability that adds up to 1
and it's going to tend to want to pick

824
01:20:14,980 --> 01:20:22,600
one thing particularly strongly okay so
that's soft mess your net could you pass

825
01:20:22,600 --> 01:20:31,180
actually bust me up we how would we do
something that as let's say you have any

826
01:20:31,180 --> 01:20:34,780
imaging you want to count in categorize
I was like cat and the dog or like has
multiple things but what kind of

827
01:20:37,600 --> 01:20:43,230
function will we try to use so happens
we're going to do that right now so

828
01:20:45,419 --> 01:20:48,840
so hope you think about why we might
want to do that and so runways where you

829
01:20:48,840 --> 01:20:53,010
might want to do that is to do
multi-label classification so we're

830
01:20:53,010 --> 01:20:57,119
looking now at listen to image models
and specifically we're going to take a
look at the planet competition satellite

831
01:21:00,419 --> 01:21:08,189
imaging competition now the satellite
imaging competition has some

832
01:21:08,189 --> 01:21:13,229
similarities to stuff we've seen before
right so before we've seen cat versus

833
01:21:13,229 --> 01:21:19,769
dog and these images are a cat or a dog
they're not Maya they're not both right

834
01:21:19,769 --> 01:21:25,559
but the satellite imaging competition
has stayed as images that look like this

835
01:21:25,559 --> 01:21:30,389
and in fact every single one of the
images is classified by whether there's

836
01:21:30,389 --> 01:21:34,260
four kinds of weather
one of which is haze and another of

837
01:21:34,260 --> 01:21:39,719
which is clear in addition to which
there is a list of features that may be

838
01:21:39,719 --> 01:21:44,880
present including agriculture which is
like some some cleared area used for

839
01:21:44,880 --> 01:21:50,999
agriculture primary which means primary
rainforest and water which means a river

840
01:21:50,999 --> 01:21:56,969
or a creek so here is a clear day
satellite image showing some agriculture

841
01:21:56,969 --> 01:22:02,550
some primary rainforest and some water
features and here's one which is in haze

842
01:22:02,550 --> 01:22:09,840
and is entirely primary rainforest so in
this case we're going to want to be able

843
01:22:09,840 --> 01:22:15,689
to show we're going to predict multiple
things and so softmax wouldn't be good

844
01:22:15,689 --> 01:22:21,749
because softmax doesn't like predicting
multiple things and like I would
definitely recommend anthropomorphizing

845
01:22:23,610 --> 01:22:28,079
your activation functions right they
have personalities okay and the

846
01:22:28,079 --> 01:22:34,559
personality of the softmax is it wants
to pick a thing and people forget this

847
01:22:34,559 --> 01:22:39,419
all the time I've seen many people even
well-regarded researchers in famous

848
01:22:39,419 --> 01:22:46,229
academic papers using like soft maps for
multi-label classification it happens

849
01:22:46,229 --> 01:22:49,590
all the time
right and it's kind of ridiculous

850
01:22:49,590 --> 01:22:55,249
because they're not understanding the
personality of their activation function

851
01:22:55,249 --> 01:23:00,550
so for multi
classification where each sample can

852
01:23:00,550 --> 01:23:05,590
belong to one or more classes we have to
change a few things but here's the good

853
01:23:05,590 --> 01:23:08,560
news
in fast AI we don't have to change

854
01:23:08,560 --> 01:23:15,940
anything right so fast AI will look at
the labels in the CSV and if there is
more than one label ever for any item it

855
01:23:21,880 --> 01:23:25,600
will automatically switch into like
multi-label mode so I'm going to show

856
01:23:25,600 --> 01:23:29,770
you how it works behind the scenes but
the good news is you don't actually have

857
01:23:29,770 --> 01:23:40,690
to care it happens anywhere so if you
have multi label images multi label
objects you obviously can't use the

858
01:23:43,150 --> 01:23:48,370
classic Kerris style approach where
things are in folders because something

859
01:23:48,370 --> 01:23:55,180
can't conveniently be in multiple
folders at the same time right so that's

860
01:23:55,180 --> 01:24:04,050
why we you basically have to use the
from CSV approach right so if we look at

861
01:24:04,050 --> 01:24:12,160
[Music]
an example actually I'll show you I tend

862
01:24:12,160 --> 01:24:16,420
to take you through it right so we can
say okay this is the CSV file containing

863
01:24:16,420 --> 01:24:21,610
our labels this looks exactly the same
as I did before but rather than side on

864
01:24:21,610 --> 01:24:27,220
its top down alright and top down I've
mentioned before that can do our

865
01:24:27,220 --> 01:24:29,800
vertical flips it actually does more
than that there's actually eight

866
01:24:29,800 --> 01:24:35,140
possible symmetries for a square which
is it can be rotated through 90 180 270

867
01:24:35,140 --> 01:24:39,670
or 0 degrees and for each of those it
can be flipped and if you think about it

868
01:24:39,670 --> 01:24:45,070
for awhile you'll realize that that's a
complete enumeration of everything that

869
01:24:45,070 --> 01:24:50,620
you can do in terms of symmetries to a
square so they're called it's called the

870
01:24:50,620 --> 01:24:55,630
dihedral group of eight so if you see in
the code there's actually a transform or

871
01:24:55,630 --> 01:25:01,690
dihedral that's why it's called that so
this transforms will basically do the
full set of eight symmetric dihedral

872
01:25:05,670 --> 01:25:11,130
rotations and flips plus everything
which we can do to dogs and cats

873
01:25:11,130 --> 01:25:16,290
you know small clinical rotations a
little bit of zooming a little bit of

874
01:25:16,290 --> 01:25:23,340
contrast and brightness adjustment so
these images are a size 256 by 256 so I

875
01:25:23,340 --> 01:25:28,440
just create a little function here to
let me quickly grab you know data loader
of any size so here's a 256 by 256 once

876
01:25:32,429 --> 01:25:37,650
you've got a data object inside it we've
already seen that there's things called

877
01:25:37,650 --> 01:25:43,230
Val D s test D s train D s there are
things that you can just index into and

878
01:25:43,230 --> 01:25:48,360
grab a particular image so you just use
square brackets 0 you'll also see that

879
01:25:48,360 --> 01:25:53,310
all of those things have a DL that's a
data loader so des is data set DL is

880
01:25:53,310 --> 01:25:58,080
data motor these are concepts from PI
watch so if you Google PI torch data set
or pipe watch data loader you can

881
01:26:00,300 --> 01:26:04,650
basically see what it means but the
basic idea is a data set gives you a
single image or a single object back a

882
01:26:08,429 --> 01:26:12,900
data loader gives you back a mini batch
and specifically it gives you back a

883
01:26:12,900 --> 01:26:20,219
transformed mini - so that's why when we
create our data object we can pass in

884
01:26:20,219 --> 01:26:25,560
num workers and transforms it's like how
many processes do you want to use what

885
01:26:25,560 --> 01:26:30,480
transforms do you want and so with with
a data loader you can't ask for an
individual image you can only get back

886
01:26:32,159 --> 01:26:37,260
at a mini batch and you can't get back a
particular mini batch you can only get

887
01:26:37,260 --> 01:26:42,719
back the next mini - so something
reverses look through grabbing a mini

888
01:26:42,719 --> 01:26:47,880
batch at a time and so in Python the
thing that does that is called a

889
01:26:47,880 --> 01:26:51,989
generator right or an iterator this
slightly different versions are the same
thing so to turn a data loader into an

890
01:26:54,480 --> 01:26:57,480
iterator you use the standard Python
function cordetta

891
01:26:57,480 --> 01:27:02,550
that's a Python function just a regular
part of the Python basic language that

892
01:27:02,550 --> 01:27:07,350
returns you an iterator and an iterator
is something that takes you can pass the

893
01:27:07,350 --> 01:27:13,290
static give pass it to the standard
Python function or statement next and

894
01:27:13,290 --> 01:27:18,800
that just says give me another batch
from this iterator

895
01:27:18,800 --> 01:27:23,070
so we're basically this is one of the
things I really like about PI torch is
it really leverages

896
01:27:24,829 --> 01:27:31,489
modern pythons kind of stuff you know in
in tensorflow they invent their whole

897
01:27:31,489 --> 01:27:38,329
new world earth ways of doing things and
so it's kind of more in a sense it's

898
01:27:38,329 --> 01:27:42,349
more like cross-platform but in another
sense like it's not a good fit to any

899
01:27:42,349 --> 01:27:49,880
platform so it's nice if you if you know
Python well PI torch comes very

900
01:27:49,880 --> 01:27:53,960
naturally if you don't know Python well
PI torches are good reason to learn

901
01:27:53,960 --> 01:28:00,559
Python well a PI torch your module
neural network module is a standard

902
01:28:00,559 --> 01:28:05,690
Python bus for example so any work you
put into learning Python better will pay
off with paid watch so here I am using

903
01:28:08,090 --> 01:28:15,619
standard Python iterators and next to
grab my next mini batch from the
validation sets data loader and that's

904
01:28:17,780 --> 01:28:20,960
going to return two things it's going to
return the images in the mini batch and

905
01:28:20,960 --> 01:28:25,360
the labels of the mini - so standard
Python approach I can pull them apart

906
01:28:25,360 --> 01:28:34,099
like so and so here is one mini batch of
labels and so not surprisingly since I

907
01:28:34,099 --> 01:28:40,239
said that my batch size let's go ahead
and find it

908
01:28:41,570 --> 01:28:47,840
Oh actually it's the batch size by
default is 64 so I didn't pass in a

909
01:28:47,840 --> 01:28:52,099
batch size and so just remember shift
tab to see like what are the things you

910
01:28:52,099 --> 01:28:57,710
can pass and what are the defaults so by
default my batch size is 64 so I've got

911
01:28:57,710 --> 01:29:07,040
that something of size 64 by 17 so there
are 17 of the possible classes right so

912
01:29:07,040 --> 01:29:15,829
let's take a look at the zeroth set of
labels so the zeroth images labels so I

913
01:29:15,829 --> 01:29:21,559
can zip again standard Python things it
takes two lists and combines it so you

914
01:29:21,559 --> 01:29:25,489
get the zero theme from the first list
as you're asking for the second list and
the first thing for the first first this

915
01:29:27,440 --> 01:29:31,190
first thing from the second list and so
forth so I can zip them together and

916
01:29:31,190 --> 01:29:36,790
that way I can find out for the zeroth
image and the validation set is

917
01:29:36,790 --> 01:29:42,599
agriculture
it's clear its primary rainforest its

918
01:29:42,599 --> 01:29:51,869
slash-and-burn its water okay so as you
can see here this is a MOLLE label you
see here's a way to do multi-label

919
01:29:53,219 --> 01:30:01,080
classification so by the same token
right if we go back to our single label

920
01:30:01,080 --> 01:30:07,230
classification it's a cat dog playing
official building behind the scenes we

921
01:30:07,230 --> 01:30:12,599
haven't actually looked at it but behind
the scenes fast AI imply torch are

922
01:30:12,599 --> 01:30:18,869
turning our labels into something called
one hot encoded labels and so if it was
actually a dog than the actual values

923
01:30:22,940 --> 01:30:29,699
would be like that right so these are
like the actuals okay so do you remember

924
01:30:29,699 --> 01:30:34,139
at the very end of at AV o--'s video he
showed how like the template had to

925
01:30:34,139 --> 01:30:39,119
match to one of the like five ABCDE
templates and so what it's actually

926
01:30:39,119 --> 01:30:44,489
doing is it's comparing when I said it's
basically doing a dot product it's

927
01:30:44,489 --> 01:30:50,040
actually a fully connected layer at the
end right that calculates an output

928
01:30:50,040 --> 01:30:56,940
activation that goes through a soft Max
and then the soft max is compared to the

929
01:30:56,940 --> 01:31:03,540
one hot encoded label right so if it was
a dog there would be a one here and then

930
01:31:03,540 --> 01:31:08,670
we take the difference between the
actuals and the softmax activation is to

931
01:31:08,670 --> 01:31:12,179
say and add those add up those
differences to say how much error is

932
01:31:12,179 --> 01:31:16,110
there essentially we're skipping over
something called a loss function that

933
01:31:16,110 --> 01:31:21,320
we'll learn about next week but
essentially we're basically doing that

934
01:31:21,680 --> 01:31:28,250
now if it's one hot encoded like there's
only one thing which have a 1 in it then

935
01:31:28,250 --> 01:31:36,530
actually storing it as 0 1 0 0 0 is
terribly inefficient right like we could

936
01:31:36,530 --> 01:31:40,700
basically say what are the index of each
of these things right so we can say it's

937
01:31:40,700 --> 01:31:48,710
like 0 1 2 3 4 like so right and so
rather than storing it is 0 1 0 0 0

938
01:31:48,710 --> 01:31:56,420
we actually just store the index value
right so if you look at the the Y values

939
01:31:56,420 --> 01:32:01,160
for the cats and dogs competition or the
dog breeds competition you won't

940
01:32:01,160 --> 01:32:04,850
actually see a big lists of ones and
zeros like this you'll see a single

941
01:32:04,850 --> 01:32:11,680
integer right which is like what what
class index is it right and internally

942
01:32:11,680 --> 01:32:17,750
inside pipe arch it will actually turn
that into a one hot encoded vector but

943
01:32:17,750 --> 01:32:23,270
like you will literally never see it
okay and and pi torch has different loss

944
01:32:23,270 --> 01:32:27,860
functions where you basically say this
thing's won this thing is one hot
encoder door this thing is not and it

945
01:32:30,050 --> 01:32:34,160
uses different bus functions
that's all hidden by the faster I

946
01:32:34,160 --> 01:32:40,040
library right so like you don't have to
worry about it but is but the the cool

947
01:32:40,040 --> 01:32:45,350
thing to realize is that this approach
for multi-label encoding with these ones

948
01:32:45,350 --> 01:32:51,080
and zeros behind the scenes the exact
same thing happens for single level

949
01:32:51,080 --> 01:32:59,000
classification does it make sense to
change the beginners of the sigmoid of

950
01:32:59,000 --> 01:33:07,940
the softmax function by changing the
base no because when you change the more

951
01:33:07,940 --> 01:33:24,200
math log base a of B equals log B over
log a so changing the base is just a

952
01:33:24,200 --> 01:33:29,670
linear scaling and linear scaling is
something which the neural net can

953
01:33:29,670 --> 01:33:33,110
with very easily

954
01:33:35,450 --> 01:33:44,250
good question okay so here is that image
right here is the image with

955
01:33:44,250 --> 01:33:49,650
slash-and-burn water etc etc one of the
things to notice here is like when I

956
01:33:49,650 --> 01:33:56,070
first displayed this image it was so
washed out I really couldn't see it

957
01:33:56,070 --> 01:34:02,970
right but remember images now you know
we know images are just matrices of

958
01:34:02,970 --> 01:34:08,670
numbers and so you can see here I just
said times 1.4 just to make it more

959
01:34:08,670 --> 01:34:12,930
visible right so like now that you kind
of it's the kind of thing I want you to

960
01:34:12,930 --> 01:34:15,840
get familiar with is the idea that this
stuff you're dealing with

961
01:34:15,840 --> 01:34:19,710
they're just matrices of numbers and you
can fiddle around with them so if you're

962
01:34:19,710 --> 01:34:22,830
looking at something like guys a bit
washed out you can just multiply it by

963
01:34:22,830 --> 01:34:27,990
something to brighten it up a bit okay
so here we can see I guess this is the

964
01:34:27,990 --> 01:34:33,300
slash-and-burn here's the river that's
the water here's the primary rainforest

965
01:34:33,300 --> 01:34:41,010
maybe that's the agriculture so forth
okay so so you know with all that

966
01:34:41,010 --> 01:34:46,350
background how do we actually use this
exactly the same way as everything we've

967
01:34:46,350 --> 01:34:52,170
done before right so you know size and
and the interesting thing about playing

968
01:34:52,170 --> 01:34:56,370
around with this planet competition is
that these images are not at all like

969
01:34:56,370 --> 01:35:02,580
image there and I would guess that the
vast majority is of stuff that the vast

970
01:35:02,580 --> 01:35:07,770
majority of you do involving
convolutional neural Nets won't actually

971
01:35:07,770 --> 01:35:14,130
be anything like image net you know
it'll be it'll be medical imaging it'll

972
01:35:14,130 --> 01:35:18,530
be like classifying different kinds of
steel tube or figuring out whether a

973
01:35:18,530 --> 01:35:25,650
world you know is going to break or not
or or looking at satellite images or you

974
01:35:25,650 --> 01:35:32,690
know whatever right so it's it's good to
experiment with stuff like this planet

975
01:35:32,690 --> 01:35:37,170
competition to get a sense of kind of
what you want to do and so you'll see

976
01:35:37,170 --> 01:35:44,910
here I start out by resizing my data to
64 by 64 it starts out at 256 by 256

977
01:35:44,910 --> 01:35:48,270
right now
I wouldn't want to do this for the cats

978
01:35:48,270 --> 01:35:52,620
and dogs competition because it cats end
on competition we start with a pre
trained imagenet Network it's it's

979
01:35:54,360 --> 01:36:00,060
nearly isn't it starts off nearly
perfect right so if we resized

980
01:36:00,060 --> 01:36:04,980
everything to 64 by 64 and then
retrained the whole set regular it we'd
basically destroy the weights that are

981
01:36:07,440 --> 01:36:12,000
already pre trained to be very good
remember imagenet most imagenet models

982
01:36:12,000 --> 01:36:17,760
are trained at either 224 by 224 or
$2.99 by 299 all right so if we like

983
01:36:17,760 --> 01:36:23,480
retrain them at 64 by 64 we're going to
we're going to kill it on the other hand
there's nothing in image net that looks

984
01:36:25,920 --> 01:36:31,260
anything like this you know there's no
satellite images so the only useful bits

985
01:36:31,260 --> 01:36:39,870
of the image net Network for us are kind
of layers like this one you know finding

986
01:36:39,870 --> 01:36:44,460
edges and gradients and this one you
know finding kind of textures and

987
01:36:44,460 --> 01:36:49,890
repeating patterns and maybe these ones
are kind of finding more complex

988
01:36:49,890 --> 01:36:57,530
textures but that's probably about it
right so so in other words you know
starting out by training very small

989
01:37:00,210 --> 01:37:04,920
images works pretty well when you're
using stuff like satellites so in this

990
01:37:04,920 --> 01:37:13,410
case I started right back at 64 by 64
grab some data built my model found out

991
01:37:13,410 --> 01:37:17,280
what learning rate to use and
interestingly it turned out to be quite

992
01:37:17,280 --> 01:37:25,920
high it seems that because like it's so
unlike imagenet I needed to do quite a

993
01:37:25,920 --> 01:37:30,390
bit more fitting with just that last
layer before it started to flatten out

994
01:37:30,390 --> 01:37:38,090
then I unfreeze dit and again this is
the difference to image net like

995
01:37:38,090 --> 01:37:44,190
datasets is my learning rate in the
initial layer i set 2/9 the middle
layers I said 2/3 where else for stuff

996
01:37:47,190 --> 01:37:52,410
like it's like image net I had a
multiple of 10 each of those you know

997
01:37:52,410 --> 01:37:58,140
again the idea being that that earlier
layers probably and not as close to what

998
01:37:58,140 --> 01:38:04,620
they need to be compared to the
like dances again

999
01:38:04,620 --> 01:38:09,120
unfreeze train for a while and you can
kind of see here

1000
01:38:09,120 --> 01:38:14,820
you know there's cycle one there's cycle
- there's cycle three and then I kind of

1001
01:38:14,820 --> 01:38:21,840
increased double the size with my images
fit for a while and freeze fit for a

1002
01:38:21,840 --> 01:38:25,950
while double the size of the images
again fit for a while I'm freeze for a

1003
01:38:25,950 --> 01:38:30,090
while and then add TTA and so as I
mentioned last time we looked at this

1004
01:38:30,090 --> 01:38:35,300
this process ends up you know getting us
about 30th place in this competition

1005
01:38:35,300 --> 01:38:39,440
which is really cool because people you
know a lot of very very smart people
just a few months ago worked very very

1006
01:38:42,150 --> 01:38:51,140
hard on this competition a couple of
things people have asked about one is

1007
01:38:51,200 --> 01:38:59,160
what is this data dot resize do so a
couple of different pieces here the

1008
01:38:59,160 --> 01:39:07,650
first is that when we say back here what
transforms do we apply and here's our

1009
01:39:07,650 --> 01:39:12,810
transforms we actually pass in a size
right so one of the things that that one

1010
01:39:12,810 --> 01:39:17,130
of the things that data loaded does is
to resize the images like on-demand

1011
01:39:17,130 --> 01:39:20,300
every time it sees them

1012
01:39:21,510 --> 01:39:25,710
it's got nothing to do with that dot
resize method right so this is this is

1013
01:39:25,710 --> 01:39:29,699
the thing that happens at the end like
whatever's passed in before it hits out

1014
01:39:29,699 --> 01:39:35,099
that before our data loader spits it out
it's going to resize it to this size if

1015
01:39:35,099 --> 01:39:41,659
the initial input is like a thousand by
a thousand reading that JPEG and
resizing it to 64 by 64 turns out to

1016
01:39:45,989 --> 01:39:51,300
actually take more time than training
the content that's for each batch all

1017
01:39:51,300 --> 01:39:57,150
right so basically all resize does is it
says hey I'm not going to be using any
images bigger than size times 1.3 so

1018
01:40:01,320 --> 01:40:08,400
just grow through once and create new
JPEGs of this size right and they're

1019
01:40:08,400 --> 01:40:14,070
rectangular right so new JPEGs where the
smallest edges of this size and again

1020
01:40:14,070 --> 01:40:19,079
it's like you never have to do this
there's no reason to ever use it if you

1021
01:40:19,079 --> 01:40:23,280
don't want to it's just a speed-up okay
but if you've got really big images

1022
01:40:23,280 --> 01:40:27,300
coming in it saves you a lot of time and
you'll often see on like Carol kernels

1023
01:40:27,300 --> 01:40:34,409
or forum posts or whatever people will
have like bash script stuff like that -
like loop through and resize images to

1024
01:40:36,809 --> 01:40:40,440
save time you never have to do that
right just you can just say dot resize
and it'll just create you know once-off

1025
01:40:44,219 --> 01:40:48,389
it'll go through and create that if it's
already there and it'll use the

1026
01:40:48,389 --> 01:40:53,880
criticized ones for you okay so it's
just it's just a speed up convenience

1027
01:40:53,880 --> 01:41:05,119
function no more okay so for those of
you that are kind of past dog breeds I

1028
01:41:05,119 --> 01:41:13,980
would be looking at planet next you know
like try it like play around with with

1029
01:41:13,980 --> 01:41:19,170
trying to get a sense of like how can
you get this as an accurate model one

1030
01:41:19,170 --> 01:41:21,840
thing to mention and I'm not really
going to go into it in details there's

1031
01:41:21,840 --> 01:41:24,900
nothing to do with deep learning
particularly is that I'm using a
different metric I didn't use metrics

1032
01:41:26,880 --> 01:41:32,570
equals accuracy but I said metrics
equals f2

1033
01:41:33,980 --> 01:41:39,400
remember from last week that confusion
matrix that like two by two you know

1034
01:41:39,400 --> 01:41:47,090
correct incorrect for each of dogs and
cats there's a lot of different ways you

1035
01:41:47,090 --> 01:41:51,140
could turn that confusion matrix into a
score you know do you care more about

1036
01:41:51,140 --> 01:41:54,260
false negatives or do you care more
about false positives and how do you

1037
01:41:54,260 --> 01:41:59,390
weight them and how do you combine them
together right there's a basic there's

1038
01:41:59,390 --> 01:42:04,370
basically a function called F beta where
the beta says how much do you weight

1039
01:42:04,370 --> 01:42:11,540
false negatives versus false positives
and so f 2 is f beta with beta equals 2

1040
01:42:11,540 --> 01:42:15,140
and it's basically as particular way of
weighting false negatives and false

1041
01:42:15,140 --> 01:42:19,700
positives and the reason we use it is
because cattle told us that planet who
are running this competition wanted to

1042
01:42:21,739 --> 01:42:28,730
use this particular F beta metric the
important thing for you to know is that

1043
01:42:28,730 --> 01:42:33,110
you can create custom metrics so in this
case you can see here it says from

1044
01:42:33,110 --> 01:42:38,120
Planet import f2 and really I've got
this here so that you can see how to do

1045
01:42:38,120 --> 01:42:47,270
it right so if you look inside courses
DL 1 you can see there's something

1046
01:42:47,270 --> 01:42:55,540
called planet py right and so if I look
at planet py you'll see there's a
function there called

1047
01:42:57,320 --> 01:43:08,690
f2 right and so f2 simply calls F beta
score from psychic or side PI and patent

1048
01:43:08,690 --> 01:43:13,400
where it came from and does a couple
little tweets that are particularly

1049
01:43:13,400 --> 01:43:19,340
important but the important thing is
like you can write any metric you like

1050
01:43:19,340 --> 01:43:25,970
right as long as it takes in set of
predictions and a set of targets and

1051
01:43:25,970 --> 01:43:30,530
they're both going to be numpy arrays
one dimensional non pyros and then you
return back a number okay and so as long

1052
01:43:33,380 --> 01:43:38,930
as you put a function that takes two
vectors and returns at number you can

1053
01:43:38,930 --> 01:43:44,290
call it as a metric and so then when we
said

1054
01:43:46,710 --> 01:43:54,490
see here learn metrics equals and then
past in that array which just contains a

1055
01:43:54,490 --> 01:44:00,580
single function f2 then it's just going
to be printed out after every for you

1056
01:44:00,580 --> 01:44:06,970
okay so in general like the the faster I
library everything is customizable so

1057
01:44:06,970 --> 01:44:15,940
kind of the idea is that everything is
everything is kind of gives you what you
might want by default but also

1058
01:44:18,520 --> 01:44:26,470
everything can be changed as well yes
you know um we have a little confusion

1059
01:44:26,470 --> 01:44:32,890
about the difference between multi-label
and a single label uh-huh

1060
01:44:32,890 --> 01:44:38,140
the vanish as an example in which
compared like similarly the example they

1061
01:44:38,140 --> 01:44:47,080
just show us ah activation function yeah
so so I'm so sorry I said I'd do that

1062
01:44:47,080 --> 01:44:52,180
then I didn't so the activation the
output activation function for a single

1063
01:44:52,180 --> 01:44:58,390
label classification is softmax but all
the reasons that we talked today but if

1064
01:44:58,390 --> 01:45:05,350
we were trying to predict something that
was like 0 0 1 1 0 then softmax would be

1065
01:45:05,350 --> 01:45:09,100
a terrible choice because it's very hard
to come up with something where both of

1066
01:45:09,100 --> 01:45:13,510
these are high in fact it's impossible
because they have to add up to 1 so the

1067
01:45:13,510 --> 01:45:19,690
closest they could be would be point 5
so for multi-label classification our
activation function is called sigmoid ok

1068
01:45:23,650 --> 01:45:28,690
and again the faster library does this
automatically for you if it notices you

1069
01:45:28,690 --> 01:45:34,210
have a multi label problem and it does
that by checking your data tip to see if

1070
01:45:34,210 --> 01:45:40,870
anything has more than one label applied
to it and so sigmoid is a function which

1071
01:45:40,870 --> 01:45:48,240
is equal to it's basically the same
thing except rather than we never add up

1072
01:45:48,240 --> 01:45:55,170
all of these X but instead we just take
this X when we say it's just equal to it

1073
01:45:55,170 --> 01:46:05,119
divided by one plus
it and so the nice thing about that is

1074
01:46:05,119 --> 01:46:15,439
that now like multiple things can be
high at once right and so generally then

1075
01:46:15,439 --> 01:46:21,229
if something is less than zero its
sigmoid is going to be less than 0.5 if

1076
01:46:21,229 --> 01:46:26,300
it's greater than zero is signal it's
going to be greater than 0.5 and so the

1077
01:46:26,300 --> 01:46:34,570
important thing to know about a sigmoid
function is that its shape is

1078
01:46:36,050 --> 01:46:44,560
something which asymptotes at the top to
one and asymptotes drew

1079
01:46:45,869 --> 01:46:53,440
asymptotes at the bottom to zero and so
therefore it's a good thing to model a

1080
01:46:53,440 --> 01:47:01,960
probability with anybody who has done
any logistic regression will be familiar

1081
01:47:01,960 --> 01:47:05,440
with this is what we do in logistic
regression so it kind of appears

1082
01:47:05,440 --> 01:47:08,949
everywhere in machine learning and
you'll see that kind of a sigmoid and a

1083
01:47:08,949 --> 01:47:15,880
softmax they're very close to each other
conceptually but this is what we want is

1084
01:47:15,880 --> 01:47:19,869
our activation function for multi-label
and this is what we want the single

1085
01:47:19,869 --> 01:47:22,659
label and again first AI does it all for
you

1086
01:47:22,659 --> 01:47:34,059
there was a question over here yes I
have a question about the initial

1087
01:47:34,059 --> 01:47:39,659
training that you do if I understand
correctly you have we have frozen the
the premium model and you only need

1088
01:47:42,989 --> 01:47:50,920
initially try to train the latest
playwright right but from the other hand

1089
01:47:50,920 --> 01:47:55,960
we said that only the initial layer so
let's last probably the first layer is

1090
01:47:55,960 --> 01:48:01,599
like important to us and the other two
are more like features that are you must

1091
01:48:01,599 --> 01:48:07,119
not related and we then apply in this
case what that they the lie is a very
important but the pre-trained weights in

1092
01:48:10,840 --> 01:48:16,210
them aren't so it's the later layers
that we really want to train the most so

1093
01:48:16,210 --> 01:48:23,769
earlier layers likely to be like already
closer to what we want okay so you

1094
01:48:23,769 --> 01:48:27,519
started with the latest one and then you
go right so if you go back to our quick

1095
01:48:27,519 --> 01:48:33,820
dogs and cats right when we create a
model from pre train from a pre train
model it returns something where all of

1096
01:48:36,190 --> 01:48:42,969
the convolutional layers are frozen and
some randomly set fully connected layers

1097
01:48:42,969 --> 01:48:50,309
we add to the end our unfrozen and so
when we go fit at first it just trains

1098
01:48:50,309 --> 01:48:56,920
the randomly set a randomly initialized
fully connected letters right

1099
01:48:56,920 --> 01:49:02,530
and if something is like really close to
imagenet that's often all we need

1100
01:49:02,530 --> 01:49:08,380
but because the early early layers are
already good at finding edges gradients

1101
01:49:08,380 --> 01:49:16,830
repeating patterns for ears and dogs
heads you know so then when we unfreeze

1102
01:49:16,830 --> 01:49:22,960
we set the learning rates for the early
layers to be really low because we don't

1103
01:49:22,960 --> 01:49:28,270
want to change the mesh for us the later
ones we set them to be higher where else

1104
01:49:28,270 --> 01:49:34,690
for satellite data right this is no
longer true you know the early layers
are still like better than the later

1105
01:49:37,420 --> 01:49:42,730
layers but we still probably need to
change them quite a bit so that's right
this learning rate is nine times smaller

1106
01:49:45,460 --> 01:49:52,150
than the final learning rate rather than
a thousand times smaller the final loan

1107
01:49:52,150 --> 01:50:00,219
rate okay you play with with the weights
of the layers yeah normally most of the

1108
01:50:00,219 --> 01:50:03,989
stuff you see online if they talk about
this at all they'll talk about
unfreezing different subsets of layers

1109
01:50:07,300 --> 01:50:13,960
and indeed we do unfreeze our randomly
generated runs but what I found is

1110
01:50:13,960 --> 01:50:18,250
although the first layer library you can
type learn dot freeze too and just

1111
01:50:18,250 --> 01:50:22,449
freeze a subset of layers this approach
of using differential learning rates

1112
01:50:22,449 --> 01:50:28,179
seems to be like more flexible to the
point that I never find myself I'm

1113
01:50:28,179 --> 01:50:34,960
freezing subsets of layers that I would
expect you to start with that with a

1114
01:50:34,960 --> 01:50:40,270
different cell the different learning
rates rather than trying to learn the

1115
01:50:40,270 --> 01:50:48,190
last layer so the reason okay so you
could skip this training just the last
layers and just go straight to

1116
01:50:49,719 --> 01:50:53,890
differential learning rates but you
probably don't want to and the reason

1117
01:50:53,890 --> 01:50:57,610
you probably don't want to is that
there's a difference the convolutional
layers all contain pre-trained weights

1118
01:51:00,910 --> 01:51:05,290
so they're like they're not random for
things that are close to imagenet

1119
01:51:05,290 --> 01:51:09,190
they're actually really good for things
that are not close to imagenet they're

1120
01:51:09,190 --> 01:51:13,250
better than that
all of our fully connected layers

1121
01:51:13,250 --> 01:51:19,969
however are totally random so therefore
you would always want to make the fully

1122
01:51:19,969 --> 01:51:24,050
connected weights better than random by
training them a bit first because

1123
01:51:24,050 --> 01:51:28,840
otherwise if you go straight to unfreeze
then you're actually going to be like

1124
01:51:28,840 --> 01:51:34,369
fiddling around of those early early can
early layer weights when the later ones
are still random that's probably not

1125
01:51:35,809 --> 01:51:41,900
what you want I think that's another
question here any possible so when we
unfreeze what are the things we're

1126
01:51:46,909 --> 01:51:54,349
trying to change there will it change
the Colonel's themselves that that's

1127
01:51:54,349 --> 01:52:02,869
always what SGD does yeah so the only
thing what training means is setting
these numbers right and these numbers

1128
01:52:09,619 --> 01:52:19,699
and these numbers the weights so the
weights are the weights of the fully

1129
01:52:19,699 --> 01:52:24,320
connected layers and the weights in
those kernels and the convolutions so

1130
01:52:24,320 --> 01:52:29,750
that's what training means it's and
we'll learn about how to do it with SGD

1131
01:52:29,750 --> 01:52:34,730
but training literally is setting those
numbers these numbers on the other hand

1132
01:52:34,730 --> 01:52:41,329
are activations they're calculated
they're calculated from the weights and

1133
01:52:41,329 --> 01:52:49,250
the previous layers activations or
amounts of questions so you can lift it

1134
01:52:49,250 --> 01:52:52,670
up higher and speak badly so in your
example of a cheerleader set of that

1135
01:52:52,670 --> 01:52:58,610
English example so you start with very
small size existed for yeah so does it

1136
01:52:58,610 --> 01:53:03,559
literally mean you know the model takes
a small area from the entire image that

1137
01:53:03,559 --> 01:53:13,489
is 64 bytes so how do we get that 64 by
64 depends on the transforms by default
our transform takes the smallest edge

1138
01:53:18,019 --> 01:53:24,070
and recites the whole thing out
samples it so the smallest edge is
societics t4 and then it takes a Center

1139
01:53:27,010 --> 01:53:33,099
crop of that okay
although when we're using data

1140
01:53:33,099 --> 01:53:40,539
augmentation it actually takes a
randomly chosen prop ie the case where

1141
01:53:40,539 --> 01:53:46,780
the image ties to multiple objects don't
in this case like would it be possible

1142
01:53:46,780 --> 01:53:50,170
that you would just lose the other
things that they try to predict yeah

1143
01:53:50,170 --> 01:53:54,070
which is why data augmentation is
important so by by and particularly

1144
01:53:54,070 --> 01:53:59,289
their test time augmentation is going to
be particularly important because you

1145
01:53:59,289 --> 01:54:03,880
would you wouldn't want to you know that
there may be a artisanal mine out in the

1146
01:54:03,880 --> 01:54:08,860
corner which if you take a center crop
you you don't see so data augmentation

1147
01:54:08,860 --> 01:54:17,499
becomes very important yeah so when we
talk on their tributaries are he

1148
01:54:17,499 --> 01:54:23,230
receiver up to that's not really what a
model choice Delton that's a great point

1149
01:54:23,230 --> 01:54:27,039
that's not the loss function yeah right
the loss function is something we'll be

1150
01:54:27,039 --> 01:54:33,429
learning about next week and it uses
cross entropy or otherwise known as like

1151
01:54:33,429 --> 01:54:39,249
negative log likelihood the metric is
just this thing that's printed so we can

1152
01:54:39,249 --> 01:54:47,079
see what's going on just next to that so
in the context of my deep pass modeling

1153
01:54:47,079 --> 01:54:51,280
cannot change data does it trading it
also have to be multiplied so can I

1154
01:54:51,280 --> 01:54:54,909
train on just like images of pure cats
and dogs and expect it at prediction

1155
01:54:54,909 --> 01:55:04,030
time to predict if I give it a picture
of both having cat eye on it over I've

1156
01:55:04,030 --> 01:55:08,409
never tried that and I've never seen an
example of something that needed it

1157
01:55:08,409 --> 01:55:14,949
I guess conceptually there's no reason
it wouldn't work but it's kind of out

1158
01:55:14,949 --> 01:55:19,389
there and you still use a sigmoid you
would have to make sure you're using a

1159
01:55:19,389 --> 01:55:22,659
sigmoid loss function so in this case
faster eyes default would not work
because by default first day I would say

1160
01:55:24,579 --> 01:55:28,150
your training data knitter has both a
cat and the dog so you would have to

1161
01:55:28,150 --> 01:55:31,230
override the loss function

1162
01:55:35,349 --> 01:55:40,729
when you use the differential learning
rates those three learning rates do they

1163
01:55:40,729 --> 01:55:46,550
just kind of spread evenly across the
layers yeah we'll talk more about this

1164
01:55:46,550 --> 01:55:50,689
later in the course but I mean the
faster I library there's a concept of

1165
01:55:50,689 --> 01:55:55,579
layer groups so in something like a
resonant 50 you know there's hundreds of
layers and I think it you don't want to

1166
01:55:57,889 --> 01:56:03,679
write down hundreds of learning rates so
I've basically decided for you how to

1167
01:56:03,679 --> 01:56:10,219
split them and the the last one always
refers just to the fully connected

1168
01:56:10,219 --> 01:56:14,929
layers that we've randomly initialized
and edit to the end and then these ones

1169
01:56:14,929 --> 01:56:19,669
are split generally about halfway
through basically I've tried to make it

1170
01:56:19,669 --> 01:56:24,019
so that these you know these ones are
kind of the ones which you hardly want

1171
01:56:24,019 --> 01:56:26,630
to change at all and these are the ones
you might want to change a little bit

1172
01:56:26,630 --> 01:56:30,469
and I don't think we're covered in the
course but if you're interested we can

1173
01:56:30,469 --> 01:56:33,860
talk about in the forum there are ways
you can override this behavior to define

1174
01:56:33,860 --> 01:56:38,869
your own layer groups if you want to and
is there any way to visualize the model

1175
01:56:38,869 --> 01:56:43,999
easily or like don't dump the layers of
the model yeah absolutely

1176
01:56:43,999 --> 01:56:54,229
you can let's make sure we've got one
here okay so if you just type learn it

1177
01:56:54,229 --> 01:57:02,479
doesn't tell you much at all but what
you can do is go learn summary and that

1178
01:57:02,479 --> 01:57:09,590
spits out basically everything there's
all the letters and so you can see in

1179
01:57:09,590 --> 01:57:14,419
this case these are the names I
mentioned how they look up names right

1180
01:57:14,419 --> 01:57:21,919
so the first layer is called con 2 d 1
and it's going to take as input this is

1181
01:57:21,919 --> 01:57:27,110
useful to actually look at it's taking
64 by 64 images which is what we told it

1182
01:57:27,110 --> 01:57:33,260
we're going to transform things to this
is three channels high torch like most

1183
01:57:33,260 --> 01:57:38,449
things have channels at the end would
say 64 by 64 by 3 ply torch music to the
front so it's 3 by 64 by 64 that's

1184
01:57:41,869 --> 01:57:46,820
because it turns out that some of the
GPU computations run faster when it

1185
01:57:46,820 --> 01:57:50,900
in that order okay but that happens all
behind-the-scenes automatic plays a part

1186
01:57:50,900 --> 01:57:56,260
of that transformation stuff that's kind
of all done automatically is to do that

1187
01:57:56,260 --> 01:58:04,280
minus one means however however big the
batch size is in care us they use the
number they use a special number none in

1188
01:58:07,580 --> 01:58:13,130
pile types that used minus one so this
is a four dimensional mini batch the

1189
01:58:13,130 --> 01:58:18,290
number of elements in the amount of
images in the mini batch is dynamic you

1190
01:58:18,290 --> 01:58:23,660
can change that the number of channels
is three number which is a 64 by 64 okay

1191
01:58:23,660 --> 01:58:29,200
and so then you can basically see that
this particular convolutional kernel

1192
01:58:29,200 --> 01:58:35,240
apparently has 64 kernels in it and it's
also having we haven't talked about this

1193
01:58:35,240 --> 01:58:39,200
but convolutions can have something
called a stride that is like Matt pullin

1194
01:58:39,200 --> 01:58:47,870
it changes the size so it's returning a
32 by 32 564 kernel tenza and so on and

1195
01:58:47,870 --> 01:58:54,080
so forth so that's summary and we'll
learn all about that's doing in detail

1196
01:58:54,080 --> 01:59:00,770
on in the second half of the course one
where I clicked in my own data set and I

1197
01:59:00,770 --> 01:59:06,260
try to use the in as a really small data
set these currencies from Google Images

1198
01:59:06,260 --> 01:59:12,650
and I tried to do a learning rate find
and then the plot and it just it gave me
some numbers which I didn't understand

1199
01:59:14,030 --> 01:59:18,260
and the learning rate font yeah and then
the plot was empty so yeah I mean let's

1200
01:59:18,260 --> 01:59:22,910
let's talk about that on the forum but
basically the learning rate finder is

1201
01:59:22,910 --> 01:59:26,450
going to go through a mini batch at a
time if you've got a tiny data set

1202
01:59:26,450 --> 01:59:30,560
there's just not enough mini batches so
the trick is to make your mini bit make

1203
01:59:30,560 --> 01:59:35,530
your batch size really small like try
making it like four

1204
01:59:38,250 --> 01:59:44,440
okay they were great questions it's not
nothing online to add you know they were

1205
01:59:44,440 --> 01:59:48,160
great questions we've got a little bit
past where I hope to but let's let's

1206
01:59:48,160 --> 01:59:52,690
quickly talk about structured data so we
can start thinking about it for next

1207
01:59:52,690 --> 02:00:02,500
week so this is really weird right to me
there's basically two types of data set

1208
02:00:02,500 --> 02:00:10,300
we use in machine learning there's a
type of data like audio images natural

1209
02:00:10,300 --> 02:00:16,600
language text where all of the all of
the things inside an object like all of

1210
02:00:16,600 --> 02:00:21,850
the pixels inside an image are all the
same kind of thing they're all pixels or

1211
02:00:21,850 --> 02:00:30,130
they're all apertures of a waveform or
they're all words I call this kind of

1212
02:00:30,130 --> 02:00:37,199
data unstructured and then there's data
sets like a profit and loss statement or
the information about a Facebook user

1213
02:00:39,960 --> 02:00:45,520
where each column is like structurally
quite different you know one thing is

1214
02:00:45,520 --> 02:00:49,449
representing like how many page views
last month another one is their sex
another one is what zip code they're in

1215
02:00:51,550 --> 02:00:57,940
and I call this structure there that
particular terminology is not unusual
like lots of people use that terminology

1216
02:01:00,480 --> 02:01:05,489
but lots of people don't
there's no particularly agreed-upon

1217
02:01:05,489 --> 02:01:11,770
terminology so when I say structured
data I'm referring to kind of columnar

1218
02:01:11,770 --> 02:01:16,210
data as you might find in a database or
a spreadsheet where different columns

1219
02:01:16,210 --> 02:01:21,840
represent different kinds of things and
each row represents an observation and

1220
02:01:21,840 --> 02:01:31,199
so structured data is probably what most
of you are analyzing most of the time
funnily enough you know academics in the

1221
02:01:34,810 --> 02:01:39,730
deep learning world don't really give a
 about structured data because it's
pretty hard to get published in fancy

1222
02:01:41,590 --> 02:01:46,179
conference proceedings if you're like if
you've got a better logistics model you

1223
02:01:46,179 --> 02:01:49,719
know it's the thing that makes the world
goes round it's a thing that makes

1224
02:01:49,719 --> 02:01:55,880
everybody you know
and efficiency and make stuff work but

1225
02:01:55,880 --> 02:02:01,309
it's largely ignored sadly so we're not
going to ignore it because we're a

1226
02:02:01,309 --> 02:02:05,749
practical deep learning and cackled
doesn't ignore it either because people

1227
02:02:05,749 --> 02:02:10,070
put prize money up on Cagle to solve
real-world problems so there are some

1228
02:02:10,070 --> 02:02:14,780
great capable competitions we can look
at there's one running right now which

1229
02:02:14,780 --> 02:02:21,130
is the grocery sales forecasting
competition for Ecuador's largest chain

1230
02:02:21,399 --> 02:02:26,360
it's always a little I've got to be a
little careful about how much I show you
about currently running competitions

1231
02:02:28,099 --> 02:02:32,659
because I don't want to you know help
you cheat but it so happens there was a
competition a year or two ago for one of

1232
02:02:35,989 --> 02:02:40,099
Germany's magistrate chains which is
almost identical so I'm going to show

1233
02:02:40,099 --> 02:02:50,539
you how to do that so that was called
the Rossman stores data and so I would

1234
02:02:50,539 --> 02:02:53,780
suggest you know first of all try
practicing what we're learning on

1235
02:02:53,780 --> 02:02:58,579
Russman right but then see if you can
get it working on on grocery because

1236
02:02:58,579 --> 02:03:03,320
currently on the leaderboard no one
seems to basically know what they're

1237
02:03:03,320 --> 02:03:10,130
doing in the groceries competition if
you look at the leaderboard the let's

1238
02:03:10,130 --> 02:03:15,769
see here yeah these ones around 5 to 9 v
3o are people that are literally finding

1239
02:03:15,769 --> 02:03:19,849
like group averages and submitting those
I know because they're kernels that

1240
02:03:19,849 --> 02:03:24,369
they're using so you know the basically
the people around 20th place are not
actually doing any machine learning so

1241
02:03:29,059 --> 02:03:34,639
yeah let's see if we can improve things
so you'll see there's a less than 3

1242
02:03:34,639 --> 02:03:40,189
rossmann notebook sure you get pull ok
in fact you know just reminder you know

1243
02:03:40,189 --> 02:03:45,289
before you start working get pull in
you're faster a repo and from time to
time Condor and update for you guys

1244
02:03:48,889 --> 02:03:52,849
during the in-person course the Condor
and update you should do it more often

1245
02:03:52,849 --> 02:03:57,769
because we're kind of changing things a
little bit um folks in the MOOC you know

1246
02:03:57,769 --> 02:04:04,010
more like once a month should be fine
so anyway I just just changed this a

1247
02:04:04,010 --> 02:04:09,560
little bit so make sure you get Paul to
get lesson 3 Rossman and there's a

1248
02:04:09,560 --> 02:04:14,600
couple of new libraries here one is fast
AI dot structure faster guided

1249
02:04:14,600 --> 02:04:18,950
structured contain stuff which is
actually not at all high torch specific
and we actually use that in the machine

1250
02:04:20,600 --> 02:04:25,000
learning course as well for doing random
forests with no tie torch at all I

1251
02:04:25,000 --> 02:04:30,230
mentioned that because you can use that
particular library without any of the

1252
02:04:30,230 --> 02:04:36,080
other parts of fast AI so that can be
handy and then we're also going to use

1253
02:04:36,080 --> 02:04:41,630
faster column data which is basically
some stuff that allows us to do fast a

1254
02:04:41,630 --> 02:04:49,100
type a torch stuff with columnar
structured data for structured data we

1255
02:04:49,100 --> 02:04:54,710
need to use pandas a lot anybody who's
used our data frames will be very

1256
02:04:54,710 --> 02:04:59,240
familiar with pandas pandas is basically
an attempt to kind of replicate data

1257
02:04:59,240 --> 02:05:08,240
friends in Python you know and a bit
more if you're not entirely familiar

1258
02:05:08,240 --> 02:05:13,690
with pandas there's a great book

1259
02:05:14,730 --> 02:05:18,020
[Music]
which I think I might have mentioned

1260
02:05:18,020 --> 02:05:24,920
before - for data analysis by Wes
McKinney there's a new addition that

1261
02:05:24,920 --> 02:05:30,619
just came out a couple of weeks ago
obviously being by the pandas author its

1262
02:05:30,619 --> 02:05:37,340
coverage of pandas is excellent but it
also covers numpy scipy matplotlib

1263
02:05:37,340 --> 02:05:45,940
scikit-learn - and Jupiter really well
okay and so I'm kind of going to assume
that you know your way around these

1264
02:05:48,949 --> 02:05:54,980
libraries to some extent also there was
the workshop we did before they started

1265
02:05:54,980 --> 02:05:59,030
and there's a video of that online where
we kind of have a brief mention of all

1266
02:05:59,030 --> 02:06:06,710
of those tools structured data is
generally shared as CSV files it was no

1267
02:06:06,710 --> 02:06:10,610
different in this competition as you'll
see there's a hyperlink to the rustman

1268
02:06:10,610 --> 02:06:15,500
data set here right now if you look at
the bottom of my screen you'll see this

1269
02:06:15,500 --> 02:06:19,579
goes to file start faster day.i because
this doesn't require any login or

1270
02:06:19,579 --> 02:06:24,409
anything to grab this data set it's as
simple as right clicking copy link

1271
02:06:24,409 --> 02:06:36,159
address head over to wherever you want
it and just type W get and the URL okay

1272
02:06:36,159 --> 02:06:44,150
so that's because you know it's it's not
behind a login or anything so you can

1273
02:06:44,150 --> 02:06:49,820
grab the grab it from there and you can
always read a CSV file with just pandas

1274
02:06:49,820 --> 02:06:55,639
don't read CSV now in this particular
case there's a lot of pre-processing

1275
02:06:55,639 --> 02:07:03,710
that we do and what I've actually done
here is I've I've actually stolen the

1276
02:07:03,710 --> 02:07:08,780
entire pipeline from the third place
winner roster okay so they made all

1277
02:07:08,780 --> 02:07:12,440
their data they're really great you know
they better get hub available with

1278
02:07:12,440 --> 02:07:16,880
everything that we need and I've ported
it all across and simplified it and

1279
02:07:16,880 --> 02:07:23,030
tried to make it pretty easy to
understand this course is about deep

1280
02:07:23,030 --> 02:07:28,460
learning not about data processing so
I'm not going to go through it but we

1281
02:07:28,460 --> 02:07:30,840
will be going through it in the machine
learning course

1282
02:07:30,840 --> 02:07:34,469
in some detail because feature
engineering is really important so if

1283
02:07:34,469 --> 02:07:41,520
you're interested you know check out the
machine learning course for that I will

1284
02:07:41,520 --> 02:07:48,780
however show you kind of what it looks
like so once we read the CSV Xin you can
see basically what's there so the key

1285
02:07:50,909 --> 02:08:04,190
one is for a particular store we have
the

1286
02:08:05,400 --> 02:08:12,270
we have the date and we have the sales
for that particular store we know

1287
02:08:12,270 --> 02:08:19,860
whether that thing is on promo or not we
know the number of customers at that
particular store had we know whether

1288
02:08:23,429 --> 02:08:26,510
that date was a school holiday

1289
02:08:29,940 --> 02:08:38,320
we also know what kind of store it is so
this is pretty common right you'll often

1290
02:08:38,320 --> 02:08:43,360
get datasets where there's some column
with like just some kind of code we

1291
02:08:43,360 --> 02:08:47,410
don't really know what the code means
and most of the time I find it doesn't

1292
02:08:47,410 --> 02:08:51,760
matter what it means like normally you
get given a data dictionary when you

1293
02:08:51,760 --> 02:08:54,550
start on a project and obviously if
you're working on an internal project

1294
02:08:54,550 --> 02:08:59,740
you can ask the people at your company
what does this column mean I kind of

1295
02:08:59,740 --> 02:09:03,400
stay away from learning too much about
it I prefer to like to see what the data

1296
02:09:03,400 --> 02:09:10,210
says first there's something about what
kind of product are we selling in this

1297
02:09:10,210 --> 02:09:12,930
particular row

1298
02:09:13,989 --> 02:09:19,239
and then there's information about like
how far away is the nearest competitor

1299
02:09:19,239 --> 02:09:31,120
how long have they been open for how
long is the promo being on for for each

1300
02:09:31,120 --> 02:09:35,380
store we can find out what state it's in
for each state we can find at the name

1301
02:09:35,380 --> 02:09:40,540
of the state this is in Germany and
interestingly they were allowed to

1302
02:09:40,540 --> 02:09:44,380
download any data external data they
wanted in this competition just very

1303
02:09:44,380 --> 02:09:48,520
common as long as you share it with
everybody else and so some folks tried

1304
02:09:48,520 --> 02:09:55,180
downloading data from Google Trends I'm
not sure exactly what it was that they

1305
02:09:55,180 --> 02:09:59,430
would check in the trend off but we have
this information from Google Trends

1306
02:09:59,430 --> 02:10:05,760
somebody downloaded the weather for
every day in Germany every state

1307
02:10:09,250 --> 02:10:20,710
and yeah that's about it right so you
can get a data frame summary with pandas

1308
02:10:20,710 --> 02:10:24,430
which kind of lets you see how many
observations and means and standard

1309
02:10:24,430 --> 02:10:30,130
deviations again I don't do a hell of a
lot with that early on but it's nice to

1310
02:10:30,130 --> 02:10:35,530
note there so what we do you know this
is called a relational data set a

1311
02:10:35,530 --> 02:10:38,920
relational data set is one where there's
quite a few tables we have to join

1312
02:10:38,920 --> 02:10:43,780
together it's very easy to do that in
pandas there's a thing called merge so

1313
02:10:43,780 --> 02:10:46,360
great little function to do that and so
I just started joining everything

1314
02:10:46,360 --> 02:10:49,380
together
joining the weather or the Google Trends

1315
02:10:49,380 --> 02:10:59,530
stores yeah that's about everything I
guess you'll see there's one thing that

1316
02:10:59,530 --> 02:11:04,300
I'm using from the FASTA a library which
is called add date part we talked about

1317
02:11:04,300 --> 02:11:07,630
this a lot in the machine learning
course but basically this is going to

1318
02:11:07,630 --> 02:11:12,970
take a date and pull out of a bunch of
columns day of week is at the start of a

1319
02:11:12,970 --> 02:11:18,460
quarter month of year so on and so forth
and add them all in to the dataset okay

1320
02:11:18,460 --> 02:11:22,020
so this is all standard pre-processing

1321
02:11:22,740 --> 02:11:26,800
all right so we join everything together
we fiddle around with some of the dates

1322
02:11:26,800 --> 02:11:29,950
a little bit some of them are in month
in year format we turn it into date

1323
02:11:29,950 --> 02:11:37,930
format we spend a lot of time trying to
take information about for example

1324
02:11:37,930 --> 02:11:42,940
holidays and add a column for like how
long until the next holiday how long has
it been since the last holiday ditto for

1325
02:11:45,130 --> 02:11:52,180
promos so on and so forth okay so we do
all that and at the very end we

1326
02:11:52,180 --> 02:11:57,630
basically save a big structured data
file that contains all that stuff

1327
02:11:57,630 --> 02:12:01,300
something that those of you that use
pandas may not be aware of is that

1328
02:12:01,300 --> 02:12:05,800
there's a very cool new format called
feather which you can save a panda's

1329
02:12:05,800 --> 02:12:11,170
data frame into this feather format it's
kind of pretty much takes it as it sits

1330
02:12:11,170 --> 02:12:16,420
in RAM and dumps it to the disk and so
it's like really really really fast the

1331
02:12:16,420 --> 02:12:20,770
reason that you need to know this is
because the ecuadorian grocery

1332
02:12:20,770 --> 02:12:26,410
competition is on now has
350 million records so you will care
about how long things take a talk I

1333
02:12:28,420 --> 02:12:32,410
believe about six seconds for me to save
three hundred and fifty million records

1334
02:12:32,410 --> 02:12:37,540
to feather format so that's pretty cool
so at the end of all that I'd save it as

1335
02:12:37,540 --> 02:12:40,540
a feather format and for the rest of
this discussion I'm just going to take

1336
02:12:40,540 --> 02:12:45,400
it as given that we've got this nicely
pre-processed feature engineered file

1337
02:12:45,400 --> 02:12:50,650
and I can just go read better okay but
for you to play along at home you will

1338
02:12:50,650 --> 02:12:57,580
have to run those previous cells oh
except the see these ones have commented

1339
02:12:57,580 --> 02:13:01,690
out you don't have to run those because
the file that you download from files

1340
02:13:01,690 --> 02:13:09,400
doc bastard AI has already done that for
you okay all right so we basically have

1341
02:13:09,400 --> 02:13:18,510
all these columns so it basically is
going to tell us you know how many of

1342
02:13:18,510 --> 02:13:25,780
this thing was sold on this date at this
store and so the goal of this
competition is to find out how many

1343
02:13:28,990 --> 02:13:36,160
things will be sold for each store for
each type of thing in the future okay

1344
02:13:36,160 --> 02:13:40,900
and so that's basically what we're going
to be trying to do and so here's an

1345
02:13:40,900 --> 02:13:48,640
example of what some of the data looks
like and so next week we're going to see

1346
02:13:48,640 --> 02:13:52,300
how to go through these steps but
basically what we're going to learn is

1347
02:13:52,300 --> 02:13:58,480
we're going to learn to split the
columns into two types some columns were
going to treat as categorical which is

1348
02:14:01,390 --> 02:14:08,050
to say store ID one and store ID - I'm
not numerically related to each other

1349
02:14:08,050 --> 02:14:13,150
they're categories right we're going to
treat day of week like that - Monday and
Tuesday day zero and day one not

1350
02:14:15,490 --> 02:14:20,620
numerically related to each other where
else distance in kilometers to the

1351
02:14:20,620 --> 02:14:26,170
nearest competitor that's a number that
we're going to treat numerically right

1352
02:14:26,170 --> 02:14:29,770
so in other words the categorical
variables we basically are going to one

1353
02:14:29,770 --> 02:14:34,039
how to encode them you can think of it
as one hot encoding on where
the continuous variables we're going to

1354
02:14:35,839 --> 02:14:43,609
be feeding into fully connected layers
just as is okay so what we'll be doing

1355
02:14:43,609 --> 02:14:49,849
is we'll be basically creating a
validation set and you'll see like a lot

1356
02:14:49,849 --> 02:14:53,419
of these are start to look familiar this
is the same function we used on planet

1357
02:14:53,419 --> 02:14:58,669
and dog breeds to create a validation
set there's some stuff that you haven't

1358
02:14:58,669 --> 02:15:02,899
seen before
where we're going to basically rather

1359
02:15:02,899 --> 02:15:09,109
than saying image data dot from CSV
we're going to say columnar data from

1360
02:15:09,109 --> 02:15:13,780
data frame right so you can see like the
basic API concepts will be the same but
they're a little different right but

1361
02:15:17,689 --> 02:15:23,659
just like before we're going to get a
learner and we're going to go lr find to
find our best learning rate and then

1362
02:15:26,359 --> 02:15:33,019
we're going to go dot fit with a metric
with a cycle length okay so the basic

1363
02:15:33,019 --> 02:15:39,079
sequence who's going to end up looking
hopefully very familiar okay so we're
out of time

1364
02:15:40,339 --> 02:15:47,479
so what I suggest you do this week is
like try to enter as many capital image

1365
02:15:47,479 --> 02:15:53,030
competitions as possible like like try
to really get this feel for like cycling
learning rates plotting things you know

1366
02:15:59,030 --> 02:16:05,869
that that post I showed you at the start
of class today that kind of took you

1367
02:16:05,869 --> 02:16:11,030
through lesson one like really go
through that on as many image datasets

1368
02:16:11,030 --> 02:16:17,059
as you can you just feel really
comfortable with it right because you

1369
02:16:17,059 --> 02:16:19,879
want to get to the point where next week
when we start talking about structured

1370
02:16:19,879 --> 02:16:24,619
data that this idea of like how learners
kind of work and data works and data

1371
02:16:24,619 --> 02:16:28,869
loaders and data sets and looking at
pictures should be really you know

1372
02:16:28,869 --> 02:16:34,119
intuitive all right good luck see you
next week

1373
02:16:35,510 --> 02:16:39,040
[Applause]