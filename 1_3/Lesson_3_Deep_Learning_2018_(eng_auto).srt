1
00:00:00,149 --> 00:00:03,600
welcome back everybody
[Music]

2
00:00:03,600 --> 00:00:09,599
I'm sure you've noticed but there's been
a lot of cool activity on the forum this

3
00:00:09,599 --> 00:00:25,920
week and one of the things that's been
really great to see is that a lot of you
have started creating really helpful
materials both for your classmates to
better understand stuff and also for you
to better understand stuff by trying to
teach what you've learned I just wanted

4
00:00:25,920 --> 00:00:32,340
to highlight a few i've actually posted
to the wiki thread of a few of these but

5
00:00:32,340 --> 00:00:42,480
there's there's lots more Russian has
posted a whole bunch of nice
introductory tutorials so for example if
you're having any trouble getting
connected with AWS she's got a whole

6
00:00:46,079 --> 00:00:57,750
step-by-step how to go about logging in
and getting everything working which i
think is a really terrific thing and so
it's a kind of thing that if you are

7
00:00:57,750 --> 00:01:05,460
writing some notes for yourself to
remind you how to do it
you may as well post them for others to
do it as well and by using a markdown
file like this and it's actually good

8
00:01:07,439 --> 00:01:13,789
practice if you haven't used github
before if you put it up on github
everybody can now use it or of course
you can just put it in the forum so more

9
00:01:13,889 --> 00:01:25,140
advanced a thing that Reshma wrote up
about is she noticed that I like using
Tmax which is a handy little thing which

10
00:01:25,140 --> 00:01:39,869
lets me lets me basically have a window
I'll show you so as soon as I log into
my computer if I run Tmax you'll see
that all of my windows pop straight up

11
00:01:39,869 --> 00:01:48,210
basically and I can like continue
running stuff in the background and I
can like I've got them over here and I
can kind of zoom into it or I can move

12
00:01:48,210 --> 00:01:53,579
over to the top which is here so Jupiter
colonel running and so forth so if that

13
00:01:53,579 --> 00:02:03,060
sounds interesting
Reshma has a tutorial here on how you
can use two maps and it's actually got a
whole bunch of stuff in her github so

14
00:02:03,060 --> 00:02:28,810
that's that's really cool I built among
has written a very nice kind of summary
basically of our last lesson
which kind of covers what are the key
things we did and why did we do them so
if you are a kind of
wondering like how does it fit together
I think this is a really helpful summary
like what if those couple of hours look

15
00:02:29,610 --> 00:02:45,069
like if we summarize it all into a page
or two I also really like Pavel has dad
kind of done a deep dive on the learning
rate finder which is a topic that a lot

16
00:02:45,069 --> 00:02:59,500
of you have been interested in learning
more about particularly those of you who
have done deep learning before I
realized that this is like a solution to
a problem that you've been having for a
long time and haven't seen before and so
it's kind of something which hasn't
really been blogged about before so this

17
00:02:59,500 --> 00:03:13,560
is the first I've seen it's logged about
so when I put this on Twitter a link to
pebbles post it's been shared you know
hundreds of times it's been really
really popular and viewed many thousands
of times so that's some great content

18
00:03:13,560 --> 00:03:20,779
radec has posted lots of cool stuff I
really like this practitioners guide to
apply torch which again this is more for

19
00:03:20,879 --> 00:03:32,260
more advanced students but it's like
digging into people who have never used
hi torch before but know a bit about
numerical programming in general and
it's a quick introduction to how high
torch is different and then there's been

20
00:03:35,560 --> 00:03:40,624
some interesting little bits of research
like what's the relationship between
learning rate and batch sites so one of

21
00:03:40,724 --> 00:03:46,030
the students actually asked me this
before class and I said oh well one of
the other students has written an

22
00:03:46,030 --> 00:04:01,235
analysis of exactly that so what he's
done is basically looked through and
tried different batch sizes and
different learning rates and tried to
see how they seemed to relate together
and these are all like cool experiments
which you know you can try yourself

23
00:04:01,335 --> 00:04:15,586
I predict again he's written something
again a kind of a research into this
question I made a claim that the the
stochastic gradient descent with
restarts finds more generalizable parts
of the function surface because they're

24
00:04:15,686 --> 00:04:25,147
kind of flatter and he's been trying to
figure out is there a way to measure
that more directly not quite successful
yet but a really interesting piece of

25
00:04:25,247 --> 00:04:32,999
research got some introductions to
convolutional neural networks and then

26
00:04:32,999 --> 00:04:57,059
something that we'll be learning about
towards the end of this course but I'm
sure you've noticed we're using
something called ResNet and a nonce aha
actually posted a pretty impressive
analysis of like watts arrest net and
why is it interesting and this one's
actually being very already shared very
widely around the internet I've seen
also so some more advanced students who
are interested in jumping ahead can look

27
00:04:57,059 --> 00:05:04,349
at that and uphill Tamang also has done
something similar so lots of yeah lots
of stuff going on on the forums

28
00:05:06,059 --> 00:05:17,479
I'm sure you've also noticed we have a
beginner forum now specifically for you
know asking questions which you know it
is always the case that there are no
dumb questions but when there's lots of

29
00:05:19,860 --> 00:05:29,819
people around you talking about advanced
topics it might not feel that way so
hopefully the beginners forum is just a
less intimidating space and if there are

30
00:05:29,819 --> 00:05:38,632
more advanced student who can help
answer those questions please do but
remember when you do answer those
questions try to answer in a way that's
friendly to people that maybe you know

31
00:05:38,732 --> 00:05:47,934
have no more than a year of programming
experience you haven't done any machine
learning before so you know I hope other

32
00:05:48,034 --> 00:05:56,159
people in the class feel like you can
contribute as well and just remember all
of the people we just looked at or many

33
00:05:56,159 --> 00:06:04,769
of them I believe have never hosted
anything to the internet before right I
mean you don't have to be a particular
kind of person to be allowed to block

34
00:06:04,769 --> 00:06:20,399
something you can just jot down your
notes throw it up there and one handy
thing is if you just put it on the forum
and you're not quite sure of some of the
details then then you know you have an
opportunity to get feedback and say like

35
00:06:20,399 --> 00:06:29,439
oh well that's not quite how that works
you know actually it works this way
instead or oh that's a really
interesting insight have you thought
about taking this further and so forth

36
00:06:29,539 --> 00:06:38,969
so what we've done so far is a kind of
an injury an introduction as a just as a
practitioner to convolutional neural

37
00:06:38,969 --> 00:06:59,319
networks for images and we haven't
really talked much at all about
the theory or why they work or the math
of them but on the other hand what we
have done is seen how to build a model
which actually works exceptionally well
compact world-class level models and

38
00:06:59,319 --> 00:07:12,280
we'll kind of review a little bit of
that today and then also today we're
going to dig in a little quite a lot
more actually into the underlying theory
of like what is a what is a CNN what's a

39
00:07:12,280 --> 00:07:16,000
convolution how does this work and then
we're going to kind of go through this

40
00:07:16,000 --> 00:07:25,930
this cycle where we're going to dig
we're going to do a little intro into a
whole bunch of application areas using
neural nets for structured data so kind

41
00:07:25,930 --> 00:07:31,449
of like logistics or forecasting or you
know financial data or that kind of

42
00:07:31,449 --> 00:07:45,550
thing and then looking at language
applications and LP applications using
recurrent neural Nets and then
collaborative filtering for
recommendations and systems and so these

43
00:07:45,550 --> 00:07:53,020
will all be like similar to what we've
done for cnn's for images would be like
here's how you can get a
state-of-the-art result without digging
into the theory but but knowing how to

44
00:07:55,150 --> 00:08:01,984
actually make a work and then we're kind
of going to go back through those almost
in reverse order so then we're going to

45
00:08:02,084 --> 00:08:09,949
dig right into collaborative filtering
in a lot of detail and see how how to
write the code underneath and how the
math works underneath and then we're

46
00:08:10,049 --> 00:08:20,620
going to do the same thing for the
structured data analysis we're going to
do the same thing for comp nets for
images and finally an in depth deep dive
into apparent neural networks so that's

47
00:08:20,620 --> 00:08:37,078
kind of where we're okay so let's start
by doing a little bit of a review and I
want to also provide a bit more detail
on some on some steps that we only
briefly slipped over so I want to make

48
00:08:37,178 --> 00:08:55,889
sure that we're all able to complete
kind of last week's assignment which was
that the dog breeze I mean to basically
apply what you've learned with another
data set and I thought the easiest one
to do with me the dog breeds cattle
competition and so I want to make sure
everybody has everything you need to do
this right now so and the

49
00:08:55,889 --> 00:09:11,339
first thing is to make sure that you
know how to download data and so there's
there's two main places at the moment
we're kind of downloading data from one
is from cattle and the other is from
like anywhere else and so I'll first of

50
00:09:11,339 --> 00:09:37,960
all do the the casual version so to
download from cattle we use something
called cattle CLI which is gear and to
install what I think it's already in the
system will shake yeah so it should
already be in your environment but to

51
00:09:38,060 --> 00:09:53,279
make sure one thing that happens is
because this is downloading from the
cattle website through experience rating
every time cap will change us the
website it breaks so anytime you try to
use it and if the cattles websites
changed recent when you'll need to make

52
00:09:53,279 --> 00:10:13,434
sure you get the most recent version so
you can always go to pip install cable -
CL I - - upgrade and so that'll just
make sure that you've got the latest
version of of it and everything that it
depends on okay and so then having done

53
00:10:13,534 --> 00:10:25,765
that you can follow the instructions
actually I think Reshma was kind enough
to they go there's a cable CLI you know
like everything you need to know can be
under Reshma's github so basically to do

54
00:10:25,865 --> 00:10:43,509
that at the next step you go kg download
and then you provide your username with
- you you provide your password with - P
and then - see it the competition name

55
00:10:43,609 --> 00:11:02,100
and a lot of people in the forum is
being confused about what to enter here
and so the key thing to note is that
when you're at a capital competition
after the /c there's a specific name
planet - understanding - etcetera right
that's the name you need okay the other

56
00:11:02,100 --> 00:11:12,650
thing you'll need to make sure is that
you've on your own computer have
attempted to click download at least
once because when you do
ask you to accept the rules if you've

57
00:11:12,750 --> 00:11:18,850
forgotten to do that kg download will
give you a hint it'll say oh it looks
like you might have forgotten the rules

58
00:11:18,850 --> 00:11:31,085
if you log into cattle with like a
Google account like anything other than
a username password this won't work so
you'll need to click forgot password on
Kaggle and get them to send you a normal
password so that's the cattle version

59
00:11:31,185 --> 00:11:38,775
right and so when you do that you end up
with a whole folder created for you with
all of that competition and data in it

60
00:11:38,875 --> 00:11:52,600
so a couple of reasons you might want to
not use that the first years that you're
using a data set that's not on cattle
the second is that you don't want all of
the data sets in a cattle competition

61
00:11:52,600 --> 00:12:11,890
for example the planet competition that
we've been looking at a little bit we'll
look at again today has data in two
formats TIFF and JPEG the TIFF is 19
gigabytes and the JPEG is 600 megabytes
so you probably don't want to download
both so I'll show you a really cool kit

62
00:12:11,890 --> 00:12:32,480
which actually somebody on the forum
taught me I think was one of the young
MSN students here at USF there's a
Chrome extension cord curl W get so you
can just search for a curl W get and
then you install it by just clicking on
installed and having an extension before

63
00:12:32,580 --> 00:13:02,944
and then from now on every time you try
to download something so I'll try and
download this file
and I'll just go ahead and cancel it
right and now you see this little yellow
button that's added up here there's a
whole command here right so I can copy
that and paste it into my window and hit
go and it's there cuz okay so what that

64
00:13:03,044 --> 00:13:10,449
does is like all of your cookies and
headers and everything else needed to
download that file is like say so this

65
00:13:10,449 --> 00:13:19,420
is not just useful for downloading data
it's also useful if you like trying to
download some I don't know TV show or
something anything where you're it's

66
00:13:19,420 --> 00:13:33,610
hidden behind a login or something you
can you can grab it and actually that is
very useful for data science because
quite often we want to analyze things
like videos on our on our consoles so
this is a good trick alright so there's

67
00:13:33,610 --> 00:13:44,019
two ways to get the data so then having
got the data you then need to build your

68
00:13:44,019 --> 00:13:57,670
model right so what I tend to do like
you'll notice that I tend to assume that
the data is in a directory called data
that's a subdirectory of wherever your
notebook is right now you don't

69
00:13:57,670 --> 00:14:04,990
necessarily actually want to put your
data there you might want to put it
directly in your home directory or you
might wanna put it on another drive or

70
00:14:04,990 --> 00:14:17,949
whatever so what I do is if you look
inside my courses do one folder you'll
see that data is actually a symbolic
link to a different drive alright so you

71
00:14:17,949 --> 00:14:24,109
can put it anywhere you like and then
you can just add a symbolic link or you
can just put it there directly it's up

72
00:14:24,209 --> 00:14:35,829
to you if you haven't used symlinks
before they're like aliases or shortcuts
on the mac or windows very handy and
there's some threads on the forum about
how to use them if you want help with

73
00:14:35,829 --> 00:14:45,490
that that for example is also how we
actually have the fast AI modules
available from the same place as our
notebooks it's just a symlink

74
00:14:45,490 --> 00:15:06,845
to where they come from anytime you want
to see like where things actually point
to in Linux you can just use the
- L flag - listing a directory and
that'll show you where the symlinks
exist still lost I'll show you which
scenes the directories so forth okay so

75
00:15:06,945 --> 00:15:20,315
one thing which may be a little unclear
based on what we've done so far is like
how little code you actually need to do
this end to it so what I've got here is

76
00:15:20,415 --> 00:15:28,292
is in a single window is an entire
end-to-end process to get a
state-of-the-art result put cats versus
dogs all right I've only step I've

77
00:15:28,392 --> 00:15:36,197
skipped is the bit where we've
downloaded it from title and then where
we unzip it all right so these are

78
00:15:36,297 --> 00:15:51,130
literally all the steps and so we import
our libraries and actually if you import
this one Kampf loner that basically
imports everything else so that's that

79
00:15:51,130 --> 00:15:57,860
we need to tell at the path of where
things are the size that we want the
batch size that we want right so then

80
00:15:57,960 --> 00:16:10,940
and we're going to learn a lot more
about what these do very shortly but
basically we say how do we want to
transform our data so we want to
transform it in a way that's suitable to
this particular kind of model and it

81
00:16:11,040 --> 00:16:18,490
assumes that the photos aside on photos
and that we're going to zoom in up to
ten percent each time we say that we

82
00:16:18,490 --> 00:16:34,360
want to get some data based on paths and
so remember this is this idea that
there's a path called cats and the path
called dogs and they're inside a path
called train and a path called valid
note that you can always overwrite these

83
00:16:34,360 --> 00:16:45,170
with other things so if your things are
in different folders
you could either rename them or you can
see here there's like a train name and a
bowel name you can always pick something

84
00:16:45,270 --> 00:16:56,380
else here also notice there's a test
name so if you want to submit some into
cattle you'll need to fill in the name
the name of the folder where the test
sentence and obviously those those won't
be labeled

85
00:16:59,770 --> 00:17:08,205
so then we create a model from a
pre-training model it's from a resonant
50 model using this data and then we
call fit and remember by default that

86
00:17:08,305 --> 00:17:17,420
has all of the layers but the last few
frozen and again we'll learn a lot more
about what that means

87
00:17:17,420 --> 00:17:23,110
and so that's that's what that does so
that that took two and a half minutes
notice here I didn't say pre-compute

88
00:17:26,119 --> 00:17:35,390
equals true again there's been some
confusion on the forums about like what
that means it's only a is only something
that makes it a little faster for this

89
00:17:35,390 --> 00:17:42,325
first step right so you can always skip
it and if you're at all confused about
it or it's causing you any problems just

90
00:17:42,425 --> 00:17:52,635
leave it off right because it's just a
it's just a shortcut which caches some
of the intermediate steps that don't
have to be recalculating each time and

91
00:17:52,735 --> 00:18:11,180
remember that when we are using pre
computed activations data or
augmentation doesn't work right so even
if you ask for a data augmentation if
you've got pre compute equals true it
doesn't actually do any data
augmentation because it's using the
cached non augmented activations so in

92
00:18:11,180 --> 00:18:14,780
this case to keep this as simple as
possible I have no pre computed anything

93
00:18:14,780 --> 00:18:22,810
going on so I do three cycles of length
one and then I can then unfreeze so it's

94
00:18:23,110 --> 00:18:27,190
now going to train the whole thing

95
00:18:27,590 --> 00:18:39,170
something we haven't seen before and
we'll learn about in the second half is
called B and freeze for now all you need
to know is that if you're using a model
like a bigger deeper model like ResNet

96
00:18:39,170 --> 00:18:49,005
50 or rez next 101 on a data set that's
very very similar to imagenet like these
cat sandbox data set sort of words it's

97
00:18:49,105 --> 00:19:02,495
like sidon photos of standard objects
you know of a similar size to image turn
and money somewhere between 200 and 500
pixels
you should probably add this line when
you unfreeze for those of you that are

98
00:19:02,595 --> 00:19:10,630
more advanced what it's doing is it's
causing the batch normalization moving
averages to not be updated

99
00:19:10,730 --> 00:19:20,409
but in the second half of this course
we're gonna learn all about why we do
that it's something that's not supported
by any other library but it turns out to
be super important anyway so we do one

100
00:19:20,409 --> 00:19:31,789
more
epoch with training the whole network
and then at the end we use test time
augmentation to ensure that we get the

101
00:19:31,889 --> 00:19:37,909
best predictions we can and that gives
us ninety nine point four five percent

102
00:19:38,009 --> 00:19:47,139
so that's that's it right so when you
try a new data set they're basically the
minimum set of steps that you would need

103
00:19:47,139 --> 00:20:02,799
to follow you'll notice this is assuming
I already know what learning wrote to
use so you'd use the learning rate
finder for that it's assuming that I
know that the directory layout and so
forth so that's kind of a minimum set

104
00:20:02,799 --> 00:20:15,499
now one of the things that I wanted to
make sure you had an understanding of
how to do is how to use other libraries
other than fast AI and so I feel like
the best thing to to look at is to look
at care us because care us is a library

105
00:20:15,599 --> 00:20:33,059
just like fast AI sits on top of pi
torch care us sits on top of actually a
whole variety of different backends it
fits mainly people nowadays use it with
tensorflow
there's also an MX net version there's

106
00:20:33,159 --> 00:20:37,450
also a Microsoft CNT K version so what

107
00:20:37,450 --> 00:20:51,024
I've got if you do a git pull you'll see
that there's a something called care us
less than one where I've attempted to
replicate at least parts of lesson one
in care us just to give you a sense of

108
00:20:51,124 --> 00:20:59,510
how that works I'm not going to talk
more about batch two norm freeze now

109
00:21:00,850 --> 00:21:11,950
other than to say if you're using
something which has got a number larger
than 34 at the end
so like ResNet 50 or res next 101 and

110
00:21:11,950 --> 00:21:24,520
you're trading a data set that has that
is very similar to image net so it's
like normal photos of normal sizes where
the thing of interest takes up most of
the frame then you probably should

111
00:21:24,520 --> 00:21:32,989
at the end fries true after unfreeze
if in doubt try trading it with and then
try trading it without more advanced

112
00:21:33,089 --> 00:21:39,910
students will certainly talk about it on
the forums this week and we will be
talking about the details of it in the

113
00:21:39,910 --> 00:21:46,720
second half of the course when we come
back to our CNN in death section in the

114
00:21:50,820 --> 00:22:01,480
second last lesson so with care us again
we import a bunch of stuff and remember

115
00:22:01,480 --> 00:22:12,151
I mentioned that this idea that you've
got a thing called train and a thing
called valid and inside that you've got
a thing called dogs and things called
cats is a standard way of providing
image labelled images so Karis does that

116
00:22:12,251 --> 00:22:22,805
too right so it's going to tell it where
the training set and the validation set
are twice what batch size to used now

117
00:22:22,905 --> 00:22:36,575
you'll notice in Karis we need much much
much more code to do the same thing more
importantly each part of that code has
many many many more things you have to

118
00:22:36,675 --> 00:22:44,650
set and if you set them wrong everything
breaks right so I'll give you a summary
of what they are

119
00:22:44,650 --> 00:22:55,030
so you're basically rather than creating
a single data object in chaos we first
of all have to define something called a
data generator to say kind of generate

120
00:22:55,030 --> 00:23:10,634
the data and so a data generator we
basically have to say what kind of data
augmentation we want to do and we also
we actually have to say what kind of
normalization do we want to do

121
00:23:10,734 --> 00:23:15,619
so we're else with fast AI we just say
whatever ResNet 50 requires just do that

122
00:23:15,719 --> 00:23:21,700
for me please we actually have to kind
of know a little bit about what's
expected of us

123
00:23:21,700 --> 00:23:30,220
generally speaking copying and pasting
cos code from the internet is a good way
to make sure you've got the right the
right stuff to make that work and again

124
00:23:30,220 --> 00:23:43,550
it doesn't have a kind of a standard set
of like here the best data augmentation
parameters to use for photos so you know
I've copied and pasted all of this from
the Kaos documentation so I don't know

125
00:23:43,550 --> 00:23:49,140
if it's I don't think it's the best set
to use it all but it's the set that
they're using in their docks so having

126
00:23:49,240 --> 00:23:56,540
said this is how I want to generate data
so horizontally fit sometimes you know
zoom sometimes sheer sometimes we then

127
00:23:56,540 --> 00:24:03,102
create a generator from that by taking
that data generator and saying I want to
generate images by looking from a

128
00:24:03,202 --> 00:24:17,985
directory we pass in the directory which
is of the same directory structure that
fast AI users and you'll see there's
some overlaps with kind of how fast AI
works here you tell it what size images
you want to create you tell at what
batch size you want in your mini batches

129
00:24:18,085 --> 00:24:31,650
and then there's something you not to
worry about too much but basically if
you're just got two possible outcomes
you would generally say binary here if
you've got multiple possible outcomes
would say categorical yeah so we've only

130
00:24:31,750 --> 00:24:36,525
got cats or dogs
so it's binary so an example of like

131
00:24:36,625 --> 00:24:48,048
where things get a little more complex
is you have to do the same thing for the
validation set so it's up to you to
create a data generator that doesn't
have data augmentation because obviously
for the validation set unless you're

132
00:24:48,148 --> 00:25:00,760
using t/ta that's going to start things
up you also when you train you randomly
reorder the images so that they're
always shown in different orders to make
it more random but with a validation

133
00:25:00,860 --> 00:25:11,270
it's vital that you don't do that
because if you shuffle the validation
set you then can't track how well you're
doing it's in a different order for the

134
00:25:11,270 --> 00:25:18,465
labels
that's a basically these are the kind of
steps you have to do every time with

135
00:25:18,565 --> 00:25:32,750
care us so again the reason I was using
ResNet 50 before is chaos doesn't have
ResNet 34 unfortunately so I just wanted
to compare like with Mike so we're going
to use resident 50 here there isn't the

136
00:25:32,750 --> 00:25:42,290
same idea with chaos of saying like
constructor model that is suitable for
this data set for me so you have to do
it by hand right so the way you do it is

137
00:25:42,290 --> 00:25:50,600
to basically say this is my base model
and then you have to construct on top of
that manually the layers that you want
to add

138
00:25:50,600 --> 00:26:01,370
and so by the end of this course you'll
understand a way it is that these
particular three layers other layers
that we add so having done that in chaos

139
00:26:01,370 --> 00:26:06,070
you basically say okay this is my model
and then again there isn't like a

140
00:26:06,070 --> 00:26:19,515
concept to it like automatically
freezing things or an API for that so
you just have to allow look through the
layers that you want to freeze and call
dot trainable equals false on them in

141
00:26:19,615 --> 00:26:27,110
Karis there's a concept we don't have in
fast AI or play a torch of compiling a
model so basically once your model is
ready to use you have to compile it

142
00:26:28,160 --> 00:26:35,360
passing in what kind of optimizer to use
what kind of loss to look for about
metric so again with fast AI you don't

143
00:26:36,439 --> 00:26:47,324
have to pass this in because we know
what loss is the write loss to use you
can always override it but for a
particular model we give you good
defaults okay so having done all that

144
00:26:47,424 --> 00:26:55,886
rather than calling fit you call
generator passing in those two
generators that you saw earlier the
Train generator in the validation

145
00:26:55,886 --> 00:27:06,681
generator for reasons I don't quite
understand chaos expects you to also
tell it how many batches there are per
epoch so the number of batches is a
quarter the size of the generator

146
00:27:07,081 --> 00:27:13,680
divided by the batch size you can tell
it how many epochs just like in fast AI

147
00:27:15,100 --> 00:27:35,639
you can say how many processes or how
many workers to use for pre-processing
[Music]
unlike fast AI the default in chaos is
basically not to use any so to get good
speed you're going to make sure you
include this and so that's basically

148
00:27:35,739 --> 00:27:43,469
enough to start fine tuning the last
layers so as you can see I got to a

149
00:27:43,569 --> 00:27:53,805
validation accuracy of 95% but as you
can also see something really weird
happened we're after one it was like 49
and then it was 69 and then 95 I don't

150
00:27:53,905 --> 00:28:01,115
know why these are so low that's not
normal
I may have there may be a bug and chaos
they may be a bug in my code I reached

151
00:28:01,215 --> 00:28:07,469
out on Twitter to see if
anybody could figure it out but they
couldn't I guess this is one of the

152
00:28:07,569 --> 00:28:15,509
challenges with using something like
this is one of the reasons I wanted to
use fast AI for this course is it's much
harder to screw things up so I don't

153
00:28:15,609 --> 00:28:18,806
know if I screwed something up or
somebody else did

154
00:28:18,906 --> 00:28:23,270
yes you know this is you've seen the
chance to float back end yeah yeah and

155
00:28:23,500 --> 00:28:50,279
if you want to run this to try it out
yourself you just can just go pip
install tensorflow - GPU Kerris okay
because it's not part of the faster I
environment by default but that should
be all you need to do to get that

156
00:28:50,379 --> 00:29:08,539
working so then there isn't a concept of
like layer groups or differential
learning rates or partial unfreezing or
whatever so you have to decide like I
had to print out all of the layers and
decide manually how many I wanted to
fine-tune so I decided to fine-tune

157
00:29:08,539 --> 00:29:17,210
everything from a layer 140 onwards so
that's why I just looked through like
this after you change that you have to
recompile the model and then after that

158
00:29:17,210 --> 00:29:26,924
I then ran another step and again I
don't know what happened here the
accuracy of the training set stayed
about the same but the validation set
totally fill in the hole and I mean the

159
00:29:27,024 --> 00:29:40,062
main thing to notice even if we put
aside the validation set we're getting I
mean I guess the main thing is there's a
hell of a lot more code here which is
kind of annoying but also the
performance is very different so where

160
00:29:40,162 --> 00:29:49,934
else is here even on the training set
we're getting like 97% after four epochs
that took a total of about eight minutes

161
00:29:50,034 --> 00:30:02,380
you know over here we had 99.5% on the
validation set and it ran a lot faster
so I was like four or five minutes right

162
00:30:02,380 --> 00:30:17,620
so
depending on what you do particularly if
you end up wanting to deploy stuff to
mobile devices at the moment the kind of
PI torch on mobile situation is very

163
00:30:17,620 --> 00:30:24,365
early so you may find yourself wanting
to use tensorflow or you may work for a
company that's kind of settled on

164
00:30:24,465 --> 00:30:37,000
tensorflow so if you need to convert
something like redo something you've
learnt here intensive flow you probably
want to do it with care us but just
recognize you know it's got to take a

165
00:30:37,000 --> 00:30:53,690
bit more work to get there and by
default it's much harder to get I mean I
to get the same state of the out results
you get the faster I you'd have to like
replicate all of the state-of-the-art
algorithms that are in first a nice so
it's hard to get the same level of

166
00:30:54,669 --> 00:31:05,990
results but you can see the basic ideas
are similar okay and it's certainly it's
certainly possible you know like there's
nothing I'm doing in fast AI that like
would be impossible but like you'd have

167
00:31:06,090 --> 00:31:20,389
to implement stochastic gradient percent
with restarts you would have to
implement differential learning rates
you would have to implement batch norm
freezing which you probably don't want
to do I know and well that's not quite

168
00:31:20,489 --> 00:31:29,199
true I think somewhat one person at
least on the forum is attempting to
create a chaos compatible version of or
a tensorflow compatible version of fast
AI which I think I hope will get there I

169
00:31:29,299 --> 00:31:43,270
actually spoke to Google about this a
few weeks ago and they're very
interested in getting faster i ported to
tensorflow so maybe by the time you're
looking at this on the mooc maybe that
will exist I certainly hope so we will

170
00:31:43,270 --> 00:32:05,900
see hey wait
so Karis is Karis intensive flow was
certainly not you know
that difficult to handle and so I don't
think you should worry if you're told
you have to learn them after this course
for some reason even let me take you a
couple of days I'm sure so that's kind

171
00:32:06,000 --> 00:32:19,890
of most of the stuff you would need to
kind of complete this this kind of
assignment from last week which was like
try to do everything you've seen already
but on the dog of reinstated said just

172
00:32:19,890 --> 00:32:26,600
to remind you that kind of last few
minutes of last week's lesson I show you
how to do much of that including like

173
00:32:30,870 --> 00:32:37,885
how I actually explored the data to find
out like what the classes were and how
big the images were and stuff like that

174
00:32:37,985 --> 00:32:41,980
right so if you've forgotten that or
didn't quite follow at all last week
check out the video from last week to

175
00:32:43,080 --> 00:32:53,794
see one thing that we didn't talk about
is how do you actually submit to Carol
so how do you actually get predictions
so I just wanted to show you that last

176
00:32:53,894 --> 00:33:01,500
piece as well and on the wiki thread
this week I've already put a little
image of this to show you these days but

177
00:33:01,500 --> 00:33:12,955
if you go to the kaggle website for
every competition there's a section
called evaluation and they tell you what
it's a bit and so I just copied and
pasted these two lines from from there

178
00:33:13,055 --> 00:33:19,810
and so it says we're expected to submit
a file where the first line contains the

179
00:33:20,410 --> 00:33:31,860
the work the word ID and then a comma
separated list of all of the possible
dog breeds and then every line after
that will contain the idea itself
followed by all the probabilities of all
the different dog breeds so how do you

180
00:33:35,610 --> 00:33:55,240
create that so recognize that inside our
data object there's a dot classes which
has got in alphabetical order all of the
four other classes and then so it's got
all of the different classes and then

181
00:33:55,340 --> 00:34:01,645
inside data dot test data set just yes
you can also see there's all the file
names so and just to remind you dogs and

182
00:34:01,745 --> 00:34:18,444
cats
sorry
cats dog breeds was not provided in the
kind of care our style format where the
dogs and cats from different folders but
instead it was provided as a CSV file of

183
00:34:18,544 --> 00:34:27,859
labels right so when you get a CSV file
of labels you use image classifier data
from CSV rather than image classifier
data from parts there isn't an

184
00:34:31,799 --> 00:34:40,139
equivalent in care us so you'll see like
on the cattle forums people share
scripts for how to convert it to a care
our style folders but in our case we

185
00:34:40,139 --> 00:34:47,574
don't have to we just go image
classifier data from CSV passing in that
CSV file and so the CSV file will you

186
00:34:47,674 --> 00:35:03,740
know has automatically told the data you
know what the masses are and then also
we can see from the folder of test
images what the file names of those are
so with those two pieces of information

187
00:35:03,740 --> 00:35:28,789
we're ready to go so I always think it's
a good idea to use TTA as you saw with
that dogs and cats example just now it
can really improve things particularly
when your model is less good so I can
say learn dot t ta and if you pass in
yeah if you pass in is test equals true

188
00:35:28,889 --> 00:35:35,369
then it's going to give you predictions
on the test set rather than the
validation set okay

189
00:35:35,369 --> 00:35:44,765
and now obviously we can't now get an
accuracy or anything because by
definition we don't know the labels for

190
00:35:44,865 --> 00:35:57,099
the test set right so by default most
high-touch models give you back the log
of the predictions so then we just have
to go X of that to get back out

191
00:35:57,199 --> 00:36:10,529
probabilities so in this case the test
set had ten thousand three hundred and
fifty seven images in it and there are
120 possible breeds all right so we get
back a matrix of of that size and so we

192
00:36:10,529 --> 00:36:17,695
now need to turn that into something
that looks like this
and so the easiest way to do that is

193
00:36:17,795 --> 00:36:27,472
with pandas if you're not familiar with
pandas there's lots of information
online about it or check out the machine
learning course intro to machine
learning that we have where we do lots
of stuff with pandas but basically we

194
00:36:27,572 --> 00:36:41,007
can describe PD data frame and pass in
that matrix and then we can say the
names of the columns are equal to data

195
00:36:41,607 --> 00:36:57,745
duck classes and then finally we can
insert a new column at position 0 called
ID that contains the file names but
you'll notice that the file names
contain five letters at the end I start
we don't want and four letters at the
end we don't want so I just subset in

196
00:36:57,845 --> 00:37:06,550
like so right so at that point I've got
a data frame that looks like this which

197
00:37:06,650 --> 00:37:24,000
is what we want so you can now call a
data frame data so social cues dated DF
not des let's fix it now data frame okay

198
00:37:24,000 --> 00:37:36,680
so you can now call data frame to CSV
and quite often you'll find these files
actually get quite big so it's a good
idea to say compression equals gzip and
that'll zip it up on the server for you

199
00:37:36,780 --> 00:37:45,082
and that's going to create a zipped up
CSV file on the server on wherever
you're running is Jupiter notebook so

200
00:37:45,182 --> 00:37:57,235
you need apps that you now need to get
that back to your computer so you can
upload it or you can use carol CLA so
you can type kgs submit and do it that
way

201
00:37:57,335 --> 00:38:04,170
I generally download it to my computer
so like how often back to this lab
double check it all looks ok so to do

202
00:38:04,170 --> 00:38:27,130
that there's a cool little theme called
file link and if you run file link with
a path on your server it gives you back
a URL which you can click on and it will
download that file from the server onto
your computer so if I click on that now
I can go ahead and save it
and then I can see in my downloads

203
00:38:33,490 --> 00:38:43,025
there it is here's my submission file
they want to open their yeah and as you

204
00:38:43,125 --> 00:38:54,095
can see it's exactly what I asked for
there's my ID and 120 different dog
breeds and then here's my first row
containing the file name and the 120
different probabilities okay so then you

205
00:38:54,195 --> 00:39:00,695
can go ahead and submit that to cattle
through their through their regular form

206
00:39:00,795 --> 00:39:15,160
and so this is also a good way you can
see we've now got a good way of both
grabbing any file off the internet and
getting a to our AWS instance or paper
space or whatever by using the cool
little extension in chrome and we've

207
00:39:17,170 --> 00:39:31,270
also got a way of grabbing stuff off our
server easily those of you that are more
command line oriented you can also use
SCP of course but I kind of like doing
everything through the notebook all

208
00:39:31,270 --> 00:39:42,910
right
one other question I had during the week
was like what if I want to just get a
single a single file that I want to you

209
00:39:42,910 --> 00:39:49,790
know get a prediction for so for example
you know maybe I want to get this first
file from my validation set

210
00:39:49,890 --> 00:39:59,350
so there's its name so you can always
look at a file just by calling image dot
open that just uses regular - imaging

211
00:39:59,350 --> 00:40:16,230
library and so what you can do is
there's actually I'll show you the
shortest version you can just call learn
predict array passing in your your image

212
00:40:16,230 --> 00:40:21,390
okay now the image needs to have been
transformed

213
00:40:21,390 --> 00:40:36,310
so you've seen transform Trent
transforms from model before normally we
just put put it all in one variable but
actually behind the scenes it was
returning to things
it was returning training transforms and
validation transforms so I can actually

214
00:40:36,310 --> 00:40:45,280
split them apart and so here you can see
I'm actually applying example my
training transforms or probably more
likely that would apply validation

215
00:40:45,280 --> 00:40:52,420
transforms that gives me back an array
containing the image the transformed
image which I can then past

216
00:40:55,440 --> 00:41:05,320
everything that gets passed to or
returned from bottles is generally
assumed to be a mini batch right it's
generally assumed to be a bunch of

217
00:41:05,320 --> 00:41:15,250
images so we'll talk more about some
numpy tricks later but basically in this
case we only have one image so we have
to turn that into a mini batch of images

218
00:41:15,250 --> 00:41:21,490
so in other words we need to create a
tensor that basically is not just rows

219
00:41:21,490 --> 00:41:26,230
by columns by channels but it's number
of image by rows by columns by channels

220
00:41:26,230 --> 00:41:31,330
and as one image so it's basically
becomes a 4 dimensional tensor so

221
00:41:31,330 --> 00:41:46,480
there's a cool little trick in numpy
that if you index into an array with
none that basically adds additional unit
access to the start so it turns it from
an image into a mini batch of one images
and so that's why we had to do that so

222
00:41:46,480 --> 00:42:02,080
if you basically find you're trying to
do things with a single image with any
kind of Pi torch or fast AI thing this
is just something you might you might
find it says like expecting four
dimensions only got three it probably

223
00:42:02,080 --> 00:42:15,040
means that or if you get back a return
value from something that has like some
weird first access that's probably why
it's probably giving you like back a
mini batch okay and so we'll learn a lot
more about this but it's just something

224
00:42:15,040 --> 00:42:38,405
to be aware of okay so that's kind of
everything you need to do in practice so
now we're going to kind of get into a
little bit of theory what's actually
going on behind the scenes with these
convolutional neural networks and you

225
00:42:38,505 --> 00:42:54,175
might remember it back in Lesson one we
actually saw our first little bit of
theory which we stole from this
fantastic websites a toaster dot IO

226
00:42:54,275 --> 00:43:03,910
either explained visually and we learnt
that a that a convolution is something
where we basically have a little matrix
in deep learning nearly always three by
three a little matrix that we basically

227
00:43:04,010 --> 00:43:16,790
multiply every element of that matrix by
every element of a three by three
section of an image add them all
together to get the result of that

228
00:43:16,890 --> 00:43:29,065
convolution at one point all right now
let's see how that all gets turned
together to create these these various
layers that we saw in the the Zeiler and

229
00:43:29,165 --> 00:43:39,570
burgers paper and to do that again I'm
going to steal off somebody who's much
smarter than I am
we're going to steal from a guy called
Ottavia good

230
00:43:39,570 --> 00:43:47,155
Ottavia oh good was the guy who created
Word Lens which nowadays is part of
Google Translate if I'm Google Translate

231
00:43:47,255 --> 00:43:58,080
you've ever like done that thing where
you point your camera at something at
something which has any kind of foreign
language on it and in real-time it
overlays it with the translation that

232
00:43:58,080 --> 00:44:06,385
was a views company that built that and
so Tokyo was kind enough to share this
fantastic video
he created he's at Google now and I want

233
00:44:06,485 --> 00:44:21,000
to kind of step you through works I
think it explains really really well
what's going on and then after we look
at the video we're going to see how to
implement the whole a whole sequence of
kintyre set of layers of convolution on
your network in Microsoft Excel

234
00:44:21,500 --> 00:44:27,720
so with you're a visual learner or a
spreadsheet learner hopefully you'll be
able to understand all this okay so

235
00:44:29,880 --> 00:44:41,610
we're going to start with an image and
something that we're going to do later
in the course is we're going to learn to
nice digits so we'll do it like end to
end we'll do the whole thing so this is
pretty similar so we're going to try and

236
00:44:41,610 --> 00:44:51,990
recognize in this case letters so here's
an A which obviously it's actually a
grid of numbers right and so there's the
creative numbers and so what we do is we

237
00:44:51,990 --> 00:44:59,035
take our first convolutional filter so
we're assuming this is all this is
assuming that these are already learned

238
00:44:59,135 --> 00:45:05,010
right and you can see this point it's
got wiped down the right-hand side right
and black down the left so it's like

239
00:45:05,010 --> 00:45:10,350
zero zero zero maybe negative 1 negative
1 negative 1 0 0 0 1 1 1 and so we're
taking each 3x3 part of the image and

240
00:45:13,140 --> 00:45:21,670
multiplying it by that 3x3 matrix not as
a matrix product that an element-wise
product and so you can see what happens

241
00:45:21,770 --> 00:45:30,180
is everywhere where the the white edge
is matching the edge of the a and the
black edge isn't we're getting green

242
00:45:30,180 --> 00:45:36,475
we're getting a positive and everywhere
where it's the opposite we're getting a
negative we're getting a red right and

243
00:45:36,575 --> 00:45:41,920
so that's the first filter creating the
first that the result of the first

244
00:45:42,020 --> 00:45:47,020
kernel right and so here's a new kernel
this one is it's got a white stripe

245
00:45:47,120 --> 00:46:02,610
along the top right so we literally scan
through every 3x3 part of the matrix
multiplying those 3 bits of the a the
neighbors of the a by the 9 bits as a
filter to find out whether it's red or
green and how red or green it is ok and

246
00:46:02,610 --> 00:46:15,570
so this is assuming we had two filters
one was a bottom edge one was a left
edge and you can see here the top edge
not surprisingly it's red here so a
bottom edge was red here and green here
the right edge right here in green here

247
00:46:15,570 --> 00:46:26,520
and then in the next step we add a
non-linearity ok the rectified linear
unit which literally means strongly the
negatives so here the Reds all gone okay

248
00:46:26,520 --> 00:46:30,780
so here's layer 1 the input
here's layup to the result of 2

249
00:46:30,780 --> 00:46:36,180
convolutional filters here's layer 3
which is which is throw away all of the
red stuff and that's called a rectified

250
00:46:38,190 --> 00:46:55,320
linear unit and then layer 4 is
something called a max pull on a layer 4
we replace every 2 by 2 part of this
grid
and we replace it with its maximum mat
so it basically makes it half the size
it's basically the same thing but half

251
00:46:55,420 --> 00:47:05,599
the size and then we can go through and
do exactly the same thing we can have
some new filter three by three filter
that we put through each of the two
results of the previous layer okay and

252
00:47:05,699 --> 00:47:21,190
again we can throw away the red bits
right so get rid of all the negatives so
we just keep the positives that's called
applying a rectified linear unit and
that gets us to our next layer of this

253
00:47:21,190 --> 00:47:30,490
convolutional neural network so you can
see that by you know at this layer back
here it was kind of very interpretive
all it's like we've either got bottom
edges or left edges but then the next

254
00:47:33,010 --> 00:47:42,559
layer was combining the results of
convolutions so it's starting to become
a lot less clear like intuitively what's
happening but it's doing the same thing

255
00:47:42,659 --> 00:47:52,180
and then we do another max pull right so
we replace every 2x2 or 3x3 section with
a single digit so here this 2x2 it's all
black so we replaced it with a black

256
00:47:53,799 --> 00:48:07,865
right and then we go and we take that
and we we compare it to basically a kind
of a template of what we would expect to
see if it was an a it was a B but the
see it was d give it an E and we see how

257
00:48:07,965 --> 00:48:19,456
closely it matches and we can do it in
exactly the same way we can multiply
every one of the values in this four by
eight matrix with every one of the four
by eight in this one and this one and

258
00:48:19,556 --> 00:48:30,614
this one and we add we just add them
together to say like how often does it
match versus how often does it not match
and then that could be converted to give
us a percentage probability that this is

259
00:48:31,114 --> 00:48:38,524
a no so in this case this particular
template matched well with a so notice

260
00:48:38,624 --> 00:48:46,119
we're not doing an each training here
right this is how it would work if we
have a pre trained model all right so

261
00:48:46,119 --> 00:49:00,484
when we download a pre trained imagenet
model off the internet and isn't on an
image without any changing to it this is
what's happening or if we take a model
that you've trained and you're applying
it to some test set or for some new
image this is what it's doing all right

262
00:49:00,584 --> 00:49:17,855
as it's basically taking it through it
buying a convolution to each layer to
each multiple convolutional filters to
each layer and then doing the rectified
linear unit so throw away the negatives
and then do the max pull and then repeat

263
00:49:17,955 --> 00:49:29,395
that a bunch of times and so then we can
do it with a new letter A or letter B or
whatever and keep going through that
process right so as you can see that's

264
00:49:29,495 --> 00:49:40,420
far nice the visualization thing and I
could have created because I'm not at a
vo so thanks to him for sharing this
with us because it's totally awesome

265
00:49:40,420 --> 00:49:49,895
he actually this is not done by hand he
actually wrote a piece of computer
software to actually do these
convolutions this is actually being
actually being done dynamically which is

266
00:49:49,995 --> 00:49:58,060
pretty cool so I'm more of a spreadsheet
guy personally I'm a simple person so
here is the same thing now in

267
00:49:58,060 --> 00:50:08,950
spreadsheet all right and so you'll find
this in the github repo so you can
either get clone the repo to your own
computer open up the spreadsheet or you

268
00:50:08,950 --> 00:50:36,880
can just go to github.com slash / ji and
click on this it's it's inside if you go
to our repo and just go to courses as
usual go to deal 1 as usual you'll see
there's an Excel section there okay and
so he lay all that so you can just
download them by clicking them or you
can clone the whole repo and we're
looking at cognitive example convolution

269
00:50:36,880 --> 00:50:55,290
example all right so you can see I have
here an input right so in this case the
input is the number 7 so I grab this
from a dataset called m-must MN ist
which we'll be looking at in a lot of
detail and I just took one of those

270
00:50:55,290 --> 00:51:09,640
digits at random and I put it into Excel
and so you can see every Hextall is
actually just a number between 9 1 okay
very often actually it'll be a bite
between Norton 255 or sometimes it might

271
00:51:13,060 --> 00:51:25,900
be a float between naught and 1 it
doesn't really matter
by the time it gets to PI torch we're
generally dealing with floats so we if
one of the steps we often will take will
be to convert it to a number between

272
00:51:25,900 --> 00:51:47,299
naught 1 so then you can see I've just
use conditional formatting in Excel to
kind of make the higher numbers more red
so you can clearly see that this is a
red this is a 7 but but it's just a
bunch of numbers that have been imported
into Excel okay so here's our input so

273
00:51:47,399 --> 00:51:55,249
remember what at a via did was he then
applied two filters right with different

274
00:51:55,349 --> 00:52:05,170
shapes so here I've created a filter
which is designed to detect top edges so
this is a 3 by 3 filter okay and I've

275
00:52:05,170 --> 00:52:10,119
got ones along the top zeroes in the
middle minus ones at the bottom right so

276
00:52:10,119 --> 00:52:16,630
let's take a look at an example that's
here right and so if I hit that - you

277
00:52:16,630 --> 00:52:24,112
can see here highlighted this is the 3
by 3 part of the input that this
particular thing is calculating right so

278
00:52:24,212 --> 00:52:44,134
here you can see it's got 1 1 1 are all
being multiplied by 1 and point 1 0 0
are all being multiplied by negative 1
okay so in other words all the positive
bits are getting a lot of positive the
negative bits are getting nearly nothing
at all so we end up with a high number

279
00:52:44,234 --> 00:52:54,670
okay where else on the other side of
this bit of the 7 right you can see how
you know this is basically zeros here or

280
00:52:54,670 --> 00:53:08,859
perhaps more interestingly on the top of
it okay here we've got high numbers at
the top but we've also got high numbers
at the bottom which are negating it ok

281
00:53:08,859 --> 00:53:15,169
so you can see that the only place that
we end up activating is where we're

282
00:53:15,669 --> 00:53:49,070
actually at an edge so in this case this
here this number 3 this is called an
activation ok so when I say an
activation I mean ah
at number a number that is calculated
and it is calculated by taking some
numbers from the input and applying some
kind of linear operation in this case a
convolutional kernel to calculate an

283
00:53:49,170 --> 00:54:03,095
output right you'll notice that other
than going inputs multiplied by kernel
and summing it together
right so here's my some and here's my x

284
00:54:03,195 --> 00:54:32,920
then take that and I go max of 0 comma
that and so that's my rectified linear
unit so it sounds very fancy rectified
linear unit but what they actually mean
is open up Excel and type equals max 0
comma C ok that's all about then you'll
see people in the biz so to say value a
so ral you means rectified linear unit
means max 0 comma thing and I'm not like

285
00:54:32,920 --> 00:54:50,490
simplifying it I really mean it like
when I say like if I'm simplifying I
always say so I'm simplifying but if I'm
not saying I'm simplifying that's the
entirety okay so a rectified linear unit
in its entirety is this and a
convolution in its entirety is is this

286
00:54:50,490 --> 00:55:03,560
okay so a single layer of a
convolutional neural network is being
implemented in its entirety
here in Excel okay and so you can see

287
00:55:03,560 --> 00:55:09,290
what it's done is it's deleted pretty
much the vertical edges and highlighted

288
00:55:09,290 --> 00:55:24,660
the horizontal edges so again this is
assuming that our network is trained and
that at the end of training it a created
a convolutional filter with these
specific line numbers in and so here is

289
00:55:24,760 --> 00:55:31,890
a second convolutional filter it's just
a different line numbers now pi torch

290
00:55:31,990 --> 00:55:54,230
doesn't store them as two separate nine
digit arrays it stores it as a tensor
right remember a tensor just means an
array with more dimensions okay you can
use the word array as well it's the same
thing but in pi torch they always use
the word tensor so I'm going to say

291
00:55:54,230 --> 00:56:03,495
cancer okay so it's just a tensor with
an additional axis which allows us to
stack each of these filters together

292
00:56:03,595 --> 00:56:19,700
right filter and kernel pretty much mean
the same thing yeah right it refers to
one of these three by three matrices or
one of these three by three slices of a
three dimensional tensor so if I take

293
00:56:19,700 --> 00:56:35,295
this one and here I've literally just
copied the formulas in Excel from above
okay and so you can see this one is now
finding a vertebra which as we would
expect

294
00:56:35,395 --> 00:56:42,030
okay so we've now created one layer
right this here is a layer them

295
00:56:42,130 --> 00:56:48,680
specifically we'd say it's a hidden
layer which is it's not an input layer
and it's not an output layer so
everything else is a hidden layer okay

296
00:56:50,810 --> 00:57:05,974
and this particular hidden layer has is
a size two on this dimension right
because it has two
filters right two kernels so what

297
00:57:06,074 --> 00:57:13,729
happens next
well let's do another one okay so as we

298
00:57:13,829 --> 00:57:34,540
kind of go along things can multiply a
little bit in complexity right because
my next filter is going to have to
contain two of these three by threes
because I'm gonna have to say how do I
want to bring Adam I want to write these
three things and at the same time how do

299
00:57:34,540 --> 00:57:53,464
I want to wait the corresponding three
things down here right because in pi
torch this is going to be this whole
thing here is going to be stored as a
multi-dimensional tensor right so you
shouldn't really think of this now as
two 3x3 kernels but one two by three by

300
00:57:53,564 --> 00:58:14,264
three eternal okay so to calculate this
value here I've got the sum product of
all of that plus the sum product of
scroll down all of that okay and so the

301
00:58:14,364 --> 00:58:22,420
top ones are being multiplied by this
part of the kernel and the bottom ones
have been multiplied by this part of the

302
00:58:22,420 --> 00:58:35,530
kernel and so over time you want to
start to get very comfortable with the
idea of these like higher dimensional
linear combinations right like it's it's

303
00:58:35,530 --> 00:58:44,855
harder to draw it on the screen like I
had to put one above the other but
conceptually just stuck it in your mind
like this that's really how you want to

304
00:58:44,955 --> 00:58:50,600
think right and actually Geoffrey Hinton
in his original 2012 neural Nets

305
00:58:50,700 --> 00:58:58,990
Coursera class has a tip which is how
all computer scientists deal with like
very high dimensional spaces which is

306
00:58:58,990 --> 00:59:06,390
that they basically just visualize the
two-dimensional space
and then say like twelve dimensions
really fast and they had lots of tires

307
00:59:06,390 --> 00:59:19,550
so that's it
right we can see two dimensions on the
screen and then you're just going to try
to trust that you can have more
dimensions like the Const
it's just you know there's there's
nothing different about them and so you

308
00:59:19,550 --> 00:59:38,900
can see in Excel you know Excel doesn't
have the ability to handle
three-dimensional tensors so I had to
like say okay take this two-dimensional
dot product add on this two-dimensional
dot product right but if there was some
kind of 3d Excel I could have to stand
that in a single line all right and then

309
00:59:38,900 --> 00:59:44,720
again apply max 0 comma otherwise known
as rectified linear unit otherwise known
as value okay so here is my second layer

310
00:59:48,710 --> 01:00:02,540
and so when people create different
architectures write an architecture
means like how big is your kernel at
layer 1
how many filters are in your kernel at

311
01:00:02,540 --> 01:00:15,250
layer 1 so here I've got a 3 by 3
where's number 1 and a 3 by 3 there's
number 2 so like this architecture I've
created starts off with 2 3 by 3

312
01:00:16,150 --> 01:00:31,895
convolutional kernels and then my second
layer has another two kernels of size 2
by 3 by 3 so there's the first one
and then down here here's a second 2 by
3 by 3 kernel okay and so remember one

313
01:00:31,995 --> 01:00:52,940
of these specific any one of these
numbers is an activation okay so this
activation is being calculated from
these three things here and other 3
things up there and we're using these
this 2 by 3 by 3 kernel okay and so what

314
01:00:52,940 --> 01:00:57,830
tends to happen is people generally give
names to their layers so I say okay

315
01:00:57,830 --> 01:01:10,650
let's call this layer here con 1 and
this layer here and this and this layer
here con - all right so that's you know

316
01:01:10,750 --> 01:01:21,990
but generally you'll just see that like
when you print out a summary of a
network every layer will have some kind
of name okay and so then what happens

317
01:01:22,090 --> 01:01:36,520
next well part of the architecture is
like do you have some max pooling where
bounces up Matt spalling happen so in
this architecture we're inventing we're
going to next step is do max fully okay

318
01:01:36,520 --> 01:01:43,569
Matt spooling is a little hard to kind
of show in Excel but we've got it so max

319
01:01:43,569 --> 01:01:53,219
pooling if I do a two by two max pooling
it's going to have the resolution both
height and width so you can see here

320
01:01:53,319 --> 01:01:59,909
that I've replaced these four numbers
with the maximum of those four numbers

321
01:02:00,009 --> 01:02:06,780
right and so because I'm having the
resolution it only makes sense to
actually have something every two cells

322
01:02:06,780 --> 01:02:19,030
okay so you can see here the way I've
got kind of the same looking shape as I
had back here okay but it's now half the
resolution so for placed every two by

323
01:02:19,030 --> 01:02:24,160
two with its max and you'll notice like
it's not every possible two by two I
skip over from here so this is like

324
01:02:26,319 --> 01:02:38,050
starting at beat Hugh and then the next
one starts at BS right so they're like
non-overlapping that's why it's
decreasing the resolution okay

325
01:02:38,050 --> 01:02:47,525
so anybody who's comfortable with
spreadsheets you know you can open this
and have a look and so after our max

326
01:02:47,625 --> 01:02:55,084
pooling there's a number of different
things we could do next and I'm going to

327
01:02:55,184 --> 01:03:00,454
show you a kind of classic old style
approach nowadays in fact what generally
happens nowadays is we do a max pool

328
01:03:00,554 --> 01:03:07,490
where we kind of like max across the
entire size right but on older
architectures and also on all the

329
01:03:07,590 --> 01:03:14,810
structured data stuff we do we actually
do something called a fully connected

330
01:03:14,910 --> 01:03:43,094
layer and so here's a fully connected
layer I'm going to take every single one
of these activations and I've got to
give every single one of them or weight
right and so then I'm going to take over
here here is the sum product of every
one of the activations by every one of
the weights for both of the
two levels of my three-dimensional

331
01:03:43,194 --> 01:03:49,490
tensor right and so this is called a
fully connected layer notice it's
different to a convolution I'm not going
through a few at a time right but I'm

332
01:03:51,980 --> 01:04:03,290
creating a really big weight matrix
right so rather than having a couple of
little 3x3 kernels my weight matrix is
now as big as the entire input and so as

333
01:04:03,290 --> 01:04:17,744
you can imagine architectures that make
heavy use of fully convolutional layers
can have a lot of weights which means
they can have trouble with overfitting
and they can also be slow and so you're

334
01:04:17,844 --> 01:04:28,465
going to see a lot an architecture
called vgg because it was the first kind
of successful deeper architecture it has
up to 19 layers and vgg actually

335
01:04:28,565 --> 01:04:42,930
contains a fully connected layer with
4096 weights connected to a hidden layer
with 4,000 sorry 4096 activations
connected to a hidden layer with 4096

336
01:04:43,030 --> 01:04:53,180
activations so you've got like 4096 by
4096 x remember or apply it by the
number of kind of kernels that we've

337
01:04:53,180 --> 01:05:09,380
calculated so in vgg there's this I
think it's like 300 million weights of
which something like 250 million of them
are in these fully connected layers so

338
01:05:09,380 --> 01:05:13,700
we'll learn later on in the course about
how we can kind of avoid using these big
fully connected layers and behind the

339
01:05:15,470 --> 01:05:23,004
scenes all the stuff that you've seen us
using like ResNet and res next none of
them use very large fully connected

340
01:05:23,104 --> 01:05:40,280
layers you know you had a question
sorry yeah come on um so could you tell
us more about for example if we had like
three channels for the input what would
be the shape yeah these filters right so

341
01:05:40,280 --> 01:05:52,160
that's a great question so if we have 3
channels of input it would look exactly
like conv one right cons one kind of has
two channels right and so you can see

342
01:05:52,160 --> 01:06:00,164
with cons one we had two channels so
therefore our filters had to have like
two channels per filter and so you could

343
01:06:00,264 --> 01:06:11,749
like imagine that this input didn't
exist you know and actually this was the
airport alright so when you have a
multi-channel input it just means that
your filters look like this and so

344
01:06:11,749 --> 01:06:17,779
images often full color they have three
red green and blue sometimes they also

345
01:06:17,779 --> 01:06:23,059
have an alpha Channel so however many
you have that's how many inputs you need

346
01:06:23,059 --> 01:06:30,764
and so something which I know Jeanette
was playing with recently was like using
a full color image net model in medical

347
01:06:30,864 --> 01:06:35,124
imaging for something called bone age
calculations which has a single channel

348
01:06:35,224 --> 01:06:51,989
and so what she did was basically take
the the input the the single channel
input and make three copies of it so you
end up with basically like one two three
versions of the same thing which is like

349
01:06:52,089 --> 01:06:59,630
it's kind of a small idea like it's kind
of redundant information that we don't
quite want but it does mean that then if

350
01:06:59,630 --> 01:07:08,640
you had a something that expected a
three channel convolutional filter you
can use it right and so at the moment

351
01:07:08,740 --> 01:07:17,660
there's a cable competition for iceberg
detection using a some funky satellite
specific data format that has two

352
01:07:17,660 --> 01:07:30,029
channels so here's how you could do that
you could either copy one of those two
channels into the third channel or I
think what people in Carroll are doing
is to take the average of the two again

353
01:07:30,129 --> 01:07:36,794
it's not ideal but it's a way that you
can use pre-trained networks yeah I've

354
01:07:36,894 --> 01:07:48,410
done a lot of fiddling around like that
you can also actually I've actually done
things where I wanted to use a three
channel image net Network on four
channel data

355
01:07:48,410 --> 01:07:53,029
I had a satellite data where the fourth
channel was near-infrared and so

356
01:07:53,029 --> 01:08:08,545
basically I added an extra kind of level
to my convolutional kernels that were
all zeros and so basically like started
off by ignoring the near-infrared band
and

357
01:08:08,645 --> 01:08:21,500
so what happens it basically and you'll
see this next week is that rather than
having these like carefully trained
filters when you're actually training
something from scratch we're actually
going to start with random numbers

358
01:08:21,600 --> 01:08:33,070
that's actually what we do we actually
start with random numbers and then we
use this thing called stochastic
gradient descent which we've kind of
seen conceptually
to slightly improve those random numbers
to make them less random and we
basically do that again and again and

359
01:08:35,350 --> 01:08:44,985
again okay great
let's take a seven minute break and
we'll come back at 7:50 all right so

360
01:08:45,085 --> 01:08:50,310
what happens next

361
01:08:50,310 --> 01:09:04,685
so we've got as far as doing a fully
connected layer right so we had our the
results of our max pooling layer got fed
to a fully connected layer and he might

362
01:09:04,785 --> 01:09:07,180
notice those of you that remember your

363
01:09:07,180 --> 01:09:11,770
linear algebra the fully connected layer
is actually doing a classic traditional

364
01:09:11,770 --> 01:09:17,920
matrix product okay so it's basically
just going through each pair in turn

365
01:09:17,920 --> 01:09:23,970
multiplying them together and then
adding them up to do a matrix product

366
01:09:23,970 --> 01:09:47,505
now in practice if we want to calculate
which one of the ten digits we're
looking at their single number we've
calculated isn't enough we would
actually calculate ten numbers so what

367
01:09:47,605 --> 01:10:06,860
we will have is rather than just having
one set of fully connected weights like
this and I say set because remember
there's like a whole 3d kind of tensor
of them we would actually need ten of
those right so you can see that these

368
01:10:06,960 --> 01:10:21,430
tensors start to get a little bit high
dimensional right and so this is where
my patients we're doing it next cell ran
out but imagine that I had done this ten
times I could now have ten different
numbers or being calculated
yeah using exactly the same process

369
01:10:21,530 --> 01:10:40,190
right we'll just be ten of these fully
connected to by m-by-n erased basically
and so then we would have ten numbers

370
01:10:40,190 --> 01:10:52,314
being spat out so what happens next so
next up we can open up a different Excel
worksheet entropy example dot XLS that's

371
01:10:52,414 --> 01:10:59,234
got two different worksheets one of them
is called soft mass and what happens

372
01:10:59,334 --> 01:11:10,820
here sorry I've changed domains rather
than predicting whether it's the number
from one not to nine I'm going to
predict whether something is a cat a dog
a plane of Fisher Building okay so out

373
01:11:10,820 --> 01:11:27,675
of our that fully connected layer we've
got this case we'd have five numbers and
notice at this point there's no rail you
okay in the last layer there's no rail
you okay so I can have negatives so I

374
01:11:27,775 --> 01:11:42,359
want to turn these five numbers H into a
probability I want to turn it into a
probability from naught to one that it's
a cat
that's a dog there's a plane that it's a
fish that it's a building and I want

375
01:11:42,459 --> 01:11:55,969
those probabilities to have a couple of
characteristics first is that each of
them should be between zero and one and
the second is that this state together
should add up to one right it's
definitely one of these five things okay

376
01:11:55,969 --> 01:12:01,400
so to do that we use a different kind of
activation function what's an activation

377
01:12:01,400 --> 01:12:16,340
function an activation function is a
function that is applied to activations
so for example max 0 comma something is
a function that I applied to an

378
01:12:16,340 --> 01:12:23,630
activation so an activation function
always takes in one number and spits out

379
01:12:23,630 --> 01:12:36,115
one number so max of 0 comma X takes in
a number X and spits out some different
number value of s
that's all an activation function is and

380
01:12:36,215 --> 01:12:44,230
if you remember back to that PowerPoint
we saw in Lesson one

381
01:12:45,590 --> 01:12:55,230
each of our layers was just a linear
function and then after every layer we
said we needed some non-linearity act as

382
01:12:58,619 --> 01:13:18,210
if you stack a bunch of linear layers
together right then all you end up with
is a linear layer okay
so somebody's talking can can you not a
slow just acting thank you
if you stack a number of linear
functions together you just end up with

383
01:13:18,210 --> 01:13:25,609
a linear function and nobody does any
cool deep learning with displaying your
functions right but remember we also

384
01:13:25,709 --> 01:13:34,769
learnt that by stacking linear functions
with between each one a non-linearity we
could create like arbitrarily complex

385
01:13:34,769 --> 01:13:47,119
shapes and so the non-linearity that
we're using after every hidden layer is
a rally rectified linear unit a
non-linearity is an activation function
an activation function is a

386
01:13:47,219 --> 01:13:57,444
non-linearity in with in deep way
obviously there's lots of other
nonlinearities and in the world but in
deep learning this is what we mean so an

387
01:13:57,544 --> 01:14:06,239
activation function is any function that
takes some activation in as a single
number and spits out some new activation
like max of 0 comma so I'm now going to

388
01:14:06,339 --> 01:14:15,744
tell you about a different activation
function it's slightly more complicated
than value but not too much it's called

389
01:14:15,844 --> 01:14:22,029
soft max soft max only ever occurs in
the final layer at the very end and the

390
01:14:22,129 --> 01:14:35,729
reason why is that soft max always spits
out numbers as an activation function
that always spits out a number between
Norton 1 and it always spits out a bunch
of numbers that add to 1 so a soft max

391
01:14:35,729 --> 01:14:53,489
gives us what we want right in theory
this isn't strictly necessary right like
we could ask our neural net to learn a
set of kernels which have you know which
which give probabilities that line up as
closely as possible with what we want

392
01:14:54,960 --> 01:15:10,170
but in general with deep learning if you
can construct your architecture so that
the desired characteristics are as easy
to express as possible you'll end up
with better models like they'll learn
more quickly with less parameters so in

393
01:15:10,170 --> 01:15:29,610
this case we know that our probabilities
should end up being between 9 1 we know
that they should end up adding to 1 so
if we construct an activation function
which always has those features then
we're going to make our neural network
do a better job
it's gonna make it easier for it it
doesn't have to learn to do those things

394
01:15:29,610 --> 01:15:51,420
because it all happen automatically okay
so in order to make this work we first
of all have to get rid of all of the
negatives right like we can't have
negative probabilities so to make things
not being negative one way we could do
it is just go into the pair of right so
here you can see my first step is to go

395
01:15:51,420 --> 01:16:03,450
X of the previous one right and I think
I've mentioned this before but of all
the math that you just need to be super
familiar with to do deep learning the

396
01:16:03,450 --> 01:16:14,050
one you really need is logarithms and
asks write all of deep learning and all
of machine learning they appear all the

397
01:16:14,150 --> 01:16:33,090
time right so for example you absolutely
need to know that log of x times y
equals log of X plus log of Y all right

398
01:16:33,090 --> 01:16:43,135
and like not just know that that's a
formula that exists but have a sense of
like what does that mean why is that
interesting oh I can turn
multiplications into additions that
could be really handy right and

399
01:16:43,235 --> 01:17:00,385
therefore log of x over y equals log of
X minus log of Y again that's going to
come in pretty handy you know rather
than dividing I can just subtract things

400
01:17:00,485 --> 01:17:17,860
right and also remember that if I've got
log of x equals y then that means a to
the y equals x in other words log log
and E to the for the inverse of each

401
01:17:17,960 --> 01:17:28,345
other okay again you just you need to
really really understand these things
and like so if you if you haven't spent
much time with logs and X for a while
try plotting them in Excel or a notebook

402
01:17:28,445 --> 01:17:38,260
have a sense of what shape they are how
they combine together just make sure
you're really comfortable with them so

403
01:17:38,260 --> 01:17:49,450
we're using it here right we're using it
here so one of the things that we know
is a to the power of something is
positive okay so that's great

404
01:17:49,450 --> 01:18:03,965
the other thing you'll notice about e to
the power of something is because it's a
power numbers that are slightly bigger
than other numbers like four is a little
bit bigger than 2.8 when you go e to the
power of it really accentuates that
difference okay

405
01:18:04,065 --> 01:18:07,850
so we're going to take advantage of both
of these features for the purpose of

406
01:18:07,950 --> 01:18:15,850
deep learning okay so we take our the
results of this fully connected layer we
go e to the power of for each of them

407
01:18:15,850 --> 01:18:28,120
and then we're gonna yeah and then we're
going to add them up okay so here is the
sum of e to the power of so then here

408
01:18:32,340 --> 01:18:40,795
we're going to take e to the power of
divided by the sum of e to the power of
so if you take all of these things

409
01:18:40,895 --> 01:18:47,610
divided by their sum then by definition
all of those things must add up to 1 and

410
01:18:47,710 --> 01:19:00,815
furthermore since we're dividing by
their sum they must always vary between
0 and 1 because they were always
positive alright and that's it so that's

411
01:19:00,915 --> 01:19:04,840
what softmax is ok so I've got this kind

412
01:19:04,840 --> 01:19:11,050
of doing random numbers each time right
and so you can see like as I as I look

413
01:19:11,050 --> 01:19:18,574
through my softmax generally has quite a
few things that are so close to zero
that they round down to zero and you
know maybe one thing that's nearly 1

414
01:19:18,674 --> 01:19:30,710
right and the reason for that is what we
just talked about that is with the X
just having one number a bit bigger than
the others tends to like push it out

415
01:19:30,810 --> 01:19:47,140
further right so even though my inputs
here around on numbers between negative
5 and 5 right my outputs from the
softmax don't really look that random at
all in the sense that they tend to have
one big number and a bunch of small

416
01:19:47,140 --> 01:20:03,755
numbers and now that's what we want
right we want to say like in terms of
like is this a cat a dog a plane a fish
or a building we really want it to say
like it's it's that you know it's it's a
dog or it's a plane not like I don't

417
01:20:03,855 --> 01:20:18,740
know okay so softmax has lots of these
cool properties right it's going to
return a probability that adds up to 1
and it's going to tend to want to pick
one thing particularly strongly okay so

418
01:20:18,840 --> 01:20:22,600
that's soft mess your net could you pass

419
01:20:22,600 --> 01:20:34,780
actually bust me up we how would we do
something that as let's say you have any
imaging you want to count in categorize
I was like cat and the dog or like has
multiple things but what kind of

420
01:20:37,600 --> 01:20:43,230
function will we try to use so happens
we're going to do that right now so

421
01:20:45,419 --> 01:20:48,840
so hope you think about why we might
want to do that and so runways where you

422
01:20:48,840 --> 01:20:53,010
might want to do that is to do
multi-label classification so we're

423
01:20:53,010 --> 01:20:57,119
looking now at listen to image models
and specifically we're going to take a
look at the planet competition satellite

424
01:21:00,419 --> 01:21:08,189
imaging competition now the satellite
imaging competition has some

425
01:21:08,189 --> 01:21:13,229
similarities to stuff we've seen before
right so before we've seen cat versus

426
01:21:13,229 --> 01:21:19,769
dog and these images are a cat or a dog
they're not Maya they're not both right

427
01:21:19,769 --> 01:21:25,559
but the satellite imaging competition
has stayed as images that look like this

428
01:21:25,559 --> 01:21:30,389
and in fact every single one of the
images is classified by whether there's

429
01:21:30,389 --> 01:21:34,260
four kinds of weather
one of which is haze and another of

430
01:21:34,260 --> 01:21:39,719
which is clear in addition to which
there is a list of features that may be

431
01:21:39,719 --> 01:21:44,880
present including agriculture which is
like some some cleared area used for

432
01:21:44,880 --> 01:21:50,999
agriculture primary which means primary
rainforest and water which means a river

433
01:21:50,999 --> 01:21:56,969
or a creek so here is a clear day
satellite image showing some agriculture

434
01:21:56,969 --> 01:22:02,550
some primary rainforest and some water
features and here's one which is in haze

435
01:22:02,550 --> 01:22:09,840
and is entirely primary rainforest so in
this case we're going to want to be able

436
01:22:09,840 --> 01:22:15,689
to show we're going to predict multiple
things and so softmax wouldn't be good

437
01:22:15,689 --> 01:22:21,749
because softmax doesn't like predicting
multiple things and like I would
definitely recommend anthropomorphizing

438
01:22:23,610 --> 01:22:28,079
your activation functions right they
have personalities okay and the

439
01:22:28,079 --> 01:22:34,559
personality of the softmax is it wants
to pick a thing and people forget this

440
01:22:34,559 --> 01:22:39,419
all the time I've seen many people even
well-regarded researchers in famous

441
01:22:39,419 --> 01:22:46,229
academic papers using like soft maps for
multi-label classification it happens

442
01:22:46,229 --> 01:22:49,590
all the time
right and it's kind of ridiculous

443
01:22:49,590 --> 01:22:55,249
because they're not understanding the
personality of their activation function

444
01:22:55,249 --> 01:23:00,550
so for multi
classification where each sample can

445
01:23:00,550 --> 01:23:05,590
belong to one or more classes we have to
change a few things but here's the good

446
01:23:05,590 --> 01:23:08,560
news
in fast AI we don't have to change

447
01:23:08,560 --> 01:23:15,940
anything right so fast AI will look at
the labels in the CSV and if there is
more than one label ever for any item it

448
01:23:21,880 --> 01:23:25,600
will automatically switch into like
multi-label mode so I'm going to show

449
01:23:25,600 --> 01:23:29,770
you how it works behind the scenes but
the good news is you don't actually have

450
01:23:29,770 --> 01:23:40,690
to care it happens anywhere so if you
have multi label images multi label
objects you obviously can't use the

451
01:23:43,150 --> 01:23:48,370
classic Kerris style approach where
things are in folders because something

452
01:23:48,370 --> 01:23:55,180
can't conveniently be in multiple
folders at the same time right so that's

453
01:23:55,180 --> 01:24:04,050
why we you basically have to use the
from CSV approach right so if we look at

454
01:24:04,050 --> 01:24:12,160
[Music]
an example actually I'll show you I tend

455
01:24:12,160 --> 01:24:16,420
to take you through it right so we can
say okay this is the CSV file containing

456
01:24:16,420 --> 01:24:21,610
our labels this looks exactly the same
as I did before but rather than side on

457
01:24:21,610 --> 01:24:27,220
its top down alright and top down I've
mentioned before that can do our

458
01:24:27,220 --> 01:24:29,800
vertical flips it actually does more
than that there's actually eight

459
01:24:29,800 --> 01:24:35,140
possible symmetries for a square which
is it can be rotated through 90 180 270

460
01:24:35,140 --> 01:24:39,670
or 0 degrees and for each of those it
can be flipped and if you think about it

461
01:24:39,670 --> 01:24:45,070
for awhile you'll realize that that's a
complete enumeration of everything that

462
01:24:45,070 --> 01:24:50,620
you can do in terms of symmetries to a
square so they're called it's called the

463
01:24:50,620 --> 01:24:55,630
dihedral group of eight so if you see in
the code there's actually a transform or

464
01:24:55,630 --> 01:25:01,690
dihedral that's why it's called that so
this transforms will basically do the
full set of eight symmetric dihedral

465
01:25:05,670 --> 01:25:11,130
rotations and flips plus everything
which we can do to dogs and cats

466
01:25:11,130 --> 01:25:16,290
you know small clinical rotations a
little bit of zooming a little bit of

467
01:25:16,290 --> 01:25:23,340
contrast and brightness adjustment so
these images are a size 256 by 256 so I

468
01:25:23,340 --> 01:25:28,440
just create a little function here to
let me quickly grab you know data loader
of any size so here's a 256 by 256 once

469
01:25:32,429 --> 01:25:37,650
you've got a data object inside it we've
already seen that there's things called

470
01:25:37,650 --> 01:25:43,230
Val D s test D s train D s there are
things that you can just index into and

471
01:25:43,230 --> 01:25:48,360
grab a particular image so you just use
square brackets 0 you'll also see that

472
01:25:48,360 --> 01:25:53,310
all of those things have a DL that's a
data loader so des is data set DL is

473
01:25:53,310 --> 01:25:58,080
data motor these are concepts from PI
watch so if you Google PI torch data set
or pipe watch data loader you can

474
01:26:00,300 --> 01:26:04,650
basically see what it means but the
basic idea is a data set gives you a
single image or a single object back a

475
01:26:08,429 --> 01:26:12,900
data loader gives you back a mini batch
and specifically it gives you back a

476
01:26:12,900 --> 01:26:20,219
transformed mini - so that's why when we
create our data object we can pass in

477
01:26:20,219 --> 01:26:25,560
num workers and transforms it's like how
many processes do you want to use what

478
01:26:25,560 --> 01:26:30,480
transforms do you want and so with with
a data loader you can't ask for an
individual image you can only get back

479
01:26:32,159 --> 01:26:37,260
at a mini batch and you can't get back a
particular mini batch you can only get

480
01:26:37,260 --> 01:26:42,719
back the next mini - so something
reverses look through grabbing a mini

481
01:26:42,719 --> 01:26:47,880
batch at a time and so in Python the
thing that does that is called a

482
01:26:47,880 --> 01:26:51,989
generator right or an iterator this
slightly different versions are the same
thing so to turn a data loader into an

483
01:26:54,480 --> 01:26:57,480
iterator you use the standard Python
function cordetta

484
01:26:57,480 --> 01:27:02,550
that's a Python function just a regular
part of the Python basic language that

485
01:27:02,550 --> 01:27:07,350
returns you an iterator and an iterator
is something that takes you can pass the

486
01:27:07,350 --> 01:27:13,290
static give pass it to the standard
Python function or statement next and

487
01:27:13,290 --> 01:27:18,800
that just says give me another batch
from this iterator

488
01:27:18,800 --> 01:27:23,070
so we're basically this is one of the
things I really like about PI torch is
it really leverages

489
01:27:24,829 --> 01:27:31,489
modern pythons kind of stuff you know in
in tensorflow they invent their whole

490
01:27:31,489 --> 01:27:38,329
new world earth ways of doing things and
so it's kind of more in a sense it's

491
01:27:38,329 --> 01:27:42,349
more like cross-platform but in another
sense like it's not a good fit to any

492
01:27:42,349 --> 01:27:49,880
platform so it's nice if you if you know
Python well PI torch comes very

493
01:27:49,880 --> 01:27:53,960
naturally if you don't know Python well
PI torches are good reason to learn

494
01:27:53,960 --> 01:28:00,559
Python well a PI torch your module
neural network module is a standard

495
01:28:00,559 --> 01:28:05,690
Python bus for example so any work you
put into learning Python better will pay
off with paid watch so here I am using

496
01:28:08,090 --> 01:28:15,619
standard Python iterators and next to
grab my next mini batch from the
validation sets data loader and that's

497
01:28:17,780 --> 01:28:20,960
going to return two things it's going to
return the images in the mini batch and

498
01:28:20,960 --> 01:28:25,360
the labels of the mini - so standard
Python approach I can pull them apart

499
01:28:25,360 --> 01:28:34,099
like so and so here is one mini batch of
labels and so not surprisingly since I

500
01:28:34,099 --> 01:28:40,239
said that my batch size let's go ahead
and find it

501
01:28:41,570 --> 01:28:47,840
Oh actually it's the batch size by
default is 64 so I didn't pass in a

502
01:28:47,840 --> 01:28:52,099
batch size and so just remember shift
tab to see like what are the things you

503
01:28:52,099 --> 01:28:57,710
can pass and what are the defaults so by
default my batch size is 64 so I've got

504
01:28:57,710 --> 01:29:07,040
that something of size 64 by 17 so there
are 17 of the possible classes right so

505
01:29:07,040 --> 01:29:15,829
let's take a look at the zeroth set of
labels so the zeroth images labels so I

506
01:29:15,829 --> 01:29:21,559
can zip again standard Python things it
takes two lists and combines it so you

507
01:29:21,559 --> 01:29:25,489
get the zero theme from the first list
as you're asking for the second list and
the first thing for the first first this

508
01:29:27,440 --> 01:29:31,190
first thing from the second list and so
forth so I can zip them together and

509
01:29:31,190 --> 01:29:36,790
that way I can find out for the zeroth
image and the validation set is

510
01:29:36,790 --> 01:29:42,599
agriculture
it's clear its primary rainforest its

511
01:29:42,599 --> 01:29:51,869
slash-and-burn its water okay so as you
can see here this is a MOLLE label you
see here's a way to do multi-label

512
01:29:53,219 --> 01:30:01,080
classification so by the same token
right if we go back to our single label

513
01:30:01,080 --> 01:30:07,230
classification it's a cat dog playing
official building behind the scenes we

514
01:30:07,230 --> 01:30:12,599
haven't actually looked at it but behind
the scenes fast AI imply torch are

515
01:30:12,599 --> 01:30:18,869
turning our labels into something called
one hot encoded labels and so if it was
actually a dog than the actual values

516
01:30:22,940 --> 01:30:29,699
would be like that right so these are
like the actuals okay so do you remember

517
01:30:29,699 --> 01:30:34,139
at the very end of at AV o--'s video he
showed how like the template had to

518
01:30:34,139 --> 01:30:39,119
match to one of the like five ABCDE
templates and so what it's actually

519
01:30:39,119 --> 01:30:44,489
doing is it's comparing when I said it's
basically doing a dot product it's

520
01:30:44,489 --> 01:30:50,040
actually a fully connected layer at the
end right that calculates an output

521
01:30:50,040 --> 01:30:56,940
activation that goes through a soft Max
and then the soft max is compared to the

522
01:30:56,940 --> 01:31:03,540
one hot encoded label right so if it was
a dog there would be a one here and then

523
01:31:03,540 --> 01:31:08,670
we take the difference between the
actuals and the softmax activation is to

524
01:31:08,670 --> 01:31:12,179
say and add those add up those
differences to say how much error is

525
01:31:12,179 --> 01:31:16,110
there essentially we're skipping over
something called a loss function that

526
01:31:16,110 --> 01:31:21,320
we'll learn about next week but
essentially we're basically doing that

527
01:31:21,680 --> 01:31:28,250
now if it's one hot encoded like there's
only one thing which have a 1 in it then

528
01:31:28,250 --> 01:31:36,530
actually storing it as 0 1 0 0 0 is
terribly inefficient right like we could

529
01:31:36,530 --> 01:31:40,700
basically say what are the index of each
of these things right so we can say it's

530
01:31:40,700 --> 01:31:48,710
like 0 1 2 3 4 like so right and so
rather than storing it is 0 1 0 0 0

531
01:31:48,710 --> 01:31:56,420
we actually just store the index value
right so if you look at the the Y values

532
01:31:56,420 --> 01:32:01,160
for the cats and dogs competition or the
dog breeds competition you won't

533
01:32:01,160 --> 01:32:04,850
actually see a big lists of ones and
zeros like this you'll see a single

534
01:32:04,850 --> 01:32:11,680
integer right which is like what what
class index is it right and internally

535
01:32:11,680 --> 01:32:17,750
inside pipe arch it will actually turn
that into a one hot encoded vector but

536
01:32:17,750 --> 01:32:23,270
like you will literally never see it
okay and and pi torch has different loss

537
01:32:23,270 --> 01:32:27,860
functions where you basically say this
thing's won this thing is one hot
encoder door this thing is not and it

538
01:32:30,050 --> 01:32:34,160
uses different bus functions
that's all hidden by the faster I

539
01:32:34,160 --> 01:32:40,040
library right so like you don't have to
worry about it but is but the the cool

540
01:32:40,040 --> 01:32:45,350
thing to realize is that this approach
for multi-label encoding with these ones

541
01:32:45,350 --> 01:32:51,080
and zeros behind the scenes the exact
same thing happens for single level

542
01:32:51,080 --> 01:32:59,000
classification does it make sense to
change the beginners of the sigmoid of

543
01:32:59,000 --> 01:33:07,940
the softmax function by changing the
base no because when you change the more

544
01:33:07,940 --> 01:33:24,200
math log base a of B equals log B over
log a so changing the base is just a

545
01:33:24,200 --> 01:33:29,670
linear scaling and linear scaling is
something which the neural net can

546
01:33:29,670 --> 01:33:33,110
with very easily

547
01:33:35,450 --> 01:33:44,250
good question okay so here is that image
right here is the image with

548
01:33:44,250 --> 01:33:49,650
slash-and-burn water etc etc one of the
things to notice here is like when I

549
01:33:49,650 --> 01:33:56,070
first displayed this image it was so
washed out I really couldn't see it

550
01:33:56,070 --> 01:34:02,970
right but remember images now you know
we know images are just matrices of

551
01:34:02,970 --> 01:34:08,670
numbers and so you can see here I just
said times 1.4 just to make it more

552
01:34:08,670 --> 01:34:12,930
visible right so like now that you kind
of it's the kind of thing I want you to

553
01:34:12,930 --> 01:34:15,840
get familiar with is the idea that this
stuff you're dealing with

554
01:34:15,840 --> 01:34:19,710
they're just matrices of numbers and you
can fiddle around with them so if you're

555
01:34:19,710 --> 01:34:22,830
looking at something like guys a bit
washed out you can just multiply it by

556
01:34:22,830 --> 01:34:27,990
something to brighten it up a bit okay
so here we can see I guess this is the

557
01:34:27,990 --> 01:34:33,300
slash-and-burn here's the river that's
the water here's the primary rainforest

558
01:34:33,300 --> 01:34:41,010
maybe that's the agriculture so forth
okay so so you know with all that

559
01:34:41,010 --> 01:34:46,350
background how do we actually use this
exactly the same way as everything we've

560
01:34:46,350 --> 01:34:52,170
done before right so you know size and
and the interesting thing about playing

561
01:34:52,170 --> 01:34:56,370
around with this planet competition is
that these images are not at all like

562
01:34:56,370 --> 01:35:02,580
image there and I would guess that the
vast majority is of stuff that the vast

563
01:35:02,580 --> 01:35:07,770
majority of you do involving
convolutional neural Nets won't actually

564
01:35:07,770 --> 01:35:14,130
be anything like image net you know
it'll be it'll be medical imaging it'll

565
01:35:14,130 --> 01:35:18,530
be like classifying different kinds of
steel tube or figuring out whether a

566
01:35:18,530 --> 01:35:25,650
world you know is going to break or not
or or looking at satellite images or you

567
01:35:25,650 --> 01:35:32,690
know whatever right so it's it's good to
experiment with stuff like this planet

568
01:35:32,690 --> 01:35:37,170
competition to get a sense of kind of
what you want to do and so you'll see

569
01:35:37,170 --> 01:35:44,910
here I start out by resizing my data to
64 by 64 it starts out at 256 by 256

570
01:35:44,910 --> 01:35:48,270
right now
I wouldn't want to do this for the cats

571
01:35:48,270 --> 01:35:52,620
and dogs competition because it cats end
on competition we start with a pre
trained imagenet Network it's it's

572
01:35:54,360 --> 01:36:00,060
nearly isn't it starts off nearly
perfect right so if we resized

573
01:36:00,060 --> 01:36:04,980
everything to 64 by 64 and then
retrained the whole set regular it we'd
basically destroy the weights that are

574
01:36:07,440 --> 01:36:12,000
already pre trained to be very good
remember imagenet most imagenet models

575
01:36:12,000 --> 01:36:17,760
are trained at either 224 by 224 or
$2.99 by 299 all right so if we like

576
01:36:17,760 --> 01:36:23,480
retrain them at 64 by 64 we're going to
we're going to kill it on the other hand
there's nothing in image net that looks

577
01:36:25,920 --> 01:36:31,260
anything like this you know there's no
satellite images so the only useful bits

578
01:36:31,260 --> 01:36:39,870
of the image net Network for us are kind
of layers like this one you know finding

579
01:36:39,870 --> 01:36:44,460
edges and gradients and this one you
know finding kind of textures and

580
01:36:44,460 --> 01:36:49,890
repeating patterns and maybe these ones
are kind of finding more complex

581
01:36:49,890 --> 01:36:57,530
textures but that's probably about it
right so so in other words you know
starting out by training very small

582
01:37:00,210 --> 01:37:04,920
images works pretty well when you're
using stuff like satellites so in this

583
01:37:04,920 --> 01:37:13,410
case I started right back at 64 by 64
grab some data built my model found out

584
01:37:13,410 --> 01:37:17,280
what learning rate to use and
interestingly it turned out to be quite

585
01:37:17,280 --> 01:37:25,920
high it seems that because like it's so
unlike imagenet I needed to do quite a

586
01:37:25,920 --> 01:37:30,390
bit more fitting with just that last
layer before it started to flatten out

587
01:37:30,390 --> 01:37:38,090
then I unfreeze dit and again this is
the difference to image net like

588
01:37:38,090 --> 01:37:44,190
datasets is my learning rate in the
initial layer i set 2/9 the middle
layers I said 2/3 where else for stuff

589
01:37:47,190 --> 01:37:52,410
like it's like image net I had a
multiple of 10 each of those you know

590
01:37:52,410 --> 01:37:58,140
again the idea being that that earlier
layers probably and not as close to what

591
01:37:58,140 --> 01:38:04,620
they need to be compared to the
like dances again

592
01:38:04,620 --> 01:38:09,120
unfreeze train for a while and you can
kind of see here

593
01:38:09,120 --> 01:38:14,820
you know there's cycle one there's cycle
- there's cycle three and then I kind of

594
01:38:14,820 --> 01:38:21,840
increased double the size with my images
fit for a while and freeze fit for a

595
01:38:21,840 --> 01:38:25,950
while double the size of the images
again fit for a while I'm freeze for a

596
01:38:25,950 --> 01:38:30,090
while and then add TTA and so as I
mentioned last time we looked at this

597
01:38:30,090 --> 01:38:35,300
this process ends up you know getting us
about 30th place in this competition

598
01:38:35,300 --> 01:38:39,440
which is really cool because people you
know a lot of very very smart people
just a few months ago worked very very

599
01:38:42,150 --> 01:38:51,140
hard on this competition a couple of
things people have asked about one is

600
01:38:51,200 --> 01:38:59,160
what is this data dot resize do so a
couple of different pieces here the

601
01:38:59,160 --> 01:39:07,650
first is that when we say back here what
transforms do we apply and here's our

602
01:39:07,650 --> 01:39:12,810
transforms we actually pass in a size
right so one of the things that that one

603
01:39:12,810 --> 01:39:17,130
of the things that data loaded does is
to resize the images like on-demand

604
01:39:17,130 --> 01:39:20,300
every time it sees them

605
01:39:21,510 --> 01:39:25,710
it's got nothing to do with that dot
resize method right so this is this is

606
01:39:25,710 --> 01:39:29,699
the thing that happens at the end like
whatever's passed in before it hits out

607
01:39:29,699 --> 01:39:35,099
that before our data loader spits it out
it's going to resize it to this size if

608
01:39:35,099 --> 01:39:41,659
the initial input is like a thousand by
a thousand reading that JPEG and
resizing it to 64 by 64 turns out to

609
01:39:45,989 --> 01:39:51,300
actually take more time than training
the content that's for each batch all

610
01:39:51,300 --> 01:39:57,150
right so basically all resize does is it
says hey I'm not going to be using any
images bigger than size times 1.3 so

611
01:40:01,320 --> 01:40:08,400
just grow through once and create new
JPEGs of this size right and they're

612
01:40:08,400 --> 01:40:14,070
rectangular right so new JPEGs where the
smallest edges of this size and again

613
01:40:14,070 --> 01:40:19,079
it's like you never have to do this
there's no reason to ever use it if you

614
01:40:19,079 --> 01:40:23,280
don't want to it's just a speed-up okay
but if you've got really big images

615
01:40:23,280 --> 01:40:27,300
coming in it saves you a lot of time and
you'll often see on like Carol kernels

616
01:40:27,300 --> 01:40:34,409
or forum posts or whatever people will
have like bash script stuff like that -
like loop through and resize images to

617
01:40:36,809 --> 01:40:40,440
save time you never have to do that
right just you can just say dot resize
and it'll just create you know once-off

618
01:40:44,219 --> 01:40:48,389
it'll go through and create that if it's
already there and it'll use the

619
01:40:48,389 --> 01:40:53,880
criticized ones for you okay so it's
just it's just a speed up convenience

620
01:40:53,880 --> 01:41:05,119
function no more okay so for those of
you that are kind of past dog breeds I

621
01:41:05,119 --> 01:41:13,980
would be looking at planet next you know
like try it like play around with with

622
01:41:13,980 --> 01:41:19,170
trying to get a sense of like how can
you get this as an accurate model one

623
01:41:19,170 --> 01:41:21,840
thing to mention and I'm not really
going to go into it in details there's

624
01:41:21,840 --> 01:41:24,900
nothing to do with deep learning
particularly is that I'm using a
different metric I didn't use metrics

625
01:41:26,880 --> 01:41:32,570
equals accuracy but I said metrics
equals f2

626
01:41:33,980 --> 01:41:39,400
remember from last week that confusion
matrix that like two by two you know

627
01:41:39,400 --> 01:41:47,090
correct incorrect for each of dogs and
cats there's a lot of different ways you

628
01:41:47,090 --> 01:41:51,140
could turn that confusion matrix into a
score you know do you care more about

629
01:41:51,140 --> 01:41:54,260
false negatives or do you care more
about false positives and how do you

630
01:41:54,260 --> 01:41:59,390
weight them and how do you combine them
together right there's a basic there's

631
01:41:59,390 --> 01:42:04,370
basically a function called F beta where
the beta says how much do you weight

632
01:42:04,370 --> 01:42:11,540
false negatives versus false positives
and so f 2 is f beta with beta equals 2

633
01:42:11,540 --> 01:42:15,140
and it's basically as particular way of
weighting false negatives and false

634
01:42:15,140 --> 01:42:19,700
positives and the reason we use it is
because cattle told us that planet who
are running this competition wanted to

635
01:42:21,739 --> 01:42:28,730
use this particular F beta metric the
important thing for you to know is that

636
01:42:28,730 --> 01:42:33,110
you can create custom metrics so in this
case you can see here it says from

637
01:42:33,110 --> 01:42:38,120
Planet import f2 and really I've got
this here so that you can see how to do

638
01:42:38,120 --> 01:42:47,270
it right so if you look inside courses
DL 1 you can see there's something

639
01:42:47,270 --> 01:42:55,540
called planet py right and so if I look
at planet py you'll see there's a
function there called

640
01:42:57,320 --> 01:43:08,690
f2 right and so f2 simply calls F beta
score from psychic or side PI and patent

641
01:43:08,690 --> 01:43:13,400
where it came from and does a couple
little tweets that are particularly

642
01:43:13,400 --> 01:43:19,340
important but the important thing is
like you can write any metric you like

643
01:43:19,340 --> 01:43:25,970
right as long as it takes in set of
predictions and a set of targets and

644
01:43:25,970 --> 01:43:30,530
they're both going to be numpy arrays
one dimensional non pyros and then you
return back a number okay and so as long

645
01:43:33,380 --> 01:43:38,930
as you put a function that takes two
vectors and returns at number you can

646
01:43:38,930 --> 01:43:44,290
call it as a metric and so then when we
said

647
01:43:46,710 --> 01:43:54,490
see here learn metrics equals and then
past in that array which just contains a

648
01:43:54,490 --> 01:44:00,580
single function f2 then it's just going
to be printed out after every for you

649
01:44:00,580 --> 01:44:06,970
okay so in general like the the faster I
library everything is customizable so

650
01:44:06,970 --> 01:44:15,940
kind of the idea is that everything is
everything is kind of gives you what you
might want by default but also

651
01:44:18,520 --> 01:44:26,470
everything can be changed as well yes
you know um we have a little confusion

652
01:44:26,470 --> 01:44:32,890
about the difference between multi-label
and a single label uh-huh

653
01:44:32,890 --> 01:44:38,140
the vanish as an example in which
compared like similarly the example they

654
01:44:38,140 --> 01:44:47,080
just show us ah activation function yeah
so so I'm so sorry I said I'd do that

655
01:44:47,080 --> 01:44:52,180
then I didn't so the activation the
output activation function for a single

656
01:44:52,180 --> 01:44:58,390
label classification is softmax but all
the reasons that we talked today but if

657
01:44:58,390 --> 01:45:05,350
we were trying to predict something that
was like 0 0 1 1 0 then softmax would be

658
01:45:05,350 --> 01:45:09,100
a terrible choice because it's very hard
to come up with something where both of

659
01:45:09,100 --> 01:45:13,510
these are high in fact it's impossible
because they have to add up to 1 so the

660
01:45:13,510 --> 01:45:19,690
closest they could be would be point 5
so for multi-label classification our
activation function is called sigmoid ok

661
01:45:23,650 --> 01:45:28,690
and again the faster library does this
automatically for you if it notices you

662
01:45:28,690 --> 01:45:34,210
have a multi label problem and it does
that by checking your data tip to see if

663
01:45:34,210 --> 01:45:40,870
anything has more than one label applied
to it and so sigmoid is a function which

664
01:45:40,870 --> 01:45:48,240
is equal to it's basically the same
thing except rather than we never add up

665
01:45:48,240 --> 01:45:55,170
all of these X but instead we just take
this X when we say it's just equal to it

666
01:45:55,170 --> 01:46:05,119
divided by one plus
it and so the nice thing about that is

667
01:46:05,119 --> 01:46:15,439
that now like multiple things can be
high at once right and so generally then

668
01:46:15,439 --> 01:46:21,229
if something is less than zero its
sigmoid is going to be less than 0.5 if

669
01:46:21,229 --> 01:46:26,300
it's greater than zero is signal it's
going to be greater than 0.5 and so the

670
01:46:26,300 --> 01:46:34,570
important thing to know about a sigmoid
function is that its shape is

671
01:46:36,050 --> 01:46:44,560
something which asymptotes at the top to
one and asymptotes drew

672
01:46:45,869 --> 01:46:53,440
asymptotes at the bottom to zero and so
therefore it's a good thing to model a

673
01:46:53,440 --> 01:47:01,960
probability with anybody who has done
any logistic regression will be familiar

674
01:47:01,960 --> 01:47:05,440
with this is what we do in logistic
regression so it kind of appears

675
01:47:05,440 --> 01:47:08,949
everywhere in machine learning and
you'll see that kind of a sigmoid and a

676
01:47:08,949 --> 01:47:15,880
softmax they're very close to each other
conceptually but this is what we want is

677
01:47:15,880 --> 01:47:19,869
our activation function for multi-label
and this is what we want the single

678
01:47:19,869 --> 01:47:22,659
label and again first AI does it all for
you

679
01:47:22,659 --> 01:47:34,059
there was a question over here yes I
have a question about the initial

680
01:47:34,059 --> 01:47:39,659
training that you do if I understand
correctly you have we have frozen the
the premium model and you only need

681
01:47:42,989 --> 01:47:50,920
initially try to train the latest
playwright right but from the other hand

682
01:47:50,920 --> 01:47:55,960
we said that only the initial layer so
let's last probably the first layer is

683
01:47:55,960 --> 01:48:01,599
like important to us and the other two
are more like features that are you must

684
01:48:01,599 --> 01:48:07,119
not related and we then apply in this
case what that they the lie is a very
important but the pre-trained weights in

685
01:48:10,840 --> 01:48:16,210
them aren't so it's the later layers
that we really want to train the most so

686
01:48:16,210 --> 01:48:23,769
earlier layers likely to be like already
closer to what we want okay so you

687
01:48:23,769 --> 01:48:27,519
started with the latest one and then you
go right so if you go back to our quick

688
01:48:27,519 --> 01:48:33,820
dogs and cats right when we create a
model from pre train from a pre train
model it returns something where all of

689
01:48:36,190 --> 01:48:42,969
the convolutional layers are frozen and
some randomly set fully connected layers

690
01:48:42,969 --> 01:48:50,309
we add to the end our unfrozen and so
when we go fit at first it just trains

691
01:48:50,309 --> 01:48:56,920
the randomly set a randomly initialized
fully connected letters right

692
01:48:56,920 --> 01:49:02,530
and if something is like really close to
imagenet that's often all we need

693
01:49:02,530 --> 01:49:08,380
but because the early early layers are
already good at finding edges gradients

694
01:49:08,380 --> 01:49:16,830
repeating patterns for ears and dogs
heads you know so then when we unfreeze

695
01:49:16,830 --> 01:49:22,960
we set the learning rates for the early
layers to be really low because we don't

696
01:49:22,960 --> 01:49:28,270
want to change the mesh for us the later
ones we set them to be higher where else

697
01:49:28,270 --> 01:49:34,690
for satellite data right this is no
longer true you know the early layers
are still like better than the later

698
01:49:37,420 --> 01:49:42,730
layers but we still probably need to
change them quite a bit so that's right
this learning rate is nine times smaller

699
01:49:45,460 --> 01:49:52,150
than the final learning rate rather than
a thousand times smaller the final loan

700
01:49:52,150 --> 01:50:00,219
rate okay you play with with the weights
of the layers yeah normally most of the

701
01:50:00,219 --> 01:50:03,989
stuff you see online if they talk about
this at all they'll talk about
unfreezing different subsets of layers

702
01:50:07,300 --> 01:50:13,960
and indeed we do unfreeze our randomly
generated runs but what I found is

703
01:50:13,960 --> 01:50:18,250
although the first layer library you can
type learn dot freeze too and just

704
01:50:18,250 --> 01:50:22,449
freeze a subset of layers this approach
of using differential learning rates

705
01:50:22,449 --> 01:50:28,179
seems to be like more flexible to the
point that I never find myself I'm

706
01:50:28,179 --> 01:50:34,960
freezing subsets of layers that I would
expect you to start with that with a

707
01:50:34,960 --> 01:50:40,270
different cell the different learning
rates rather than trying to learn the

708
01:50:40,270 --> 01:50:48,190
last layer so the reason okay so you
could skip this training just the last
layers and just go straight to

709
01:50:49,719 --> 01:50:53,890
differential learning rates but you
probably don't want to and the reason

710
01:50:53,890 --> 01:50:57,610
you probably don't want to is that
there's a difference the convolutional
layers all contain pre-trained weights

711
01:51:00,910 --> 01:51:05,290
so they're like they're not random for
things that are close to imagenet

712
01:51:05,290 --> 01:51:09,190
they're actually really good for things
that are not close to imagenet they're

713
01:51:09,190 --> 01:51:13,250
better than that
all of our fully connected layers

714
01:51:13,250 --> 01:51:19,969
however are totally random so therefore
you would always want to make the fully

715
01:51:19,969 --> 01:51:24,050
connected weights better than random by
training them a bit first because

716
01:51:24,050 --> 01:51:28,840
otherwise if you go straight to unfreeze
then you're actually going to be like

717
01:51:28,840 --> 01:51:34,369
fiddling around of those early early can
early layer weights when the later ones
are still random that's probably not

718
01:51:35,809 --> 01:51:41,900
what you want I think that's another
question here any possible so when we
unfreeze what are the things we're

719
01:51:46,909 --> 01:51:54,349
trying to change there will it change
the Colonel's themselves that that's

720
01:51:54,349 --> 01:52:02,869
always what SGD does yeah so the only
thing what training means is setting
these numbers right and these numbers

721
01:52:09,619 --> 01:52:19,699
and these numbers the weights so the
weights are the weights of the fully

722
01:52:19,699 --> 01:52:24,320
connected layers and the weights in
those kernels and the convolutions so

723
01:52:24,320 --> 01:52:29,750
that's what training means it's and
we'll learn about how to do it with SGD

724
01:52:29,750 --> 01:52:34,730
but training literally is setting those
numbers these numbers on the other hand

725
01:52:34,730 --> 01:52:41,329
are activations they're calculated
they're calculated from the weights and

726
01:52:41,329 --> 01:52:49,250
the previous layers activations or
amounts of questions so you can lift it

727
01:52:49,250 --> 01:52:52,670
up higher and speak badly so in your
example of a cheerleader set of that

728
01:52:52,670 --> 01:52:58,610
English example so you start with very
small size existed for yeah so does it

729
01:52:58,610 --> 01:53:03,559
literally mean you know the model takes
a small area from the entire image that

730
01:53:03,559 --> 01:53:13,489
is 64 bytes so how do we get that 64 by
64 depends on the transforms by default
our transform takes the smallest edge

731
01:53:18,019 --> 01:53:24,070
and recites the whole thing out
samples it so the smallest edge is
societics t4 and then it takes a Center

732
01:53:27,010 --> 01:53:33,099
crop of that okay
although when we're using data

733
01:53:33,099 --> 01:53:40,539
augmentation it actually takes a
randomly chosen prop ie the case where

734
01:53:40,539 --> 01:53:46,780
the image ties to multiple objects don't
in this case like would it be possible

735
01:53:46,780 --> 01:53:50,170
that you would just lose the other
things that they try to predict yeah

736
01:53:50,170 --> 01:53:54,070
which is why data augmentation is
important so by by and particularly

737
01:53:54,070 --> 01:53:59,289
their test time augmentation is going to
be particularly important because you

738
01:53:59,289 --> 01:54:03,880
would you wouldn't want to you know that
there may be a artisanal mine out in the

739
01:54:03,880 --> 01:54:08,860
corner which if you take a center crop
you you don't see so data augmentation

740
01:54:08,860 --> 01:54:17,499
becomes very important yeah so when we
talk on their tributaries are he

741
01:54:17,499 --> 01:54:23,230
receiver up to that's not really what a
model choice Delton that's a great point

742
01:54:23,230 --> 01:54:27,039
that's not the loss function yeah right
the loss function is something we'll be

743
01:54:27,039 --> 01:54:33,429
learning about next week and it uses
cross entropy or otherwise known as like

744
01:54:33,429 --> 01:54:39,249
negative log likelihood the metric is
just this thing that's printed so we can

745
01:54:39,249 --> 01:54:47,079
see what's going on just next to that so
in the context of my deep pass modeling

746
01:54:47,079 --> 01:54:51,280
cannot change data does it trading it
also have to be multiplied so can I

747
01:54:51,280 --> 01:54:54,909
train on just like images of pure cats
and dogs and expect it at prediction

748
01:54:54,909 --> 01:55:04,030
time to predict if I give it a picture
of both having cat eye on it over I've

749
01:55:04,030 --> 01:55:08,409
never tried that and I've never seen an
example of something that needed it

750
01:55:08,409 --> 01:55:14,949
I guess conceptually there's no reason
it wouldn't work but it's kind of out

751
01:55:14,949 --> 01:55:19,389
there and you still use a sigmoid you
would have to make sure you're using a

752
01:55:19,389 --> 01:55:22,659
sigmoid loss function so in this case
faster eyes default would not work
because by default first day I would say

753
01:55:24,579 --> 01:55:28,150
your training data knitter has both a
cat and the dog so you would have to

754
01:55:28,150 --> 01:55:31,230
override the loss function

755
01:55:35,349 --> 01:55:40,729
when you use the differential learning
rates those three learning rates do they

756
01:55:40,729 --> 01:55:46,550
just kind of spread evenly across the
layers yeah we'll talk more about this

757
01:55:46,550 --> 01:55:50,689
later in the course but I mean the
faster I library there's a concept of

758
01:55:50,689 --> 01:55:55,579
layer groups so in something like a
resonant 50 you know there's hundreds of
layers and I think it you don't want to

759
01:55:57,889 --> 01:56:03,679
write down hundreds of learning rates so
I've basically decided for you how to

760
01:56:03,679 --> 01:56:10,219
split them and the the last one always
refers just to the fully connected

761
01:56:10,219 --> 01:56:14,929
layers that we've randomly initialized
and edit to the end and then these ones

762
01:56:14,929 --> 01:56:19,669
are split generally about halfway
through basically I've tried to make it

763
01:56:19,669 --> 01:56:24,019
so that these you know these ones are
kind of the ones which you hardly want

764
01:56:24,019 --> 01:56:26,630
to change at all and these are the ones
you might want to change a little bit

765
01:56:26,630 --> 01:56:30,469
and I don't think we're covered in the
course but if you're interested we can

766
01:56:30,469 --> 01:56:33,860
talk about in the forum there are ways
you can override this behavior to define

767
01:56:33,860 --> 01:56:38,869
your own layer groups if you want to and
is there any way to visualize the model

768
01:56:38,869 --> 01:56:43,999
easily or like don't dump the layers of
the model yeah absolutely

769
01:56:43,999 --> 01:56:54,229
you can let's make sure we've got one
here okay so if you just type learn it

770
01:56:54,229 --> 01:57:02,479
doesn't tell you much at all but what
you can do is go learn summary and that

771
01:57:02,479 --> 01:57:09,590
spits out basically everything there's
all the letters and so you can see in

772
01:57:09,590 --> 01:57:14,419
this case these are the names I
mentioned how they look up names right

773
01:57:14,419 --> 01:57:21,919
so the first layer is called con 2 d 1
and it's going to take as input this is

774
01:57:21,919 --> 01:57:27,110
useful to actually look at it's taking
64 by 64 images which is what we told it

775
01:57:27,110 --> 01:57:33,260
we're going to transform things to this
is three channels high torch like most

776
01:57:33,260 --> 01:57:38,449
things have channels at the end would
say 64 by 64 by 3 ply torch music to the
front so it's 3 by 64 by 64 that's

777
01:57:41,869 --> 01:57:46,820
because it turns out that some of the
GPU computations run faster when it

778
01:57:46,820 --> 01:57:50,900
in that order okay but that happens all
behind-the-scenes automatic plays a part

779
01:57:50,900 --> 01:57:56,260
of that transformation stuff that's kind
of all done automatically is to do that

780
01:57:56,260 --> 01:58:04,280
minus one means however however big the
batch size is in care us they use the
number they use a special number none in

781
01:58:07,580 --> 01:58:13,130
pile types that used minus one so this
is a four dimensional mini batch the

782
01:58:13,130 --> 01:58:18,290
number of elements in the amount of
images in the mini batch is dynamic you

783
01:58:18,290 --> 01:58:23,660
can change that the number of channels
is three number which is a 64 by 64 okay

784
01:58:23,660 --> 01:58:29,200
and so then you can basically see that
this particular convolutional kernel

785
01:58:29,200 --> 01:58:35,240
apparently has 64 kernels in it and it's
also having we haven't talked about this

786
01:58:35,240 --> 01:58:39,200
but convolutions can have something
called a stride that is like Matt pullin

787
01:58:39,200 --> 01:58:47,870
it changes the size so it's returning a
32 by 32 564 kernel tenza and so on and

788
01:58:47,870 --> 01:58:54,080
so forth so that's summary and we'll
learn all about that's doing in detail

789
01:58:54,080 --> 01:59:00,770
on in the second half of the course one
where I clicked in my own data set and I

790
01:59:00,770 --> 01:59:06,260
try to use the in as a really small data
set these currencies from Google Images

791
01:59:06,260 --> 01:59:12,650
and I tried to do a learning rate find
and then the plot and it just it gave me
some numbers which I didn't understand

792
01:59:14,030 --> 01:59:18,260
and the learning rate font yeah and then
the plot was empty so yeah I mean let's

793
01:59:18,260 --> 01:59:22,910
let's talk about that on the forum but
basically the learning rate finder is

794
01:59:22,910 --> 01:59:26,450
going to go through a mini batch at a
time if you've got a tiny data set

795
01:59:26,450 --> 01:59:30,560
there's just not enough mini batches so
the trick is to make your mini bit make

796
01:59:30,560 --> 01:59:35,530
your batch size really small like try
making it like four

797
01:59:38,250 --> 01:59:44,440
okay they were great questions it's not
nothing online to add you know they were

798
01:59:44,440 --> 01:59:48,160
great questions we've got a little bit
past where I hope to but let's let's

799
01:59:48,160 --> 01:59:52,690
quickly talk about structured data so we
can start thinking about it for next

800
01:59:52,690 --> 02:00:02,500
week so this is really weird right to me
there's basically two types of data set

801
02:00:02,500 --> 02:00:10,300
we use in machine learning there's a
type of data like audio images natural

802
02:00:10,300 --> 02:00:16,600
language text where all of the all of
the things inside an object like all of

803
02:00:16,600 --> 02:00:21,850
the pixels inside an image are all the
same kind of thing they're all pixels or

804
02:00:21,850 --> 02:00:30,130
they're all apertures of a waveform or
they're all words I call this kind of

805
02:00:30,130 --> 02:00:37,199
data unstructured and then there's data
sets like a profit and loss statement or
the information about a Facebook user

806
02:00:39,960 --> 02:00:45,520
where each column is like structurally
quite different you know one thing is

807
02:00:45,520 --> 02:00:49,449
representing like how many page views
last month another one is their sex
another one is what zip code they're in

808
02:00:51,550 --> 02:00:57,940
and I call this structure there that
particular terminology is not unusual
like lots of people use that terminology

809
02:01:00,480 --> 02:01:05,489
but lots of people don't
there's no particularly agreed-upon

810
02:01:05,489 --> 02:01:11,770
terminology so when I say structured
data I'm referring to kind of columnar

811
02:01:11,770 --> 02:01:16,210
data as you might find in a database or
a spreadsheet where different columns

812
02:01:16,210 --> 02:01:21,840
represent different kinds of things and
each row represents an observation and

813
02:01:21,840 --> 02:01:31,199
so structured data is probably what most
of you are analyzing most of the time
funnily enough you know academics in the

814
02:01:34,810 --> 02:01:39,730
deep learning world don't really give a
about structured data because it's
pretty hard to get published in fancy

815
02:01:41,590 --> 02:01:46,179
conference proceedings if you're like if
you've got a better logistics model you

816
02:01:46,179 --> 02:01:49,719
know it's the thing that makes the world
goes round it's a thing that makes

817
02:01:49,719 --> 02:01:55,880
everybody you know
and efficiency and make stuff work but

818
02:01:55,880 --> 02:02:01,309
it's largely ignored sadly so we're not
going to ignore it because we're a

819
02:02:01,309 --> 02:02:05,749
practical deep learning and cackled
doesn't ignore it either because people

820
02:02:05,749 --> 02:02:10,070
put prize money up on Cagle to solve
real-world problems so there are some

821
02:02:10,070 --> 02:02:14,780
great capable competitions we can look
at there's one running right now which

822
02:02:14,780 --> 02:02:21,130
is the grocery sales forecasting
competition for Ecuador's largest chain

823
02:02:21,399 --> 02:02:26,360
it's always a little I've got to be a
little careful about how much I show you
about currently running competitions

824
02:02:28,099 --> 02:02:32,659
because I don't want to you know help
you cheat but it so happens there was a
competition a year or two ago for one of

825
02:02:35,989 --> 02:02:40,099
Germany's magistrate chains which is
almost identical so I'm going to show

826
02:02:40,099 --> 02:02:50,539
you how to do that so that was called
the Rossman stores data and so I would

827
02:02:50,539 --> 02:02:53,780
suggest you know first of all try
practicing what we're learning on

828
02:02:53,780 --> 02:02:58,579
Russman right but then see if you can
get it working on on grocery because

829
02:02:58,579 --> 02:03:03,320
currently on the leaderboard no one
seems to basically know what they're

830
02:03:03,320 --> 02:03:10,130
doing in the groceries competition if
you look at the leaderboard the let's

831
02:03:10,130 --> 02:03:15,769
see here yeah these ones around 5 to 9 v
3o are people that are literally finding

832
02:03:15,769 --> 02:03:19,849
like group averages and submitting those
I know because they're kernels that

833
02:03:19,849 --> 02:03:24,369
they're using so you know the basically
the people around 20th place are not
actually doing any machine learning so

834
02:03:29,059 --> 02:03:34,639
yeah let's see if we can improve things
so you'll see there's a less than 3

835
02:03:34,639 --> 02:03:40,189
rossmann notebook sure you get pull ok
in fact you know just reminder you know

836
02:03:40,189 --> 02:03:45,289
before you start working get pull in
you're faster a repo and from time to
time Condor and update for you guys

837
02:03:48,889 --> 02:03:52,849
during the in-person course the Condor
and update you should do it more often

838
02:03:52,849 --> 02:03:57,769
because we're kind of changing things a
little bit um folks in the MOOC you know

839
02:03:57,769 --> 02:04:04,010
more like once a month should be fine
so anyway I just just changed this a

840
02:04:04,010 --> 02:04:09,560
little bit so make sure you get Paul to
get lesson 3 Rossman and there's a

841
02:04:09,560 --> 02:04:14,600
couple of new libraries here one is fast
AI dot structure faster guided

842
02:04:14,600 --> 02:04:18,950
structured contain stuff which is
actually not at all high torch specific
and we actually use that in the machine

843
02:04:20,600 --> 02:04:25,000
learning course as well for doing random
forests with no tie torch at all I

844
02:04:25,000 --> 02:04:30,230
mentioned that because you can use that
particular library without any of the

845
02:04:30,230 --> 02:04:36,080
other parts of fast AI so that can be
handy and then we're also going to use

846
02:04:36,080 --> 02:04:41,630
faster column data which is basically
some stuff that allows us to do fast a

847
02:04:41,630 --> 02:04:49,100
type a torch stuff with columnar
structured data for structured data we

848
02:04:49,100 --> 02:04:54,710
need to use pandas a lot anybody who's
used our data frames will be very

849
02:04:54,710 --> 02:04:59,240
familiar with pandas pandas is basically
an attempt to kind of replicate data

850
02:04:59,240 --> 02:05:08,240
friends in Python you know and a bit
more if you're not entirely familiar

851
02:05:08,240 --> 02:05:13,690
with pandas there's a great book

852
02:05:14,730 --> 02:05:18,020
[Music]
which I think I might have mentioned

853
02:05:18,020 --> 02:05:24,920
before - for data analysis by Wes
McKinney there's a new addition that

854
02:05:24,920 --> 02:05:30,619
just came out a couple of weeks ago
obviously being by the pandas author its

855
02:05:30,619 --> 02:05:37,340
coverage of pandas is excellent but it
also covers numpy scipy matplotlib

856
02:05:37,340 --> 02:05:45,940
scikit-learn - and Jupiter really well
okay and so I'm kind of going to assume
that you know your way around these

857
02:05:48,949 --> 02:05:54,980
libraries to some extent also there was
the workshop we did before they started

858
02:05:54,980 --> 02:05:59,030
and there's a video of that online where
we kind of have a brief mention of all

859
02:05:59,030 --> 02:06:06,710
of those tools structured data is
generally shared as CSV files it was no

860
02:06:06,710 --> 02:06:10,610
different in this competition as you'll
see there's a hyperlink to the rustman

861
02:06:10,610 --> 02:06:15,500
data set here right now if you look at
the bottom of my screen you'll see this

862
02:06:15,500 --> 02:06:19,579
goes to file start faster day.i because
this doesn't require any login or

863
02:06:19,579 --> 02:06:24,409
anything to grab this data set it's as
simple as right clicking copy link

864
02:06:24,409 --> 02:06:36,159
address head over to wherever you want
it and just type W get and the URL okay

865
02:06:36,159 --> 02:06:44,150
so that's because you know it's it's not
behind a login or anything so you can

866
02:06:44,150 --> 02:06:49,820
grab the grab it from there and you can
always read a CSV file with just pandas

867
02:06:49,820 --> 02:06:55,639
don't read CSV now in this particular
case there's a lot of pre-processing

868
02:06:55,639 --> 02:07:03,710
that we do and what I've actually done
here is I've I've actually stolen the

869
02:07:03,710 --> 02:07:08,780
entire pipeline from the third place
winner roster okay so they made all

870
02:07:08,780 --> 02:07:12,440
their data they're really great you know
they better get hub available with

871
02:07:12,440 --> 02:07:16,880
everything that we need and I've ported
it all across and simplified it and

872
02:07:16,880 --> 02:07:23,030
tried to make it pretty easy to
understand this course is about deep

873
02:07:23,030 --> 02:07:28,460
learning not about data processing so
I'm not going to go through it but we

874
02:07:28,460 --> 02:07:30,840
will be going through it in the machine
learning course

875
02:07:30,840 --> 02:07:34,469
in some detail because feature
engineering is really important so if

876
02:07:34,469 --> 02:07:41,520
you're interested you know check out the
machine learning course for that I will

877
02:07:41,520 --> 02:07:48,780
however show you kind of what it looks
like so once we read the CSV Xin you can
see basically what's there so the key

878
02:07:50,909 --> 02:08:04,190
one is for a particular store we have
the

879
02:08:05,400 --> 02:08:12,270
we have the date and we have the sales
for that particular store we know

880
02:08:12,270 --> 02:08:19,860
whether that thing is on promo or not we
know the number of customers at that
particular store had we know whether

881
02:08:23,429 --> 02:08:26,510
that date was a school holiday

882
02:08:29,940 --> 02:08:38,320
we also know what kind of store it is so
this is pretty common right you'll often

883
02:08:38,320 --> 02:08:43,360
get datasets where there's some column
with like just some kind of code we

884
02:08:43,360 --> 02:08:47,410
don't really know what the code means
and most of the time I find it doesn't

885
02:08:47,410 --> 02:08:51,760
matter what it means like normally you
get given a data dictionary when you

886
02:08:51,760 --> 02:08:54,550
start on a project and obviously if
you're working on an internal project

887
02:08:54,550 --> 02:08:59,740
you can ask the people at your company
what does this column mean I kind of

888
02:08:59,740 --> 02:09:03,400
stay away from learning too much about
it I prefer to like to see what the data

889
02:09:03,400 --> 02:09:10,210
says first there's something about what
kind of product are we selling in this

890
02:09:10,210 --> 02:09:12,930
particular row

891
02:09:13,989 --> 02:09:19,239
and then there's information about like
how far away is the nearest competitor

892
02:09:19,239 --> 02:09:31,120
how long have they been open for how
long is the promo being on for for each

893
02:09:31,120 --> 02:09:35,380
store we can find out what state it's in
for each state we can find at the name

894
02:09:35,380 --> 02:09:40,540
of the state this is in Germany and
interestingly they were allowed to

895
02:09:40,540 --> 02:09:44,380
download any data external data they
wanted in this competition just very

896
02:09:44,380 --> 02:09:48,520
common as long as you share it with
everybody else and so some folks tried

897
02:09:48,520 --> 02:09:55,180
downloading data from Google Trends I'm
not sure exactly what it was that they

898
02:09:55,180 --> 02:09:59,430
would check in the trend off but we have
this information from Google Trends

899
02:09:59,430 --> 02:10:05,760
somebody downloaded the weather for
every day in Germany every state

900
02:10:09,250 --> 02:10:20,710
and yeah that's about it right so you
can get a data frame summary with pandas

901
02:10:20,710 --> 02:10:24,430
which kind of lets you see how many
observations and means and standard

902
02:10:24,430 --> 02:10:30,130
deviations again I don't do a hell of a
lot with that early on but it's nice to

903
02:10:30,130 --> 02:10:35,530
note there so what we do you know this
is called a relational data set a

904
02:10:35,530 --> 02:10:38,920
relational data set is one where there's
quite a few tables we have to join

905
02:10:38,920 --> 02:10:43,780
together it's very easy to do that in
pandas there's a thing called merge so

906
02:10:43,780 --> 02:10:46,360
great little function to do that and so
I just started joining everything

907
02:10:46,360 --> 02:10:49,380
together
joining the weather or the Google Trends

908
02:10:49,380 --> 02:10:59,530
stores yeah that's about everything I
guess you'll see there's one thing that

909
02:10:59,530 --> 02:11:04,300
I'm using from the FASTA a library which
is called add date part we talked about

910
02:11:04,300 --> 02:11:07,630
this a lot in the machine learning
course but basically this is going to

911
02:11:07,630 --> 02:11:12,970
take a date and pull out of a bunch of
columns day of week is at the start of a

912
02:11:12,970 --> 02:11:18,460
quarter month of year so on and so forth
and add them all in to the dataset okay

913
02:11:18,460 --> 02:11:22,020
so this is all standard pre-processing

914
02:11:22,740 --> 02:11:26,800
all right so we join everything together
we fiddle around with some of the dates

915
02:11:26,800 --> 02:11:29,950
a little bit some of them are in month
in year format we turn it into date

916
02:11:29,950 --> 02:11:37,930
format we spend a lot of time trying to
take information about for example

917
02:11:37,930 --> 02:11:42,940
holidays and add a column for like how
long until the next holiday how long has
it been since the last holiday ditto for

918
02:11:45,130 --> 02:11:52,180
promos so on and so forth okay so we do
all that and at the very end we

919
02:11:52,180 --> 02:11:57,630
basically save a big structured data
file that contains all that stuff

920
02:11:57,630 --> 02:12:01,300
something that those of you that use
pandas may not be aware of is that

921
02:12:01,300 --> 02:12:05,800
there's a very cool new format called
feather which you can save a panda's

922
02:12:05,800 --> 02:12:11,170
data frame into this feather format it's
kind of pretty much takes it as it sits

923
02:12:11,170 --> 02:12:16,420
in RAM and dumps it to the disk and so
it's like really really really fast the

924
02:12:16,420 --> 02:12:20,770
reason that you need to know this is
because the ecuadorian grocery

925
02:12:20,770 --> 02:12:26,410
competition is on now has
350 million records so you will care
about how long things take a talk I

926
02:12:28,420 --> 02:12:32,410
believe about six seconds for me to save
three hundred and fifty million records

927
02:12:32,410 --> 02:12:37,540
to feather format so that's pretty cool
so at the end of all that I'd save it as

928
02:12:37,540 --> 02:12:40,540
a feather format and for the rest of
this discussion I'm just going to take

929
02:12:40,540 --> 02:12:45,400
it as given that we've got this nicely
pre-processed feature engineered file

930
02:12:45,400 --> 02:12:50,650
and I can just go read better okay but
for you to play along at home you will

931
02:12:50,650 --> 02:12:57,580
have to run those previous cells oh
except the see these ones have commented

932
02:12:57,580 --> 02:13:01,690
out you don't have to run those because
the file that you download from files

933
02:13:01,690 --> 02:13:09,400
doc bastard AI has already done that for
you okay all right so we basically have

934
02:13:09,400 --> 02:13:18,510
all these columns so it basically is
going to tell us you know how many of

935
02:13:18,510 --> 02:13:25,780
this thing was sold on this date at this
store and so the goal of this
competition is to find out how many

936
02:13:28,990 --> 02:13:36,160
things will be sold for each store for
each type of thing in the future okay

937
02:13:36,160 --> 02:13:40,900
and so that's basically what we're going
to be trying to do and so here's an

938
02:13:40,900 --> 02:13:48,640
example of what some of the data looks
like and so next week we're going to see

939
02:13:48,640 --> 02:13:52,300
how to go through these steps but
basically what we're going to learn is

940
02:13:52,300 --> 02:13:58,480
we're going to learn to split the
columns into two types some columns were
going to treat as categorical which is

941
02:14:01,390 --> 02:14:08,050
to say store ID one and store ID - I'm
not numerically related to each other

942
02:14:08,050 --> 02:14:13,150
they're categories right we're going to
treat day of week like that - Monday and
Tuesday day zero and day one not

943
02:14:15,490 --> 02:14:20,620
numerically related to each other where
else distance in kilometers to the

944
02:14:20,620 --> 02:14:26,170
nearest competitor that's a number that
we're going to treat numerically right

945
02:14:26,170 --> 02:14:29,770
so in other words the categorical
variables we basically are going to one

946
02:14:29,770 --> 02:14:34,039
how to encode them you can think of it
as one hot encoding on where
the continuous variables we're going to

947
02:14:35,839 --> 02:14:43,609
be feeding into fully connected layers
just as is okay so what we'll be doing

948
02:14:43,609 --> 02:14:49,849
is we'll be basically creating a
validation set and you'll see like a lot

949
02:14:49,849 --> 02:14:53,419
of these are start to look familiar this
is the same function we used on planet

950
02:14:53,419 --> 02:14:58,669
and dog breeds to create a validation
set there's some stuff that you haven't

951
02:14:58,669 --> 02:15:02,899
seen before
where we're going to basically rather

952
02:15:02,899 --> 02:15:09,109
than saying image data dot from CSV
we're going to say columnar data from

953
02:15:09,109 --> 02:15:13,780
data frame right so you can see like the
basic API concepts will be the same but
they're a little different right but

954
02:15:17,689 --> 02:15:23,659
just like before we're going to get a
learner and we're going to go lr find to
find our best learning rate and then

955
02:15:26,359 --> 02:15:33,019
we're going to go dot fit with a metric
with a cycle length okay so the basic

956
02:15:33,019 --> 02:15:39,079
sequence who's going to end up looking
hopefully very familiar okay so we're
out of time

957
02:15:40,339 --> 02:15:47,479
so what I suggest you do this week is
like try to enter as many capital image

958
02:15:47,479 --> 02:15:53,030
competitions as possible like like try
to really get this feel for like cycling
learning rates plotting things you know

959
02:15:59,030 --> 02:16:05,869
that that post I showed you at the start
of class today that kind of took you

960
02:16:05,869 --> 02:16:11,030
through lesson one like really go
through that on as many image datasets

961
02:16:11,030 --> 02:16:17,059
as you can you just feel really
comfortable with it right because you

962
02:16:17,059 --> 02:16:19,879
want to get to the point where next week
when we start talking about structured

963
02:16:19,879 --> 02:16:24,619
data that this idea of like how learners
kind of work and data works and data

964
02:16:24,619 --> 02:16:28,869
loaders and data sets and looking at
pictures should be really you know

965
02:16:28,869 --> 02:16:34,119
intuitive all right good luck see you
next week

