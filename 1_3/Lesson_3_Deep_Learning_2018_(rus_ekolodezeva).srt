1
00:00:00,149 --> 00:00:03,600
Добро пожаловать на третью неделю курса.

2
00:00:03,600 --> 00:00:09,599
Вы наверняка заметили, что на этой неделе на форумах происходило много интересного.

3
00:00:09,599 --> 00:00:25,920
Многие участники курса подготовили материалы по курсу, чтобы помочь своим одногруппникам и лучше разобраться самим.

4
00:00:25,920 --> 00:00:32,340
Я хочу рассказать про некоторые, про что-то я уже писал на вики, но материалов очень много.

5
00:00:32,340 --> 00:00:42,480
Пользователь reshamas создала много полезных заметок — например, что делать, если не получается подключиться к AWS,

6
00:00:46,079 --> 00:00:57,750
она расписала всё в мелочах, я считаю, что это очень круто.

7
00:00:57,750 --> 00:01:05,460
Если вы делаете какие-то заметки для себя — поделитесь ими на форуме, это удобно делать в файлах разметки Markdown.

8
00:01:07,439 --> 00:01:13,789
Если вы загрузите свои заметки на GitHub, все смогут ими пользоваться. Или загрузите их на наш форум,

9
00:01:13,889 --> 00:01:25,140
reshamas сделала так со своей заметкой про tmux.

10
00:01:25,140 --> 00:01:39,869
tmux — консольная утилита, позволяющая показывать несколько терминалов на одном экране.

11
00:01:39,869 --> 00:01:48,210
Здесь в одном терминале у меня модули библиотеки, открытые редактором vim,

12
00:01:48,210 --> 00:01:53,579
в другом запущен Jupyter Notebook и так далее.

13
00:01:53,579 --> 00:02:03,060
Если интересно, reshamas написала на эту тему туториал, в её аккаунте на GitHub ещё много интересного.

14
00:02:03,060 --> 00:02:28,810
Apil Tamang написал хороший сжатый конспект предыдущей лекции.

15
00:02:29,610 --> 00:02:45,069
Pavel Surmenok написал пост про алгоритм поиска скорости обучения.

16
00:02:45,069 --> 00:02:59,500
Это очень круто, потому что об этом алгоритме ещё нигде не писали, а он очень полезный.

17
00:02:59,500 --> 00:03:13,560
Когда я поделился ссылкой на его пост в Twitter, им поделились сотни раз, у поста тысячи просмотров, отличная работа.

18
00:03:13,560 --> 00:03:20,779
Radek написал несколько полезных постов, мне особенно понравился гайд по PyTorch,

19
00:03:20,879 --> 00:03:32,260
он подойдёт для продвинутых студентов, например, тех, кто никогда не использовал PyTorch, но уже что-то знает про численное программирование.

20
00:03:35,560 --> 00:03:40,624
Есть интересная статья про связь между скоростью обучения и размером минибатча.

21
00:03:40,724 --> 00:03:46,030
Один студент недавно задал мне этот вопрос, поэтому я вспомнил этот пост.

22
00:03:46,030 --> 00:04:01,235
Автор поста пробовал различные скорости обучения и размеры минибатча и проанализировал, как они связаны, можете сами попробовать.

23
00:04:01,335 --> 00:04:15,586
Radek написал пост на тему моего утверждения о том, что SDGR находит более плоские области поверхности потерь и в них модель лучше обобщает.

24
00:04:15,686 --> 00:04:25,147
Он попробовал описать эту закономерность более точно, не очень удачно, но пост интересный.

25
00:04:25,247 --> 00:04:32,999
Есть пост по введению в свёрточные нейронные сети.

26
00:04:32,999 --> 00:04:57,059
Anand Saha написал отличный анализ архитектуры ResNet, мы ещё обсудим это в курсе, продвинутые студенты могут читать уже сейчас.

27
00:04:57,059 --> 00:05:04,349
Apil Tamang написал похожий пост. В общем, на форумах много чего происходит.

28
00:05:06,059 --> 00:05:17,479
Мы также создали форум для новичков. Тупых вопросов не бывает, но иногда страшно задавать вопрос про что-то простое,

29
00:05:19,860 --> 00:05:29,819
когда вокруг обсуждаются очень сложные вещи. Надеемся, что форум новичков будет не таким устрашающим.

30
00:05:29,819 --> 00:05:38,632
Если вы продвинутый студент и можете отвечать на такие вопросы, пожалуйста, делайте это дружелюбно —

31
00:05:38,732 --> 00:05:47,934
у людей за плечами может быть всего год опыта программирования и никакого опыта машинного обучения.

32
00:05:48,034 --> 00:05:56,159
Если вам хочется написать какой-то вспомогательный материал, не стесняйтесь — большинство авторов только что упомянутых заметок

33
00:05:56,159 --> 00:06:04,769
никогда не публиковали ничего в интернете, они такие же люди, как и вы.

34
00:06:04,769 --> 00:06:20,399
Если вы не уверены в каких-то деталях, запостите сначала на форум, люди помогут вам улучшить ваш материал фразами вроде

35
00:06:20,399 --> 00:06:29,439
«Вот здесь всё устроено немного по-другому, сейчас расскажу» или «Это очень интересно, вы не думали углубиться в эту тему?».

36
00:06:29,539 --> 00:06:38,969
К текущему моменту мы немного обсудили свёрточные нейронные сети.

37
00:06:38,969 --> 00:06:59,319
Мы не углублялись в детали того, как они работают, зато построили на их основе превосходного качества модель.

38
00:06:59,319 --> 00:07:12,280
Сегодня мы ещё раз посмотрим на модели и наконец-то подойдём к теории — что такое свёрточная нейронная сеть,

39
00:07:12,280 --> 00:07:16,000
что такое свёртка, как и почему это работает.

40
00:07:16,000 --> 00:07:25,930
Дальше мы продвинемся по нашему плану и обсудим использование нейронных сетей для анализа структурированных данных —

41
00:07:25,930 --> 00:07:31,449
логистика, прогнозы, анализ рынка и прочее.

42
00:07:31,449 --> 00:07:45,550
Потом посмотрим на обработку естественного языка, потом на коллаборативную фильтрацию для рекомендательных систем.

43
00:07:45,550 --> 00:07:53,020
Это будет проходить в том же формате, что и обсуждение CNN — без углубления в теорию, но с построением качественных моделей.

44
00:07:55,150 --> 00:08:01,984
Потом мы пройдёмся по этим же темам, но в другом порядке и более углубленно —

45
00:08:02,084 --> 00:08:09,949
рассмотрим коллаборативную фильтрацию, поймём, как написан соответствующий код и какая за этим стоит математика,

46
00:08:10,049 --> 00:08:20,620
потом сделаем то же самое для анализа структурированных данных, свёрточных нейронных сетей и обработки естественного языка.

47
00:08:20,620 --> 00:08:37,078
Давайте проговорим некоторые вещи с прошлых лекций ещё раз.

48
00:08:37,178 --> 00:08:55,889
Я хочу убедиться, что все смогут повторить модель с прошлой лекции для различения пород собак.

49
00:08:55,889 --> 00:09:11,339
Для этого нужно скачать данные. Данные можно скачивать либо с Kaggle, либо откуда-то ещё.

50
00:09:11,339 --> 00:09:37,960
Для скачивания данных с Kaggle мы используем kaggle-cli, он должен был установиться при загрузке материалов для курса.

51
00:09:38,060 --> 00:09:53,279
Если в процессе скачивания данных на сайте Kaggle что-то меняется, kaggle-cli падает,

52
00:09:53,279 --> 00:10:13,434
с этим можно справиться командой pip install kaggle-cli --upgrade.

53
00:10:13,534 --> 00:10:25,765
После этого следуйте инструкциям reshamas для скачивания данных.

54
00:10:25,865 --> 00:10:43,509
Команда для скачивания данных: kg download -u  -p  -c .

55
00:10:43,609 --> 00:11:02,100
— это фраза, идущая после /c/ в адресной строке на странице соревнования.

56
00:11:02,100 --> 00:11:12,650
Перед использованием kaggle-cli убедитесь, что приняли правила использования, то есть скачивали данные вручную хотя бы однажды.

57
00:11:12,750 --> 00:11:18,850
Если правила не приняты, kaggle-cli вам об этом скажет.

58
00:11:18,850 --> 00:11:31,085
Если вы используете аккаунт Google для входа в Kaggle, kaggle-cli не будет работать, используйте форму восстановления пароля Kaggle.

59
00:11:31,185 --> 00:11:38,775
kaggle-cli создаст папку на вашем компьютере и скачает туда данные.

60
00:11:38,875 --> 00:11:52,600
kaggle-cli не подходит, если вы используете датасет не с Kaggle или если вам не нужны все предоставленные там данные.

61
00:11:52,600 --> 00:12:11,890
Например, в этом соревновании данные предоставлены в форматах TIFF (19 ГБ) и JPG (600 МБ), возможно, вы не захотите использовать оба.

62
00:12:11,890 --> 00:12:32,480
Для таких случаев есть расширение CurlWget для Google Chrome.

63
00:12:32,580 --> 00:13:02,944
Для всех страниц, откуда можно что-нибудь скачать, CurlWget предоставит вам ссылку для скачивания из терминала.

64
00:13:03,044 --> 00:13:10,449
В ссылке содержатся куки вашего браузера, для активации нужно скачать что-то вручную один раз.

65
00:13:10,449 --> 00:13:19,420
С помощью этого расширения можно скачать что угодно — например, ваш любимый сериал.

66
00:13:19,420 --> 00:13:33,610
Это очень полезный инструмент для анализа данных, потому что зачастую приходится анализировать видео.

67
00:13:33,610 --> 00:13:44,019
Итак, есть два способа получить данные. После этого можно начинать обучать модель.

68
00:13:44,019 --> 00:13:57,670
В переменной PATH указано, что данные лежат в директории data рабочей директории Jupyter ноутбука.

69
00:13:57,670 --> 00:14:04,990
Это не всегда удобно — данные могут лежать в другом месте или даже на другом диске,

70
00:14:04,990 --> 00:14:17,949
поэтому можно создать символическую ссылку на директорию с данными.

71
00:14:17,949 --> 00:14:24,109
Вы можете сложить ваши данные куда угодно и добавить на них ссылку или сложить их в рабочую директорию Jupyter ноутбука.

72
00:14:24,209 --> 00:14:35,829
Символические ссылки — это очень удобно, можете почитать про них на нашем форуме, если не работали с ними раньше.

73
00:14:35,829 --> 00:14:45,490
Как видите, модули библиотеки fast.ai доступны из Jupyter ноутбука таким же образом.

74
00:14:45,490 --> 00:15:06,845
Чтобы вывести список файлов и директорий с указанием ссылок в Linux, используйте команду ls -l.

75
00:15:06,945 --> 00:15:20,315
Вам могло показаться, что для обучения модели нужно больше кода, чем на самом деле.

76
00:15:20,415 --> 00:15:28,292
Здесь на одном экране показаны все шаги, которые я проделал для обучения модели для классификации кошек и собак.

77
00:15:28,392 --> 00:15:36,197
Здесь не показаны скачивание и распаковка данных с Kaggle.

78
00:15:36,297 --> 00:15:51,130
Это все необходимые этапы. Сначала мы импортируем библиотеки — fastai.conv_learner импортирует всё необходимое сам.

79
00:15:51,130 --> 00:15:57,860
Нужно указать путь к файлам, размер изображений и размер минибатча.

80
00:15:57,960 --> 00:16:10,940
В этой строке мы говорим, как нужно преобразовать изображения — какая будет модель, какого они должны быть размера,

81
00:16:11,040 --> 00:16:18,490
какой алгоритм дополнения данных использовать, какое максимальное увеличение использовать.

82
00:16:18,490 --> 00:16:34,360
Здесь мы говорим, что данные рассортированы на обучающую и валидационную выборку, а внутри каждой выборки — на кошек и собак.

83
00:16:34,360 --> 00:16:45,170
Если папки с обучающей и валидационной выборкой названы нестандартно, их названия можно передать через параметры trn_name и val_name.

84
00:16:45,270 --> 00:16:56,380
Название папки с неразмеченной тестовой выборкой test_name необходимо указывать, если вы будете отправлять модель на Kaggle.

85
00:16:59,770 --> 00:17:08,205
После этого мы создаём модель с архитектурой ResNet на основе уже обученной и вызываем метод .fit().

86
00:17:08,305 --> 00:17:17,420
Напомню, что по умолчанию все слои, кроме последних, заморожены, мы ещё будем про это говорить.

87
00:17:17,420 --> 00:17:23,110
Метод .fit() работал около двух с половиной минут. Я не выставлял precompute=True.

88
00:17:26,119 --> 00:17:35,390
На форумах было много уточняющих вопросов по поводу значения этого параметра, повторюсь — это просто небольшое ускорение.

89
00:17:35,390 --> 00:17:42,325
Если не до конца понятно, просто пропускайте и оставляйте precompute=False по умолчанию.

90
00:17:42,425 --> 00:17:52,635
precompute=True кэширует некоторые промежуточные результаты, чтобы их не нужно было пересчитывать каждый раз.

91
00:17:52,735 --> 00:18:11,180
Помните, что с precomputed=True не работает дополнение данных, потому что дополнение данных не работает с предвычисленными активациями.

92
00:18:11,180 --> 00:18:14,780
Я максимально упростил процесс, поэтому precompute=False.

93
00:18:14,780 --> 00:18:22,810
Последний слой обучается 3 цикла с длиной цикла в одну эпоху (cycle_len=1).

94
00:18:23,110 --> 00:18:27,190
После этого я размораживаю модель и обучаю уже все слои.

95
00:18:27,590 --> 00:18:39,170
Метод .bn_freeze() мы ещё обсудим, это особенность сложных архитектур типа ResNet50 или ResNeXt101.

96
00:18:39,170 --> 00:18:49,005
Эту строку имеет добавлять после разморозки, если вы используете датасеты, подобные изображениям ImageNet —

97
00:18:49,105 --> 00:19:02,495
фотографии обычных объектов со стороны размера от 200 до 500 пикселей.

98
00:19:02,595 --> 00:19:10,630
Для продвинутых студентов — это фиксирует нормализацию скользящего среднего.

99
00:19:10,730 --> 00:19:20,409
Мы ещё обсудим это во второй части курса, в других библиотеках этого нет, но это очень важный аспект.

100
00:19:20,409 --> 00:19:31,789
После этого все слои нейронной сети обучаются в течение одной эпохи, и применяется дополнение тестовых данных.

101
00:19:31,889 --> 00:19:37,909
Это даёт долю правильных ответов в 99.45%.

102
00:19:38,009 --> 00:19:47,139
Это минимальные шаги, которые можно выполнить при работе с новым датасетом,

103
00:19:47,139 --> 00:20:02,799
при условии, что вы уже подобрали скорость обучения, знаете, как устроены данные и так далее.

104
00:20:02,799 --> 00:20:15,499
Я хотел показать вам, как устроены другие библиотеки, и выбрал для этого Keras.

105
00:20:15,599 --> 00:20:33,059
Библиотека fast.ai построена на основе PyTorch, а Keras поддерживает TensorFlow, MXNet, CNTK и многие другие библиотеки,

106
00:20:33,159 --> 00:20:37,450
большинство людей используют Keras с TensorFlow.

107
00:20:37,450 --> 00:20:51,024
В Jupyter ноутбуке keras_lesson1.ipynb я постараюсь воссоздать модель с первой лекции на Keras, чтобы вы увидели, как это можно делать.

108
00:20:51,124 --> 00:20:59,510
Я не буду пока ничего говорить про метод .bn_freeze(), кроме показаний к применению —

109
00:21:00,850 --> 00:21:11,950
это архитектура нейронной сети с числом больше 34, как ResNet50 или ResNeXt101,

110
00:21:11,950 --> 00:21:24,520
и датасет, похожий на изображения ImageNet, где объект занимает большую часть изображений нормального размера.

111
00:21:24,520 --> 00:21:32,989
Если вы не уверены в его необходимости, попробуйте убрать и сравнить результаты.

112
00:21:33,089 --> 00:21:39,910
Продвинутые студенты наверняка начнут обсуждать это на форумах уже сейчас, а мы дойдём до этого

113
00:21:39,910 --> 00:21:46,720
только во второй части курса, когда вернёмся к свёрточным нейронным сетям.

114
00:21:50,820 --> 00:22:01,480
Итак, для работы с Keras нужно импортировать необходимые модули.

115
00:22:01,480 --> 00:22:12,151
Keras поддерживает стандартный способ сортировки данных на обучающую и валидационную выборки и на классы внутри выборок.

116
00:22:12,251 --> 00:22:22,805
Здесь мы указываем пути к папкам с обучающей и валидационной выборкой.

117
00:22:22,905 --> 00:22:36,575
Вы заметите, что с Keras обучение модели требует больше кода и различных параметров,

118
00:22:36,675 --> 00:22:44,650
и очень просто задать неправильные параметры, поэтому я постараюсь подробно всё показать.

119
00:22:44,650 --> 00:22:55,030
Для подготовки данных необходимо создать генератор данных конструктором ImageDataGenerator().

120
00:22:55,030 --> 00:23:10,634
В параметры генератора данных необходимо передать параметры дополнения данных и нормализации.

121
00:23:10,734 --> 00:23:15,619
В библиотеке fast.ai достаточно указать архитектуру, например, ResNet50, и необходимые параметры выставляются автоматически,

122
00:23:15,719 --> 00:23:21,700
здесь надо примерно понимать, что для этого требуется.

123
00:23:21,700 --> 00:23:30,220
В принципе копирования кода из интернета достаточно, чтобы всё работало.

124
00:23:30,220 --> 00:23:43,550
Нет общепринятых стандартов по поводу этих параметров дополнения данных, я скопировал эту строку из документации Keras.

125
00:23:43,550 --> 00:23:49,140
Я не знаю, хороший ли это набор параметров, но в документации используется такой.

126
00:23:49,240 --> 00:23:56,540
Параметры говорят, отражать ли изображения относительно горизонтали, как увеличивать, как сдвигать.

127
00:23:56,540 --> 00:24:03,102
Из генератора данных создаётся генератор методом .flow_from_directory().

128
00:24:03,202 --> 00:24:17,985
В его параметры передаётся путь к файлам, размер изображений, размер минибатча и параметр class_mode.

129
00:24:18,085 --> 00:24:31,650
Параметр class_mode указывает вид задачи классификации — двухклассовая или многоклассовая, 'binary' или 'categorial'.

130
00:24:31,750 --> 00:24:36,525
У нас два класса — кошки и собаки, class_mode='binary'.

131
00:24:36,625 --> 00:24:48,048
Необходимо отдельно создать генератор данных без дополнения данных для валидации

132
00:24:48,148 --> 00:25:00,760
и создать соответствующий генератор с параметром shuffle=False, чтобы валидационная выборка не перемешивалась —

133
00:25:00,860 --> 00:25:11,270
это полезно для обучающей выборки, но на валидационной помешает отслеживать прогресс в обучении.

134
00:25:11,270 --> 00:25:18,465
Эти шаги в Keras необходимо делать каждый раз.

135
00:25:18,565 --> 00:25:32,750
Keras не поддерживает ResNet34, поэтому в конце прошлой лекции я поменял ResNet34 на ResNet50, чтобы мы могли сравнить fastai и Keras на одной архитектуре.

136
00:25:32,750 --> 00:25:42,290
В Keras модель не подстраивается под датасет автоматически, это нужно делать вручную.

137
00:25:42,290 --> 00:25:50,600
Для этого конструктором ResNet50() создаётся базовая модель и к ней вручную добавляются дополнительные слои.

138
00:25:50,600 --> 00:26:01,370
К концу этого курса вы поймёте, почему мы добавили именно эти три слоя.

139
00:26:01,370 --> 00:26:06,070
Модель создаётся конструктором Model().

140
00:26:06,070 --> 00:26:19,515
В Keras нет встроенной функции заморозки, поэтому мы проходим по всем слоям и выставляем поле .trainable=False.

141
00:26:19,615 --> 00:26:27,110
В Keras необходимо компилировать модель после создания методом .compile().

142
00:26:28,160 --> 00:26:35,360
Метод принимает как параметры вид оптимизации, функцию потерь и метрику оценки качества модели.

143
00:26:36,439 --> 00:26:47,324
В fast.ai эти значения передаются по умолчанию, хотя есть возможность заменить их своими.

144
00:26:47,424 --> 00:26:55,886
Вместо метода .fit() вызывается метод .fit_generator(), он принимает как параметры два только что созданных генератора,

145
00:26:55,886 --> 00:27:06,681
зачем-то просит количество минибатчей в одной эпохе — это размер генератора, делённый на размер минибатча.

146
00:27:07,081 --> 00:27:13,680
Как и в fast.ai, задаётся количество эпох.

147
00:27:15,100 --> 00:27:35,639
Задаётся количество воркеров. В отличие от fast.ai, по умолчанию они не используются, важно не забыть про этот параметр для достижения хорошей скорости.

148
00:27:35,739 --> 00:27:43,469
Этого достаточно, чтобы начать тонкую настройку последних слоёв.

149
00:27:43,569 --> 00:27:53,805
На валидационной выборке доля правильных ответов получилась 95%, но на первой и второй эпохах она была 49% и 69%.

150
00:27:53,905 --> 00:28:01,115
Я не знаю, почему это так — возможно, ошибка в Keras, возможно, в моём коде.

151
00:28:01,215 --> 00:28:07,469
Я писал про это в Twitter, но там никто не смог разобраться.

152
00:28:07,569 --> 00:28:15,509
Это одна из причин, почему я использую fast.ai в этом курсе — там гораздо сложнее всё испортить.

153
00:28:15,609 --> 00:28:18,806
Я не знаю, в чём тут ошибка.

154
00:28:18,906 --> 00:28:23,270
Янет: Здесь используется TensorFlow?

155
00:28:23,500 --> 00:28:50,279
Да, здесь используется TensorFlow, для этого нужно установить его командой pip install tensorflow-gpu keras.

156
00:28:50,379 --> 00:29:08,539
В Keras нет дифференциальных скоростей обучения и частичного размораживания, поэтому нужно вручную отделить последние слои.

157
00:29:08,539 --> 00:29:17,210
Я буду настраивать все слои, начиная со 140-го, для этого прохожу по всем слоям и замораживаю или размораживаю их, после этого снова вызываю метод .compile().

158
00:29:17,210 --> 00:29:26,924
После этого я снова обучаю модель, доля правильных ответов на обучающей выборке примерно такая же, а на валидационной опять ерунда.

159
00:29:27,024 --> 00:29:40,062
Даже если не учитывать это, Keras проигрывает в сравнении с fastai — кода гораздо больше и результаты хуже:

160
00:29:40,162 --> 00:29:49,934
модель на Keras за восемь минут достигла доли правильных ответов в 97% на обучающей выборке,

161
00:29:50,034 --> 00:30:02,380
а модель на fastai за четыре или пять минут — 99.5% на валидационной выборке.

162
00:30:02,380 --> 00:30:17,620
Используйте TensorFlow, если вы разрабатываете что-то для смартфонов, PyTorch пока плохо это поддерживает.

163
00:30:17,620 --> 00:30:24,365
Вам придётся использовать TensorFlow, если это делает компания, где вы работаете.

164
00:30:24,465 --> 00:30:37,000
Если вам нужно повторить что-то из этого курса с TensorFlow, используйте Keras и будьте готовы к тому,

165
00:30:37,000 --> 00:30:53,690
что будет больше кода, и будет сложнее повторить результаты, которые легко достигаются в fast.ai.

166
00:30:52,190 --> 00:30:53,690
===========================================

167
00:30:54,669 --> 00:31:05,990
В fast.ai нет ничего, что нельзя было бы повторить в Keras,

168
00:31:06,090 --> 00:31:20,389
но каждый раз заново писать SGDR, дифференциальные скорости обучения и всё остальное - неудобно.

169
00:31:20,489 --> 00:31:29,199
На форуме один человек работает над интеграцией Keras/TensorFlow и fast.ai, я надеюсь, что из этого что-то выйдет.

170
00:31:29,299 --> 00:31:43,270
Я разговаривал с Google и они тоже в этом заинтересованы. Возможно, к тому моменту, как этот курс появится на MOOC-платформе, это сделают.

171
00:31:43,270 --> 00:32:05,900
Keras/TensorFlow не очень сложны для обучения, для миграции с fast.ai после этого курса вам понадобится пара дней.

172
00:32:06,000 --> 00:32:19,890
Всего сказанного должно хватить для того, чтобы вы смогли обучить модель на датасете с породами собак.

173
00:32:19,890 --> 00:32:26,600
Большую часть того, что нужно сделать, я показал в конце прошлой лекции -

174
00:32:30,870 --> 00:32:37,885
например, как я изучал данные, чтобы понять, как устроены классы и какого размера изображения.

175
00:32:37,985 --> 00:32:41,980
Если вы что-то забыли, пересмотрите прошлую лекцию.

176
00:32:42,080 --> 00:32:53,794
Мы не успели обсудить, как отправить модель на Kaggle, сейчас покажу.

177
00:32:53,894 --> 00:33:01,500
Я уже написал про это в вики.

178
00:33:01,500 --> 00:33:12,955
На Kaggle для каждого соревнования есть вкладка Evaluation, в ней написано, какой формат вывода ожидается.

179
00:33:13,055 --> 00:33:18,910
В этом соревновании на выходе должен быть файл, где в заголовке - ID и все возможные породы сбоак,

180
00:33:19,010 --> 00:33:31,860
а в остальных строках - ID файлов тестовой выборки и вероятности того, что на этих файлах различные породы собак.

181
00:33:35,610 --> 00:33:55,240
Объект data.classes содержит названия всех классов в алфавитном порядке.

182
00:33:55,340 --> 00:34:01,645
Объект data.test_ds содержит тестовую выборку, названия файлов лежат в data.test_ds.fnames.

183
00:34:01,745 --> 00:34:18,444
Напоминаю, что изображения не разложены по папкам в стиле Keras, а размечены в csv-файле,

184
00:34:18,544 --> 00:34:27,859
поэтому мы используем метод ImageDataClassifier.from_csv(), а не ImageDataClassifier.from_paths().

185
00:34:31,799 --> 00:34:40,139
Keras не поддерживает такой формат, поэтому на форумах Kaggle люди выкладывают скрипты для сортировки данных по папкам,

186
00:34:40,139 --> 00:34:47,574
но нам этого делать не придётся.

187
00:34:47,674 --> 00:35:03,740
Итак, у нас есть названия пород и имена файлов тестовой выборки.

188
00:35:03,740 --> 00:35:28,789
Я всегда использую TTA при предсказании тестовой выборки, для этого в метод .TTA() передаётся параметр is_test=True,

189
00:35:28,889 --> 00:35:35,369
и предсказания делаются на тестовой выборке, а не на валидационной.

190
00:35:35,369 --> 00:35:44,765
Мы не знаем, какая получилась доля правильных ответов, потому что тестовая выборка не размечена.

191
00:35:44,865 --> 00:35:57,099
Большинство моделей PyTorch возвращает логарифмы вероятностей, метод np.exp() переводит их в обычные вероятности.

192
00:35:57,199 --> 00:36:10,529
В тестовой выборке 10357 изображений, принадлежащих 120 различным породам собак, это размеры полученной матрицы предсказаний.

193
00:36:10,529 --> 00:36:17,695
Из этой матрицы мы создадим файл необходимого формата с помощью pandas.

194
00:36:17,795 --> 00:36:27,472
Если вы не работали с pandas, погуглите или посмотрите наш курс по машинному обучению, там это часто используется.

195
00:36:27,572 --> 00:36:34,807
Мы создаём датафрейм pandas из матрицы методом pandas.DataFrame(), присваиваем колонкам названия пород собак

196
00:36:34,907 --> 00:36:57,745
и вставляем нулевую колонку с названиями файлов. Названия содержат 'test/'в начале и '.jpg' в конце, эти части мы обрезаем.

197
00:36:57,845 --> 00:37:06,550
Итоговый датафрейм выглядит так.

198
00:37:06,650 --> 00:37:24,000
Датафреймы с данными обычно называют df (data frame).

199
00:37:24,000 --> 00:37:36,680
С помощью метода .to_csv() можно записать получившийся датафрейм в csv-файл. Имеет смысл включить сжатие параметром compression='gzip'.

200
00:37:36,780 --> 00:37:45,082
После этого на сервере Jupyter Notebook появится csv-файл.

201
00:37:45,182 --> 00:37:57,235
После этого вы можете либо скачать файл с сервера и отправить его вручную, либо использовать kaggle-cli.

202
00:37:57,335 --> 00:38:04,170
Я обычно скачиваю файл себе на компьютер, чтобы посмотреть на него перед отправкой.

203
00:38:04,170 --> 00:38:27,130
Есть удобная утилита FileLink, которая создаёт ссылку, по которой можно скачать файл с сервера Jupyter Notebook на ваш компьютер.

204
00:38:33,490 --> 00:38:43,025
Архив с файлом скачался.

205
00:38:43,125 --> 00:38:54,095
Формат именно такой, какой нужно - в заголовке строке ID и породы собак, остальные строки содержат имя файла и вероятности.

206
00:38:54,195 --> 00:39:00,695
Теперь можно загрузить его на Kaggle через веб-интерфейс.

207
00:39:00,795 --> 00:39:15,160
Итак, теперь мы умеем скачивать файлы из интернета на сервер Jupyter Notebook через CurlWget в Google Chrome

208
00:39:17,170 --> 00:39:31,270
и умеем скачивать файлы с сервера на свой компьютер. Можно использовать scp в терминале, но мне нравится делать это в Jupyter ноутбуке.

209
00:39:31,270 --> 00:39:42,910
На этой неделе меня спросили, как получить предсказание только для одного файла.

210
00:39:42,910 --> 00:39:49,790
Допустим, я хочу получить предсказание для первого изображения валидационной выборки.

211
00:39:49,890 --> 00:39:59,350
Вот имя файла, я могу его вывести методом Image.open() стандартной библиотеки Python.

212
00:39:59,350 --> 00:40:16,230
Самое простое, что вы можете сделать - вызвать метод .predict_array().

213
00:40:16,230 --> 00:40:21,390
Для этого надо сначала применить к изображению дополнение данных.

214
00:40:21,390 --> 00:40:36,310
Функция tfms_from_model возвращает преобразования данных отдельно для обучающей и валидационной выборки.

215
00:40:36,310 --> 00:40:45,280
После этого мы применяем на изображении преобразования данных для обучающей выборки. Нет, лучше для валидационной.

216
00:40:45,280 --> 00:40:51,220
Полученный массив можно передавать в метод .predict_array().

217
00:40:55,440 --> 00:41:05,320
Данные можно подавать в модель и получать от модели только в минибатчах.

218
00:41:05,320 --> 00:41:15,250
У нас только одно изображение, и мы ходим превратить его в минибатч,

219
00:41:15,250 --> 00:41:21,490
то есть превратить из тензора размерности (количество строк)x(количество столбцов)х(цветовые каналы)

220
00:41:21,490 --> 00:41:26,230
в тензор размерности (количество изображений)x(количество строк)x(количество столбцов)х(цветовые каналы).

221
00:41:26,230 --> 00:41:31,330
У нас трёхмерная матрица, а должна быть четырёхмерная.

222
00:41:31,330 --> 00:41:46,480
Если в numpy индексировать массив im как im[None], вернётся массив размерностью больше на 1, так мы превратили изображение в минибатч.

223
00:41:46,480 --> 00:42:02,080
Если вы забудете это сделать при использовании PyTorch или fast.ai, получите ошибку вроде "expected 4 dimensions but got 3".

224
00:42:02,080 --> 00:42:15,040
Модели не только принимают, но и возвращают минибатчи, это тоже учтите.

225
00:42:15,040 --> 00:42:38,405
На этом мы закончим с практикой и перейдём к теории свёрточный нейронных сетей.

226
00:42:38,505 --> 00:42:54,175
На первой лекции мы немного поговорили про теорию, используя демонстрацию setosa.io/ev/, Explained Visually.

227
00:42:54,275 --> 00:43:03,910
Мы узнали, что свёртка - это алгоритм, который рассматривает область 3х3 пикселя за раз и умножает значение каждого пикселя

228
00:43:04,010 --> 00:43:16,790
на соответствующее значение матрицы свёртки, а потом складывает числа внутри области для получения нового значения в центре области.

229
00:43:16,890 --> 00:43:29,065
Давайте посмотрим, как с помощью этого алгоритма создаются слои нейронной сети, которые мы видели в статье Зайлера и Фергюса.

230
00:43:29,165 --> 00:43:39,570
Для этого я покажу вам работу человека, который гораздо умнее меня - Отавио Гуда.

231
00:43:39,570 --> 00:43:47,155
Отавио Гуд создал Word Lens - это система распознавания текста, которая сейчас используется в Google Translate,

232
00:43:47,255 --> 00:43:58,080
когда вы наводите камеру телефона на текст на незнакомом языке и поверх изображения показвыается перевод.

233
00:43:58,080 --> 00:44:06,385
Отавио разработал эту систему и создал превосходную демонстрацию её работы, сейчас он работает в Google.

234
00:44:06,485 --> 00:44:21,000
Я прокомментирую эту демонстрацию, а потом мы посмотрим на нечто похожее в таблице в Microsoft Excel.

235
00:44:21,500 --> 00:44:27,720
Надеюсь, что принцип работы будет ясен и тем, кто любит видео, и тем, кто любит таблички.

236
00:44:29,880 --> 00:44:41,610
Эта демонстрация распознавания текста, дальше в курсе мы займёмся распознаванием цифр, задачи очень похожи.

237
00:44:41,610 --> 00:44:51,990
На изображении - буква А, изображение - это матрица чисел.

238
00:44:51,990 --> 00:44:59,035
К этой матрице чисел применяется первый свёрточный фильтр. Предполагается, что все фильтры уже вычислены, модель обучена.

239
00:44:59,135 --> 00:45:05,010
Фильтр чёрный слева и белый справа, это значит, что матрица свёртки выглядит примерно так:

240
00:45:05,010 --> 00:45:10,350
[[-1, 0, 1],
[-1, 0, 1],
[-1, 0, 1]].

241
00:45:13,140 --> 00:45:21,670
Каждая область 3х3 поэлементно умножается на эту матрицу, полученные результаты складываются.

242
00:45:21,770 --> 00:45:30,180
Везде, где чёрное переходит в белое, мы получаем положительные значения, они показаны зелёным,

243
00:45:30,180 --> 00:45:36,475
а там, где белое переходит в чёрное - негативные значения, они показаны красным.

244
00:45:36,575 --> 00:45:41,920
Это результат работы первого ядра свёртки.

245
00:45:42,020 --> 00:45:47,020
Вот новое ядро, с белой полосой наверху, а не справа.

246
00:45:47,120 --> 00:46:02,610
Фильтр проходит через каждую область изображения 3x3 и определяет, насколько красным или зелёным получится новый пиксель.

247
00:46:02,610 --> 00:46:15,570
Допустим, у нас было всего два фильтра, видно, что результат отражает вид матриц свёртки.

248
00:46:15,570 --> 00:46:26,520
Полученные результаты пропускаются через выпрямитель (ReLU), который убирает отрицательные значения.

249
00:46:26,520 --> 00:46:30,780
Здесь показан первый входной слой, второй слой - результат работы свёрточных фильтров,

250
00:46:30,780 --> 00:46:36,180
третий слой - результат работы выпрямителя,

251
00:46:38,190 --> 00:46:55,320
четвёртый слой - подвыборка максимумом: каждая область 2х2 заменяется на максимальный элемент этой области, такой алгоритм уменьшения.

252
00:46:55,420 --> 00:47:05,599
После этого процедура повторяется. Новые фильтры свёртки проходят уже оба полученных фильтра с предыдущего слоя,

253
00:47:05,699 --> 00:47:21,190
полученные результаты пропускаются через выпрямитель, получается ещё один слой свёрточной нейронной сети.

254
00:47:21,190 --> 00:47:30,490
Видно, что на первом слое свёртки выделялись горизонтальные или вертикальные края.

255
00:47:33,010 --> 00:47:42,559
Результат работы следующего слоя уже не так очевиден, но принцип его работы такой же.

256
00:47:42,659 --> 00:47:52,180
После этого опять выполняется подвыборка максимумом, каждая область 2x2 заменяется одним числом.

257
00:47:53,799 --> 00:48:07,865
Полученное изображение сравнивается с шаблонами возможных классов, одному из которых оно принадлежит.

258
00:48:07,965 --> 00:48:19,456
Это делается таким же образом - изображение 4x8 поэлементно умножается на шаблон, соответствующий какому-то классу,

259
00:48:19,556 --> 00:48:30,614
результаты складываются и потом конвертируются в вероятность.

260
00:48:31,114 --> 00:48:38,524
Это изображение на 92% совпало с шаблоном класса А, модель считает, что на изображении буква А.

261
00:48:38,624 --> 00:48:46,119
Демонстрация показывает работу уже обученной модели на тестовой выборке,

262
00:48:46,119 --> 00:49:00,484
например, вашей модели или модели, преобученной на изображениях ImageNet.

263
00:49:00,584 --> 00:49:17,855
Ещё раз: на каждом слое сначала работают свёрточные фильтры, потом выпрямитель, потом подвыборка максимумом.

264
00:49:17,955 --> 00:49:29,395
После работы многих слоёв результат сравнивается с шаблонами и получается предсказание.

265
00:49:29,495 --> 00:49:40,420
Как видите, демонстрация очень крутая, я не смог бы такое нарисовать, спасибо Отавио за то, что поделился этим.

266
00:49:40,420 --> 00:49:49,895
Это видео демонстрирует работу реальной свёрточной нейронной сети, Отавио написал для этого специальную программу.

267
00:49:49,995 --> 00:49:58,060
Я человек простой и предпочитаю таблицы в Excel.

268
00:49:58,060 --> 00:50:08,950
Эта таблица есть в репозитории fast.ai на GitHub, можете клонировать весь репозиторий или скачать её отдельно,

269
00:50:08,950 --> 00:50:36,880
она доступна по адресу github.com/fastai/fastai/tree/master/courses/dl1/excel/conv-example.xlsx.

270
00:50:36,880 --> 00:50:44,710


271
00:50:44,710 --> 00:50:51,310


272
00:50:51,310 --> 00:50:55,290


273
00:50:55,290 --> 00:51:01,360


274
00:51:01,360 --> 00:51:09,640


275
00:51:13,060 --> 00:51:16,089


276
00:51:16,089 --> 00:51:22,449


277
00:51:22,449 --> 00:51:25,900


278
00:51:25,900 --> 00:51:32,170


279
00:51:32,170 --> 00:51:36,489


280
00:51:36,489 --> 00:51:42,400


281
00:51:42,400 --> 00:51:52,299


282
00:51:52,299 --> 00:51:58,299


283
00:51:58,299 --> 00:52:05,170


284
00:52:05,170 --> 00:52:10,119


285
00:52:10,119 --> 00:52:16,630


286
00:52:16,630 --> 00:52:22,029


287
00:52:22,029 --> 00:52:28,239


288
00:52:28,239 --> 00:52:35,249


289
00:52:37,989 --> 00:52:41,319


290
00:52:41,319 --> 00:52:47,049


291
00:52:47,049 --> 00:52:54,670


292
00:52:54,670 --> 00:53:04,420


293
00:53:04,420 --> 00:53:08,859


294
00:53:08,859 --> 00:53:14,769


295
00:53:14,769 --> 00:53:22,359


296
00:53:22,359 --> 00:53:28,990


297
00:53:28,990 --> 00:53:38,500


298
00:53:38,500 --> 00:53:46,000


299
00:53:46,000 --> 00:53:52,240


300
00:53:52,240 --> 00:53:59,230


301
00:53:59,230 --> 00:54:07,060


302
00:54:07,060 --> 00:54:13,750


303
00:54:13,750 --> 00:54:17,710


304
00:54:17,710 --> 00:54:25,480


305
00:54:25,480 --> 00:54:32,920


306
00:54:32,920 --> 00:54:36,610


307
00:54:36,610 --> 00:54:40,930


308
00:54:42,970 --> 00:54:50,490


309
00:54:50,490 --> 00:54:57,370


310
00:54:57,370 --> 00:55:03,560


311
00:55:03,560 --> 00:55:09,290


312
00:55:09,290 --> 00:55:15,920


313
00:55:15,920 --> 00:55:20,750


314
00:55:20,750 --> 00:55:28,670


315
00:55:28,670 --> 00:55:35,210


316
00:55:35,210 --> 00:55:41,540


317
00:55:41,540 --> 00:55:50,060


318
00:55:50,060 --> 00:55:54,230


319
00:55:54,230 --> 00:55:59,870


320
00:55:59,870 --> 00:56:07,220


321
00:56:07,220 --> 00:56:13,490


322
00:56:13,490 --> 00:56:19,700


323
00:56:19,700 --> 00:56:23,990


324
00:56:23,990 --> 00:56:30,680


325
00:56:30,680 --> 00:56:40,010


326
00:56:40,010 --> 00:56:44,150


327
00:56:44,150 --> 00:56:48,680


328
00:56:50,810 --> 00:56:59,210


329
00:56:59,210 --> 00:57:02,080


330
00:57:02,200 --> 00:57:09,849


331
00:57:09,849 --> 00:57:17,710


332
00:57:17,710 --> 00:57:23,589


333
00:57:23,589 --> 00:57:28,930


334
00:57:28,930 --> 00:57:34,540


335
00:57:34,540 --> 00:57:38,589


336
00:57:38,589 --> 00:57:44,109


337
00:57:44,109 --> 00:57:48,430


338
00:57:48,430 --> 00:57:58,599


339
00:57:58,599 --> 00:58:09,510


340
00:58:09,510 --> 00:58:19,119


341
00:58:19,119 --> 00:58:22,420


342
00:58:22,420 --> 00:58:28,000


343
00:58:28,000 --> 00:58:35,530


344
00:58:35,530 --> 00:58:40,720


345
00:58:42,670 --> 00:58:47,140


346
00:58:47,140 --> 00:58:54,160


347
00:58:54,160 --> 00:58:58,990


348
00:58:58,990 --> 00:59:01,930


349
00:59:01,930 --> 00:59:06,390


350
00:59:06,390 --> 00:59:09,760


351
00:59:09,760 --> 00:59:14,470


352
00:59:15,800 --> 00:59:19,550


353
00:59:19,550 --> 00:59:23,090


354
00:59:23,090 --> 00:59:27,730


355
00:59:27,730 --> 00:59:33,170


356
00:59:33,170 --> 00:59:38,900


357
00:59:38,900 --> 00:59:44,720


358
00:59:48,710 --> 00:59:54,740


359
00:59:59,300 --> 01:00:02,540


360
01:00:02,540 --> 01:00:08,510


361
01:00:08,510 --> 01:00:15,250


362
01:00:15,250 --> 01:00:25,100


363
01:00:25,100 --> 01:00:30,650


364
01:00:30,650 --> 01:00:35,930


365
01:00:40,640 --> 01:00:45,680


366
01:00:45,680 --> 01:00:52,940


367
01:00:52,940 --> 01:00:57,830


368
01:00:57,830 --> 01:01:07,670


369
01:01:07,670 --> 01:01:13,730


370
01:01:13,730 --> 01:01:17,930


371
01:01:17,930 --> 01:01:26,150


372
01:01:26,150 --> 01:01:30,400


373
01:01:30,400 --> 01:01:36,520


374
01:01:36,520 --> 01:01:43,569


375
01:01:43,569 --> 01:01:49,180


376
01:01:49,180 --> 01:01:57,359


377
01:01:57,359 --> 01:02:02,560


378
01:02:02,560 --> 01:02:06,780


379
01:02:06,780 --> 01:02:13,690


380
01:02:13,690 --> 01:02:19,030


381
01:02:19,030 --> 01:02:24,160


382
01:02:26,319 --> 01:02:33,460


383
01:02:33,460 --> 01:02:38,050


384
01:02:38,050 --> 01:02:43,000


385
01:02:43,000 --> 01:02:52,150


386
01:02:52,150 --> 01:02:58,119


387
01:02:58,119 --> 01:03:02,890


388
01:03:04,900 --> 01:03:10,180


389
01:03:12,790 --> 01:03:16,930


390
01:03:16,930 --> 01:03:22,300


391
01:03:22,300 --> 01:03:28,150


392
01:03:28,150 --> 01:03:35,650


393
01:03:35,650 --> 01:03:40,440


394
01:03:40,670 --> 01:03:45,619


395
01:03:45,619 --> 01:03:49,490


396
01:03:51,980 --> 01:03:57,079


397
01:03:57,079 --> 01:04:03,290


398
01:04:03,290 --> 01:04:09,920


399
01:04:09,920 --> 01:04:15,049


400
01:04:15,049 --> 01:04:20,540


401
01:04:20,540 --> 01:04:25,280


402
01:04:25,280 --> 01:04:31,750


403
01:04:31,750 --> 01:04:39,710


404
01:04:39,710 --> 01:04:46,250


405
01:04:46,250 --> 01:04:53,180


406
01:04:53,180 --> 01:05:03,319


407
01:05:03,319 --> 01:05:09,380


408
01:05:09,380 --> 01:05:13,700


409
01:05:15,470 --> 01:05:20,059


410
01:05:20,059 --> 01:05:26,049


411
01:05:26,049 --> 01:05:32,210


412
01:05:32,210 --> 01:05:40,280


413
01:05:40,280 --> 01:05:44,960


414
01:05:44,960 --> 01:05:52,160


415
01:05:52,160 --> 01:05:57,440


416
01:05:57,440 --> 01:06:02,989


417
01:06:02,989 --> 01:06:06,950


418
01:06:06,950 --> 01:06:11,749


419
01:06:11,749 --> 01:06:17,779


420
01:06:17,779 --> 01:06:23,059


421
01:06:23,059 --> 01:06:27,529


422
01:06:27,529 --> 01:06:34,099


423
01:06:34,099 --> 01:06:38,499


424
01:06:42,559 --> 01:06:48,619


425
01:06:48,619 --> 01:06:55,460


426
01:06:55,460 --> 01:06:59,630


427
01:06:59,630 --> 01:07:05,960


428
01:07:05,960 --> 01:07:11,420


429
01:07:11,420 --> 01:07:17,660


430
01:07:17,660 --> 01:07:23,719


431
01:07:23,719 --> 01:07:27,380


432
01:07:27,380 --> 01:07:32,779


433
01:07:32,779 --> 01:07:40,910


434
01:07:40,910 --> 01:07:45,440


435
01:07:45,440 --> 01:07:48,410


436
01:07:48,410 --> 01:07:53,029


437
01:07:53,029 --> 01:08:02,239


438
01:08:02,239 --> 01:08:06,890


439
01:08:06,890 --> 01:08:10,300


440
01:08:10,300 --> 01:08:15,940


441
01:08:15,940 --> 01:08:19,870


442
01:08:19,870 --> 01:08:23,230


443
01:08:23,229 --> 01:08:26,469


444
01:08:26,470 --> 01:08:29,050


445
01:08:29,050 --> 01:08:33,070


446
01:08:35,350 --> 01:08:39,760


447
01:08:39,760 --> 01:08:50,310


448
01:08:50,310 --> 01:09:00,339


449
01:09:02,290 --> 01:09:07,180


450
01:09:07,180 --> 01:09:11,770


451
01:09:11,770 --> 01:09:17,920


452
01:09:17,920 --> 01:09:23,970


453
01:09:23,970 --> 01:09:36,460


454
01:09:36,460 --> 01:09:44,740


455
01:09:44,740 --> 01:09:50,370


456
01:09:50,370 --> 01:09:57,130


457
01:09:57,130 --> 01:10:04,030


458
01:10:04,030 --> 01:10:09,790


459
01:10:09,790 --> 01:10:14,680


460
01:10:14,680 --> 01:10:19,810


461
01:10:19,810 --> 01:10:23,150


462
01:10:27,110 --> 01:10:40,190


463
01:10:40,190 --> 01:10:48,190


464
01:10:48,190 --> 01:10:56,539


465
01:10:56,539 --> 01:11:02,030


466
01:11:02,030 --> 01:11:06,199


467
01:11:06,199 --> 01:11:10,820


468
01:11:10,820 --> 01:11:16,989


469
01:11:16,989 --> 01:11:23,150


470
01:11:23,150 --> 01:11:32,300


471
01:11:32,300 --> 01:11:37,039


472
01:11:37,039 --> 01:11:40,340


473
01:11:40,340 --> 01:11:44,479


474
01:11:44,479 --> 01:11:48,249


475
01:11:50,510 --> 01:11:55,969


476
01:11:55,969 --> 01:12:01,400


477
01:12:01,400 --> 01:12:07,449


478
01:12:07,449 --> 01:12:16,340


479
01:12:16,340 --> 01:12:23,630


480
01:12:23,630 --> 01:12:29,780


481
01:12:29,780 --> 01:12:32,920


482
01:12:33,070 --> 01:12:39,260


483
01:12:39,260 --> 01:12:44,230


484
01:12:45,590 --> 01:12:55,230


485
01:12:58,619 --> 01:13:04,500


486
01:13:04,500 --> 01:13:09,840


487
01:13:09,840 --> 01:13:13,340


488
01:13:13,829 --> 01:13:18,210


489
01:13:18,210 --> 01:13:22,139


490
01:13:22,139 --> 01:13:29,179


491
01:13:29,179 --> 01:13:34,769


492
01:13:34,769 --> 01:13:39,599


493
01:13:39,599 --> 01:13:45,570


494
01:13:45,570 --> 01:13:51,269


495
01:13:51,269 --> 01:13:54,749


496
01:13:54,749 --> 01:14:00,239


497
01:14:00,239 --> 01:14:04,979


498
01:14:04,979 --> 01:14:10,320


499
01:14:12,300 --> 01:14:19,289


500
01:14:19,289 --> 01:14:24,869


501
01:14:24,869 --> 01:14:29,550


502
01:14:29,550 --> 01:14:35,729


503
01:14:35,729 --> 01:14:43,289


504
01:14:46,139 --> 01:14:53,489


505
01:14:54,960 --> 01:14:59,940


506
01:14:59,940 --> 01:15:06,090


507
01:15:06,090 --> 01:15:10,170


508
01:15:10,170 --> 01:15:16,199


509
01:15:18,630 --> 01:15:23,280


510
01:15:23,280 --> 01:15:26,670


511
01:15:26,670 --> 01:15:29,610


512
01:15:29,610 --> 01:15:37,470


513
01:15:37,470 --> 01:15:41,760


514
01:15:41,760 --> 01:15:46,500


515
01:15:46,500 --> 01:15:51,420


516
01:15:51,420 --> 01:15:58,650


517
01:15:58,650 --> 01:16:03,450


518
01:16:03,450 --> 01:16:08,670


519
01:16:08,670 --> 01:16:19,530


520
01:16:19,530 --> 01:16:33,090


521
01:16:33,090 --> 01:16:37,260


522
01:16:37,260 --> 01:16:41,010


523
01:16:41,010 --> 01:16:45,360


524
01:16:50,820 --> 01:16:57,480


525
01:16:57,480 --> 01:17:03,390


526
01:17:03,390 --> 01:17:14,370


527
01:17:14,370 --> 01:17:21,450


528
01:17:21,450 --> 01:17:25,140


529
01:17:25,140 --> 01:17:31,650


530
01:17:33,780 --> 01:17:38,260


531
01:17:38,260 --> 01:17:44,590


532
01:17:44,590 --> 01:17:49,450


533
01:17:49,450 --> 01:17:52,630


534
01:17:52,630 --> 01:17:58,210


535
01:17:58,210 --> 01:18:02,710


536
01:18:02,710 --> 01:18:05,320


537
01:18:05,320 --> 01:18:10,480


538
01:18:10,480 --> 01:18:15,850


539
01:18:15,850 --> 01:18:28,120


540
01:18:32,340 --> 01:18:37,830


541
01:18:37,830 --> 01:18:43,860


542
01:18:43,860 --> 01:18:51,460


543
01:18:51,460 --> 01:18:56,890


544
01:18:56,890 --> 01:19:04,840


545
01:19:04,840 --> 01:19:11,050


546
01:19:11,050 --> 01:19:16,510


547
01:19:16,510 --> 01:19:20,739


548
01:19:23,110 --> 01:19:28,420


549
01:19:28,420 --> 01:19:33,100


550
01:19:33,100 --> 01:19:39,190


551
01:19:41,140 --> 01:19:47,140


552
01:19:47,140 --> 01:19:53,680


553
01:19:53,680 --> 01:19:57,700


554
01:20:00,100 --> 01:20:07,510


555
01:20:07,510 --> 01:20:12,610


556
01:20:14,980 --> 01:20:22,600


557
01:20:22,600 --> 01:20:31,180


558
01:20:31,180 --> 01:20:34,780


559
01:20:37,600 --> 01:20:43,230


560
01:20:45,419 --> 01:20:48,840


561
01:20:48,840 --> 01:20:53,010


562
01:20:53,010 --> 01:20:57,119


563
01:21:00,419 --> 01:21:08,189


564
01:21:08,189 --> 01:21:13,229


565
01:21:13,229 --> 01:21:19,769


566
01:21:19,769 --> 01:21:25,559


567
01:21:25,559 --> 01:21:30,389


568
01:21:30,389 --> 01:21:34,260


569
01:21:34,260 --> 01:21:39,719


570
01:21:39,719 --> 01:21:44,880


571
01:21:44,880 --> 01:21:50,999


572
01:21:50,999 --> 01:21:56,969


573
01:21:56,969 --> 01:22:02,550


574
01:22:02,550 --> 01:22:09,840


575
01:22:09,840 --> 01:22:15,689


576
01:22:15,689 --> 01:22:21,749


577
01:22:23,610 --> 01:22:28,079


578
01:22:28,079 --> 01:22:34,559


579
01:22:34,559 --> 01:22:39,419


580
01:22:39,419 --> 01:22:46,229


581
01:22:46,229 --> 01:22:49,590


582
01:22:49,590 --> 01:22:55,249


583
01:22:55,249 --> 01:23:00,550


584
01:23:00,550 --> 01:23:05,590


585
01:23:05,590 --> 01:23:08,560


586
01:23:08,560 --> 01:23:15,940


587
01:23:21,880 --> 01:23:25,600


588
01:23:25,600 --> 01:23:29,770


589
01:23:29,770 --> 01:23:40,690


590
01:23:43,150 --> 01:23:48,370


591
01:23:48,370 --> 01:23:55,180


592
01:23:55,180 --> 01:24:04,050


593
01:24:04,050 --> 01:24:12,160


594
01:24:12,160 --> 01:24:16,420


595
01:24:16,420 --> 01:24:21,610


596
01:24:21,610 --> 01:24:27,220


597
01:24:27,220 --> 01:24:29,800


598
01:24:29,800 --> 01:24:35,140


599
01:24:35,140 --> 01:24:39,670


600
01:24:39,670 --> 01:24:45,070


601
01:24:45,070 --> 01:24:50,620


602
01:24:50,620 --> 01:24:55,630


603
01:24:55,630 --> 01:25:01,690


604
01:25:05,670 --> 01:25:11,130


605
01:25:11,130 --> 01:25:16,290


606
01:25:16,290 --> 01:25:23,340


607
01:25:23,340 --> 01:25:28,440


608
01:25:32,429 --> 01:25:37,650


609
01:25:37,650 --> 01:25:43,230


610
01:25:43,230 --> 01:25:48,360


611
01:25:48,360 --> 01:25:53,310


612
01:25:53,310 --> 01:25:58,080


613
01:26:00,300 --> 01:26:04,650


614
01:26:08,429 --> 01:26:12,900


615
01:26:12,900 --> 01:26:20,219


616
01:26:20,219 --> 01:26:25,560


617
01:26:25,560 --> 01:26:30,480


618
01:26:32,159 --> 01:26:37,260


619
01:26:37,260 --> 01:26:42,719


620
01:26:42,719 --> 01:26:47,880


621
01:26:47,880 --> 01:26:51,989


622
01:26:54,480 --> 01:26:57,480


623
01:26:57,480 --> 01:27:02,550


624
01:27:02,550 --> 01:27:07,350


625
01:27:07,350 --> 01:27:13,290


626
01:27:13,290 --> 01:27:18,800


627
01:27:18,800 --> 01:27:23,070


628
01:27:24,829 --> 01:27:31,489


629
01:27:31,489 --> 01:27:38,329


630
01:27:38,329 --> 01:27:42,349


631
01:27:42,349 --> 01:27:49,880


632
01:27:49,880 --> 01:27:53,960


633
01:27:53,960 --> 01:28:00,559


634
01:28:00,559 --> 01:28:05,690


635
01:28:08,090 --> 01:28:15,619


636
01:28:17,780 --> 01:28:20,960


637
01:28:20,960 --> 01:28:25,360


638
01:28:25,360 --> 01:28:34,099


639
01:28:34,099 --> 01:28:40,239


640
01:28:41,570 --> 01:28:47,840


641
01:28:47,840 --> 01:28:52,099


642
01:28:52,099 --> 01:28:57,710


643
01:28:57,710 --> 01:29:07,040


644
01:29:07,040 --> 01:29:15,829


645
01:29:15,829 --> 01:29:21,559


646
01:29:21,559 --> 01:29:25,489


647
01:29:27,440 --> 01:29:31,190


648
01:29:31,190 --> 01:29:36,790


649
01:29:36,790 --> 01:29:42,599


650
01:29:42,599 --> 01:29:51,869


651
01:29:53,219 --> 01:30:01,080


652
01:30:01,080 --> 01:30:07,230


653
01:30:07,230 --> 01:30:12,599


654
01:30:12,599 --> 01:30:18,869


655
01:30:22,940 --> 01:30:29,699


656
01:30:29,699 --> 01:30:34,139


657
01:30:34,139 --> 01:30:39,119


658
01:30:39,119 --> 01:30:44,489


659
01:30:44,489 --> 01:30:50,040


660
01:30:50,040 --> 01:30:56,940


661
01:30:56,940 --> 01:31:03,540


662
01:31:03,540 --> 01:31:08,670


663
01:31:08,670 --> 01:31:12,179


664
01:31:12,179 --> 01:31:16,110


665
01:31:16,110 --> 01:31:21,320


666
01:31:21,680 --> 01:31:28,250


667
01:31:28,250 --> 01:31:36,530


668
01:31:36,530 --> 01:31:40,700


669
01:31:40,700 --> 01:31:48,710


670
01:31:48,710 --> 01:31:56,420


671
01:31:56,420 --> 01:32:01,160


672
01:32:01,160 --> 01:32:04,850


673
01:32:04,850 --> 01:32:11,680


674
01:32:11,680 --> 01:32:17,750


675
01:32:17,750 --> 01:32:23,270


676
01:32:23,270 --> 01:32:27,860


677
01:32:30,050 --> 01:32:34,160


678
01:32:34,160 --> 01:32:40,040


679
01:32:40,040 --> 01:32:45,350


680
01:32:45,350 --> 01:32:51,080


681
01:32:51,080 --> 01:32:59,000


682
01:32:59,000 --> 01:33:07,940


683
01:33:07,940 --> 01:33:24,200


684
01:33:24,200 --> 01:33:29,670


685
01:33:29,670 --> 01:33:33,110


686
01:33:35,450 --> 01:33:44,250


687
01:33:44,250 --> 01:33:49,650


688
01:33:49,650 --> 01:33:56,070


689
01:33:56,070 --> 01:34:02,970


690
01:34:02,970 --> 01:34:08,670


691
01:34:08,670 --> 01:34:12,930


692
01:34:12,930 --> 01:34:15,840


693
01:34:15,840 --> 01:34:19,710


694
01:34:19,710 --> 01:34:22,830


695
01:34:22,830 --> 01:34:27,990


696
01:34:27,990 --> 01:34:33,300


697
01:34:33,300 --> 01:34:41,010


698
01:34:41,010 --> 01:34:46,350


699
01:34:46,350 --> 01:34:52,170


700
01:34:52,170 --> 01:34:56,370


701
01:34:56,370 --> 01:35:02,580


702
01:35:02,580 --> 01:35:07,770


703
01:35:07,770 --> 01:35:14,130


704
01:35:14,130 --> 01:35:18,530


705
01:35:18,530 --> 01:35:25,650


706
01:35:25,650 --> 01:35:32,690


707
01:35:32,690 --> 01:35:37,170


708
01:35:37,170 --> 01:35:44,910


709
01:35:44,910 --> 01:35:48,270


710
01:35:48,270 --> 01:35:52,620


711
01:35:54,360 --> 01:36:00,060


712
01:36:00,060 --> 01:36:04,980


713
01:36:07,440 --> 01:36:12,000


714
01:36:12,000 --> 01:36:17,760


715
01:36:17,760 --> 01:36:23,480


716
01:36:25,920 --> 01:36:31,260


717
01:36:31,260 --> 01:36:39,870


718
01:36:39,870 --> 01:36:44,460


719
01:36:44,460 --> 01:36:49,890


720
01:36:49,890 --> 01:36:57,530


721
01:37:00,210 --> 01:37:04,920


722
01:37:04,920 --> 01:37:13,410


723
01:37:13,410 --> 01:37:17,280


724
01:37:17,280 --> 01:37:25,920


725
01:37:25,920 --> 01:37:30,390


726
01:37:30,390 --> 01:37:38,090


727
01:37:38,090 --> 01:37:44,190


728
01:37:47,190 --> 01:37:52,410


729
01:37:52,410 --> 01:37:58,140


730
01:37:58,140 --> 01:38:04,620


731
01:38:04,620 --> 01:38:09,120


732
01:38:09,120 --> 01:38:14,820


733
01:38:14,820 --> 01:38:21,840


734
01:38:21,840 --> 01:38:25,950


735
01:38:25,950 --> 01:38:30,090


736
01:38:30,090 --> 01:38:35,300


737
01:38:35,300 --> 01:38:39,440


738
01:38:42,150 --> 01:38:51,140


739
01:38:51,200 --> 01:38:59,160


740
01:38:59,160 --> 01:39:07,650


741
01:39:07,650 --> 01:39:12,810


742
01:39:12,810 --> 01:39:17,130


743
01:39:17,130 --> 01:39:20,300


744
01:39:21,510 --> 01:39:25,710


745
01:39:25,710 --> 01:39:29,699


746
01:39:29,699 --> 01:39:35,099


747
01:39:35,099 --> 01:39:41,659


748
01:39:45,989 --> 01:39:51,300


749
01:39:51,300 --> 01:39:57,150


750
01:40:01,320 --> 01:40:08,400


751
01:40:08,400 --> 01:40:14,070


752
01:40:14,070 --> 01:40:19,079


753
01:40:19,079 --> 01:40:23,280


754
01:40:23,280 --> 01:40:27,300


755
01:40:27,300 --> 01:40:34,409


756
01:40:36,809 --> 01:40:40,440


757
01:40:44,219 --> 01:40:48,389


758
01:40:48,389 --> 01:40:53,880


759
01:40:53,880 --> 01:41:05,119


760
01:41:05,119 --> 01:41:13,980


761
01:41:13,980 --> 01:41:19,170


762
01:41:19,170 --> 01:41:21,840


763
01:41:21,840 --> 01:41:24,900


764
01:41:26,880 --> 01:41:32,570


765
01:41:33,980 --> 01:41:39,400


766
01:41:39,400 --> 01:41:47,090


767
01:41:47,090 --> 01:41:51,140


768
01:41:51,140 --> 01:41:54,260


769
01:41:54,260 --> 01:41:59,390


770
01:41:59,390 --> 01:42:04,370


771
01:42:04,370 --> 01:42:11,540


772
01:42:11,540 --> 01:42:15,140


773
01:42:15,140 --> 01:42:19,700


774
01:42:21,739 --> 01:42:28,730


775
01:42:28,730 --> 01:42:33,110


776
01:42:33,110 --> 01:42:38,120


777
01:42:38,120 --> 01:42:47,270


778
01:42:47,270 --> 01:42:55,540


779
01:42:57,320 --> 01:43:08,690


780
01:43:08,690 --> 01:43:13,400


781
01:43:13,400 --> 01:43:19,340


782
01:43:19,340 --> 01:43:25,970


783
01:43:25,970 --> 01:43:30,530


784
01:43:33,380 --> 01:43:38,930


785
01:43:38,930 --> 01:43:44,290


786
01:43:46,710 --> 01:43:54,490


787
01:43:54,490 --> 01:44:00,580


788
01:44:00,580 --> 01:44:06,970


789
01:44:06,970 --> 01:44:15,940


790
01:44:18,520 --> 01:44:26,470


791
01:44:26,470 --> 01:44:32,890


792
01:44:32,890 --> 01:44:38,140


793
01:44:38,140 --> 01:44:47,080


794
01:44:47,080 --> 01:44:52,180


795
01:44:52,180 --> 01:44:58,390


796
01:44:58,390 --> 01:45:05,350


797
01:45:05,350 --> 01:45:09,100


798
01:45:09,100 --> 01:45:13,510


799
01:45:13,510 --> 01:45:19,690


800
01:45:23,650 --> 01:45:28,690


801
01:45:28,690 --> 01:45:34,210


802
01:45:34,210 --> 01:45:40,870


803
01:45:40,870 --> 01:45:48,240


804
01:45:48,240 --> 01:45:55,170


805
01:45:55,170 --> 01:46:05,119


806
01:46:05,119 --> 01:46:15,439


807
01:46:15,439 --> 01:46:21,229


808
01:46:21,229 --> 01:46:26,300


809
01:46:26,300 --> 01:46:34,570


810
01:46:36,050 --> 01:46:44,560


811
01:46:45,869 --> 01:46:53,440


812
01:46:53,440 --> 01:47:01,960


813
01:47:01,960 --> 01:47:05,440


814
01:47:05,440 --> 01:47:08,949


815
01:47:08,949 --> 01:47:15,880


816
01:47:15,880 --> 01:47:19,869


817
01:47:19,869 --> 01:47:22,659


818
01:47:22,659 --> 01:47:34,059


819
01:47:34,059 --> 01:47:39,659


820
01:47:42,989 --> 01:47:50,920


821
01:47:50,920 --> 01:47:55,960


822
01:47:55,960 --> 01:48:01,599


823
01:48:01,599 --> 01:48:07,119


824
01:48:10,840 --> 01:48:16,210


825
01:48:16,210 --> 01:48:23,769


826
01:48:23,769 --> 01:48:27,519


827
01:48:27,519 --> 01:48:33,820


828
01:48:36,190 --> 01:48:42,969


829
01:48:42,969 --> 01:48:50,309


830
01:48:50,309 --> 01:48:56,920


831
01:48:56,920 --> 01:49:02,530


832
01:49:02,530 --> 01:49:08,380


833
01:49:08,380 --> 01:49:16,830


834
01:49:16,830 --> 01:49:22,960


835
01:49:22,960 --> 01:49:28,270


836
01:49:28,270 --> 01:49:34,690


837
01:49:37,420 --> 01:49:42,730


838
01:49:45,460 --> 01:49:52,150


839
01:49:52,150 --> 01:50:00,219


840
01:50:00,219 --> 01:50:03,989


841
01:50:07,300 --> 01:50:13,960


842
01:50:13,960 --> 01:50:18,250


843
01:50:18,250 --> 01:50:22,449


844
01:50:22,449 --> 01:50:28,179


845
01:50:28,179 --> 01:50:34,960


846
01:50:34,960 --> 01:50:40,270


847
01:50:40,270 --> 01:50:48,190


848
01:50:49,719 --> 01:50:53,890


849
01:50:53,890 --> 01:50:57,610


850
01:51:00,910 --> 01:51:05,290


851
01:51:05,290 --> 01:51:09,190


852
01:51:09,190 --> 01:51:13,250


853
01:51:13,250 --> 01:51:19,969


854
01:51:19,969 --> 01:51:24,050


855
01:51:24,050 --> 01:51:28,840


856
01:51:28,840 --> 01:51:34,369


857
01:51:35,809 --> 01:51:41,900


858
01:51:46,909 --> 01:51:54,349


859
01:51:54,349 --> 01:52:02,869


860
01:52:09,619 --> 01:52:19,699


861
01:52:19,699 --> 01:52:24,320


862
01:52:24,320 --> 01:52:29,750


863
01:52:29,750 --> 01:52:34,730


864
01:52:34,730 --> 01:52:41,329


865
01:52:41,329 --> 01:52:49,250


866
01:52:49,250 --> 01:52:52,670


867
01:52:52,670 --> 01:52:58,610


868
01:52:58,610 --> 01:53:03,559


869
01:53:03,559 --> 01:53:13,489


870
01:53:18,019 --> 01:53:24,070


871
01:53:27,010 --> 01:53:33,099


872
01:53:33,099 --> 01:53:40,539


873
01:53:40,539 --> 01:53:46,780


874
01:53:46,780 --> 01:53:50,170


875
01:53:50,170 --> 01:53:54,070


876
01:53:54,070 --> 01:53:59,289


877
01:53:59,289 --> 01:54:03,880


878
01:54:03,880 --> 01:54:08,860


879
01:54:08,860 --> 01:54:17,499


880
01:54:17,499 --> 01:54:23,230


881
01:54:23,230 --> 01:54:27,039


882
01:54:27,039 --> 01:54:33,429


883
01:54:33,429 --> 01:54:39,249


884
01:54:39,249 --> 01:54:47,079


885
01:54:47,079 --> 01:54:51,280


886
01:54:51,280 --> 01:54:54,909


887
01:54:54,909 --> 01:55:04,030


888
01:55:04,030 --> 01:55:08,409


889
01:55:08,409 --> 01:55:14,949


890
01:55:14,949 --> 01:55:19,389


891
01:55:19,389 --> 01:55:22,659


892
01:55:24,579 --> 01:55:28,150


893
01:55:28,150 --> 01:55:31,230


894
01:55:35,349 --> 01:55:40,729


895
01:55:40,729 --> 01:55:46,550


896
01:55:46,550 --> 01:55:50,689


897
01:55:50,689 --> 01:55:55,579


898
01:55:57,889 --> 01:56:03,679


899
01:56:03,679 --> 01:56:10,219


900
01:56:10,219 --> 01:56:14,929


901
01:56:14,929 --> 01:56:19,669


902
01:56:19,669 --> 01:56:24,019


903
01:56:24,019 --> 01:56:26,630


904
01:56:26,630 --> 01:56:30,469


905
01:56:30,469 --> 01:56:33,860


906
01:56:33,860 --> 01:56:38,869


907
01:56:38,869 --> 01:56:43,999


908
01:56:43,999 --> 01:56:54,229


909
01:56:54,229 --> 01:57:02,479


910
01:57:02,479 --> 01:57:09,590


911
01:57:09,590 --> 01:57:14,419


912
01:57:14,419 --> 01:57:21,919


913
01:57:21,919 --> 01:57:27,110


914
01:57:27,110 --> 01:57:33,260


915
01:57:33,260 --> 01:57:38,449


916
01:57:41,869 --> 01:57:46,820


917
01:57:46,820 --> 01:57:50,900


918
01:57:50,900 --> 01:57:56,260


919
01:57:56,260 --> 01:58:04,280


920
01:58:07,580 --> 01:58:13,130


921
01:58:13,130 --> 01:58:18,290


922
01:58:18,290 --> 01:58:23,660


923
01:58:23,660 --> 01:58:29,200


924
01:58:29,200 --> 01:58:35,240


925
01:58:35,240 --> 01:58:39,200


926
01:58:39,200 --> 01:58:47,870


927
01:58:47,870 --> 01:58:54,080


928
01:58:54,080 --> 01:59:00,770


929
01:59:00,770 --> 01:59:06,260


930
01:59:06,260 --> 01:59:12,650


931
01:59:14,030 --> 01:59:18,260


932
01:59:18,260 --> 01:59:22,910


933
01:59:22,910 --> 01:59:26,450


934
01:59:26,450 --> 01:59:30,560


935
01:59:30,560 --> 01:59:35,530


936
01:59:38,250 --> 01:59:44,440


937
01:59:44,440 --> 01:59:48,160


938
01:59:48,160 --> 01:59:52,690


939
01:59:52,690 --> 02:00:02,500


940
02:00:02,500 --> 02:00:10,300


941
02:00:10,300 --> 02:00:16,600


942
02:00:16,600 --> 02:00:21,850


943
02:00:21,850 --> 02:00:30,130


944
02:00:30,130 --> 02:00:37,199


945
02:00:39,960 --> 02:00:45,520


946
02:00:45,520 --> 02:00:49,449


947
02:00:51,550 --> 02:00:57,940


948
02:01:00,480 --> 02:01:05,489


949
02:01:05,489 --> 02:01:11,770


950
02:01:11,770 --> 02:01:16,210


951
02:01:16,210 --> 02:01:21,840


952
02:01:21,840 --> 02:01:31,199


953
02:01:34,810 --> 02:01:39,730


954
02:01:41,590 --> 02:01:46,179


955
02:01:46,179 --> 02:01:49,719


956
02:01:49,719 --> 02:01:55,880


957
02:01:55,880 --> 02:02:01,309


958
02:02:01,309 --> 02:02:05,749


959
02:02:05,749 --> 02:02:10,070


960
02:02:10,070 --> 02:02:14,780


961
02:02:14,780 --> 02:02:21,130


962
02:02:21,399 --> 02:02:26,360


963
02:02:28,099 --> 02:02:32,659


964
02:02:35,989 --> 02:02:40,099


965
02:02:40,099 --> 02:02:50,539


966
02:02:50,539 --> 02:02:53,780


967
02:02:53,780 --> 02:02:58,579


968
02:02:58,579 --> 02:03:03,320


969
02:03:03,320 --> 02:03:10,130


970
02:03:10,130 --> 02:03:15,769


971
02:03:15,769 --> 02:03:19,849


972
02:03:19,849 --> 02:03:24,369


973
02:03:29,059 --> 02:03:34,639


974
02:03:34,639 --> 02:03:40,189


975
02:03:40,189 --> 02:03:45,289


976
02:03:48,889 --> 02:03:52,849


977
02:03:52,849 --> 02:03:57,769


978
02:03:57,769 --> 02:04:04,010


979
02:04:04,010 --> 02:04:09,560


980
02:04:09,560 --> 02:04:14,600


981
02:04:14,600 --> 02:04:18,950


982
02:04:20,600 --> 02:04:25,000


983
02:04:25,000 --> 02:04:30,230


984
02:04:30,230 --> 02:04:36,080


985
02:04:36,080 --> 02:04:41,630


986
02:04:41,630 --> 02:04:49,100


987
02:04:49,100 --> 02:04:54,710


988
02:04:54,710 --> 02:04:59,240


989
02:04:59,240 --> 02:05:08,240


990
02:05:08,240 --> 02:05:13,690


991
02:05:14,730 --> 02:05:18,020


992
02:05:18,020 --> 02:05:24,920


993
02:05:24,920 --> 02:05:30,619


994
02:05:30,619 --> 02:05:37,340


995
02:05:37,340 --> 02:05:45,940


996
02:05:48,949 --> 02:05:54,980


997
02:05:54,980 --> 02:05:59,030


998
02:05:59,030 --> 02:06:06,710


999
02:06:06,710 --> 02:06:10,610


1000
02:06:10,610 --> 02:06:15,500


1001
02:06:15,500 --> 02:06:19,579


1002
02:06:19,579 --> 02:06:24,409


1003
02:06:24,409 --> 02:06:36,159


1004
02:06:36,159 --> 02:06:44,150


1005
02:06:44,150 --> 02:06:49,820


1006
02:06:49,820 --> 02:06:55,639


1007
02:06:55,639 --> 02:07:03,710


1008
02:07:03,710 --> 02:07:08,780


1009
02:07:08,780 --> 02:07:12,440


1010
02:07:12,440 --> 02:07:16,880


1011
02:07:16,880 --> 02:07:23,030


1012
02:07:23,030 --> 02:07:28,460


1013
02:07:28,460 --> 02:07:30,840


1014
02:07:30,840 --> 02:07:34,469


1015
02:07:34,469 --> 02:07:41,520


1016
02:07:41,520 --> 02:07:48,780


1017
02:07:50,909 --> 02:08:04,190


1018
02:08:05,400 --> 02:08:12,270


1019
02:08:12,270 --> 02:08:19,860


1020
02:08:23,429 --> 02:08:26,510


1021
02:08:29,940 --> 02:08:38,320


1022
02:08:38,320 --> 02:08:43,360


1023
02:08:43,360 --> 02:08:47,410


1024
02:08:47,410 --> 02:08:51,760


1025
02:08:51,760 --> 02:08:54,550


1026
02:08:54,550 --> 02:08:59,740


1027
02:08:59,740 --> 02:09:03,400


1028
02:09:03,400 --> 02:09:10,210


1029
02:09:10,210 --> 02:09:12,930


1030
02:09:13,989 --> 02:09:19,239


1031
02:09:19,239 --> 02:09:31,120


1032
02:09:31,120 --> 02:09:35,380


1033
02:09:35,380 --> 02:09:40,540


1034
02:09:40,540 --> 02:09:44,380


1035
02:09:44,380 --> 02:09:48,520


1036
02:09:48,520 --> 02:09:55,180


1037
02:09:55,180 --> 02:09:59,430


1038
02:09:59,430 --> 02:10:05,760


1039
02:10:09,250 --> 02:10:20,710


1040
02:10:20,710 --> 02:10:24,430


1041
02:10:24,430 --> 02:10:30,130


1042
02:10:30,130 --> 02:10:35,530


1043
02:10:35,530 --> 02:10:38,920


1044
02:10:38,920 --> 02:10:43,780


1045
02:10:43,780 --> 02:10:46,360


1046
02:10:46,360 --> 02:10:49,380


1047
02:10:49,380 --> 02:10:59,530


1048
02:10:59,530 --> 02:11:04,300


1049
02:11:04,300 --> 02:11:07,630


1050
02:11:07,630 --> 02:11:12,970


1051
02:11:12,970 --> 02:11:18,460


1052
02:11:18,460 --> 02:11:22,020


1053
02:11:22,740 --> 02:11:26,800


1054
02:11:26,800 --> 02:11:29,950


1055
02:11:29,950 --> 02:11:37,930


1056
02:11:37,930 --> 02:11:42,940


1057
02:11:45,130 --> 02:11:52,180


1058
02:11:52,180 --> 02:11:57,630


1059
02:11:57,630 --> 02:12:01,300


1060
02:12:01,300 --> 02:12:05,800


1061
02:12:05,800 --> 02:12:11,170


1062
02:12:11,170 --> 02:12:16,420


1063
02:12:16,420 --> 02:12:20,770


1064
02:12:20,770 --> 02:12:26,410


1065
02:12:28,420 --> 02:12:32,410


1066
02:12:32,410 --> 02:12:37,540


1067
02:12:37,540 --> 02:12:40,540


1068
02:12:40,540 --> 02:12:45,400


1069
02:12:45,400 --> 02:12:50,650


1070
02:12:50,650 --> 02:12:57,580


1071
02:12:57,580 --> 02:13:01,690


1072
02:13:01,690 --> 02:13:09,400


1073
02:13:09,400 --> 02:13:18,510


1074
02:13:18,510 --> 02:13:25,780


1075
02:13:28,990 --> 02:13:36,160


1076
02:13:36,160 --> 02:13:40,900


1077
02:13:40,900 --> 02:13:48,640


1078
02:13:48,640 --> 02:13:52,300


1079
02:13:52,300 --> 02:13:58,480


1080
02:14:01,390 --> 02:14:08,050


1081
02:14:08,050 --> 02:14:13,150


1082
02:14:15,490 --> 02:14:20,620


1083
02:14:20,620 --> 02:14:26,170


1084
02:14:26,170 --> 02:14:29,770


1085
02:14:29,770 --> 02:14:34,039


1086
02:14:35,839 --> 02:14:43,609


1087
02:14:43,609 --> 02:14:49,849


1088
02:14:49,849 --> 02:14:53,419


1089
02:14:53,419 --> 02:14:58,669


1090
02:14:58,669 --> 02:15:02,899


1091
02:15:02,899 --> 02:15:09,109


1092
02:15:09,109 --> 02:15:13,780


1093
02:15:17,689 --> 02:15:23,659


1094
02:15:26,359 --> 02:15:33,019


1095
02:15:33,019 --> 02:15:39,079


1096
02:15:40,339 --> 02:15:47,479


1097
02:15:47,479 --> 02:15:53,030


1098
02:15:59,030 --> 02:16:05,869


1099
02:16:05,869 --> 02:16:11,030


1100
02:16:11,030 --> 02:16:17,059


1101
02:16:17,059 --> 02:16:19,879


1102
02:16:19,879 --> 02:16:24,619


1103
02:16:24,619 --> 02:16:28,869


1104
02:16:28,869 --> 02:16:34,119


