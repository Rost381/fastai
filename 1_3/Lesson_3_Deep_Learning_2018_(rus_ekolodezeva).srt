1
00:00:00,149 --> 00:00:03,600
Добро пожаловать на третью неделю курса.

2
00:00:03,600 --> 00:00:09,599
Вы наверняка заметили, что на этой неделе на форумах происходило много интересного.

3
00:00:09,599 --> 00:00:25,920
Многие участники курса подготовили материалы по курсу, чтобы помочь своим одногруппникам и лучше разобраться самим.

4
00:00:25,920 --> 00:00:32,340
Я хочу рассказать про некоторые, про что-то я уже писал на вики, но материалов очень много.

5
00:00:32,340 --> 00:00:42,480
Пользователь reshamas создала много полезных заметок — например, что делать, если не получается подключиться к AWS,

6
00:00:46,079 --> 00:00:57,750
она расписала всё в мелочах, я считаю, что это очень круто.

7
00:00:57,750 --> 00:01:05,460
Если вы делаете какие-то заметки для себя — поделитесь ими на форуме, это удобно делать в файлах разметки Markdown.

8
00:01:07,439 --> 00:01:13,789
Если вы загрузите свои заметки на GitHub, все смогут ими пользоваться. Или загрузите их на наш форум,

9
00:01:13,889 --> 00:01:25,140
reshamas сделала так со своей заметкой про tmux.

10
00:01:25,140 --> 00:01:39,869
tmux — консольная утилита, позволяющая показывать несколько терминалов на одном экране.

11
00:01:39,869 --> 00:01:48,210
Здесь в одном терминале у меня модули библиотеки, открытые редактором vim,

12
00:01:48,210 --> 00:01:53,579
в другом запущен Jupyter Notebook и так далее.

13
00:01:53,579 --> 00:02:03,060
Если интересно, reshamas написала на эту тему туториал, в её аккаунте на GitHub ещё много интересного.

14
00:02:03,060 --> 00:02:28,810
Apil Tamang написал хороший сжатый конспект предыдущей лекции.

15
00:02:29,610 --> 00:02:45,069
Pavel Surmenok написал пост про алгоритм поиска скорости обучения.

16
00:02:45,069 --> 00:02:59,500
Это очень круто, потому что об этом алгоритме ещё нигде не писали, а он очень полезный.

17
00:02:59,500 --> 00:03:13,560
Когда я поделился ссылкой на его пост в Twitter, им поделились сотни раз, у поста тысячи просмотров, отличная работа.

18
00:03:13,560 --> 00:03:20,779
Radek написал несколько полезных постов, мне особенно понравился гайд по PyTorch,

19
00:03:20,879 --> 00:03:32,260
он подойдёт для продвинутых студентов, например, тех, кто никогда не использовал PyTorch, но уже что-то знает про численное программирование.

20
00:03:35,560 --> 00:03:40,624
Есть интересная статья про связь между скоростью обучения и размером минибатча.

21
00:03:40,724 --> 00:03:46,030
Один студент недавно задал мне этот вопрос, поэтому я вспомнил этот пост.

22
00:03:46,030 --> 00:04:01,235
Автор поста пробовал различные скорости обучения и размеры минибатча и проанализировал, как они связаны, можете сами попробовать.

23
00:04:01,335 --> 00:04:15,586
Radek написал пост на тему моего утверждения о том, что SDGR находит более плоские области поверхности потерь и в них модель лучше обобщает.

24
00:04:15,686 --> 00:04:25,147
Он попробовал описать эту закономерность более точно, не очень удачно, но пост интересный.

25
00:04:25,247 --> 00:04:32,999
Есть пост по введению в свёрточные нейронные сети.

26
00:04:32,999 --> 00:04:57,059
Anand Saha написал отличный анализ архитектуры ResNet, мы ещё обсудим это в курсе, продвинутые студенты могут читать уже сейчас.

27
00:04:57,059 --> 00:05:04,349
Apil Tamang написал похожий пост. В общем, на форумах много чего происходит.

28
00:05:06,059 --> 00:05:17,479
Мы также создали форум для новичков. Тупых вопросов не бывает, но иногда страшно задавать вопрос про что-то простое,

29
00:05:19,860 --> 00:05:29,819
когда вокруг обсуждаются очень сложные вещи. Надеемся, что форум новичков будет не таким устрашающим.

30
00:05:29,819 --> 00:05:38,632
Если вы продвинутый студент и можете отвечать на такие вопросы, пожалуйста, делайте это дружелюбно —

31
00:05:38,732 --> 00:05:47,934
у людей за плечами может быть всего год опыта программирования и никакого опыта машинного обучения.

32
00:05:48,034 --> 00:05:56,159
Если вам хочется написать какой-то вспомогательный материал, не стесняйтесь — большинство авторов только что упомянутых заметок

33
00:05:56,159 --> 00:06:04,769
никогда не публиковали ничего в интернете, они такие же люди, как и вы.

34
00:06:04,769 --> 00:06:20,399
Если вы не уверены в каких-то деталях, запостите сначала на форум, люди помогут вам улучшить ваш материал фразами вроде

35
00:06:20,399 --> 00:06:29,439
«Вот здесь всё устроено немного по-другому, сейчас расскажу» или «Это очень интересно, вы не думали углубиться в эту тему?».

36
00:06:29,539 --> 00:06:38,969
К текущему моменту мы немного обсудили свёрточные нейронные сети.

37
00:06:38,969 --> 00:06:59,319
Мы не углублялись в детали того, как они работают, зато построили на их основе превосходного качества модель.

38
00:06:59,319 --> 00:07:12,280
Сегодня мы ещё раз посмотрим на модели и наконец-то подойдём к теории — что такое свёрточная нейронная сеть,

39
00:07:12,280 --> 00:07:16,000
что такое свёртка, как и почему это работает.

40
00:07:16,000 --> 00:07:25,930
Дальше мы продвинемся по нашему плану и обсудим использование нейронных сетей для анализа структурированных данных —

41
00:07:25,930 --> 00:07:31,449
логистика, прогнозы, анализ рынка и прочее.

42
00:07:31,449 --> 00:07:45,550
Потом посмотрим на обработку естественного языка, потом на коллаборативную фильтрацию для рекомендательных систем.

43
00:07:45,550 --> 00:07:53,020
Это будет проходить в том же формате, что и обсуждение CNN — без углубления в теорию, но с построением качественных моделей.

44
00:07:55,150 --> 00:08:01,984
Потом мы пройдёмся по этим же темам, но в другом порядке и более углубленно —

45
00:08:02,084 --> 00:08:09,949
рассмотрим коллаборативную фильтрацию, поймём, как написан соответствующий код и какая за этим стоит математика,

46
00:08:10,049 --> 00:08:20,620
потом сделаем то же самое для анализа структурированных данных, свёрточных нейронных сетей и обработки естественного языка.

47
00:08:20,620 --> 00:08:37,078
Давайте проговорим некоторые вещи с прошлых лекций ещё раз.

48
00:08:37,178 --> 00:08:55,889
Я хочу убедиться, что все смогут повторить модель с прошлой лекции для различения пород собак.

49
00:08:55,889 --> 00:09:11,339
Для этого нужно скачать данные. Данные можно скачивать либо с Kaggle, либо откуда-то ещё.

50
00:09:11,339 --> 00:09:37,960
Для скачивания данных с Kaggle мы используем kaggle-cli, он должен был установиться при загрузке материалов для курса.

51
00:09:38,060 --> 00:09:53,279
Если в процессе скачивания данных на сайте Kaggle что-то меняется, kaggle-cli падает,

52
00:09:53,279 --> 00:10:13,434
с этим можно справиться командой pip install kaggle-cli --upgrade.

53
00:10:13,534 --> 00:10:25,765
После этого следуйте инструкциям reshamas для скачивания данных.

54
00:10:25,865 --> 00:10:43,509
Команда для скачивания данных: kg download -u  -p  -c .

55
00:10:43,609 --> 00:11:02,100
— это фраза, идущая после /c/ в адресной строке на странице соревнования.

56
00:11:02,100 --> 00:11:12,650
Перед использованием kaggle-cli убедитесь, что приняли правила использования, то есть скачивали данные вручную хотя бы однажды.

57
00:11:12,750 --> 00:11:18,850
Если правила не приняты, kaggle-cli вам об этом скажет.

58
00:11:18,850 --> 00:11:31,085
Если вы используете аккаунт Google для входа в Kaggle, kaggle-cli не будет работать, используйте форму восстановления пароля Kaggle.

59
00:11:31,185 --> 00:11:38,775
kaggle-cli создаст папку на вашем компьютере и скачает туда данные.

60
00:11:38,875 --> 00:11:52,600
kaggle-cli не подходит, если вы используете датасет не с Kaggle или если вам не нужны все предоставленные там данные.

61
00:11:52,600 --> 00:12:11,890
Например, в этом соревновании данные предоставлены в форматах TIFF (19 ГБ) и JPG (600 МБ), возможно, вы не захотите использовать оба.

62
00:12:11,890 --> 00:12:32,480
Для таких случаев есть расширение CurlWget для Google Chrome.

63
00:12:32,580 --> 00:13:02,944
Для всех страниц, откуда можно что-нибудь скачать, CurlWget предоставит вам ссылку для скачивания из терминала.

64
00:13:03,044 --> 00:13:10,449
В ссылке содержатся куки вашего браузера, для активации нужно скачать что-то вручную один раз.

65
00:13:10,449 --> 00:13:19,420
С помощью этого расширения можно скачать что угодно — например, ваш любимый сериал.

66
00:13:19,420 --> 00:13:33,610
Это очень полезный инструмент для анализа данных, потому что зачастую приходится анализировать видео.

67
00:13:33,610 --> 00:13:44,019
Итак, есть два способа получить данные. После этого можно начинать обучать модель.

68
00:13:44,019 --> 00:13:57,670
В переменной PATH указано, что данные лежат в директории data рабочей директории Jupyter ноутбука.

69
00:13:57,670 --> 00:14:04,990
Это не всегда удобно — данные могут лежать в другом месте или даже на другом диске,

70
00:14:04,990 --> 00:14:17,949
поэтому можно создать символическую ссылку на директорию с данными.

71
00:14:17,949 --> 00:14:24,109
Вы можете сложить ваши данные куда угодно и добавить на них ссылку или сложить их в рабочую директорию Jupyter ноутбука.

72
00:14:24,209 --> 00:14:35,829
Символические ссылки — это очень удобно, можете почитать про них на нашем форуме, если не работали с ними раньше.

73
00:14:35,829 --> 00:14:45,490
Как видите, модули библиотеки fast.ai доступны из Jupyter ноутбука таким же образом.

74
00:14:45,490 --> 00:15:06,845
Чтобы вывести список файлов и директорий с указанием ссылок в Linux, используйте команду ls -l.

75
00:15:06,945 --> 00:15:20,315
Вам могло показаться, что для обучения модели нужно больше кода, чем на самом деле.

76
00:15:20,415 --> 00:15:28,292
Здесь на одном экране показаны все шаги, которые я проделал для обучения модели для классификации кошек и собак.

77
00:15:28,392 --> 00:15:36,197
Здесь не показаны скачивание и распаковка данных с Kaggle.

78
00:15:36,297 --> 00:15:51,130
Это все необходимые этапы. Сначала мы импортируем библиотеки — fastai.conv_learner импортирует всё необходимое сам.

79
00:15:51,130 --> 00:15:57,860
Нужно указать путь к файлам, размер изображений и размер минибатча.

80
00:15:57,960 --> 00:16:10,940
В этой строке мы говорим, как нужно преобразовать изображения — какая будет модель, какого они должны быть размера,

81
00:16:11,040 --> 00:16:18,490
какой алгоритм дополнения данных использовать, какое максимальное увеличение использовать.

82
00:16:18,490 --> 00:16:34,360
Здесь мы говорим, что данные рассортированы на обучающую и валидационную выборку, а внутри каждой выборки — на кошек и собак.

83
00:16:34,360 --> 00:16:45,170
Если папки с обучающей и валидационной выборкой названы нестандартно, их названия можно передать через параметры trn_name и val_name.

84
00:16:45,270 --> 00:16:56,380
Название папки с неразмеченной тестовой выборкой test_name необходимо указывать, если вы будете отправлять модель на Kaggle.

85
00:16:59,770 --> 00:17:08,205
После этого мы создаём модель с архитектурой ResNet на основе уже обученной и вызываем метод .fit().

86
00:17:08,305 --> 00:17:17,420
Напомню, что по умолчанию все слои, кроме последних, заморожены, мы ещё будем про это говорить.

87
00:17:17,420 --> 00:17:23,110
Метод .fit() работал около двух с половиной минут. Я не выставлял precompute=True.

88
00:17:26,119 --> 00:17:35,390
На форумах было много уточняющих вопросов по поводу значения этого параметра, повторюсь — это просто небольшое ускорение.

89
00:17:35,390 --> 00:17:42,325
Если не до конца понятно, просто пропускайте и оставляйте precompute=False по умолчанию.

90
00:17:42,425 --> 00:17:52,635
precompute=True кэширует некоторые промежуточные результаты, чтобы их не нужно было пересчитывать каждый раз.

91
00:17:52,735 --> 00:18:11,180
Помните, что с precomputed=True не работает дополнение данных, потому что дополнение данных не работает с предвычисленными активациями.

92
00:18:11,180 --> 00:18:14,780
Я максимально упростил процесс, поэтому precompute=False.

93
00:18:14,780 --> 00:18:22,810
Последний слой обучается 3 цикла с длиной цикла в одну эпоху (cycle_len=1).

94
00:18:23,110 --> 00:18:27,190
После этого я размораживаю модель и обучаю уже все слои.

95
00:18:27,590 --> 00:18:39,170
Метод .bn_freeze() мы ещё обсудим, это особенность сложных архитектур типа ResNet50 или ResNeXt101.

96
00:18:39,170 --> 00:18:49,005
Эту строку имеет добавлять после разморозки, если вы используете датасеты, подобные изображениям ImageNet —

97
00:18:49,105 --> 00:19:02,495
фотографии обычных объектов со стороны размера от 200 до 500 пикселей.

98
00:19:02,595 --> 00:19:10,630
Для продвинутых студентов — это фиксирует нормализацию скользящего среднего.

99
00:19:10,730 --> 00:19:20,409
Мы ещё обсудим это во второй части курса, в других библиотеках этого нет, но это очень важный аспект.

100
00:19:20,409 --> 00:19:31,789
После этого все слои нейронной сети обучаются в течение одной эпохи, и применяется дополнение тестовых данных.

101
00:19:31,889 --> 00:19:37,909
Это даёт долю правильных ответов в 99.45%.

102
00:19:38,009 --> 00:19:47,139
Это минимальные шаги, которые можно выполнить при работе с новым датасетом,

103
00:19:47,139 --> 00:20:02,799
при условии, что вы уже подобрали скорость обучения, знаете, как устроены данные и так далее.

104
00:20:02,799 --> 00:20:15,499
Я хотел показать вам, как устроены другие библиотеки, и выбрал для этого Keras.

105
00:20:15,599 --> 00:20:33,059
Библиотека fast.ai построена на основе PyTorch, а Keras поддерживает TensorFlow, MXNet, CNTK и многие другие библиотеки,

106
00:20:33,159 --> 00:20:37,450
большинство людей используют Keras с TensorFlow.

107
00:20:37,450 --> 00:20:51,024
В Jupyter ноутбуке keras_lesson1.ipynb я постараюсь воссоздать модель с первой лекции на Keras, чтобы вы увидели, как это можно делать.

108
00:20:51,124 --> 00:20:59,510
Я не буду пока ничего говорить про метод .bn_freeze(), кроме показаний к применению —

109
00:21:00,850 --> 00:21:11,950
это архитектура нейронной сети с числом больше 34, как ResNet50 или ResNeXt101,

110
00:21:11,950 --> 00:21:24,520
и датасет, похожий на изображения ImageNet, где объект занимает большую часть изображений нормального размера.

111
00:21:24,520 --> 00:21:32,989
Если вы не уверены в его необходимости, попробуйте убрать и сравнить результаты.

112
00:21:33,089 --> 00:21:39,910
Продвинутые студенты наверняка начнут обсуждать это на форумах уже сейчас, а мы дойдём до этого

113
00:21:39,910 --> 00:21:46,720
только во второй части курса, когда вернёмся к свёрточным нейронным сетям.

114
00:21:50,820 --> 00:22:01,480
Итак, для работы с Keras нужно импортировать необходимые модули.

115
00:22:01,480 --> 00:22:12,151
Keras поддерживает стандартный способ сортировки данных на обучающую и валидационную выборки и на классы внутри выборок.

116
00:22:12,251 --> 00:22:22,805
Здесь мы указываем пути к папкам с обучающей и валидационной выборкой.

117
00:22:22,905 --> 00:22:36,575
Вы заметите, что с Keras обучение модели требует больше кода и различных параметров,

118
00:22:36,675 --> 00:22:44,650
и очень просто задать неправильные параметры, поэтому я постараюсь подробно всё показать.

119
00:22:44,650 --> 00:22:55,030
Для подготовки данных необходимо создать генератор данных конструктором ImageDataGenerator().

120
00:22:55,030 --> 00:23:10,634
В параметры генератора данных необходимо передать параметры дополнения данных и нормализации.

121
00:23:10,734 --> 00:23:15,619
В библиотеке fast.ai достаточно указать архитектуру, например, ResNet50, и необходимые параметры выставляются автоматически,

122
00:23:15,719 --> 00:23:21,700
здесь надо примерно понимать, что для этого требуется.

123
00:23:21,700 --> 00:23:30,220
В принципе копирования кода из интернета достаточно, чтобы всё работало.

124
00:23:30,220 --> 00:23:43,550
Нет общепринятых стандартов по поводу этих параметров дополнения данных, я скопировал эту строку из документации Keras.

125
00:23:43,550 --> 00:23:49,140
Я не знаю, хороший ли это набор параметров, но в документации используется такой.

126
00:23:49,240 --> 00:23:56,540
Параметры говорят, отражать ли изображения относительно горизонтали, как увеличивать, как сдвигать.

127
00:23:56,540 --> 00:24:03,102
Из генератора данных создаётся генератор методом .flow_from_directory().

128
00:24:03,202 --> 00:24:17,985
В его параметры передаётся путь к файлам, размер изображений, размер минибатча и параметр class_mode.

129
00:24:18,085 --> 00:24:31,650
Параметр class_mode указывает вид задачи классификации — двухклассовая или многоклассовая, 'binary' или 'categorial'.

130
00:24:31,750 --> 00:24:36,525
У нас два класса — кошки и собаки, class_mode='binary'.

131
00:24:36,625 --> 00:24:48,048
Необходимо отдельно создать генератор данных без дополнения данных для валидации

132
00:24:48,148 --> 00:25:00,760
и создать соответствующий генератор с параметром shuffle=False, чтобы валидационная выборка не перемешивалась —

133
00:25:00,860 --> 00:25:11,270
это полезно для обучающей выборки, но на валидационной помешает отслеживать прогресс в обучении.

134
00:25:11,270 --> 00:25:18,465
Эти шаги в Keras необходимо делать каждый раз.

135
00:25:18,565 --> 00:25:32,750
Keras не поддерживает ResNet34, поэтому в конце прошлой лекции я поменял ResNet34 на ResNet50, чтобы мы могли сравнить fastai и Keras на одной архитектуре.

136
00:25:32,750 --> 00:25:42,290
В Keras модель не подстраивается под датасет автоматически, это нужно делать вручную.

137
00:25:42,290 --> 00:25:50,600
Для этого конструктором ResNet50() создаётся базовая модель и к ней вручную добавляются дополнительные слои.

138
00:25:50,600 --> 00:26:01,370
К концу этого курса вы поймёте, почему мы добавили именно эти три слоя.

139
00:26:01,370 --> 00:26:06,070
Модель создаётся конструктором Model().

140
00:26:06,070 --> 00:26:19,515
В Keras нет встроенной функции заморозки, поэтому мы проходим по всем слоям и выставляем поле .trainable=False.

141
00:26:19,615 --> 00:26:27,110
В Keras необходимо компилировать модель после создания методом .compile().

142
00:26:28,160 --> 00:26:35,360
Метод принимает как параметры вид оптимизации, функцию потерь и метрику оценки качества модели.

143
00:26:36,439 --> 00:26:47,324
В fast.ai эти значения передаются по умолчанию, хотя есть возможность заменить их своими.

144
00:26:47,424 --> 00:26:55,886
Вместо метода .fit() вызывается метод .fit_generator(), он принимает как параметры два только что созданных генератора,

145
00:26:55,886 --> 00:27:06,681
зачем-то просит количество минибатчей в одной эпохе — это размер генератора, делённый на размер минибатча.

146
00:27:07,081 --> 00:27:13,680
Как и в fast.ai, задаётся количество эпох.

147
00:27:15,100 --> 00:27:35,639
Задаётся количество воркеров. В отличие от fast.ai, по умолчанию они не используются, важно не забыть про этот параметр для достижения хорошей скорости.

148
00:27:35,739 --> 00:27:43,469
Этого достаточно, чтобы начать тонкую настройку последних слоёв.

149
00:27:43,569 --> 00:27:53,805
На валидационной выборке доля правильных ответов получилась 95%, но на первой и второй эпохах она была 49% и 69%.

150
00:27:53,905 --> 00:28:01,115
Я не знаю, почему это так — возможно, ошибка в Keras, возможно, в моём коде.

151
00:28:01,215 --> 00:28:07,469
Я писал про это в Twitter, но там никто не смог разобраться.

152
00:28:07,569 --> 00:28:15,509
Это одна из причин, почему я использую fast.ai в этом курсе — там гораздо сложнее всё испортить.

153
00:28:15,609 --> 00:28:18,806
Я не знаю, в чём тут ошибка.

154
00:28:18,906 --> 00:28:23,270
Янет: Здесь используется TensorFlow?

155
00:28:23,500 --> 00:28:50,279
Да, здесь используется TensorFlow, для этого нужно установить его командой pip install tensorflow-gpu keras.

156
00:28:50,379 --> 00:29:08,539
В Keras нет дифференциальных скоростей обучения и частичного размораживания, поэтому нужно вручную отделить последние слои.

157
00:29:08,539 --> 00:29:17,210
Я буду настраивать все слои, начиная со 140-го, для этого прохожу по всем слоям и замораживаю или размораживаю их, после этого снова вызываю метод .compile().

158
00:29:17,210 --> 00:29:26,924
После этого я снова обучаю модель, доля правильных ответов на обучающей выборке примерно такая же, а на валидационной опять ерунда.

159
00:29:27,024 --> 00:29:40,062
Даже если не учитывать это, Keras проигрывает в сравнении с fastai — кода гораздо больше и результаты хуже:

160
00:29:40,162 --> 00:29:49,934
модель на Keras за восемь минут достигла доли правильных ответов в 97% на обучающей выборке,

161
00:29:50,034 --> 00:30:02,380
а модель на fastai за четыре или пять минут — 99.5% на валидационной выборке.

162
00:30:02,380 --> 00:30:17,620
Используйте TensorFlow, если вы разрабатываете что-то для смартфонов, PyTorch пока плохо это поддерживает.

163
00:30:17,620 --> 00:30:24,365
Вам придётся использовать TensorFlow, если это делает компания, где вы работаете.

164
00:30:24,465 --> 00:30:37,000
Если вам нужно повторить что-то из этого курса с TensorFlow, используйте Keras и будьте готовы к тому,

165
00:30:37,000 --> 00:30:53,690
что будет больше кода, и будет сложнее повторить результаты, которые легко достигаются в fast.ai.

166
00:30:52,190 --> 00:30:53,690
===========================================

167
00:30:54,669 --> 00:31:05,990
В fast.ai нет ничего, что нельзя было бы повторить в Keras,

168
00:31:06,090 --> 00:31:20,389
но каждый раз заново писать SGDR, дифференциальные скорости обучения и всё остальное - неудобно.

169
00:31:20,489 --> 00:31:29,199
На форуме один человек работает над интеграцией Keras/TensorFlow и fast.ai, я надеюсь, что из этого что-то выйдет.

170
00:31:29,299 --> 00:31:43,270
Я разговаривал с Google и они тоже в этом заинтересованы. Возможно, к тому моменту, как этот курс появится на MOOC-платформе, это сделают.

171
00:31:43,270 --> 00:32:05,900
Keras/TensorFlow не очень сложны для обучения, для миграции с fast.ai после этого курса вам понадобится пара дней.

172
00:32:06,000 --> 00:32:19,890
Всего сказанного должно хватить для того, чтобы вы смогли обучить модель на датасете с породами собак.

173
00:32:19,890 --> 00:32:26,600
Большую часть того, что нужно сделать, я показал в конце прошлой лекции -

174
00:32:30,870 --> 00:32:37,885
например, как я изучал данные, чтобы понять, как устроены классы и какого размера изображения.

175
00:32:37,985 --> 00:32:41,980
Если вы что-то забыли, пересмотрите прошлую лекцию.

176
00:32:42,080 --> 00:32:53,794
Мы не успели обсудить, как отправить модель на Kaggle, сейчас покажу.

177
00:32:53,894 --> 00:33:01,500
Я уже написал про это в вики.

178
00:33:01,500 --> 00:33:12,955
На Kaggle для каждого соревнования есть вкладка Evaluation, в ней написано, какой формат вывода ожидается.

179
00:33:13,055 --> 00:33:18,910
В этом соревновании на выходе должен быть файл, где в заголовке - ID и все возможные породы сбоак,

180
00:33:19,010 --> 00:33:31,860
а в остальных строках - ID файлов тестовой выборки и вероятности того, что на этих файлах различные породы собак.

181
00:33:35,610 --> 00:33:55,240
Объект data.classes содержит названия всех классов в алфавитном порядке.

182
00:33:55,340 --> 00:34:01,645
Объект data.test_ds содержит тестовую выборку, названия файлов лежат в data.test_ds.fnames.

183
00:34:01,745 --> 00:34:18,444
Напоминаю, что изображения не разложены по папкам в стиле Keras, а размечены в csv-файле,

184
00:34:18,544 --> 00:34:27,859
поэтому мы используем метод ImageDataClassifier.from_csv(), а не ImageDataClassifier.from_paths().

185
00:34:31,799 --> 00:34:40,139
Keras не поддерживает такой формат, поэтому на форумах Kaggle люди выкладывают скрипты для сортировки данных по папкам,

186
00:34:40,139 --> 00:34:47,574
но нам этого делать не придётся.

187
00:34:47,674 --> 00:35:03,740
Итак, у нас есть названия пород и имена файлов тестовой выборки.

188
00:35:03,740 --> 00:35:28,789
Я всегда использую TTA при предсказании тестовой выборки, для этого в метод .TTA() передаётся параметр is_test=True,

189
00:35:28,889 --> 00:35:35,369
и предсказания делаются на тестовой выборке, а не на валидационной.

190
00:35:35,369 --> 00:35:44,765
Мы не знаем, какая получилась доля правильных ответов, потому что тестовая выборка не размечена.

191
00:35:44,865 --> 00:35:57,099
Большинство моделей PyTorch возвращает логарифмы вероятностей, метод np.exp() переводит их в обычные вероятности.

192
00:35:57,199 --> 00:36:10,529
В тестовой выборке 10357 изображений, принадлежащих 120 различным породам собак, это размеры полученной матрицы предсказаний.

193
00:36:10,529 --> 00:36:17,695
Из этой матрицы мы создадим файл необходимого формата с помощью pandas.

194
00:36:17,795 --> 00:36:27,472
Если вы не работали с pandas, погуглите или посмотрите наш курс по машинному обучению, там это часто используется.

195
00:36:27,572 --> 00:36:34,807
Мы создаём датафрейм pandas из матрицы методом pandas.DataFrame(), присваиваем колонкам названия пород собак

196
00:36:34,907 --> 00:36:57,745
и вставляем нулевую колонку с названиями файлов. Названия содержат 'test/'в начале и '.jpg' в конце, эти части мы обрезаем.

197
00:36:57,845 --> 00:37:06,550
Итоговый датафрейм выглядит так.

198
00:37:06,650 --> 00:37:24,000
Датафреймы с данными обычно называют df (data frame).

199
00:37:24,000 --> 00:37:36,680
С помощью метода .to_csv() можно записать получившийся датафрейм в csv-файл. Имеет смысл включить сжатие параметром compression='gzip'.

200
00:37:36,780 --> 00:37:45,082
После этого на сервере Jupyter Notebook появится csv-файл.

201
00:37:45,182 --> 00:37:57,235
После этого вы можете либо скачать файл с сервера и отправить его вручную, либо использовать kaggle-cli.

202
00:37:57,335 --> 00:38:04,170
Я обычно скачиваю файл себе на компьютер, чтобы посмотреть на него перед отправкой.

203
00:38:04,170 --> 00:38:27,130
Есть удобная утилита FileLink, которая создаёт ссылку, по которой можно скачать файл с сервера Jupyter Notebook на ваш компьютер.

204
00:38:33,490 --> 00:38:43,025
Архив с файлом скачался.

205
00:38:43,125 --> 00:38:54,095
Формат именно такой, какой нужно - в заголовке строке ID и породы собак, остальные строки содержат имя файла и вероятности.

206
00:38:54,195 --> 00:39:00,695
Теперь можно загрузить его на Kaggle через веб-интерфейс.

207
00:39:00,795 --> 00:39:15,160
Итак, теперь мы умеем скачивать файлы из интернета на сервер Jupyter Notebook через CurlWget в Google Chrome

208
00:39:17,170 --> 00:39:31,270
и умеем скачивать файлы с сервера на свой компьютер. Можно использовать scp в терминале, но мне нравится делать это в Jupyter ноутбуке.

209
00:39:31,270 --> 00:39:42,910
На этой неделе меня спросили, как получить предсказание только для одного файла.

210
00:39:42,910 --> 00:39:49,790
Допустим, я хочу получить предсказание для первого изображения валидационной выборки.

211
00:39:49,890 --> 00:39:59,350
Вот имя файла, я могу его вывести методом Image.open() стандартной библиотеки Python.

212
00:39:59,350 --> 00:40:16,230
Самое простое, что вы можете сделать - вызвать метод .predict_array().

213
00:40:16,230 --> 00:40:21,390
Для этого надо сначала применить к изображению дополнение данных.

214
00:40:21,390 --> 00:40:36,310
Функция tfms_from_model возвращает преобразования данных отдельно для обучающей и валидационной выборки.

215
00:40:36,310 --> 00:40:45,280
После этого мы применяем на изображении преобразования данных для обучающей выборки. Нет, лучше для валидационной.

216
00:40:45,280 --> 00:40:51,220
Полученный массив можно передавать в метод .predict_array().

217
00:40:55,440 --> 00:41:05,320
Данные можно подавать в модель и получать от модели только в минибатчах.

218
00:41:05,320 --> 00:41:15,250
У нас только одно изображение, и мы ходим превратить его в минибатч,

219
00:41:15,250 --> 00:41:21,490
то есть превратить из матрицы размерности (количество строк)x(количество столбцов)х(цветовые каналы)

220
00:41:21,490 --> 00:41:26,230
в матрицу (количество изображений)x(количество строк)x(количество столбцов)х(цветовые каналы).

221
00:41:26,230 --> 00:41:31,330
У нас трёхмерная матрица, а должна быть четырёхмерная.

222
00:41:31,330 --> 00:41:46,480
Если в numpy индексировать массив im как im[None], вернётся массив размерностью больше на 1, так мы превратили изображение в минибатч.

223
00:41:46,480 --> 00:42:02,080
Если вы забудете это сделать при использовании PyTorch или fast.ai, получите ошибку вроде "expected 4 dimensions but got 3".

224
00:42:02,080 --> 00:42:15,040
Модели не только принимают, но и возвращают минибатчи, это тоже учтите.

225
00:42:15,040 --> 00:42:38,405
На этом мы закончим с практикой и перейдём к теории свёрточный нейронных сетей.

226
00:42:38,505 --> 00:42:42,410


227
00:42:43,850 --> 00:42:51,720


228
00:42:51,720 --> 00:42:56,730


229
00:42:56,730 --> 00:43:01,320


230
00:43:01,320 --> 00:43:06,600


231
00:43:09,030 --> 00:43:14,010


232
00:43:14,010 --> 00:43:19,670


233
00:43:19,670 --> 00:43:26,370


234
00:43:26,370 --> 00:43:31,860


235
00:43:31,860 --> 00:43:35,010


236
00:43:35,010 --> 00:43:39,570


237
00:43:39,570 --> 00:43:45,540


238
00:43:45,540 --> 00:43:48,870


239
00:43:48,870 --> 00:43:54,600


240
00:43:54,600 --> 00:43:58,080


241
00:43:58,080 --> 00:44:03,630


242
00:44:03,630 --> 00:44:09,240


243
00:44:10,350 --> 00:44:14,310


244
00:44:14,310 --> 00:44:18,710


245
00:44:18,710 --> 00:44:23,400


246
00:44:23,400 --> 00:44:27,720


247
00:44:29,880 --> 00:44:33,540


248
00:44:33,540 --> 00:44:36,990


249
00:44:36,990 --> 00:44:41,610


250
00:44:41,610 --> 00:44:46,290


251
00:44:46,290 --> 00:44:51,990


252
00:44:51,990 --> 00:44:57,150


253
00:44:57,150 --> 00:45:01,020


254
00:45:01,020 --> 00:45:05,010


255
00:45:05,010 --> 00:45:10,350


256
00:45:13,140 --> 00:45:18,990


257
00:45:18,990 --> 00:45:24,450


258
00:45:24,450 --> 00:45:30,180


259
00:45:30,180 --> 00:45:33,870


260
00:45:33,870 --> 00:45:39,180


261
00:45:39,180 --> 00:45:44,760


262
00:45:44,760 --> 00:45:49,380


263
00:45:49,380 --> 00:45:55,890


264
00:45:57,450 --> 00:46:02,610


265
00:46:02,610 --> 00:46:07,080


266
00:46:07,080 --> 00:46:10,800


267
00:46:10,800 --> 00:46:15,570


268
00:46:15,570 --> 00:46:20,460


269
00:46:20,460 --> 00:46:26,520


270
00:46:26,520 --> 00:46:30,780


271
00:46:30,780 --> 00:46:36,180


272
00:46:38,190 --> 00:46:43,890


273
00:46:43,890 --> 00:46:48,700


274
00:46:48,700 --> 00:46:53,040


275
00:46:53,040 --> 00:46:57,700


276
00:46:57,700 --> 00:47:02,619


277
00:47:02,619 --> 00:47:08,680


278
00:47:10,510 --> 00:47:14,349


279
00:47:14,349 --> 00:47:21,190


280
00:47:21,190 --> 00:47:26,530


281
00:47:26,530 --> 00:47:30,490


282
00:47:33,010 --> 00:47:37,660


283
00:47:40,089 --> 00:47:45,130


284
00:47:45,130 --> 00:47:52,180


285
00:47:53,799 --> 00:48:00,430


286
00:48:00,430 --> 00:48:05,140


287
00:48:05,140 --> 00:48:10,690


288
00:48:10,690 --> 00:48:15,700


289
00:48:18,579 --> 00:48:22,390


290
00:48:24,579 --> 00:48:29,799


291
00:48:29,799 --> 00:48:35,530


292
00:48:35,530 --> 00:48:41,619


293
00:48:41,619 --> 00:48:46,119


294
00:48:46,119 --> 00:48:50,170


295
00:48:50,170 --> 00:48:54,460


296
00:48:54,460 --> 00:48:58,569


297
00:48:58,569 --> 00:49:02,500


298
00:49:02,500 --> 00:49:08,710


299
00:49:08,710 --> 00:49:14,680


300
00:49:14,680 --> 00:49:21,130


301
00:49:21,130 --> 00:49:28,030


302
00:49:28,030 --> 00:49:33,790


303
00:49:35,200 --> 00:49:40,420


304
00:49:40,420 --> 00:49:43,480


305
00:49:43,480 --> 00:49:47,410


306
00:49:47,410 --> 00:49:52,480


307
00:49:52,480 --> 00:49:58,060


308
00:49:58,060 --> 00:50:03,490


309
00:50:03,490 --> 00:50:08,950


310
00:50:08,950 --> 00:50:21,070


311
00:50:21,070 --> 00:50:26,890


312
00:50:29,170 --> 00:50:32,230


313
00:50:32,230 --> 00:50:36,880


314
00:50:36,880 --> 00:50:44,710


315
00:50:44,710 --> 00:50:51,310


316
00:50:51,310 --> 00:50:55,290


317
00:50:55,290 --> 00:51:01,360


318
00:51:01,360 --> 00:51:09,640


319
00:51:13,060 --> 00:51:16,089


320
00:51:16,089 --> 00:51:22,449


321
00:51:22,449 --> 00:51:25,900


322
00:51:25,900 --> 00:51:32,170


323
00:51:32,170 --> 00:51:36,489


324
00:51:36,489 --> 00:51:42,400


325
00:51:42,400 --> 00:51:52,299


326
00:51:52,299 --> 00:51:58,299


327
00:51:58,299 --> 00:52:05,170


328
00:52:05,170 --> 00:52:10,119


329
00:52:10,119 --> 00:52:16,630


330
00:52:16,630 --> 00:52:22,029


331
00:52:22,029 --> 00:52:28,239


332
00:52:28,239 --> 00:52:35,249


333
00:52:37,989 --> 00:52:41,319


334
00:52:41,319 --> 00:52:47,049


335
00:52:47,049 --> 00:52:54,670


336
00:52:54,670 --> 00:53:04,420


337
00:53:04,420 --> 00:53:08,859


338
00:53:08,859 --> 00:53:14,769


339
00:53:14,769 --> 00:53:22,359


340
00:53:22,359 --> 00:53:28,990


341
00:53:28,990 --> 00:53:38,500


342
00:53:38,500 --> 00:53:46,000


343
00:53:46,000 --> 00:53:52,240


344
00:53:52,240 --> 00:53:59,230


345
00:53:59,230 --> 00:54:07,060


346
00:54:07,060 --> 00:54:13,750


347
00:54:13,750 --> 00:54:17,710


348
00:54:17,710 --> 00:54:25,480


349
00:54:25,480 --> 00:54:32,920


350
00:54:32,920 --> 00:54:36,610


351
00:54:36,610 --> 00:54:40,930


352
00:54:42,970 --> 00:54:50,490


353
00:54:50,490 --> 00:54:57,370


354
00:54:57,370 --> 00:55:03,560


355
00:55:03,560 --> 00:55:09,290


356
00:55:09,290 --> 00:55:15,920


357
00:55:15,920 --> 00:55:20,750


358
00:55:20,750 --> 00:55:28,670


359
00:55:28,670 --> 00:55:35,210


360
00:55:35,210 --> 00:55:41,540


361
00:55:41,540 --> 00:55:50,060


362
00:55:50,060 --> 00:55:54,230


363
00:55:54,230 --> 00:55:59,870


364
00:55:59,870 --> 00:56:07,220


365
00:56:07,220 --> 00:56:13,490


366
00:56:13,490 --> 00:56:19,700


367
00:56:19,700 --> 00:56:23,990


368
00:56:23,990 --> 00:56:30,680


369
00:56:30,680 --> 00:56:40,010


370
00:56:40,010 --> 00:56:44,150


371
00:56:44,150 --> 00:56:48,680


372
00:56:50,810 --> 00:56:59,210


373
00:56:59,210 --> 00:57:02,080


374
00:57:02,200 --> 00:57:09,849


375
00:57:09,849 --> 00:57:17,710


376
00:57:17,710 --> 00:57:23,589


377
00:57:23,589 --> 00:57:28,930


378
00:57:28,930 --> 00:57:34,540


379
00:57:34,540 --> 00:57:38,589


380
00:57:38,589 --> 00:57:44,109


381
00:57:44,109 --> 00:57:48,430


382
00:57:48,430 --> 00:57:58,599


383
00:57:58,599 --> 00:58:09,510


384
00:58:09,510 --> 00:58:19,119


385
00:58:19,119 --> 00:58:22,420


386
00:58:22,420 --> 00:58:28,000


387
00:58:28,000 --> 00:58:35,530


388
00:58:35,530 --> 00:58:40,720


389
00:58:42,670 --> 00:58:47,140


390
00:58:47,140 --> 00:58:54,160


391
00:58:54,160 --> 00:58:58,990


392
00:58:58,990 --> 00:59:01,930


393
00:59:01,930 --> 00:59:06,390


394
00:59:06,390 --> 00:59:09,760


395
00:59:09,760 --> 00:59:14,470


396
00:59:15,800 --> 00:59:19,550


397
00:59:19,550 --> 00:59:23,090


398
00:59:23,090 --> 00:59:27,730


399
00:59:27,730 --> 00:59:33,170


400
00:59:33,170 --> 00:59:38,900


401
00:59:38,900 --> 00:59:44,720


402
00:59:48,710 --> 00:59:54,740


403
00:59:59,300 --> 01:00:02,540


404
01:00:02,540 --> 01:00:08,510


405
01:00:08,510 --> 01:00:15,250


406
01:00:15,250 --> 01:00:25,100


407
01:00:25,100 --> 01:00:30,650


408
01:00:30,650 --> 01:00:35,930


409
01:00:40,640 --> 01:00:45,680


410
01:00:45,680 --> 01:00:52,940


411
01:00:52,940 --> 01:00:57,830


412
01:00:57,830 --> 01:01:07,670


413
01:01:07,670 --> 01:01:13,730


414
01:01:13,730 --> 01:01:17,930


415
01:01:17,930 --> 01:01:26,150


416
01:01:26,150 --> 01:01:30,400


417
01:01:30,400 --> 01:01:36,520


418
01:01:36,520 --> 01:01:43,569


419
01:01:43,569 --> 01:01:49,180


420
01:01:49,180 --> 01:01:57,359


421
01:01:57,359 --> 01:02:02,560


422
01:02:02,560 --> 01:02:06,780


423
01:02:06,780 --> 01:02:13,690


424
01:02:13,690 --> 01:02:19,030


425
01:02:19,030 --> 01:02:24,160


426
01:02:26,319 --> 01:02:33,460


427
01:02:33,460 --> 01:02:38,050


428
01:02:38,050 --> 01:02:43,000


429
01:02:43,000 --> 01:02:52,150


430
01:02:52,150 --> 01:02:58,119


431
01:02:58,119 --> 01:03:02,890


432
01:03:04,900 --> 01:03:10,180


433
01:03:12,790 --> 01:03:16,930


434
01:03:16,930 --> 01:03:22,300


435
01:03:22,300 --> 01:03:28,150


436
01:03:28,150 --> 01:03:35,650


437
01:03:35,650 --> 01:03:40,440


438
01:03:40,670 --> 01:03:45,619


439
01:03:45,619 --> 01:03:49,490


440
01:03:51,980 --> 01:03:57,079


441
01:03:57,079 --> 01:04:03,290


442
01:04:03,290 --> 01:04:09,920


443
01:04:09,920 --> 01:04:15,049


444
01:04:15,049 --> 01:04:20,540


445
01:04:20,540 --> 01:04:25,280


446
01:04:25,280 --> 01:04:31,750


447
01:04:31,750 --> 01:04:39,710


448
01:04:39,710 --> 01:04:46,250


449
01:04:46,250 --> 01:04:53,180


450
01:04:53,180 --> 01:05:03,319


451
01:05:03,319 --> 01:05:09,380


452
01:05:09,380 --> 01:05:13,700


453
01:05:15,470 --> 01:05:20,059


454
01:05:20,059 --> 01:05:26,049


455
01:05:26,049 --> 01:05:32,210


456
01:05:32,210 --> 01:05:40,280


457
01:05:40,280 --> 01:05:44,960


458
01:05:44,960 --> 01:05:52,160


459
01:05:52,160 --> 01:05:57,440


460
01:05:57,440 --> 01:06:02,989


461
01:06:02,989 --> 01:06:06,950


462
01:06:06,950 --> 01:06:11,749


463
01:06:11,749 --> 01:06:17,779


464
01:06:17,779 --> 01:06:23,059


465
01:06:23,059 --> 01:06:27,529


466
01:06:27,529 --> 01:06:34,099


467
01:06:34,099 --> 01:06:38,499


468
01:06:42,559 --> 01:06:48,619


469
01:06:48,619 --> 01:06:55,460


470
01:06:55,460 --> 01:06:59,630


471
01:06:59,630 --> 01:07:05,960


472
01:07:05,960 --> 01:07:11,420


473
01:07:11,420 --> 01:07:17,660


474
01:07:17,660 --> 01:07:23,719


475
01:07:23,719 --> 01:07:27,380


476
01:07:27,380 --> 01:07:32,779


477
01:07:32,779 --> 01:07:40,910


478
01:07:40,910 --> 01:07:45,440


479
01:07:45,440 --> 01:07:48,410


480
01:07:48,410 --> 01:07:53,029


481
01:07:53,029 --> 01:08:02,239


482
01:08:02,239 --> 01:08:06,890


483
01:08:06,890 --> 01:08:10,300


484
01:08:10,300 --> 01:08:15,940


485
01:08:15,940 --> 01:08:19,870


486
01:08:19,870 --> 01:08:23,230


487
01:08:23,229 --> 01:08:26,469


488
01:08:26,470 --> 01:08:29,050


489
01:08:29,050 --> 01:08:33,070


490
01:08:35,350 --> 01:08:39,760


491
01:08:39,760 --> 01:08:50,310


492
01:08:50,310 --> 01:09:00,339


493
01:09:02,290 --> 01:09:07,180


494
01:09:07,180 --> 01:09:11,770


495
01:09:11,770 --> 01:09:17,920


496
01:09:17,920 --> 01:09:23,970


497
01:09:23,970 --> 01:09:36,460


498
01:09:36,460 --> 01:09:44,740


499
01:09:44,740 --> 01:09:50,370


500
01:09:50,370 --> 01:09:57,130


501
01:09:57,130 --> 01:10:04,030


502
01:10:04,030 --> 01:10:09,790


503
01:10:09,790 --> 01:10:14,680


504
01:10:14,680 --> 01:10:19,810


505
01:10:19,810 --> 01:10:23,150


506
01:10:27,110 --> 01:10:40,190


507
01:10:40,190 --> 01:10:48,190


508
01:10:48,190 --> 01:10:56,539


509
01:10:56,539 --> 01:11:02,030


510
01:11:02,030 --> 01:11:06,199


511
01:11:06,199 --> 01:11:10,820


512
01:11:10,820 --> 01:11:16,989


513
01:11:16,989 --> 01:11:23,150


514
01:11:23,150 --> 01:11:32,300


515
01:11:32,300 --> 01:11:37,039


516
01:11:37,039 --> 01:11:40,340


517
01:11:40,340 --> 01:11:44,479


518
01:11:44,479 --> 01:11:48,249


519
01:11:50,510 --> 01:11:55,969


520
01:11:55,969 --> 01:12:01,400


521
01:12:01,400 --> 01:12:07,449


522
01:12:07,449 --> 01:12:16,340


523
01:12:16,340 --> 01:12:23,630


524
01:12:23,630 --> 01:12:29,780


525
01:12:29,780 --> 01:12:32,920


526
01:12:33,070 --> 01:12:39,260


527
01:12:39,260 --> 01:12:44,230


528
01:12:45,590 --> 01:12:55,230


529
01:12:58,619 --> 01:13:04,500


530
01:13:04,500 --> 01:13:09,840


531
01:13:09,840 --> 01:13:13,340


532
01:13:13,829 --> 01:13:18,210


533
01:13:18,210 --> 01:13:22,139


534
01:13:22,139 --> 01:13:29,179


535
01:13:29,179 --> 01:13:34,769


536
01:13:34,769 --> 01:13:39,599


537
01:13:39,599 --> 01:13:45,570


538
01:13:45,570 --> 01:13:51,269


539
01:13:51,269 --> 01:13:54,749


540
01:13:54,749 --> 01:14:00,239


541
01:14:00,239 --> 01:14:04,979


542
01:14:04,979 --> 01:14:10,320


543
01:14:12,300 --> 01:14:19,289


544
01:14:19,289 --> 01:14:24,869


545
01:14:24,869 --> 01:14:29,550


546
01:14:29,550 --> 01:14:35,729


547
01:14:35,729 --> 01:14:43,289


548
01:14:46,139 --> 01:14:53,489


549
01:14:54,960 --> 01:14:59,940


550
01:14:59,940 --> 01:15:06,090


551
01:15:06,090 --> 01:15:10,170


552
01:15:10,170 --> 01:15:16,199


553
01:15:18,630 --> 01:15:23,280


554
01:15:23,280 --> 01:15:26,670


555
01:15:26,670 --> 01:15:29,610


556
01:15:29,610 --> 01:15:37,470


557
01:15:37,470 --> 01:15:41,760


558
01:15:41,760 --> 01:15:46,500


559
01:15:46,500 --> 01:15:51,420


560
01:15:51,420 --> 01:15:58,650


561
01:15:58,650 --> 01:16:03,450


562
01:16:03,450 --> 01:16:08,670


563
01:16:08,670 --> 01:16:19,530


564
01:16:19,530 --> 01:16:33,090


565
01:16:33,090 --> 01:16:37,260


566
01:16:37,260 --> 01:16:41,010


567
01:16:41,010 --> 01:16:45,360


568
01:16:50,820 --> 01:16:57,480


569
01:16:57,480 --> 01:17:03,390


570
01:17:03,390 --> 01:17:14,370


571
01:17:14,370 --> 01:17:21,450


572
01:17:21,450 --> 01:17:25,140


573
01:17:25,140 --> 01:17:31,650


574
01:17:33,780 --> 01:17:38,260


575
01:17:38,260 --> 01:17:44,590


576
01:17:44,590 --> 01:17:49,450


577
01:17:49,450 --> 01:17:52,630


578
01:17:52,630 --> 01:17:58,210


579
01:17:58,210 --> 01:18:02,710


580
01:18:02,710 --> 01:18:05,320


581
01:18:05,320 --> 01:18:10,480


582
01:18:10,480 --> 01:18:15,850


583
01:18:15,850 --> 01:18:28,120


584
01:18:32,340 --> 01:18:37,830


585
01:18:37,830 --> 01:18:43,860


586
01:18:43,860 --> 01:18:51,460


587
01:18:51,460 --> 01:18:56,890


588
01:18:56,890 --> 01:19:04,840


589
01:19:04,840 --> 01:19:11,050


590
01:19:11,050 --> 01:19:16,510


591
01:19:16,510 --> 01:19:20,739


592
01:19:23,110 --> 01:19:28,420


593
01:19:28,420 --> 01:19:33,100


594
01:19:33,100 --> 01:19:39,190


595
01:19:41,140 --> 01:19:47,140


596
01:19:47,140 --> 01:19:53,680


597
01:19:53,680 --> 01:19:57,700


598
01:20:00,100 --> 01:20:07,510


599
01:20:07,510 --> 01:20:12,610


600
01:20:14,980 --> 01:20:22,600


601
01:20:22,600 --> 01:20:31,180


602
01:20:31,180 --> 01:20:34,780


603
01:20:37,600 --> 01:20:43,230


604
01:20:45,419 --> 01:20:48,840


605
01:20:48,840 --> 01:20:53,010


606
01:20:53,010 --> 01:20:57,119


607
01:21:00,419 --> 01:21:08,189


608
01:21:08,189 --> 01:21:13,229


609
01:21:13,229 --> 01:21:19,769


610
01:21:19,769 --> 01:21:25,559


611
01:21:25,559 --> 01:21:30,389


612
01:21:30,389 --> 01:21:34,260


613
01:21:34,260 --> 01:21:39,719


614
01:21:39,719 --> 01:21:44,880


615
01:21:44,880 --> 01:21:50,999


616
01:21:50,999 --> 01:21:56,969


617
01:21:56,969 --> 01:22:02,550


618
01:22:02,550 --> 01:22:09,840


619
01:22:09,840 --> 01:22:15,689


620
01:22:15,689 --> 01:22:21,749


621
01:22:23,610 --> 01:22:28,079


622
01:22:28,079 --> 01:22:34,559


623
01:22:34,559 --> 01:22:39,419


624
01:22:39,419 --> 01:22:46,229


625
01:22:46,229 --> 01:22:49,590


626
01:22:49,590 --> 01:22:55,249


627
01:22:55,249 --> 01:23:00,550


628
01:23:00,550 --> 01:23:05,590


629
01:23:05,590 --> 01:23:08,560


630
01:23:08,560 --> 01:23:15,940


631
01:23:21,880 --> 01:23:25,600


632
01:23:25,600 --> 01:23:29,770


633
01:23:29,770 --> 01:23:40,690


634
01:23:43,150 --> 01:23:48,370


635
01:23:48,370 --> 01:23:55,180


636
01:23:55,180 --> 01:24:04,050


637
01:24:04,050 --> 01:24:12,160


638
01:24:12,160 --> 01:24:16,420


639
01:24:16,420 --> 01:24:21,610


640
01:24:21,610 --> 01:24:27,220


641
01:24:27,220 --> 01:24:29,800


642
01:24:29,800 --> 01:24:35,140


643
01:24:35,140 --> 01:24:39,670


644
01:24:39,670 --> 01:24:45,070


645
01:24:45,070 --> 01:24:50,620


646
01:24:50,620 --> 01:24:55,630


647
01:24:55,630 --> 01:25:01,690


648
01:25:05,670 --> 01:25:11,130


649
01:25:11,130 --> 01:25:16,290


650
01:25:16,290 --> 01:25:23,340


651
01:25:23,340 --> 01:25:28,440


652
01:25:32,429 --> 01:25:37,650


653
01:25:37,650 --> 01:25:43,230


654
01:25:43,230 --> 01:25:48,360


655
01:25:48,360 --> 01:25:53,310


656
01:25:53,310 --> 01:25:58,080


657
01:26:00,300 --> 01:26:04,650


658
01:26:08,429 --> 01:26:12,900


659
01:26:12,900 --> 01:26:20,219


660
01:26:20,219 --> 01:26:25,560


661
01:26:25,560 --> 01:26:30,480


662
01:26:32,159 --> 01:26:37,260


663
01:26:37,260 --> 01:26:42,719


664
01:26:42,719 --> 01:26:47,880


665
01:26:47,880 --> 01:26:51,989


666
01:26:54,480 --> 01:26:57,480


667
01:26:57,480 --> 01:27:02,550


668
01:27:02,550 --> 01:27:07,350


669
01:27:07,350 --> 01:27:13,290


670
01:27:13,290 --> 01:27:18,800


671
01:27:18,800 --> 01:27:23,070


672
01:27:24,829 --> 01:27:31,489


673
01:27:31,489 --> 01:27:38,329


674
01:27:38,329 --> 01:27:42,349


675
01:27:42,349 --> 01:27:49,880


676
01:27:49,880 --> 01:27:53,960


677
01:27:53,960 --> 01:28:00,559


678
01:28:00,559 --> 01:28:05,690


679
01:28:08,090 --> 01:28:15,619


680
01:28:17,780 --> 01:28:20,960


681
01:28:20,960 --> 01:28:25,360


682
01:28:25,360 --> 01:28:34,099


683
01:28:34,099 --> 01:28:40,239


684
01:28:41,570 --> 01:28:47,840


685
01:28:47,840 --> 01:28:52,099


686
01:28:52,099 --> 01:28:57,710


687
01:28:57,710 --> 01:29:07,040


688
01:29:07,040 --> 01:29:15,829


689
01:29:15,829 --> 01:29:21,559


690
01:29:21,559 --> 01:29:25,489


691
01:29:27,440 --> 01:29:31,190


692
01:29:31,190 --> 01:29:36,790


693
01:29:36,790 --> 01:29:42,599


694
01:29:42,599 --> 01:29:51,869


695
01:29:53,219 --> 01:30:01,080


696
01:30:01,080 --> 01:30:07,230


697
01:30:07,230 --> 01:30:12,599


698
01:30:12,599 --> 01:30:18,869


699
01:30:22,940 --> 01:30:29,699


700
01:30:29,699 --> 01:30:34,139


701
01:30:34,139 --> 01:30:39,119


702
01:30:39,119 --> 01:30:44,489


703
01:30:44,489 --> 01:30:50,040


704
01:30:50,040 --> 01:30:56,940


705
01:30:56,940 --> 01:31:03,540


706
01:31:03,540 --> 01:31:08,670


707
01:31:08,670 --> 01:31:12,179


708
01:31:12,179 --> 01:31:16,110


709
01:31:16,110 --> 01:31:21,320


710
01:31:21,680 --> 01:31:28,250


711
01:31:28,250 --> 01:31:36,530


712
01:31:36,530 --> 01:31:40,700


713
01:31:40,700 --> 01:31:48,710


714
01:31:48,710 --> 01:31:56,420


715
01:31:56,420 --> 01:32:01,160


716
01:32:01,160 --> 01:32:04,850


717
01:32:04,850 --> 01:32:11,680


718
01:32:11,680 --> 01:32:17,750


719
01:32:17,750 --> 01:32:23,270


720
01:32:23,270 --> 01:32:27,860


721
01:32:30,050 --> 01:32:34,160


722
01:32:34,160 --> 01:32:40,040


723
01:32:40,040 --> 01:32:45,350


724
01:32:45,350 --> 01:32:51,080


725
01:32:51,080 --> 01:32:59,000


726
01:32:59,000 --> 01:33:07,940


727
01:33:07,940 --> 01:33:24,200


728
01:33:24,200 --> 01:33:29,670


729
01:33:29,670 --> 01:33:33,110


730
01:33:35,450 --> 01:33:44,250


731
01:33:44,250 --> 01:33:49,650


732
01:33:49,650 --> 01:33:56,070


733
01:33:56,070 --> 01:34:02,970


734
01:34:02,970 --> 01:34:08,670


735
01:34:08,670 --> 01:34:12,930


736
01:34:12,930 --> 01:34:15,840


737
01:34:15,840 --> 01:34:19,710


738
01:34:19,710 --> 01:34:22,830


739
01:34:22,830 --> 01:34:27,990


740
01:34:27,990 --> 01:34:33,300


741
01:34:33,300 --> 01:34:41,010


742
01:34:41,010 --> 01:34:46,350


743
01:34:46,350 --> 01:34:52,170


744
01:34:52,170 --> 01:34:56,370


745
01:34:56,370 --> 01:35:02,580


746
01:35:02,580 --> 01:35:07,770


747
01:35:07,770 --> 01:35:14,130


748
01:35:14,130 --> 01:35:18,530


749
01:35:18,530 --> 01:35:25,650


750
01:35:25,650 --> 01:35:32,690


751
01:35:32,690 --> 01:35:37,170


752
01:35:37,170 --> 01:35:44,910


753
01:35:44,910 --> 01:35:48,270


754
01:35:48,270 --> 01:35:52,620


755
01:35:54,360 --> 01:36:00,060


756
01:36:00,060 --> 01:36:04,980


757
01:36:07,440 --> 01:36:12,000


758
01:36:12,000 --> 01:36:17,760


759
01:36:17,760 --> 01:36:23,480


760
01:36:25,920 --> 01:36:31,260


761
01:36:31,260 --> 01:36:39,870


762
01:36:39,870 --> 01:36:44,460


763
01:36:44,460 --> 01:36:49,890


764
01:36:49,890 --> 01:36:57,530


765
01:37:00,210 --> 01:37:04,920


766
01:37:04,920 --> 01:37:13,410


767
01:37:13,410 --> 01:37:17,280


768
01:37:17,280 --> 01:37:25,920


769
01:37:25,920 --> 01:37:30,390


770
01:37:30,390 --> 01:37:38,090


771
01:37:38,090 --> 01:37:44,190


772
01:37:47,190 --> 01:37:52,410


773
01:37:52,410 --> 01:37:58,140


774
01:37:58,140 --> 01:38:04,620


775
01:38:04,620 --> 01:38:09,120


776
01:38:09,120 --> 01:38:14,820


777
01:38:14,820 --> 01:38:21,840


778
01:38:21,840 --> 01:38:25,950


779
01:38:25,950 --> 01:38:30,090


780
01:38:30,090 --> 01:38:35,300


781
01:38:35,300 --> 01:38:39,440


782
01:38:42,150 --> 01:38:51,140


783
01:38:51,200 --> 01:38:59,160


784
01:38:59,160 --> 01:39:07,650


785
01:39:07,650 --> 01:39:12,810


786
01:39:12,810 --> 01:39:17,130


787
01:39:17,130 --> 01:39:20,300


788
01:39:21,510 --> 01:39:25,710


789
01:39:25,710 --> 01:39:29,699


790
01:39:29,699 --> 01:39:35,099


791
01:39:35,099 --> 01:39:41,659


792
01:39:45,989 --> 01:39:51,300


793
01:39:51,300 --> 01:39:57,150


794
01:40:01,320 --> 01:40:08,400


795
01:40:08,400 --> 01:40:14,070


796
01:40:14,070 --> 01:40:19,079


797
01:40:19,079 --> 01:40:23,280


798
01:40:23,280 --> 01:40:27,300


799
01:40:27,300 --> 01:40:34,409


800
01:40:36,809 --> 01:40:40,440


801
01:40:44,219 --> 01:40:48,389


802
01:40:48,389 --> 01:40:53,880


803
01:40:53,880 --> 01:41:05,119


804
01:41:05,119 --> 01:41:13,980


805
01:41:13,980 --> 01:41:19,170


806
01:41:19,170 --> 01:41:21,840


807
01:41:21,840 --> 01:41:24,900


808
01:41:26,880 --> 01:41:32,570


809
01:41:33,980 --> 01:41:39,400


810
01:41:39,400 --> 01:41:47,090


811
01:41:47,090 --> 01:41:51,140


812
01:41:51,140 --> 01:41:54,260


813
01:41:54,260 --> 01:41:59,390


814
01:41:59,390 --> 01:42:04,370


815
01:42:04,370 --> 01:42:11,540


816
01:42:11,540 --> 01:42:15,140


817
01:42:15,140 --> 01:42:19,700


818
01:42:21,739 --> 01:42:28,730


819
01:42:28,730 --> 01:42:33,110


820
01:42:33,110 --> 01:42:38,120


821
01:42:38,120 --> 01:42:47,270


822
01:42:47,270 --> 01:42:55,540


823
01:42:57,320 --> 01:43:08,690


824
01:43:08,690 --> 01:43:13,400


825
01:43:13,400 --> 01:43:19,340


826
01:43:19,340 --> 01:43:25,970


827
01:43:25,970 --> 01:43:30,530


828
01:43:33,380 --> 01:43:38,930


829
01:43:38,930 --> 01:43:44,290


830
01:43:46,710 --> 01:43:54,490


831
01:43:54,490 --> 01:44:00,580


832
01:44:00,580 --> 01:44:06,970


833
01:44:06,970 --> 01:44:15,940


834
01:44:18,520 --> 01:44:26,470


835
01:44:26,470 --> 01:44:32,890


836
01:44:32,890 --> 01:44:38,140


837
01:44:38,140 --> 01:44:47,080


838
01:44:47,080 --> 01:44:52,180


839
01:44:52,180 --> 01:44:58,390


840
01:44:58,390 --> 01:45:05,350


841
01:45:05,350 --> 01:45:09,100


842
01:45:09,100 --> 01:45:13,510


843
01:45:13,510 --> 01:45:19,690


844
01:45:23,650 --> 01:45:28,690


845
01:45:28,690 --> 01:45:34,210


846
01:45:34,210 --> 01:45:40,870


847
01:45:40,870 --> 01:45:48,240


848
01:45:48,240 --> 01:45:55,170


849
01:45:55,170 --> 01:46:05,119


850
01:46:05,119 --> 01:46:15,439


851
01:46:15,439 --> 01:46:21,229


852
01:46:21,229 --> 01:46:26,300


853
01:46:26,300 --> 01:46:34,570


854
01:46:36,050 --> 01:46:44,560


855
01:46:45,869 --> 01:46:53,440


856
01:46:53,440 --> 01:47:01,960


857
01:47:01,960 --> 01:47:05,440


858
01:47:05,440 --> 01:47:08,949


859
01:47:08,949 --> 01:47:15,880


860
01:47:15,880 --> 01:47:19,869


861
01:47:19,869 --> 01:47:22,659


862
01:47:22,659 --> 01:47:34,059


863
01:47:34,059 --> 01:47:39,659


864
01:47:42,989 --> 01:47:50,920


865
01:47:50,920 --> 01:47:55,960


866
01:47:55,960 --> 01:48:01,599


867
01:48:01,599 --> 01:48:07,119


868
01:48:10,840 --> 01:48:16,210


869
01:48:16,210 --> 01:48:23,769


870
01:48:23,769 --> 01:48:27,519


871
01:48:27,519 --> 01:48:33,820


872
01:48:36,190 --> 01:48:42,969


873
01:48:42,969 --> 01:48:50,309


874
01:48:50,309 --> 01:48:56,920


875
01:48:56,920 --> 01:49:02,530


876
01:49:02,530 --> 01:49:08,380


877
01:49:08,380 --> 01:49:16,830


878
01:49:16,830 --> 01:49:22,960


879
01:49:22,960 --> 01:49:28,270


880
01:49:28,270 --> 01:49:34,690


881
01:49:37,420 --> 01:49:42,730


882
01:49:45,460 --> 01:49:52,150


883
01:49:52,150 --> 01:50:00,219


884
01:50:00,219 --> 01:50:03,989


885
01:50:07,300 --> 01:50:13,960


886
01:50:13,960 --> 01:50:18,250


887
01:50:18,250 --> 01:50:22,449


888
01:50:22,449 --> 01:50:28,179


889
01:50:28,179 --> 01:50:34,960


890
01:50:34,960 --> 01:50:40,270


891
01:50:40,270 --> 01:50:48,190


892
01:50:49,719 --> 01:50:53,890


893
01:50:53,890 --> 01:50:57,610


894
01:51:00,910 --> 01:51:05,290


895
01:51:05,290 --> 01:51:09,190


896
01:51:09,190 --> 01:51:13,250


897
01:51:13,250 --> 01:51:19,969


898
01:51:19,969 --> 01:51:24,050


899
01:51:24,050 --> 01:51:28,840


900
01:51:28,840 --> 01:51:34,369


901
01:51:35,809 --> 01:51:41,900


902
01:51:46,909 --> 01:51:54,349


903
01:51:54,349 --> 01:52:02,869


904
01:52:09,619 --> 01:52:19,699


905
01:52:19,699 --> 01:52:24,320


906
01:52:24,320 --> 01:52:29,750


907
01:52:29,750 --> 01:52:34,730


908
01:52:34,730 --> 01:52:41,329


909
01:52:41,329 --> 01:52:49,250


910
01:52:49,250 --> 01:52:52,670


911
01:52:52,670 --> 01:52:58,610


912
01:52:58,610 --> 01:53:03,559


913
01:53:03,559 --> 01:53:13,489


914
01:53:18,019 --> 01:53:24,070


915
01:53:27,010 --> 01:53:33,099


916
01:53:33,099 --> 01:53:40,539


917
01:53:40,539 --> 01:53:46,780


918
01:53:46,780 --> 01:53:50,170


919
01:53:50,170 --> 01:53:54,070


920
01:53:54,070 --> 01:53:59,289


921
01:53:59,289 --> 01:54:03,880


922
01:54:03,880 --> 01:54:08,860


923
01:54:08,860 --> 01:54:17,499


924
01:54:17,499 --> 01:54:23,230


925
01:54:23,230 --> 01:54:27,039


926
01:54:27,039 --> 01:54:33,429


927
01:54:33,429 --> 01:54:39,249


928
01:54:39,249 --> 01:54:47,079


929
01:54:47,079 --> 01:54:51,280


930
01:54:51,280 --> 01:54:54,909


931
01:54:54,909 --> 01:55:04,030


932
01:55:04,030 --> 01:55:08,409


933
01:55:08,409 --> 01:55:14,949


934
01:55:14,949 --> 01:55:19,389


935
01:55:19,389 --> 01:55:22,659


936
01:55:24,579 --> 01:55:28,150


937
01:55:28,150 --> 01:55:31,230


938
01:55:35,349 --> 01:55:40,729


939
01:55:40,729 --> 01:55:46,550


940
01:55:46,550 --> 01:55:50,689


941
01:55:50,689 --> 01:55:55,579


942
01:55:57,889 --> 01:56:03,679


943
01:56:03,679 --> 01:56:10,219


944
01:56:10,219 --> 01:56:14,929


945
01:56:14,929 --> 01:56:19,669


946
01:56:19,669 --> 01:56:24,019


947
01:56:24,019 --> 01:56:26,630


948
01:56:26,630 --> 01:56:30,469


949
01:56:30,469 --> 01:56:33,860


950
01:56:33,860 --> 01:56:38,869


951
01:56:38,869 --> 01:56:43,999


952
01:56:43,999 --> 01:56:54,229


953
01:56:54,229 --> 01:57:02,479


954
01:57:02,479 --> 01:57:09,590


955
01:57:09,590 --> 01:57:14,419


956
01:57:14,419 --> 01:57:21,919


957
01:57:21,919 --> 01:57:27,110


958
01:57:27,110 --> 01:57:33,260


959
01:57:33,260 --> 01:57:38,449


960
01:57:41,869 --> 01:57:46,820


961
01:57:46,820 --> 01:57:50,900


962
01:57:50,900 --> 01:57:56,260


963
01:57:56,260 --> 01:58:04,280


964
01:58:07,580 --> 01:58:13,130


965
01:58:13,130 --> 01:58:18,290


966
01:58:18,290 --> 01:58:23,660


967
01:58:23,660 --> 01:58:29,200


968
01:58:29,200 --> 01:58:35,240


969
01:58:35,240 --> 01:58:39,200


970
01:58:39,200 --> 01:58:47,870


971
01:58:47,870 --> 01:58:54,080


972
01:58:54,080 --> 01:59:00,770


973
01:59:00,770 --> 01:59:06,260


974
01:59:06,260 --> 01:59:12,650


975
01:59:14,030 --> 01:59:18,260


976
01:59:18,260 --> 01:59:22,910


977
01:59:22,910 --> 01:59:26,450


978
01:59:26,450 --> 01:59:30,560


979
01:59:30,560 --> 01:59:35,530


980
01:59:38,250 --> 01:59:44,440


981
01:59:44,440 --> 01:59:48,160


982
01:59:48,160 --> 01:59:52,690


983
01:59:52,690 --> 02:00:02,500


984
02:00:02,500 --> 02:00:10,300


985
02:00:10,300 --> 02:00:16,600


986
02:00:16,600 --> 02:00:21,850


987
02:00:21,850 --> 02:00:30,130


988
02:00:30,130 --> 02:00:37,199


989
02:00:39,960 --> 02:00:45,520


990
02:00:45,520 --> 02:00:49,449


991
02:00:51,550 --> 02:00:57,940


992
02:01:00,480 --> 02:01:05,489


993
02:01:05,489 --> 02:01:11,770


994
02:01:11,770 --> 02:01:16,210


995
02:01:16,210 --> 02:01:21,840


996
02:01:21,840 --> 02:01:31,199


997
02:01:34,810 --> 02:01:39,730


998
02:01:41,590 --> 02:01:46,179


999
02:01:46,179 --> 02:01:49,719


1000
02:01:49,719 --> 02:01:55,880


1001
02:01:55,880 --> 02:02:01,309


1002
02:02:01,309 --> 02:02:05,749


1003
02:02:05,749 --> 02:02:10,070


1004
02:02:10,070 --> 02:02:14,780


1005
02:02:14,780 --> 02:02:21,130


1006
02:02:21,399 --> 02:02:26,360


1007
02:02:28,099 --> 02:02:32,659


1008
02:02:35,989 --> 02:02:40,099


1009
02:02:40,099 --> 02:02:50,539


1010
02:02:50,539 --> 02:02:53,780


1011
02:02:53,780 --> 02:02:58,579


1012
02:02:58,579 --> 02:03:03,320


1013
02:03:03,320 --> 02:03:10,130


1014
02:03:10,130 --> 02:03:15,769


1015
02:03:15,769 --> 02:03:19,849


1016
02:03:19,849 --> 02:03:24,369


1017
02:03:29,059 --> 02:03:34,639


1018
02:03:34,639 --> 02:03:40,189


1019
02:03:40,189 --> 02:03:45,289


1020
02:03:48,889 --> 02:03:52,849


1021
02:03:52,849 --> 02:03:57,769


1022
02:03:57,769 --> 02:04:04,010


1023
02:04:04,010 --> 02:04:09,560


1024
02:04:09,560 --> 02:04:14,600


1025
02:04:14,600 --> 02:04:18,950


1026
02:04:20,600 --> 02:04:25,000


1027
02:04:25,000 --> 02:04:30,230


1028
02:04:30,230 --> 02:04:36,080


1029
02:04:36,080 --> 02:04:41,630


1030
02:04:41,630 --> 02:04:49,100


1031
02:04:49,100 --> 02:04:54,710


1032
02:04:54,710 --> 02:04:59,240


1033
02:04:59,240 --> 02:05:08,240


1034
02:05:08,240 --> 02:05:13,690


1035
02:05:14,730 --> 02:05:18,020


1036
02:05:18,020 --> 02:05:24,920


1037
02:05:24,920 --> 02:05:30,619


1038
02:05:30,619 --> 02:05:37,340


1039
02:05:37,340 --> 02:05:45,940


1040
02:05:48,949 --> 02:05:54,980


1041
02:05:54,980 --> 02:05:59,030


1042
02:05:59,030 --> 02:06:06,710


1043
02:06:06,710 --> 02:06:10,610


1044
02:06:10,610 --> 02:06:15,500


1045
02:06:15,500 --> 02:06:19,579


1046
02:06:19,579 --> 02:06:24,409


1047
02:06:24,409 --> 02:06:36,159


1048
02:06:36,159 --> 02:06:44,150


1049
02:06:44,150 --> 02:06:49,820


1050
02:06:49,820 --> 02:06:55,639


1051
02:06:55,639 --> 02:07:03,710


1052
02:07:03,710 --> 02:07:08,780


1053
02:07:08,780 --> 02:07:12,440


1054
02:07:12,440 --> 02:07:16,880


1055
02:07:16,880 --> 02:07:23,030


1056
02:07:23,030 --> 02:07:28,460


1057
02:07:28,460 --> 02:07:30,840


1058
02:07:30,840 --> 02:07:34,469


1059
02:07:34,469 --> 02:07:41,520


1060
02:07:41,520 --> 02:07:48,780


1061
02:07:50,909 --> 02:08:04,190


1062
02:08:05,400 --> 02:08:12,270


1063
02:08:12,270 --> 02:08:19,860


1064
02:08:23,429 --> 02:08:26,510


1065
02:08:29,940 --> 02:08:38,320


1066
02:08:38,320 --> 02:08:43,360


1067
02:08:43,360 --> 02:08:47,410


1068
02:08:47,410 --> 02:08:51,760


1069
02:08:51,760 --> 02:08:54,550


1070
02:08:54,550 --> 02:08:59,740


1071
02:08:59,740 --> 02:09:03,400


1072
02:09:03,400 --> 02:09:10,210


1073
02:09:10,210 --> 02:09:12,930


1074
02:09:13,989 --> 02:09:19,239


1075
02:09:19,239 --> 02:09:31,120


1076
02:09:31,120 --> 02:09:35,380


1077
02:09:35,380 --> 02:09:40,540


1078
02:09:40,540 --> 02:09:44,380


1079
02:09:44,380 --> 02:09:48,520


1080
02:09:48,520 --> 02:09:55,180


1081
02:09:55,180 --> 02:09:59,430


1082
02:09:59,430 --> 02:10:05,760


1083
02:10:09,250 --> 02:10:20,710


1084
02:10:20,710 --> 02:10:24,430


1085
02:10:24,430 --> 02:10:30,130


1086
02:10:30,130 --> 02:10:35,530


1087
02:10:35,530 --> 02:10:38,920


1088
02:10:38,920 --> 02:10:43,780


1089
02:10:43,780 --> 02:10:46,360


1090
02:10:46,360 --> 02:10:49,380


1091
02:10:49,380 --> 02:10:59,530


1092
02:10:59,530 --> 02:11:04,300


1093
02:11:04,300 --> 02:11:07,630


1094
02:11:07,630 --> 02:11:12,970


1095
02:11:12,970 --> 02:11:18,460


1096
02:11:18,460 --> 02:11:22,020


1097
02:11:22,740 --> 02:11:26,800


1098
02:11:26,800 --> 02:11:29,950


1099
02:11:29,950 --> 02:11:37,930


1100
02:11:37,930 --> 02:11:42,940


1101
02:11:45,130 --> 02:11:52,180


1102
02:11:52,180 --> 02:11:57,630


1103
02:11:57,630 --> 02:12:01,300


1104
02:12:01,300 --> 02:12:05,800


1105
02:12:05,800 --> 02:12:11,170


1106
02:12:11,170 --> 02:12:16,420


1107
02:12:16,420 --> 02:12:20,770


1108
02:12:20,770 --> 02:12:26,410


1109
02:12:28,420 --> 02:12:32,410


1110
02:12:32,410 --> 02:12:37,540


1111
02:12:37,540 --> 02:12:40,540


1112
02:12:40,540 --> 02:12:45,400


1113
02:12:45,400 --> 02:12:50,650


1114
02:12:50,650 --> 02:12:57,580


1115
02:12:57,580 --> 02:13:01,690


1116
02:13:01,690 --> 02:13:09,400


1117
02:13:09,400 --> 02:13:18,510


1118
02:13:18,510 --> 02:13:25,780


1119
02:13:28,990 --> 02:13:36,160


1120
02:13:36,160 --> 02:13:40,900


1121
02:13:40,900 --> 02:13:48,640


1122
02:13:48,640 --> 02:13:52,300


1123
02:13:52,300 --> 02:13:58,480


1124
02:14:01,390 --> 02:14:08,050


1125
02:14:08,050 --> 02:14:13,150


1126
02:14:15,490 --> 02:14:20,620


1127
02:14:20,620 --> 02:14:26,170


1128
02:14:26,170 --> 02:14:29,770


1129
02:14:29,770 --> 02:14:34,039


1130
02:14:35,839 --> 02:14:43,609


1131
02:14:43,609 --> 02:14:49,849


1132
02:14:49,849 --> 02:14:53,419


1133
02:14:53,419 --> 02:14:58,669


1134
02:14:58,669 --> 02:15:02,899


1135
02:15:02,899 --> 02:15:09,109


1136
02:15:09,109 --> 02:15:13,780


1137
02:15:17,689 --> 02:15:23,659


1138
02:15:26,359 --> 02:15:33,019


1139
02:15:33,019 --> 02:15:39,079


1140
02:15:40,339 --> 02:15:47,479


1141
02:15:47,479 --> 02:15:53,030


1142
02:15:59,030 --> 02:16:05,869


1143
02:16:05,869 --> 02:16:11,030


1144
02:16:11,030 --> 02:16:17,059


1145
02:16:17,059 --> 02:16:19,879


1146
02:16:19,879 --> 02:16:24,619


1147
02:16:24,619 --> 02:16:28,869


1148
02:16:28,869 --> 02:16:34,119


