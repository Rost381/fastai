1
00:00:00,149 --> 00:00:03,600
Добро пожаловать на третью неделю курса.

2
00:00:03,600 --> 00:00:09,599
Вы наверняка заметили, что на этой неделе на форумах происходило много интересного.

3
00:00:09,599 --> 00:00:25,920
Многие участники курса подготовили материалы по курсу, чтобы помочь своим одногруппникам и лучше разобраться самим.

4
00:00:25,920 --> 00:00:32,340
Я хочу рассказать про некоторые, про что-то я уже писал на вики, но материалов очень много.

5
00:00:32,340 --> 00:00:42,480
Пользователь reshamas создала много полезных заметок — например, что делать, если не получается подключиться к AWS,

6
00:00:46,079 --> 00:00:57,750
она расписала всё в мелочах, я считаю, что это очень круто.

7
00:00:57,750 --> 00:01:05,460
Если вы делаете какие-то заметки для себя — поделитесь ими на форуме, это удобно делать в файлах разметки Markdown.

8
00:01:07,439 --> 00:01:13,789
Если вы загрузите свои заметки на GitHub, все смогут ими пользоваться. Или загрузите их на наш форум,

9
00:01:13,889 --> 00:01:25,140
reshamas сделала так со своей заметкой про tmux.

10
00:01:25,140 --> 00:01:39,869
tmux — консольная утилита, позволяющая показывать несколько терминалов на одном экране.

11
00:01:39,869 --> 00:01:48,210
Здесь в одном терминале у меня модули библиотеки, открытые редактором vim,

12
00:01:48,210 --> 00:01:53,579
в другом запущен Jupyter Notebook и так далее.

13
00:01:53,579 --> 00:02:03,060
Если интересно, reshamas написала на эту тему туториал, в её аккаунте на GitHub ещё много интересного.

14
00:02:03,060 --> 00:02:28,810
Apil Tamang написал хороший сжатый конспект предыдущей лекции.

15
00:02:29,610 --> 00:02:45,069
Pavel Surmenok написал пост про алгоритм поиска скорости обучения.

16
00:02:45,069 --> 00:02:59,500
Это очень круто, потому что об этом алгоритме ещё нигде не писали, а он очень полезный.

17
00:02:59,500 --> 00:03:13,560
Когда я поделился ссылкой на его пост в Twitter, им поделились сотни раз, у поста тысячи просмотров, отличная работа.

18
00:03:13,560 --> 00:03:20,779
Radek написал несколько полезных постов, мне особенно понравился гайд по PyTorch,

19
00:03:20,879 --> 00:03:32,260
он подойдёт для продвинутых студентов, например, тех, кто никогда не использовал PyTorch, но уже что-то знает про численное программирование.

20
00:03:35,560 --> 00:03:40,624
Есть интересная статья про связь между скоростью обучения и размером минибатча.

21
00:03:40,724 --> 00:03:46,030
Один студент недавно задал мне этот вопрос, поэтому я вспомнил этот пост.

22
00:03:46,030 --> 00:04:01,235
Автор поста пробовал различные скорости обучения и размеры минибатча и проанализировал, как они связаны, можете сами попробовать.

23
00:04:01,335 --> 00:04:15,586
Radek написал пост на тему моего утверждения о том, что SDGR находит более плоские области поверхности потерь и в них модель лучше обобщает.

24
00:04:15,686 --> 00:04:25,147
Он попробовал описать эту закономерность более точно, не очень удачно, но пост интересный.

25
00:04:25,247 --> 00:04:32,999
Есть пост по введению в свёрточные нейронные сети.

26
00:04:32,999 --> 00:04:57,059
Anand Saha написал отличный анализ архитектуры ResNet, мы ещё обсудим это в курсе, продвинутые студенты могут читать уже сейчас.

27
00:04:57,059 --> 00:05:04,349
Apil Tamang написал похожий пост. В общем, на форумах много чего происходит.

28
00:05:06,059 --> 00:05:17,479
Мы также создали форум для новичков. Тупых вопросов не бывает, но иногда страшно задавать вопрос про что-то простое,

29
00:05:19,860 --> 00:05:29,819
когда вокруг обсуждаются очень сложные вещи. Надеемся, что форум новичков будет не таким устрашающим.

30
00:05:29,819 --> 00:05:38,632
Если вы продвинутый студент и можете отвечать на такие вопросы, пожалуйста, делайте это дружелюбно —

31
00:05:38,732 --> 00:05:47,934
у людей за плечами может быть всего год опыта программирования и никакого опыта машинного обучения.

32
00:05:48,034 --> 00:05:56,159
Если вам хочется написать какой-то вспомогательный материал, не стесняйтесь — большинство авторов только что упомянутых заметок

33
00:05:56,159 --> 00:06:04,769
никогда не публиковали ничего в интернете, они такие же люди, как и вы.

34
00:06:04,769 --> 00:06:20,399
Если вы не уверены в каких-то деталях, запостите сначала на форум, люди помогут вам улучшить ваш материал фразами вроде

35
00:06:20,399 --> 00:06:29,439
«Вот здесь всё устроено немного по-другому, сейчас расскажу» или «Это очень интересно, вы не думали углубиться в эту тему?».

36
00:06:29,539 --> 00:06:38,969
К текущему моменту мы немного обсудили свёрточные нейронные сети.

37
00:06:38,969 --> 00:06:59,319
Мы не углублялись в детали того, как они работают, зато построили на их основе превосходного качества модель.

38
00:06:59,319 --> 00:07:12,280
Сегодня мы ещё раз посмотрим на модели и наконец-то подойдём к теории — что такое свёрточная нейронная сеть,

39
00:07:12,280 --> 00:07:16,000
что такое свёртка, как и почему это работает.

40
00:07:16,000 --> 00:07:25,930
Дальше мы продвинемся по нашему плану и обсудим использование нейронных сетей для анализа структурированных данных —

41
00:07:25,930 --> 00:07:31,449
логистика, прогнозы, анализ рынка и прочее.

42
00:07:31,449 --> 00:07:45,550
Потом посмотрим на обработку естественного языка, потом на коллаборативную фильтрацию для рекомендательных систем.

43
00:07:45,550 --> 00:07:53,020
Это будет проходить в том же формате, что и обсуждение CNN — без углубления в теорию, но с построением качественных моделей.

44
00:07:55,150 --> 00:08:01,984
Потом мы пройдёмся по этим же темам, но в другом порядке и более углубленно —

45
00:08:02,084 --> 00:08:09,949
рассмотрим коллаборативную фильтрацию, поймём, как написан соответствующий код и какая за этим стоит математика,

46
00:08:10,049 --> 00:08:20,620
потом сделаем то же самое для анализа структурированных данных, свёрточных нейронных сетей и обработки естественного языка.

47
00:08:20,620 --> 00:08:37,078
Давайте проговорим некоторые вещи с прошлых лекций ещё раз.

48
00:08:37,178 --> 00:08:55,889
Я хочу убедиться, что все смогут повторить модель с прошлой лекции для различения пород собак.

49
00:08:55,889 --> 00:09:11,339
Для этого нужно скачать данные. Данные можно скачивать либо с Kaggle, либо откуда-то ещё.

50
00:09:11,339 --> 00:09:37,960
Для скачивания данных с Kaggle мы используем kaggle-cli, он должен был установиться при загрузке материалов для курса.

51
00:09:38,060 --> 00:09:53,279
Если в процессе скачивания данных на сайте Kaggle что-то меняется, kaggle-cli падает,

52
00:09:53,279 --> 00:10:13,434
с этим можно справиться командой pip install kaggle-cli --upgrade.

53
00:10:13,534 --> 00:10:25,765
После этого следуйте инструкциям reshamas для скачивания данных.

54
00:10:25,865 --> 00:10:43,509
Команда для скачивания данных: kg download -u  -p  -c .

55
00:10:43,609 --> 00:11:02,100
— это фраза, идущая после /c/ в адресной строке на странице соревнования.

56
00:11:02,100 --> 00:11:12,650
Перед использованием kaggle-cli убедитесь, что приняли правила использования, то есть скачивали данные вручную хотя бы однажды.

57
00:11:12,750 --> 00:11:18,850
Если правила не приняты, kaggle-cli вам об этом скажет.

58
00:11:18,850 --> 00:11:31,085
Если вы используете аккаунт Google для входа в Kaggle, kaggle-cli не будет работать, используйте форму восстановления пароля Kaggle.

59
00:11:31,185 --> 00:11:38,775
kaggle-cli создаст папку на вашем компьютере и скачает туда данные.

60
00:11:38,875 --> 00:11:52,600
kaggle-cli не подходит, если вы используете датасет не с Kaggle или если вам не нужны все предоставленные там данные.

61
00:11:52,600 --> 00:12:11,890
Например, в этом соревновании данные предоставлены в форматах TIFF (19 ГБ) и JPG (600 МБ), возможно, вы не захотите использовать оба.

62
00:12:11,890 --> 00:12:32,480
Для таких случаев есть расширение CurlWget для Google Chrome.

63
00:12:32,580 --> 00:13:02,944
Для всех страниц, откуда можно что-нибудь скачать, CurlWget предоставит вам ссылку для скачивания из терминала.

64
00:13:03,044 --> 00:13:10,449
В ссылке содержатся куки вашего браузера, для активации нужно скачать что-то вручную один раз.

65
00:13:10,449 --> 00:13:19,420
С помощью этого расширения можно скачать что угодно — например, ваш любимый сериал.

66
00:13:19,420 --> 00:13:33,610
Это очень полезный инструмент для анализа данных, потому что зачастую приходится анализировать видео.

67
00:13:33,610 --> 00:13:44,019
Итак, есть два способа получить данные. После этого можно начинать обучать модель.

68
00:13:44,019 --> 00:13:57,670
В переменной PATH указано, что данные лежат в директории data рабочей директории Jupyter ноутбука.

69
00:13:57,670 --> 00:14:04,990
Это не всегда удобно — данные могут лежать в другом месте или даже на другом диске,

70
00:14:04,990 --> 00:14:17,949
поэтому можно создать символическую ссылку на директорию с данными.

71
00:14:17,949 --> 00:14:24,109
Вы можете сложить ваши данные куда угодно и добавить на них ссылку или сложить их в рабочую директорию Jupyter ноутбука.

72
00:14:24,209 --> 00:14:35,829
Символические ссылки — это очень удобно, можете почитать про них на нашем форуме, если не работали с ними раньше.

73
00:14:35,829 --> 00:14:45,490
Как видите, модули библиотеки fast.ai доступны из Jupyter ноутбука таким же образом.

74
00:14:45,490 --> 00:15:06,845
Чтобы вывести список файлов и директорий с указанием ссылок в Linux, используйте команду ls -l.

75
00:15:06,945 --> 00:15:20,315
Вам могло показаться, что для обучения модели нужно больше кода, чем на самом деле.

76
00:15:20,415 --> 00:15:28,292
Здесь на одном экране показаны все шаги, которые я проделал для обучения модели для классификации кошек и собак.

77
00:15:28,392 --> 00:15:36,197
Здесь не показаны скачивание и распаковка данных с Kaggle.

78
00:15:36,297 --> 00:15:51,130
Это все необходимые этапы. Сначала мы импортируем библиотеки — fastai.conv_learner импортирует всё необходимое сам.

79
00:15:51,130 --> 00:15:57,860
Нужно указать путь к файлам, размер изображений и размер минибатча.

80
00:15:57,960 --> 00:16:10,940
В этой строке мы говорим, как нужно преобразовать изображения — какая будет модель, какого они должны быть размера,

81
00:16:11,040 --> 00:16:18,490
какой алгоритм дополнения данных использовать, какое максимальное увеличение использовать.

82
00:16:18,490 --> 00:16:34,360
Здесь мы говорим, что данные рассортированы на обучающую и валидационную выборку, а внутри каждой выборки — на кошек и собак.

83
00:16:34,360 --> 00:16:45,170
Если папки с обучающей и валидационной выборкой названы нестандартно, их названия можно передать через параметры trn_name и val_name.

84
00:16:45,270 --> 00:16:56,380
Название папки с неразмеченной тестовой выборкой test_name необходимо указывать, если вы будете отправлять модель на Kaggle.

85
00:16:59,770 --> 00:17:08,205
После этого мы создаём модель с архитектурой ResNet на основе уже обученной и вызываем метод .fit().

86
00:17:08,305 --> 00:17:17,420
Напомню, что по умолчанию все слои, кроме последних, заморожены, мы ещё будем про это говорить.

87
00:17:17,420 --> 00:17:23,110
Метод .fit() работал около двух с половиной минут. Я не выставлял precompute=True.

88
00:17:26,119 --> 00:17:35,390
На форумах было много уточняющих вопросов по поводу значения этого параметра, повторюсь — это просто небольшое ускорение.

89
00:17:35,390 --> 00:17:42,325
Если не до конца понятно, просто пропускайте и оставляйте precompute=False по умолчанию.

90
00:17:42,425 --> 00:17:52,635
precompute=True кэширует некоторые промежуточные результаты, чтобы их не нужно было пересчитывать каждый раз.

91
00:17:52,735 --> 00:18:11,180
Помните, что с precomputed=True не работает дополнение данных, потому что дополнение данных не работает с предвычисленными активациями.

92
00:18:11,180 --> 00:18:14,780
Я максимально упростил процесс, поэтому precompute=False.

93
00:18:14,780 --> 00:18:22,810
Последний слой обучается 3 цикла с длиной цикла в одну эпоху (cycle_len=1).

94
00:18:23,110 --> 00:18:27,190
После этого я размораживаю модель и обучаю уже все слои.

95
00:18:27,590 --> 00:18:39,170
Метод .bn_freeze() мы ещё обсудим, это особенность сложных архитектур типа ResNet50 или ResNeXt101.

96
00:18:39,170 --> 00:18:49,005
Эту строку имеет добавлять после разморозки, если вы используете датасеты, подобные изображениям ImageNet —

97
00:18:49,105 --> 00:19:02,495
фотографии обычных объектов со стороны размера от 200 до 500 пикселей.

98
00:19:02,595 --> 00:19:10,630
Для продвинутых студентов — это фиксирует нормализацию скользящего среднего.

99
00:19:10,730 --> 00:19:20,409
Мы ещё обсудим это во второй части курса, в других библиотеках этого нет, но это очень важный аспект.

100
00:19:20,409 --> 00:19:31,789
После этого все слои нейронной сети обучаются в течение одной эпохи, и применяется дополнение тестовых данных.

101
00:19:31,889 --> 00:19:37,909
Это даёт долю правильных ответов в 99.45%.

102
00:19:38,009 --> 00:19:47,139
Это минимальные шаги, которые можно выполнить при работе с новым датасетом,

103
00:19:47,139 --> 00:20:02,799
при условии, что вы уже подобрали скорость обучения, знаете, как устроены данные и так далее.

104
00:20:02,799 --> 00:20:15,499
Я хотел показать вам, как устроены другие библиотеки, и выбрал для этого Keras.

105
00:20:15,599 --> 00:20:33,059
Библиотека fast.ai построена на основе PyTorch, а Keras поддерживает TensorFlow, MXNet, CNTK и многие другие библиотеки,

106
00:20:33,159 --> 00:20:37,450
большинство людей используют Keras с TensorFlow.

107
00:20:37,450 --> 00:20:51,024
В Jupyter ноутбуке keras_lesson1.ipynb я постараюсь воссоздать модель с первой лекции на Keras, чтобы вы увидели, как это можно делать.

108
00:20:51,124 --> 00:20:59,510
Я не буду пока ничего говорить про метод .bn_freeze(), кроме показаний к применению —

109
00:21:00,850 --> 00:21:11,950
это архитектура нейронной сети с числом больше 34, как ResNet50 или ResNeXt101,

110
00:21:11,950 --> 00:21:24,520
и датасет, похожий на изображения ImageNet, где объект занимает большую часть изображений нормального размера.

111
00:21:24,520 --> 00:21:32,989
Если вы не уверены в его необходимости, попробуйте убрать и сравнить результаты.

112
00:21:33,089 --> 00:21:39,910
Продвинутые студенты наверняка начнут обсуждать это на форумах уже сейчас, а мы дойдём до этого

113
00:21:39,910 --> 00:21:46,720
только во второй части курса, когда вернёмся к свёрточным нейронным сетям.

114
00:21:50,820 --> 00:22:01,480
Итак, для работы с Keras нужно импортировать необходимые модули.

115
00:22:01,480 --> 00:22:12,151
Keras поддерживает стандартный способ сортировки данных на обучающую и валидационную выборки и на классы внутри выборок.

116
00:22:12,251 --> 00:22:22,805
Здесь мы указываем пути к папкам с обучающей и валидационной выборкой.

117
00:22:22,905 --> 00:22:36,575
Вы заметите, что с Keras обучение модели требует больше кода и различных параметров,

118
00:22:36,675 --> 00:22:44,650
и очень просто задать неправильные параметры, поэтому я постараюсь подробно всё показать.

119
00:22:44,650 --> 00:22:55,030
Для подготовки данных необходимо создать генератор данных конструктором ImageDataGenerator().

120
00:22:55,030 --> 00:23:10,634
В параметры генератора данных необходимо передать параметры дополнения данных и нормализации.

121
00:23:10,734 --> 00:23:15,619
В библиотеке fast.ai достаточно указать архитектуру, например, ResNet50, и необходимые параметры выставляются автоматически,

122
00:23:15,719 --> 00:23:21,700
здесь надо примерно понимать, что для этого требуется.

123
00:23:21,700 --> 00:23:30,220
В принципе копирования кода из интернета достаточно, чтобы всё работало.

124
00:23:30,220 --> 00:23:43,550
Нет общепринятых стандартов по поводу этих параметров дополнения данных, я скопировал эту строку из документации Keras.

125
00:23:43,550 --> 00:23:49,140
Я не знаю, хороший ли это набор параметров, но в документации используется такой.

126
00:23:49,240 --> 00:23:56,540
Параметры говорят, отражать ли изображения относительно горизонтали, как увеличивать, как сдвигать.

127
00:23:56,540 --> 00:24:03,102
Из генератора данных создаётся генератор методом .flow_from_directory().

128
00:24:03,202 --> 00:24:17,985
В его параметры передаётся путь к файлам, размер изображений, размер минибатча и параметр class_mode.

129
00:24:18,085 --> 00:24:31,650
Параметр class_mode указывает вид задачи классификации — двухклассовая или многоклассовая, 'binary' или 'categorial'.

130
00:24:31,750 --> 00:24:36,525
У нас два класса — кошки и собаки, class_mode='binary'.

131
00:24:36,625 --> 00:24:48,048
Необходимо отдельно создать генератор данных без дополнения данных для валидации

132
00:24:48,148 --> 00:25:00,760
и создать соответствующий генератор с параметром shuffle=False, чтобы валидационная выборка не перемешивалась —

133
00:25:00,860 --> 00:25:11,270
это полезно для обучающей выборки, но на валидационной помешает отслеживать прогресс в обучении.

134
00:25:11,270 --> 00:25:18,465
Эти шаги в Keras необходимо делать каждый раз.

135
00:25:18,565 --> 00:25:32,750
Keras не поддерживает ResNet34, поэтому в конце прошлой лекции я поменял ResNet34 на ResNet50, чтобы мы могли сравнить fastai и Keras на одной архитектуре.

136
00:25:32,750 --> 00:25:42,290
В Keras модель не подстраивается под датасет автоматически, это нужно делать вручную.

137
00:25:42,290 --> 00:25:50,600
Для этого конструктором ResNet50() создаётся базовая модель и к ней вручную добавляются дополнительные слои.

138
00:25:50,600 --> 00:26:01,370
К концу этого курса вы поймёте, почему мы добавили именно эти три слоя.

139
00:26:01,370 --> 00:26:06,070
Модель создаётся конструктором Model().

140
00:26:06,070 --> 00:26:19,515
В Keras нет встроенной функции заморозки, поэтому мы проходим по всем слоям и выставляем поле .trainable=False.

141
00:26:19,615 --> 00:26:27,110
В Keras необходимо компилировать модель после создания методом .compile().

142
00:26:28,160 --> 00:26:35,360
Метод принимает как параметры вид оптимизации, функцию потерь и метрику оценки качества модели.

143
00:26:36,439 --> 00:26:47,324
В fast.ai эти значения передаются по умолчанию, хотя есть возможность заменить их своими.

144
00:26:47,424 --> 00:26:55,886
Вместо метода .fit() вызывается метод .fit_generator(), он принимает как параметры два только что созданных генератора,

145
00:26:55,886 --> 00:27:06,681
зачем-то просит количество минибатчей в одной эпохе — это размер генератора, делённый на размер минибатча.

146
00:27:07,081 --> 00:27:13,680
Как и в fast.ai, задаётся количество эпох.

147
00:27:15,100 --> 00:27:35,639
Задаётся количество воркеров. В отличие от fast.ai, по умолчанию они не используются, важно не забыть про этот параметр для достижения хорошей скорости.

148
00:27:35,739 --> 00:27:43,469
Этого достаточно, чтобы начать тонкую настройку последних слоёв.

149
00:27:43,569 --> 00:27:53,805
На валидационной выборке доля правильных ответов получилась 95%, но на первой и второй эпохах она была 49% и 69%.

150
00:27:53,905 --> 00:28:01,115
Я не знаю, почему это так — возможно, ошибка в Keras, возможно, в моём коде.

151
00:28:01,215 --> 00:28:07,469
Я писал про это в Twitter, но там никто не смог разобраться.

152
00:28:07,569 --> 00:28:15,509
Это одна из причин, почему я использую fast.ai в этом курсе — там гораздо сложнее всё испортить.

153
00:28:15,609 --> 00:28:18,806
Я не знаю, в чём тут ошибка.

154
00:28:18,906 --> 00:28:23,270
Янет: Здесь используется TensorFlow?

155
00:28:23,500 --> 00:28:50,279
Да, здесь используется TensorFlow, для этого нужно установить его командой pip install tensorflow-gpu keras.

156
00:28:50,379 --> 00:29:08,539
В Keras нет дифференциальных скоростей обучения и частичного размораживания, поэтому нужно вручную отделить последние слои.

157
00:29:08,539 --> 00:29:17,210
Я буду настраивать все слои, начиная со 140-го, для этого прохожу по всем слоям и замораживаю или размораживаю их, после этого снова вызываю метод .compile().

158
00:29:17,210 --> 00:29:26,924
После этого я снова обучаю модель, доля правильных ответов на обучающей выборке примерно такая же, а на валидационной опять ерунда.

159
00:29:27,024 --> 00:29:40,062
Даже если не учитывать это, Keras проигрывает в сравнении с fastai — кода гораздо больше и результаты хуже:

160
00:29:40,162 --> 00:29:49,934
модель на Keras за восемь минут достигла доли правильных ответов в 97% на обучающей выборке,

161
00:29:50,034 --> 00:30:02,380
а модель на fastai за четыре или пять минут — 99.5% на валидационной выборке.

162
00:30:02,380 --> 00:30:17,620
Используйте TensorFlow, если вы разрабатываете что-то для смартфонов, PyTorch пока плохо это поддерживает.

163
00:30:17,620 --> 00:30:24,365
Вам придётся использовать TensorFlow, если это делает компания, где вы работаете.

164
00:30:24,465 --> 00:30:37,000
Если вам нужно повторить что-то из этого курса с TensorFlow, используйте Keras и будьте готовы к тому,

165
00:30:37,000 --> 00:30:53,690
что будет больше кода, и будет сложнее повторить результаты, которые легко достигаются в fast.ai.

166
00:30:54,669 --> 00:31:05,990
В fast.ai нет ничего, что нельзя было бы повторить в Keras,

167
00:31:06,090 --> 00:31:20,389
но каждый раз заново писать SGDR, дифференциальные скорости обучения и всё остальное — неудобно.

168
00:31:20,489 --> 00:31:29,199
На форуме один человек работает над интеграцией Keras/TensorFlow и fast.ai, я надеюсь, что из этого что-то выйдет.

169
00:31:29,299 --> 00:31:43,270
Я разговаривал с Google и они тоже в этом заинтересованы. Возможно, к тому моменту, как этот курс появится на MOOC-платформе, это сделают.

170
00:31:43,270 --> 00:32:05,900
Keras/TensorFlow не очень сложные, для перехода с fast.ai после этого курса вам понадобится пара дней.

171
00:32:06,000 --> 00:32:19,890
Всего сказанного должно хватить для того, чтобы вы смогли обучить модель на датасете с породами собак.

172
00:32:19,890 --> 00:32:26,600
Большую часть того, что нужно сделать, я показал в конце прошлой лекции —

173
00:32:30,870 --> 00:32:37,885
например, как я изучал данные, чтобы понять, как устроены классы и какого размера изображения.

174
00:32:37,985 --> 00:32:41,980
Если вы что-то забыли, пересмотрите прошлую лекцию.

175
00:32:43,080 --> 00:32:53,794
Мы не успели обсудить, как отправить модель на Kaggle, сейчас покажу.

176
00:32:53,894 --> 00:33:01,500
Я уже написал про это в вики.

177
00:33:01,500 --> 00:33:12,955
На Kaggle для каждого соревнования есть вкладка Evaluation, в ней написано, какой формат нужен.

178
00:33:13,055 --> 00:33:19,810
В этом соревновании на выходе должен быть файл, где в заголовке — ID и все возможные породы собак,

179
00:33:20,410 --> 00:33:31,860
а в остальных строках — ID файлов тестовой выборки и вероятности того, что на этих файлах различные породы собак.

180
00:33:35,610 --> 00:33:55,240
Объект data.classes содержит названия всех классов в алфавитном порядке.

181
00:33:55,340 --> 00:34:01,645
Объект data.test_ds содержит тестовую выборку, названия файлов лежат в data.test_ds.fnames.

182
00:34:01,745 --> 00:34:18,444
Напоминаю, что изображения не разложены по папкам в стиле Keras, а размечены в csv-файле,

183
00:34:18,544 --> 00:34:27,859
поэтому мы используем метод ImageDataClassifier.from_csv(), а не ImageDataClassifier.from_paths().

184
00:34:31,799 --> 00:34:40,139
Keras не поддерживает такой формат, поэтому на форумах Kaggle люди выкладывают скрипты для сортировки данных по папкам,

185
00:34:40,139 --> 00:34:47,574
но нам этого делать не придётся.

186
00:34:47,674 --> 00:35:03,740
Итак, у нас есть названия пород и имена файлов тестовой выборки.

187
00:35:03,740 --> 00:35:28,789
Я всегда использую TTA при предсказании тестовой выборки, для этого в метод .TTA() передаётся параметр is_test=True,

188
00:35:28,889 --> 00:35:35,369
чтобы предсказания делались на тестовой выборке, а не на валидационной.

189
00:35:35,369 --> 00:35:44,765
Мы не знаем, какая получилась доля правильных ответов, потому что тестовая выборка не размечена.

190
00:35:44,865 --> 00:35:57,099
Большинство моделей PyTorch возвращает логарифмы вероятностей, метод np.exp() переводит их в обычные вероятности.

191
00:35:57,199 --> 00:36:10,529
В тестовой выборке 10357 изображений, принадлежащих 120 различным породам собак, это размеры полученной матрицы предсказаний probs.

192
00:36:10,529 --> 00:36:17,695
Из этой матрицы мы создадим файл необходимого формата с помощью pandas.

193
00:36:17,795 --> 00:36:27,472
Если вы не работали с pandas, погуглите или посмотрите наш курс по машинному обучению, там это часто используется.

194
00:36:27,572 --> 00:36:41,007
Мы создаём датафрейм pandas из матрицы конструктором pandas.DataFrame(), присваиваем колонкам названия пород собак

195
00:36:41,607 --> 00:36:57,745
и вставляем нулевую колонку с названиями файлов. Названия содержат 'test/' в начале и '.jpg' в конце, эти части мы обрезаем.

196
00:36:57,845 --> 00:37:06,550
Итоговый датафрейм выглядит так.

197
00:37:06,650 --> 00:37:24,000
Датафреймы с данными обычно называют df (data frame).

198
00:37:24,000 --> 00:37:36,680
С помощью метода .to_csv() можно записать получившийся датафрейм в csv-файл. Имеет смысл включить сжатие параметром compression='gzip'.

199
00:37:36,780 --> 00:37:45,082
После этого на сервере Jupyter Notebook появится csv-файл.

200
00:37:45,182 --> 00:37:57,235
После этого вы можете либо скачать файл с сервера и отправить его вручную, либо использовать kaggle-cli.

201
00:37:57,335 --> 00:38:04,170
Я обычно скачиваю файл себе на компьютер, чтобы посмотреть на него перед отправкой.

202
00:38:04,170 --> 00:38:27,130
Есть удобная утилита FileLink, которая создаёт ссылку, по которой можно скачать файл с сервера Jupyter Notebook на ваш компьютер.

203
00:38:33,490 --> 00:38:43,025
Архив с файлом скачался.

204
00:38:43,125 --> 00:38:54,095
Формат именно такой, какой нужно — в заголовке строке ID и породы собак, остальные строки содержат имя файла и вероятности.

205
00:38:54,195 --> 00:39:00,695
Теперь можно загрузить его на Kaggle через веб-интерфейс.

206
00:39:00,795 --> 00:39:15,160
Итак, теперь мы умеем скачивать файлы из интернета на сервер Jupyter Notebook через CurlWget в Google Chrome

207
00:39:17,170 --> 00:39:31,270
и умеем скачивать файлы с сервера на свой компьютер. Можно использовать scp в терминале, но мне нравится делать это в Jupyter ноутбуке.

208
00:39:31,270 --> 00:39:42,910
На этой неделе меня спросили, как получить предсказание только для одного файла.

209
00:39:42,910 --> 00:39:49,790
Допустим, я хочу получить предсказание для первого изображения валидационной выборки.

210
00:39:49,890 --> 00:39:59,350
Вот имя файла, я могу его вывести методом Image.open() стандартной библиотеки Python.

211
00:39:59,350 --> 00:40:16,230
Самое простое, что вы можете сделать — вызвать метод .predict_array().

212
00:40:16,230 --> 00:40:21,390
Для этого надо сначала применить к изображению дополнение данных.

213
00:40:21,390 --> 00:40:36,310
Функция tfms_from_model возвращает преобразования данных отдельно для обучающей и валидационной выборки.

214
00:40:36,310 --> 00:40:45,280
После этого мы применяем на изображении дополнение данных для обучающей выборки. Нет, лучше для валидационной.

215
00:40:45,280 --> 00:40:52,420
Полученный массив можно передавать в метод .predict_array().

216
00:40:55,440 --> 00:41:05,320
Данные можно подавать в модель и получать от модели только в минибатчах.

217
00:41:05,320 --> 00:41:15,250
У нас только одно изображение, и мы хотим превратить его в минибатч,

218
00:41:15,250 --> 00:41:21,490
то есть превратить из тензора размерности (количество строк)x(количество столбцов)х(цветовые каналы)

219
00:41:21,490 --> 00:41:26,230
в тензор размерности (количество изображений)x(количество строк)x(количество столбцов)х(цветовые каналы).

220
00:41:26,230 --> 00:41:31,330
У нас трёхмерная матрица, а должна быть четырёхмерная.

221
00:41:31,330 --> 00:41:46,480
Если в numpy индексировать массив im как im[None], вернётся массив размерностью больше на 1, так мы превратили изображение в минибатч.

222
00:41:46,480 --> 00:42:02,080
Если вы забудете это сделать при использовании PyTorch или fast.ai, получите ошибку вроде «expected 4 dimensions but got 3».

223
00:42:02,080 --> 00:42:15,040
Модели не только принимают, но и возвращают минибатчи, это тоже учтите.

224
00:42:15,040 --> 00:42:38,405
На этом мы закончим с практикой и перейдём к теории свёрточных нейронных сетей.

225
00:42:38,505 --> 00:42:54,175
На первой лекции мы немного поговорили про теорию, используя демонстрацию setosa.io/ev/, Explained Visually.

226
00:42:54,275 --> 00:43:03,910
Мы узнали, что свёртка — это алгоритм, который рассматривает область 3х3 пикселя за раз и умножает значение каждого пикселя

227
00:43:04,010 --> 00:43:16,790
на соответствующее значение матрицы свёртки, а потом складывает числа внутри области для получения нового значения в центре области.

228
00:43:16,890 --> 00:43:29,065
Давайте посмотрим, как с помощью этого алгоритма создаются слои нейронной сети, которые мы видели в статье Зайлера и Фергюса.

229
00:43:29,165 --> 00:43:39,570
Для этого я покажу вам работу человека, который гораздо умнее меня — Отавио Гуда.

230
00:43:39,570 --> 00:43:47,155
Отавио Гуд создал Word Lens — это система распознавания текста, которая сейчас используется в Google Translate,

231
00:43:47,255 --> 00:43:58,080
когда вы наводите камеру телефона на текст на незнакомом языке и поверх изображения показывается перевод.

232
00:43:58,080 --> 00:44:06,385
Отавио разработал эту систему и создал превосходную демонстрацию её работы, сейчас он работает в Google.

233
00:44:06,485 --> 00:44:21,000
Я прокомментирую эту демонстрацию, а потом мы посмотрим на нечто похожее в таблице в Microsoft Excel.

234
00:44:21,500 --> 00:44:27,720
Надеюсь, что принцип работы будет ясен и тем, кто любит видео, и тем, кто любит таблицы.

235
00:44:29,880 --> 00:44:41,610
Эта демонстрация распознавания текста, дальше в курсе мы займёмся распознаванием цифр, задачи очень похожи.

236
00:44:41,610 --> 00:44:51,990
На изображении — буква А, изображение — это матрица чисел.

237
00:44:51,990 --> 00:44:59,035
К этой матрице чисел применяется первый свёрточный фильтр. Предполагается, что все фильтры уже вычислены, модель обучена.

238
00:44:59,135 --> 00:45:05,010
Фильтр чёрный слева и белый справа, это значит, что матрица свёртки выглядит примерно так:

239
00:45:05,010 --> 00:45:10,350
[[-1, 0, 1],
[-1, 0, 1],
[-1, 0, 1]].

240
00:45:13,140 --> 00:45:21,670
Каждая область 3х3 поэлементно умножается на эту матрицу, полученные результаты складываются.

241
00:45:21,770 --> 00:45:30,180
Везде, где чёрное переходит в белое, мы получаем положительные значения, они показаны зелёным,

242
00:45:30,180 --> 00:45:36,475
а там, где белое переходит в чёрное — отрицательные значения, они показаны красным.

243
00:45:36,575 --> 00:45:41,920
Это результат работы первого ядра свёртки.

244
00:45:42,020 --> 00:45:47,020
Вот новое ядро, с белой полосой наверху, а не справа.

245
00:45:47,120 --> 00:46:02,610
Фильтр проходит через каждую область изображения 3x3 и определяет, насколько красным или зелёным получится новый пиксель.

246
00:46:02,610 --> 00:46:15,570
Допустим, у нас было всего два фильтра, видно, что результат отражает вид матриц свёртки.

247
00:46:15,570 --> 00:46:26,520
Полученные результаты пропускаются через выпрямитель (ReLU), который убирает отрицательные значения.

248
00:46:26,520 --> 00:46:30,780
Здесь показан первый входной слой, второй слой — результат работы свёрточных фильтров,

249
00:46:30,780 --> 00:46:36,180
третий слой — результат работы выпрямителя,

250
00:46:38,190 --> 00:46:55,320
четвёртый слой — подвыборка максимумом: каждая область 2х2 заменяется на максимальный элемент этой области, такой алгоритм уменьшения.

251
00:46:55,420 --> 00:47:05,599
После этого процедура повторяется. Новые фильтры свёртки проходят уже оба полученных фильтра с предыдущего слоя,

252
00:47:05,699 --> 00:47:21,190
полученные результаты пропускаются через выпрямитель, получается ещё один слой свёрточной нейронной сети.

253
00:47:21,190 --> 00:47:30,490
Видно, что на первом слое свёртки выделялись горизонтальные или вертикальные края.

254
00:47:33,010 --> 00:47:42,559
Результат работы следующего слоя уже не так очевиден, но принцип его работы такой же.

255
00:47:42,659 --> 00:47:52,180
После этого опять выполняется подвыборка максимумом, каждая область 2x2 заменяется одним числом.

256
00:47:53,799 --> 00:48:07,865
Полученное изображение сравнивается с шаблонами возможных классов, одному из которых оно принадлежит.

257
00:48:07,965 --> 00:48:19,456
Это делается таким же образом — изображение 4x8 поэлементно умножается на шаблон, соответствующий какому-то классу,

258
00:48:19,556 --> 00:48:30,614
результаты складываются и потом конвертируются в вероятность.

259
00:48:31,114 --> 00:48:38,524
Это изображение на 92% совпало с шаблоном класса А, модель считает, что на изображении буква А.

260
00:48:38,624 --> 00:48:46,119
Демонстрация показывает работу уже обученной модели на тестовой выборке,

261
00:48:46,119 --> 00:49:00,484
например, вашей модели или модели, преобученной на изображениях ImageNet.

262
00:49:00,584 --> 00:49:17,855
Ещё раз: на каждом слое сначала работают свёрточные фильтры, потом выпрямитель, потом подвыборка максимумом.

263
00:49:17,955 --> 00:49:29,395
После работы многих слоёв результат сравнивается с шаблонами и получается предсказание.

264
00:49:29,495 --> 00:49:40,420
Как видите, демонстрация очень крутая, я не смог бы такое нарисовать, спасибо Отавио за то, что поделился этим.

265
00:49:40,420 --> 00:49:49,895
Это видео демонстрирует работу реальной свёрточной нейронной сети, Отавио написал для этого специальную программу.

266
00:49:49,995 --> 00:49:58,060
Я человек простой и предпочитаю таблицы в Excel.

267
00:49:58,060 --> 00:50:08,950
Эта таблица есть в репозитории fast.ai на GitHub, можете клонировать весь репозиторий или скачать её отдельно,

268
00:50:08,950 --> 00:50:36,880
она доступна по адресу github.com/fastai/fastai/tree/master/courses/dl1/excel/conv-example.xlsx.

269
00:50:36,880 --> 00:50:55,290
В качестве входных данных я взял изображение цифры 7 из базы данных MNIST, мы с ней ещё будем работать.

270
00:50:55,290 --> 00:51:09,640
Каждая ячейка представляет чёрно-белый пиксель, число от 0 до 1. Иногда используются числа от 0 до 255,

271
00:51:13,060 --> 00:51:25,900
это не важно, в PyTorch всё конвертируется в действительные числа от 0 до 1.

272
00:51:25,900 --> 00:51:47,299
Я использовал условное форматирование и сделал высокие значения красными, видно, что здесь изображена цифра 7.

273
00:51:47,399 --> 00:51:55,249
В демонстрации Отавио в первом слое использовались два свёрточных фильтра.

274
00:51:55,349 --> 00:52:05,170
Я создал фильтр, который выделяет верхние границы изображения, матрица свёртки выглядит так:

275
00:52:05,170 --> 00:52:10,119
[[1, 1, 1],
[0, 0, 0],
[-1, -1, -1]].

276
00:52:10,119 --> 00:52:16,630
Давайте выберем один пиксель и посмотрим, что происходит.

277
00:52:16,630 --> 00:52:24,112
Область изображения 3x3 поэлементно умножается на матрицу свёртки.

278
00:52:24,212 --> 00:52:44,134
В верхнем ряду все единицы, в нижнем почти все нули, поэтому итоговое значение получается высоким.

279
00:52:44,234 --> 00:52:54,670
Если сдвинуться на два пикселя вниз, в области уже почти нет ненулевых значений, в итоге ноль.

280
00:52:54,670 --> 00:53:08,859
Более интересный пример: здесь высокие значения как в верхнем ряду, так и в нижнем, в сумме тоже ноль.

281
00:53:08,859 --> 00:53:15,169
Как видно, такой фильтр активирует только горизонтальные края.

282
00:53:15,669 --> 00:53:49,070
Это число 3 называется активацией. Активация — число, полученное после выполнения линейной операции над изображением.

283
00:53:49,170 --> 00:54:03,095
В моей формуле есть и свёртка, и выпрямитель. Выпрямитель отбрасывает отрицательные значения,

284
00:54:03,195 --> 00:54:32,920
это записывается формулой y = MAX(0, x). Выглядит слишком просто, но это так и есть, здесь я ничего не упрощаю.

285
00:54:32,920 --> 00:54:50,490
Когда я упрощаю что-то, я обязательно говорю, что это упрощение, но выпрямитель и свёртка действительно работают очень просто.

286
00:54:50,490 --> 00:55:03,560
На экране сейчас полностью воплощён один слой свёрточной нейронной сети.

287
00:55:03,560 --> 00:55:09,290
Этот свёрточный фильтр выделяет горизонтальные границы.

288
00:55:09,290 --> 00:55:24,660
Опять же, подразумевается, что модель обучили и в процессе обучения появились эти фильтры.

289
00:55:24,760 --> 00:55:31,890
Вот второй фильтр того же слоя, в матрице другие числа.

290
00:55:31,990 --> 00:55:54,230
PyTorch не хранит фильтры как отдельные матрицы 3x3, он хранит их в виде тензора. Тензор — это многомерная матрица.

291
00:55:54,230 --> 00:56:03,495
Фильтры хранятся в виде тензора, что позволяет создавать из них слои.

292
00:56:03,595 --> 00:56:19,700
Понятия свёрточный фильтр и свёрточное ядро взаимозаменяемы — они означают одну из матриц 3x3, одну из частей трёхмерного тензора.

293
00:56:19,700 --> 00:56:35,295
Как видно, этот фильтр находит вертикальные границы изображения.

294
00:56:35,395 --> 00:56:42,030
Два свёрточных фильтра с двумя выпрямителями образуют слой свёрточной нейронной сети.

295
00:56:42,130 --> 00:56:48,680
Этот слой — скрытый, так называются все слои, кроме входного и выходного.

296
00:56:50,810 --> 00:57:05,974
Размер этого слоя — 2, потому что он содержит два свёрточных фильтра.

297
00:57:06,074 --> 00:57:13,729
Переходим к следующему слою.

298
00:57:13,829 --> 00:57:34,540
Он устроен сложнее, потому что на вход теперь подаются оба изображения с предыдущего слоя, поэтому здесь два фильтра.

299
00:57:34,540 --> 00:57:53,464
PyTorch хранит эти две матрицы в одном тензоре 2x3x3, поэтому стоит думать о них, как об одном фильтре.

300
00:57:53,564 --> 00:58:14,264
Для получения активации складываются результаты от двух матриц 3x3.

301
00:58:14,364 --> 00:58:22,420
Верхнее изображение умножается на одну часть фильтра, нижнее — на другую.

302
00:58:22,420 --> 00:58:35,530
Со временем вам придётся привыкнуть к многомерным линейным пространствам.

303
00:58:35,530 --> 00:58:44,855
Я нарисовал свёрточные фильтры один под другим, но на самом деле они как бы наложены друг на друга.

304
00:58:44,955 --> 00:58:50,600
Джеффри Хинтон в своём курсе «Нейронные сети для машинного обучения» на Coursera в 2012 году рассказал,

305
00:58:50,700 --> 00:58:58,990
как все специалисты по анализу данных представляют себе многомерные пространства.

306
00:58:58,990 --> 00:59:06,390
Они представляют двумерное пространство, а потом очень быстро произносят: «Двенадцатимерное пространство!» и всё получается.

307
00:59:06,390 --> 00:59:19,550
Мы дошли до второго слоя, вам придётся просто поверить, что все остальные устроены так же,

308
00:59:19,550 --> 00:59:38,900
потому что Excel не поддерживает трёхмерные матрицы. Если бы поддерживал, эту операцию я бы провёл одним умножением, без суммирования.

309
00:59:38,900 --> 00:59:44,720
Опять же, после свёртки я использую выпрямитель, он же ReLU.

310
00:59:48,710 --> 01:00:02,540
Архитектура нашей нейронной сети такова:

311
01:00:02,540 --> 01:00:15,250
на первом слое 2 свёрточных фильтра 3x3, вот первый и второй,

312
01:00:16,150 --> 01:00:31,895
на втором слое 2 свёрточных фильтра 2x3x3, вот первый и второй.

313
01:00:31,995 --> 01:00:52,940
Все числа в больших таблицах — активации, эта активация вычислена по двум областям 3x3 с помощью фильтра 2x3x3.

314
01:00:52,940 --> 01:00:57,830
Обычно слои свёрточной нейронной сети как-то называют.

315
01:00:57,830 --> 01:01:10,650
Первый слой мы назовём Conv1, второй — Conv2.

316
01:01:10,750 --> 01:01:21,990
Названия слоёв у различных архитектур выводятся в их описании.

317
01:01:22,090 --> 01:01:36,520
Следующая часть архитектуры после слоёв — подвыборка максимумом, этот слой мы назовём Maxpool.

318
01:01:36,520 --> 01:01:43,569
Подвыборку максимумом сложно показать в Excel, но я старался.

319
01:01:43,569 --> 01:01:53,219
При подвыборке максимумом высота и ширина изображения уменьшаются вдвое.

320
01:01:53,319 --> 01:01:59,909
Из каждых четырёх чисел в области 2x2 выбирается максимальное.

321
01:02:00,009 --> 01:02:06,780
Так как изображение уменьшится в два раза, я заполняю только половину ячеек.

322
01:02:06,780 --> 01:02:19,030
Изображение получается похожим, но в два раза меньше по обоим измерениям.

323
01:02:19,030 --> 01:02:24,160
При этом рассматриваются не все возможные области 2x2, изображение как бы разрезается на эти области —

324
01:02:26,319 --> 01:02:38,050
одна область начинается в столбце BQ, а следующая — в столбце BS, они не накладываются друг на друга, поэтому изображение уменьшается.

325
01:02:38,050 --> 01:02:47,525
Если кто-то из вас любит таблицы, можете изучить внимательнее.

326
01:02:47,625 --> 01:02:52,150


327
01:02:52,150 --> 01:02:58,119


328
01:02:58,119 --> 01:03:02,890


329
01:03:04,900 --> 01:03:10,180


330
01:03:12,790 --> 01:03:16,930


331
01:03:16,930 --> 01:03:22,300


332
01:03:22,300 --> 01:03:28,150


333
01:03:28,150 --> 01:03:35,650


334
01:03:35,650 --> 01:03:40,440


335
01:03:40,670 --> 01:03:45,619


336
01:03:45,619 --> 01:03:49,490


337
01:03:51,980 --> 01:03:57,079


338
01:03:57,079 --> 01:04:03,290


339
01:04:03,290 --> 01:04:09,920


340
01:04:09,920 --> 01:04:15,049


341
01:04:15,049 --> 01:04:20,540


342
01:04:20,540 --> 01:04:25,280


343
01:04:25,280 --> 01:04:31,750


344
01:04:31,750 --> 01:04:39,710


345
01:04:39,710 --> 01:04:46,250


346
01:04:46,250 --> 01:04:53,180


347
01:04:53,180 --> 01:05:03,319


348
01:05:03,319 --> 01:05:09,380


349
01:05:09,380 --> 01:05:13,700


350
01:05:15,470 --> 01:05:20,059


351
01:05:20,059 --> 01:05:26,049


352
01:05:26,049 --> 01:05:32,210


353
01:05:32,210 --> 01:05:40,280


354
01:05:40,280 --> 01:05:44,960


355
01:05:44,960 --> 01:05:52,160


356
01:05:52,160 --> 01:05:57,440


357
01:05:57,440 --> 01:06:02,989


358
01:06:02,989 --> 01:06:06,950


359
01:06:06,950 --> 01:06:11,749


360
01:06:11,749 --> 01:06:17,779


361
01:06:17,779 --> 01:06:23,059


362
01:06:23,059 --> 01:06:27,529


363
01:06:27,529 --> 01:06:34,099


364
01:06:34,099 --> 01:06:38,499


365
01:06:42,559 --> 01:06:48,619


366
01:06:48,619 --> 01:06:55,460


367
01:06:55,460 --> 01:06:59,630


368
01:06:59,630 --> 01:07:05,960


369
01:07:05,960 --> 01:07:11,420


370
01:07:11,420 --> 01:07:17,660


371
01:07:17,660 --> 01:07:23,719


372
01:07:23,719 --> 01:07:27,380


373
01:07:27,380 --> 01:07:32,779


374
01:07:32,779 --> 01:07:40,910


375
01:07:40,910 --> 01:07:45,440


376
01:07:45,440 --> 01:07:48,410


377
01:07:48,410 --> 01:07:53,029


378
01:07:53,029 --> 01:08:02,239


379
01:08:02,239 --> 01:08:06,890


380
01:08:06,890 --> 01:08:10,300


381
01:08:10,300 --> 01:08:15,940


382
01:08:15,940 --> 01:08:19,870


383
01:08:19,870 --> 01:08:23,230


384
01:08:23,229 --> 01:08:26,469


385
01:08:26,470 --> 01:08:29,050


386
01:08:29,050 --> 01:08:33,070


387
01:08:35,350 --> 01:08:39,760


388
01:08:39,760 --> 01:08:50,310


389
01:08:50,310 --> 01:09:00,339


390
01:09:02,290 --> 01:09:07,180


391
01:09:07,180 --> 01:09:11,770


392
01:09:11,770 --> 01:09:17,920


393
01:09:17,920 --> 01:09:23,970


394
01:09:23,970 --> 01:09:36,460


395
01:09:36,460 --> 01:09:44,740


396
01:09:44,740 --> 01:09:50,370


397
01:09:50,370 --> 01:09:57,130


398
01:09:57,130 --> 01:10:04,030


399
01:10:04,030 --> 01:10:09,790


400
01:10:09,790 --> 01:10:14,680


401
01:10:14,680 --> 01:10:19,810


402
01:10:19,810 --> 01:10:23,150


403
01:10:27,110 --> 01:10:40,190


404
01:10:40,190 --> 01:10:48,190


405
01:10:48,190 --> 01:10:56,539


406
01:10:56,539 --> 01:11:02,030


407
01:11:02,030 --> 01:11:06,199


408
01:11:06,199 --> 01:11:10,820


409
01:11:10,820 --> 01:11:16,989


410
01:11:16,989 --> 01:11:23,150


411
01:11:23,150 --> 01:11:32,300


412
01:11:32,300 --> 01:11:37,039


413
01:11:37,039 --> 01:11:40,340


414
01:11:40,340 --> 01:11:44,479


415
01:11:44,479 --> 01:11:48,249


416
01:11:50,510 --> 01:11:55,969


417
01:11:55,969 --> 01:12:01,400


418
01:12:01,400 --> 01:12:07,449


419
01:12:07,449 --> 01:12:16,340


420
01:12:16,340 --> 01:12:23,630


421
01:12:23,630 --> 01:12:29,780


422
01:12:29,780 --> 01:12:32,920


423
01:12:33,070 --> 01:12:39,260


424
01:12:39,260 --> 01:12:44,230


425
01:12:45,590 --> 01:12:55,230


426
01:12:58,619 --> 01:13:04,500


427
01:13:04,500 --> 01:13:09,840


428
01:13:09,840 --> 01:13:13,340


429
01:13:13,829 --> 01:13:18,210


430
01:13:18,210 --> 01:13:22,139


431
01:13:22,139 --> 01:13:29,179


432
01:13:29,179 --> 01:13:34,769


433
01:13:34,769 --> 01:13:39,599


434
01:13:39,599 --> 01:13:45,570


435
01:13:45,570 --> 01:13:51,269


436
01:13:51,269 --> 01:13:54,749


437
01:13:54,749 --> 01:14:00,239


438
01:14:00,239 --> 01:14:04,979


439
01:14:04,979 --> 01:14:10,320


440
01:14:12,300 --> 01:14:19,289


441
01:14:19,289 --> 01:14:24,869


442
01:14:24,869 --> 01:14:29,550


443
01:14:29,550 --> 01:14:35,729


444
01:14:35,729 --> 01:14:43,289


445
01:14:46,139 --> 01:14:53,489


446
01:14:54,960 --> 01:14:59,940


447
01:14:59,940 --> 01:15:06,090


448
01:15:06,090 --> 01:15:10,170


449
01:15:10,170 --> 01:15:16,199


450
01:15:18,630 --> 01:15:23,280


451
01:15:23,280 --> 01:15:26,670


452
01:15:26,670 --> 01:15:29,610


453
01:15:29,610 --> 01:15:37,470


454
01:15:37,470 --> 01:15:41,760


455
01:15:41,760 --> 01:15:46,500


456
01:15:46,500 --> 01:15:51,420


457
01:15:51,420 --> 01:15:58,650


458
01:15:58,650 --> 01:16:03,450


459
01:16:03,450 --> 01:16:08,670


460
01:16:08,670 --> 01:16:19,530


461
01:16:19,530 --> 01:16:33,090


462
01:16:33,090 --> 01:16:37,260


463
01:16:37,260 --> 01:16:41,010


464
01:16:41,010 --> 01:16:45,360


465
01:16:50,820 --> 01:16:57,480


466
01:16:57,480 --> 01:17:03,390


467
01:17:03,390 --> 01:17:14,370


468
01:17:14,370 --> 01:17:21,450


469
01:17:21,450 --> 01:17:25,140


470
01:17:25,140 --> 01:17:31,650


471
01:17:33,780 --> 01:17:38,260


472
01:17:38,260 --> 01:17:44,590


473
01:17:44,590 --> 01:17:49,450


474
01:17:49,450 --> 01:17:52,630


475
01:17:52,630 --> 01:17:58,210


476
01:17:58,210 --> 01:18:02,710


477
01:18:02,710 --> 01:18:05,320


478
01:18:05,320 --> 01:18:10,480


479
01:18:10,480 --> 01:18:15,850


480
01:18:15,850 --> 01:18:28,120


481
01:18:32,340 --> 01:18:37,830


482
01:18:37,830 --> 01:18:43,860


483
01:18:43,860 --> 01:18:51,460


484
01:18:51,460 --> 01:18:56,890


485
01:18:56,890 --> 01:19:04,840


486
01:19:04,840 --> 01:19:11,050


487
01:19:11,050 --> 01:19:16,510


488
01:19:16,510 --> 01:19:20,739


489
01:19:23,110 --> 01:19:28,420


490
01:19:28,420 --> 01:19:33,100


491
01:19:33,100 --> 01:19:39,190


492
01:19:41,140 --> 01:19:47,140


493
01:19:47,140 --> 01:19:53,680


494
01:19:53,680 --> 01:19:57,700


495
01:20:00,100 --> 01:20:07,510


496
01:20:07,510 --> 01:20:12,610


497
01:20:14,980 --> 01:20:22,600


498
01:20:22,600 --> 01:20:31,180


499
01:20:31,180 --> 01:20:34,780


500
01:20:37,600 --> 01:20:43,230


501
01:20:45,419 --> 01:20:48,840


502
01:20:48,840 --> 01:20:53,010


503
01:20:53,010 --> 01:20:57,119


504
01:21:00,419 --> 01:21:08,189


505
01:21:08,189 --> 01:21:13,229


506
01:21:13,229 --> 01:21:19,769


507
01:21:19,769 --> 01:21:25,559


508
01:21:25,559 --> 01:21:30,389


509
01:21:30,389 --> 01:21:34,260


510
01:21:34,260 --> 01:21:39,719


511
01:21:39,719 --> 01:21:44,880


512
01:21:44,880 --> 01:21:50,999


513
01:21:50,999 --> 01:21:56,969


514
01:21:56,969 --> 01:22:02,550


515
01:22:02,550 --> 01:22:09,840


516
01:22:09,840 --> 01:22:15,689


517
01:22:15,689 --> 01:22:21,749


518
01:22:23,610 --> 01:22:28,079


519
01:22:28,079 --> 01:22:34,559


520
01:22:34,559 --> 01:22:39,419


521
01:22:39,419 --> 01:22:46,229


522
01:22:46,229 --> 01:22:49,590


523
01:22:49,590 --> 01:22:55,249


524
01:22:55,249 --> 01:23:00,550


525
01:23:00,550 --> 01:23:05,590


526
01:23:05,590 --> 01:23:08,560


527
01:23:08,560 --> 01:23:15,940


528
01:23:21,880 --> 01:23:25,600


529
01:23:25,600 --> 01:23:29,770


530
01:23:29,770 --> 01:23:40,690


531
01:23:43,150 --> 01:23:48,370


532
01:23:48,370 --> 01:23:55,180


533
01:23:55,180 --> 01:24:04,050


534
01:24:04,050 --> 01:24:12,160


535
01:24:12,160 --> 01:24:16,420


536
01:24:16,420 --> 01:24:21,610


537
01:24:21,610 --> 01:24:27,220


538
01:24:27,220 --> 01:24:29,800


539
01:24:29,800 --> 01:24:35,140


540
01:24:35,140 --> 01:24:39,670


541
01:24:39,670 --> 01:24:45,070


542
01:24:45,070 --> 01:24:50,620


543
01:24:50,620 --> 01:24:55,630


544
01:24:55,630 --> 01:25:01,690


545
01:25:05,670 --> 01:25:11,130


546
01:25:11,130 --> 01:25:16,290


547
01:25:16,290 --> 01:25:23,340


548
01:25:23,340 --> 01:25:28,440


549
01:25:32,429 --> 01:25:37,650


550
01:25:37,650 --> 01:25:43,230


551
01:25:43,230 --> 01:25:48,360


552
01:25:48,360 --> 01:25:53,310


553
01:25:53,310 --> 01:25:58,080


554
01:26:00,300 --> 01:26:04,650


555
01:26:08,429 --> 01:26:12,900


556
01:26:12,900 --> 01:26:20,219


557
01:26:20,219 --> 01:26:25,560


558
01:26:25,560 --> 01:26:30,480


559
01:26:32,159 --> 01:26:37,260


560
01:26:37,260 --> 01:26:42,719


561
01:26:42,719 --> 01:26:47,880


562
01:26:47,880 --> 01:26:51,989


563
01:26:54,480 --> 01:26:57,480


564
01:26:57,480 --> 01:27:02,550


565
01:27:02,550 --> 01:27:07,350


566
01:27:07,350 --> 01:27:13,290


567
01:27:13,290 --> 01:27:18,800


568
01:27:18,800 --> 01:27:23,070


569
01:27:24,829 --> 01:27:31,489


570
01:27:31,489 --> 01:27:38,329


571
01:27:38,329 --> 01:27:42,349


572
01:27:42,349 --> 01:27:49,880


573
01:27:49,880 --> 01:27:53,960


574
01:27:53,960 --> 01:28:00,559


575
01:28:00,559 --> 01:28:05,690


576
01:28:08,090 --> 01:28:15,619


577
01:28:17,780 --> 01:28:20,960


578
01:28:20,960 --> 01:28:25,360


579
01:28:25,360 --> 01:28:34,099


580
01:28:34,099 --> 01:28:40,239


581
01:28:41,570 --> 01:28:47,840


582
01:28:47,840 --> 01:28:52,099


583
01:28:52,099 --> 01:28:57,710


584
01:28:57,710 --> 01:29:07,040


585
01:29:07,040 --> 01:29:15,829


586
01:29:15,829 --> 01:29:21,559


587
01:29:21,559 --> 01:29:25,489


588
01:29:27,440 --> 01:29:31,190


589
01:29:31,190 --> 01:29:36,790


590
01:29:36,790 --> 01:29:42,599


591
01:29:42,599 --> 01:29:51,869


592
01:29:53,219 --> 01:30:01,080


593
01:30:01,080 --> 01:30:07,230


594
01:30:07,230 --> 01:30:12,599


595
01:30:12,599 --> 01:30:18,869


596
01:30:22,940 --> 01:30:29,699


597
01:30:29,699 --> 01:30:34,139


598
01:30:34,139 --> 01:30:39,119


599
01:30:39,119 --> 01:30:44,489


600
01:30:44,489 --> 01:30:50,040


601
01:30:50,040 --> 01:30:56,940


602
01:30:56,940 --> 01:31:03,540


603
01:31:03,540 --> 01:31:08,670


604
01:31:08,670 --> 01:31:12,179


605
01:31:12,179 --> 01:31:16,110


606
01:31:16,110 --> 01:31:21,320


607
01:31:21,680 --> 01:31:28,250


608
01:31:28,250 --> 01:31:36,530


609
01:31:36,530 --> 01:31:40,700


610
01:31:40,700 --> 01:31:48,710


611
01:31:48,710 --> 01:31:56,420


612
01:31:56,420 --> 01:32:01,160


613
01:32:01,160 --> 01:32:04,850


614
01:32:04,850 --> 01:32:11,680


615
01:32:11,680 --> 01:32:17,750


616
01:32:17,750 --> 01:32:23,270


617
01:32:23,270 --> 01:32:27,860


618
01:32:30,050 --> 01:32:34,160


619
01:32:34,160 --> 01:32:40,040


620
01:32:40,040 --> 01:32:45,350


621
01:32:45,350 --> 01:32:51,080


622
01:32:51,080 --> 01:32:59,000


623
01:32:59,000 --> 01:33:07,940


624
01:33:07,940 --> 01:33:24,200


625
01:33:24,200 --> 01:33:29,670


626
01:33:29,670 --> 01:33:33,110


627
01:33:35,450 --> 01:33:44,250


628
01:33:44,250 --> 01:33:49,650


629
01:33:49,650 --> 01:33:56,070


630
01:33:56,070 --> 01:34:02,970


631
01:34:02,970 --> 01:34:08,670


632
01:34:08,670 --> 01:34:12,930


633
01:34:12,930 --> 01:34:15,840


634
01:34:15,840 --> 01:34:19,710


635
01:34:19,710 --> 01:34:22,830


636
01:34:22,830 --> 01:34:27,990


637
01:34:27,990 --> 01:34:33,300


638
01:34:33,300 --> 01:34:41,010


639
01:34:41,010 --> 01:34:46,350


640
01:34:46,350 --> 01:34:52,170


641
01:34:52,170 --> 01:34:56,370


642
01:34:56,370 --> 01:35:02,580


643
01:35:02,580 --> 01:35:07,770


644
01:35:07,770 --> 01:35:14,130


645
01:35:14,130 --> 01:35:18,530


646
01:35:18,530 --> 01:35:25,650


647
01:35:25,650 --> 01:35:32,690


648
01:35:32,690 --> 01:35:37,170


649
01:35:37,170 --> 01:35:44,910


650
01:35:44,910 --> 01:35:48,270


651
01:35:48,270 --> 01:35:52,620


652
01:35:54,360 --> 01:36:00,060


653
01:36:00,060 --> 01:36:04,980


654
01:36:07,440 --> 01:36:12,000


655
01:36:12,000 --> 01:36:17,760


656
01:36:17,760 --> 01:36:23,480


657
01:36:25,920 --> 01:36:31,260


658
01:36:31,260 --> 01:36:39,870


659
01:36:39,870 --> 01:36:44,460


660
01:36:44,460 --> 01:36:49,890


661
01:36:49,890 --> 01:36:57,530


662
01:37:00,210 --> 01:37:04,920


663
01:37:04,920 --> 01:37:13,410


664
01:37:13,410 --> 01:37:17,280


665
01:37:17,280 --> 01:37:25,920


666
01:37:25,920 --> 01:37:30,390


667
01:37:30,390 --> 01:37:38,090


668
01:37:38,090 --> 01:37:44,190


669
01:37:47,190 --> 01:37:52,410


670
01:37:52,410 --> 01:37:58,140


671
01:37:58,140 --> 01:38:04,620


672
01:38:04,620 --> 01:38:09,120


673
01:38:09,120 --> 01:38:14,820


674
01:38:14,820 --> 01:38:21,840


675
01:38:21,840 --> 01:38:25,950


676
01:38:25,950 --> 01:38:30,090


677
01:38:30,090 --> 01:38:35,300


678
01:38:35,300 --> 01:38:39,440


679
01:38:42,150 --> 01:38:51,140


680
01:38:51,200 --> 01:38:59,160


681
01:38:59,160 --> 01:39:07,650


682
01:39:07,650 --> 01:39:12,810


683
01:39:12,810 --> 01:39:17,130


684
01:39:17,130 --> 01:39:20,300


685
01:39:21,510 --> 01:39:25,710


686
01:39:25,710 --> 01:39:29,699


687
01:39:29,699 --> 01:39:35,099


688
01:39:35,099 --> 01:39:41,659


689
01:39:45,989 --> 01:39:51,300


690
01:39:51,300 --> 01:39:57,150


691
01:40:01,320 --> 01:40:08,400


692
01:40:08,400 --> 01:40:14,070


693
01:40:14,070 --> 01:40:19,079


694
01:40:19,079 --> 01:40:23,280


695
01:40:23,280 --> 01:40:27,300


696
01:40:27,300 --> 01:40:34,409


697
01:40:36,809 --> 01:40:40,440


698
01:40:44,219 --> 01:40:48,389


699
01:40:48,389 --> 01:40:53,880


700
01:40:53,880 --> 01:41:05,119


701
01:41:05,119 --> 01:41:13,980


702
01:41:13,980 --> 01:41:19,170


703
01:41:19,170 --> 01:41:21,840


704
01:41:21,840 --> 01:41:24,900


705
01:41:26,880 --> 01:41:32,570


706
01:41:33,980 --> 01:41:39,400


707
01:41:39,400 --> 01:41:47,090


708
01:41:47,090 --> 01:41:51,140


709
01:41:51,140 --> 01:41:54,260


710
01:41:54,260 --> 01:41:59,390


711
01:41:59,390 --> 01:42:04,370


712
01:42:04,370 --> 01:42:11,540


713
01:42:11,540 --> 01:42:15,140


714
01:42:15,140 --> 01:42:19,700


715
01:42:21,739 --> 01:42:28,730


716
01:42:28,730 --> 01:42:33,110


717
01:42:33,110 --> 01:42:38,120


718
01:42:38,120 --> 01:42:47,270


719
01:42:47,270 --> 01:42:55,540


720
01:42:57,320 --> 01:43:08,690


721
01:43:08,690 --> 01:43:13,400


722
01:43:13,400 --> 01:43:19,340


723
01:43:19,340 --> 01:43:25,970


724
01:43:25,970 --> 01:43:30,530


725
01:43:33,380 --> 01:43:38,930


726
01:43:38,930 --> 01:43:44,290


727
01:43:46,710 --> 01:43:54,490


728
01:43:54,490 --> 01:44:00,580


729
01:44:00,580 --> 01:44:06,970


730
01:44:06,970 --> 01:44:15,940


731
01:44:18,520 --> 01:44:26,470


732
01:44:26,470 --> 01:44:32,890


733
01:44:32,890 --> 01:44:38,140


734
01:44:38,140 --> 01:44:47,080


735
01:44:47,080 --> 01:44:52,180


736
01:44:52,180 --> 01:44:58,390


737
01:44:58,390 --> 01:45:05,350


738
01:45:05,350 --> 01:45:09,100


739
01:45:09,100 --> 01:45:13,510


740
01:45:13,510 --> 01:45:19,690


741
01:45:23,650 --> 01:45:28,690


742
01:45:28,690 --> 01:45:34,210


743
01:45:34,210 --> 01:45:40,870


744
01:45:40,870 --> 01:45:48,240


745
01:45:48,240 --> 01:45:55,170


746
01:45:55,170 --> 01:46:05,119


747
01:46:05,119 --> 01:46:15,439


748
01:46:15,439 --> 01:46:21,229


749
01:46:21,229 --> 01:46:26,300


750
01:46:26,300 --> 01:46:34,570


751
01:46:36,050 --> 01:46:44,560


752
01:46:45,869 --> 01:46:53,440


753
01:46:53,440 --> 01:47:01,960


754
01:47:01,960 --> 01:47:05,440


755
01:47:05,440 --> 01:47:08,949


756
01:47:08,949 --> 01:47:15,880


757
01:47:15,880 --> 01:47:19,869


758
01:47:19,869 --> 01:47:22,659


759
01:47:22,659 --> 01:47:34,059


760
01:47:34,059 --> 01:47:39,659


761
01:47:42,989 --> 01:47:50,920


762
01:47:50,920 --> 01:47:55,960


763
01:47:55,960 --> 01:48:01,599


764
01:48:01,599 --> 01:48:07,119


765
01:48:10,840 --> 01:48:16,210


766
01:48:16,210 --> 01:48:23,769


767
01:48:23,769 --> 01:48:27,519


768
01:48:27,519 --> 01:48:33,820


769
01:48:36,190 --> 01:48:42,969


770
01:48:42,969 --> 01:48:50,309


771
01:48:50,309 --> 01:48:56,920


772
01:48:56,920 --> 01:49:02,530


773
01:49:02,530 --> 01:49:08,380


774
01:49:08,380 --> 01:49:16,830


775
01:49:16,830 --> 01:49:22,960


776
01:49:22,960 --> 01:49:28,270


777
01:49:28,270 --> 01:49:34,690


778
01:49:37,420 --> 01:49:42,730


779
01:49:45,460 --> 01:49:52,150


780
01:49:52,150 --> 01:50:00,219


781
01:50:00,219 --> 01:50:03,989


782
01:50:07,300 --> 01:50:13,960


783
01:50:13,960 --> 01:50:18,250


784
01:50:18,250 --> 01:50:22,449


785
01:50:22,449 --> 01:50:28,179


786
01:50:28,179 --> 01:50:34,960


787
01:50:34,960 --> 01:50:40,270


788
01:50:40,270 --> 01:50:48,190


789
01:50:49,719 --> 01:50:53,890


790
01:50:53,890 --> 01:50:57,610


791
01:51:00,910 --> 01:51:05,290


792
01:51:05,290 --> 01:51:09,190


793
01:51:09,190 --> 01:51:13,250


794
01:51:13,250 --> 01:51:19,969


795
01:51:19,969 --> 01:51:24,050


796
01:51:24,050 --> 01:51:28,840


797
01:51:28,840 --> 01:51:34,369


798
01:51:35,809 --> 01:51:41,900


799
01:51:46,909 --> 01:51:54,349


800
01:51:54,349 --> 01:52:02,869


801
01:52:09,619 --> 01:52:19,699


802
01:52:19,699 --> 01:52:24,320


803
01:52:24,320 --> 01:52:29,750


804
01:52:29,750 --> 01:52:34,730


805
01:52:34,730 --> 01:52:41,329


806
01:52:41,329 --> 01:52:49,250


807
01:52:49,250 --> 01:52:52,670


808
01:52:52,670 --> 01:52:58,610


809
01:52:58,610 --> 01:53:03,559


810
01:53:03,559 --> 01:53:13,489


811
01:53:18,019 --> 01:53:24,070


812
01:53:27,010 --> 01:53:33,099


813
01:53:33,099 --> 01:53:40,539


814
01:53:40,539 --> 01:53:46,780


815
01:53:46,780 --> 01:53:50,170


816
01:53:50,170 --> 01:53:54,070


817
01:53:54,070 --> 01:53:59,289


818
01:53:59,289 --> 01:54:03,880


819
01:54:03,880 --> 01:54:08,860


820
01:54:08,860 --> 01:54:17,499


821
01:54:17,499 --> 01:54:23,230


822
01:54:23,230 --> 01:54:27,039


823
01:54:27,039 --> 01:54:33,429


824
01:54:33,429 --> 01:54:39,249


825
01:54:39,249 --> 01:54:47,079


826
01:54:47,079 --> 01:54:51,280


827
01:54:51,280 --> 01:54:54,909


828
01:54:54,909 --> 01:55:04,030


829
01:55:04,030 --> 01:55:08,409


830
01:55:08,409 --> 01:55:14,949


831
01:55:14,949 --> 01:55:19,389


832
01:55:19,389 --> 01:55:22,659


833
01:55:24,579 --> 01:55:28,150


834
01:55:28,150 --> 01:55:31,230


835
01:55:35,349 --> 01:55:40,729


836
01:55:40,729 --> 01:55:46,550


837
01:55:46,550 --> 01:55:50,689


838
01:55:50,689 --> 01:55:55,579


839
01:55:57,889 --> 01:56:03,679


840
01:56:03,679 --> 01:56:10,219


841
01:56:10,219 --> 01:56:14,929


842
01:56:14,929 --> 01:56:19,669


843
01:56:19,669 --> 01:56:24,019


844
01:56:24,019 --> 01:56:26,630


845
01:56:26,630 --> 01:56:30,469


846
01:56:30,469 --> 01:56:33,860


847
01:56:33,860 --> 01:56:38,869


848
01:56:38,869 --> 01:56:43,999


849
01:56:43,999 --> 01:56:54,229


850
01:56:54,229 --> 01:57:02,479


851
01:57:02,479 --> 01:57:09,590


852
01:57:09,590 --> 01:57:14,419


853
01:57:14,419 --> 01:57:21,919


854
01:57:21,919 --> 01:57:27,110


855
01:57:27,110 --> 01:57:33,260


856
01:57:33,260 --> 01:57:38,449


857
01:57:41,869 --> 01:57:46,820


858
01:57:46,820 --> 01:57:50,900


859
01:57:50,900 --> 01:57:56,260


860
01:57:56,260 --> 01:58:04,280


861
01:58:07,580 --> 01:58:13,130


862
01:58:13,130 --> 01:58:18,290


863
01:58:18,290 --> 01:58:23,660


864
01:58:23,660 --> 01:58:29,200


865
01:58:29,200 --> 01:58:35,240


866
01:58:35,240 --> 01:58:39,200


867
01:58:39,200 --> 01:58:47,870


868
01:58:47,870 --> 01:58:54,080


869
01:58:54,080 --> 01:59:00,770


870
01:59:00,770 --> 01:59:06,260


871
01:59:06,260 --> 01:59:12,650


872
01:59:14,030 --> 01:59:18,260


873
01:59:18,260 --> 01:59:22,910


874
01:59:22,910 --> 01:59:26,450


875
01:59:26,450 --> 01:59:30,560


876
01:59:30,560 --> 01:59:35,530


877
01:59:38,250 --> 01:59:44,440


878
01:59:44,440 --> 01:59:48,160


879
01:59:48,160 --> 01:59:52,690


880
01:59:52,690 --> 02:00:02,500


881
02:00:02,500 --> 02:00:10,300


882
02:00:10,300 --> 02:00:16,600


883
02:00:16,600 --> 02:00:21,850


884
02:00:21,850 --> 02:00:30,130


885
02:00:30,130 --> 02:00:37,199


886
02:00:39,960 --> 02:00:45,520


887
02:00:45,520 --> 02:00:49,449


888
02:00:51,550 --> 02:00:57,940


889
02:01:00,480 --> 02:01:05,489


890
02:01:05,489 --> 02:01:11,770


891
02:01:11,770 --> 02:01:16,210


892
02:01:16,210 --> 02:01:21,840


893
02:01:21,840 --> 02:01:31,199


894
02:01:34,810 --> 02:01:39,730


895
02:01:41,590 --> 02:01:46,179


896
02:01:46,179 --> 02:01:49,719


897
02:01:49,719 --> 02:01:55,880


898
02:01:55,880 --> 02:02:01,309


899
02:02:01,309 --> 02:02:05,749


900
02:02:05,749 --> 02:02:10,070


901
02:02:10,070 --> 02:02:14,780


902
02:02:14,780 --> 02:02:21,130


903
02:02:21,399 --> 02:02:26,360


904
02:02:28,099 --> 02:02:32,659


905
02:02:35,989 --> 02:02:40,099


906
02:02:40,099 --> 02:02:50,539


907
02:02:50,539 --> 02:02:53,780


908
02:02:53,780 --> 02:02:58,579


909
02:02:58,579 --> 02:03:03,320


910
02:03:03,320 --> 02:03:10,130


911
02:03:10,130 --> 02:03:15,769


912
02:03:15,769 --> 02:03:19,849


913
02:03:19,849 --> 02:03:24,369


914
02:03:29,059 --> 02:03:34,639


915
02:03:34,639 --> 02:03:40,189


916
02:03:40,189 --> 02:03:45,289


917
02:03:48,889 --> 02:03:52,849


918
02:03:52,849 --> 02:03:57,769


919
02:03:57,769 --> 02:04:04,010


920
02:04:04,010 --> 02:04:09,560


921
02:04:09,560 --> 02:04:14,600


922
02:04:14,600 --> 02:04:18,950


923
02:04:20,600 --> 02:04:25,000


924
02:04:25,000 --> 02:04:30,230


925
02:04:30,230 --> 02:04:36,080


926
02:04:36,080 --> 02:04:41,630


927
02:04:41,630 --> 02:04:49,100


928
02:04:49,100 --> 02:04:54,710


929
02:04:54,710 --> 02:04:59,240


930
02:04:59,240 --> 02:05:08,240


931
02:05:08,240 --> 02:05:13,690


932
02:05:14,730 --> 02:05:18,020


933
02:05:18,020 --> 02:05:24,920


934
02:05:24,920 --> 02:05:30,619


935
02:05:30,619 --> 02:05:37,340


936
02:05:37,340 --> 02:05:45,940


937
02:05:48,949 --> 02:05:54,980


938
02:05:54,980 --> 02:05:59,030


939
02:05:59,030 --> 02:06:06,710


940
02:06:06,710 --> 02:06:10,610


941
02:06:10,610 --> 02:06:15,500


942
02:06:15,500 --> 02:06:19,579


943
02:06:19,579 --> 02:06:24,409


944
02:06:24,409 --> 02:06:36,159


945
02:06:36,159 --> 02:06:44,150


946
02:06:44,150 --> 02:06:49,820


947
02:06:49,820 --> 02:06:55,639


948
02:06:55,639 --> 02:07:03,710


949
02:07:03,710 --> 02:07:08,780


950
02:07:08,780 --> 02:07:12,440


951
02:07:12,440 --> 02:07:16,880


952
02:07:16,880 --> 02:07:23,030


953
02:07:23,030 --> 02:07:28,460


954
02:07:28,460 --> 02:07:30,840


955
02:07:30,840 --> 02:07:34,469


956
02:07:34,469 --> 02:07:41,520


957
02:07:41,520 --> 02:07:48,780


958
02:07:50,909 --> 02:08:04,190


959
02:08:05,400 --> 02:08:12,270


960
02:08:12,270 --> 02:08:19,860


961
02:08:23,429 --> 02:08:26,510


962
02:08:29,940 --> 02:08:38,320


963
02:08:38,320 --> 02:08:43,360


964
02:08:43,360 --> 02:08:47,410


965
02:08:47,410 --> 02:08:51,760


966
02:08:51,760 --> 02:08:54,550


967
02:08:54,550 --> 02:08:59,740


968
02:08:59,740 --> 02:09:03,400


969
02:09:03,400 --> 02:09:10,210


970
02:09:10,210 --> 02:09:12,930


971
02:09:13,989 --> 02:09:19,239


972
02:09:19,239 --> 02:09:31,120


973
02:09:31,120 --> 02:09:35,380


974
02:09:35,380 --> 02:09:40,540


975
02:09:40,540 --> 02:09:44,380


976
02:09:44,380 --> 02:09:48,520


977
02:09:48,520 --> 02:09:55,180


978
02:09:55,180 --> 02:09:59,430


979
02:09:59,430 --> 02:10:05,760


980
02:10:09,250 --> 02:10:20,710


981
02:10:20,710 --> 02:10:24,430


982
02:10:24,430 --> 02:10:30,130


983
02:10:30,130 --> 02:10:35,530


984
02:10:35,530 --> 02:10:38,920


985
02:10:38,920 --> 02:10:43,780


986
02:10:43,780 --> 02:10:46,360


987
02:10:46,360 --> 02:10:49,380


988
02:10:49,380 --> 02:10:59,530


989
02:10:59,530 --> 02:11:04,300


990
02:11:04,300 --> 02:11:07,630


991
02:11:07,630 --> 02:11:12,970


992
02:11:12,970 --> 02:11:18,460


993
02:11:18,460 --> 02:11:22,020


994
02:11:22,740 --> 02:11:26,800


995
02:11:26,800 --> 02:11:29,950


996
02:11:29,950 --> 02:11:37,930


997
02:11:37,930 --> 02:11:42,940


998
02:11:45,130 --> 02:11:52,180


999
02:11:52,180 --> 02:11:57,630


1000
02:11:57,630 --> 02:12:01,300


1001
02:12:01,300 --> 02:12:05,800


1002
02:12:05,800 --> 02:12:11,170


1003
02:12:11,170 --> 02:12:16,420


1004
02:12:16,420 --> 02:12:20,770


1005
02:12:20,770 --> 02:12:26,410


1006
02:12:28,420 --> 02:12:32,410


1007
02:12:32,410 --> 02:12:37,540


1008
02:12:37,540 --> 02:12:40,540


1009
02:12:40,540 --> 02:12:45,400


1010
02:12:45,400 --> 02:12:50,650


1011
02:12:50,650 --> 02:12:57,580


1012
02:12:57,580 --> 02:13:01,690


1013
02:13:01,690 --> 02:13:09,400


1014
02:13:09,400 --> 02:13:18,510


1015
02:13:18,510 --> 02:13:25,780


1016
02:13:28,990 --> 02:13:36,160


1017
02:13:36,160 --> 02:13:40,900


1018
02:13:40,900 --> 02:13:48,640


1019
02:13:48,640 --> 02:13:52,300


1020
02:13:52,300 --> 02:13:58,480


1021
02:14:01,390 --> 02:14:08,050


1022
02:14:08,050 --> 02:14:13,150


1023
02:14:15,490 --> 02:14:20,620


1024
02:14:20,620 --> 02:14:26,170


1025
02:14:26,170 --> 02:14:29,770


1026
02:14:29,770 --> 02:14:34,039


1027
02:14:35,839 --> 02:14:43,609


1028
02:14:43,609 --> 02:14:49,849


1029
02:14:49,849 --> 02:14:53,419


1030
02:14:53,419 --> 02:14:58,669


1031
02:14:58,669 --> 02:15:02,899


1032
02:15:02,899 --> 02:15:09,109


1033
02:15:09,109 --> 02:15:13,780


1034
02:15:17,689 --> 02:15:23,659


1035
02:15:26,359 --> 02:15:33,019


1036
02:15:33,019 --> 02:15:39,079


1037
02:15:40,339 --> 02:15:47,479


1038
02:15:47,479 --> 02:15:53,030


1039
02:15:59,030 --> 02:16:05,869


1040
02:16:05,869 --> 02:16:11,030


1041
02:16:11,030 --> 02:16:17,059


1042
02:16:17,059 --> 02:16:19,879


1043
02:16:19,879 --> 02:16:24,619


1044
02:16:24,619 --> 02:16:28,869


1045
02:16:28,869 --> 02:16:34,119


