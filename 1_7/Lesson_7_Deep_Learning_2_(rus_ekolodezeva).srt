1
00:00:00,050 --> 00:00:04,965
Добро пожаловать на последнюю лекцию первой части курса.

2
00:00:05,065 --> 00:00:12,484
Темы первой части курса — классификация и регрессия с использованием глубокого обучения

3
00:00:12,584 --> 00:00:19,439
с уклоном в изучение лучших практик.

4
00:00:19,439 --> 00:00:25,500
Мы начали с использования непонятных трёх строк кода для решения задачи классификации изображений,

5
00:00:25,500 --> 00:00:36,420
потом перешли к NLP, структурированным данным и коллаборативной фильтрации,

6
00:00:36,420 --> 00:00:44,450
уточняя необходимые теоретические моменты и обучаясь создавать хорошо работающие модели.

7
00:00:44,450 --> 00:00:51,400
Последние три лекции курса мы посвятили более глубокому изучению этих тем в другом порядке —

8
00:00:51,500 --> 00:01:02,430
поняли, как всё работает, посмотрели на код и реализовали большинство элементов с нуля.

9
00:01:02,430 --> 00:01:17,875
Во второй части курса мы отойдём от задач классификации и регрессии, которые предсказывают несколько чисел,

10
00:01:17,975 --> 00:01:25,259
и перейдём к генеративным моделям, создающим большие объёмы данных —

11
00:01:25,259 --> 00:01:34,659
связные предложение при переводе, или описании изображений, или ответах на вопросы;

12
00:01:34,759 --> 00:01:42,469
изображения при передаче художественного стиля, семантические сегменты и так далее.

13
00:01:42,569 --> 00:02:18,400
Во второй части курса мы перейдём от испытанных техник к изучению непроверенных методов из свежих статей.

14
00:02:18,500 --> 00:02:33,789
Если вы хотите научиться читать статьи и реализовывать новейшие методы, пройдите вторую часть курса.

15
00:02:33,889 --> 00:02:39,325
Вторая часть, как и первая, не требует математики выше школьного уровня,

16
00:02:39,425 --> 00:02:49,869
но предполагается, что у вас есть время и терпение разбираться с необходимыми идеями и воплощать их в коде.

17
00:02:49,969 --> 00:03:00,540
Самое сложное в преподавании RNN — убедить студентов, что рекуррентные нейронные сети

18
00:03:00,540 --> 00:03:08,709
ничем не отличаются от обычных полносвязных нейронных сетей.

19
00:03:08,809 --> 00:03:15,750
Так выглядит полносвязная нейронная сеть.

20
00:03:15,750 --> 00:03:25,984
Напомню, что стрелки показывают операции слоя — как правило, связку линейный слой + нелинейная функция активации.

21
00:03:26,084 --> 00:03:33,139
Здесь это умножение матриц + выпрямитель или гиперболический тангенс.

22
00:03:33,239 --> 00:03:40,054
Стрелки одного цвета используют одну матрицу весов.

23
00:03:40,154 --> 00:03:45,369
Единственное отличие от полносвязных сетей, которые мы видели раньше —

24
00:03:45,469 --> 00:03:54,389
есть несколько входных слоёв в разных местах сети.

25
00:03:54,389 --> 00:04:01,180
Мы пробовали прибавлять входные данные к активациям полносвязного слоя, потом — склеивать их вместе.

26
00:04:01,280 --> 00:04:06,540
Принцип работы модели не меняется из-за наличия нескольких входных слоёв.

27
00:04:06,640 --> 00:04:15,030
Эта версия модели реализована в классе Char3Model.

28
00:04:15,030 --> 00:04:19,909
Здесь заданы стрелки разных цветов как три матрицы весов,

29
00:04:19,909 --> 00:04:29,874
конструктор nn.Linear() объединяет матрицу весов и вектор смещений.

30
00:04:29,974 --> 00:04:46,730
Мы создали эмбеддинги, пропустили их через линейные и скрытые слои.

31
00:04:46,830 --> 00:04:57,270
В первый слой не входила оранжевая стрелка, поэтому мы сместили первый входной слой

32
00:04:57,270 --> 00:05:02,009
и добавили пустую матрицу, чтобы все входные слои обрабатывались одинаково.

33
00:05:02,009 --> 00:05:16,389
В классе CharLoopModel мы обернули обработку входных данных в цикл.

34
00:05:16,489 --> 00:05:30,750
Также мы увеличили количество символов с 3 до 8, с циклом это легко сделать.

35
00:05:30,750 --> 00:05:41,380
Класс CharLoopModel — то же самое, что и касс Char3Model, только символов теперь 8, а не 3.

36
00:05:41,480 --> 00:05:51,750
В классе CharRnn заменили петлю цикла на единственный вызов конструктора nn.RNN(),

37
00:05:51,750 --> 00:06:04,299
который также сохраняет активации скрытых слоёв в переменной h.

38
00:06:04,399 --> 00:06:12,560
Это тоже был просто рефакторинг, не меняющий принципа работы модели.

39
00:06:14,000 --> 00:06:21,210
После этого слегка сократили время обучения.

40
00:06:21,210 --> 00:06:49,760
Мы разрезали датасет отзывов к фильмам на отрезки по 8 символов и предсказывали девятый.

41
00:06:49,860 --> 00:07:13,420
Чтобы использовать все данные, раньше для предсказания использовались накладывающиеся отрезки.

42
00:07:13,420 --> 00:07:24,670
Это вычислительно неэффективно, потому что приходится очень много пересчитывать.

43
00:07:24,670 --> 00:07:34,065
Мы разделили данные на ненакладывающиеся отрезки

44
00:07:34,165 --> 00:07:53,440
и по каждым 8 символам стали предсказывать 8 следующих символов, а не 1.

45
00:07:53,440 --> 00:08:06,335
Посмотрев на первый символ, предсказываем второй; посмотрев на второй — третий и так далее.

46
00:08:06,435 --> 00:08:30,070
Кто-то из вас высказал правильное замечание, что в начале отрезка матрица активаций h обнуляется,

47
00:08:30,070 --> 00:08:48,820
поэтому использующее её предсказание по первому символу отрезка будет очень неточным.

48
00:08:48,920 --> 00:09:08,520
После этого мы решили не выбрасывать h на каждом минибатче, то есть при каждом вызове метода forward().

49
00:09:08,889 --> 00:09:23,520
Матрица h — состояние скрытого слоя, оранжевые круги, они обнулялись после каждого минибатча,

50
00:09:23,520 --> 00:09:34,800
как будто все предыдущие действия не имели значения, это неправильно.

51
00:09:34,800 --> 00:09:58,205
Мы передвинем строку обнуления матрицы h в конструктор и занесём её в поле класса, чтобы отслеживать изменения.

52
00:09:58,305 --> 00:10:12,029
Эта версия модели реализована в классе CharSeqStatefulRnn.

53
00:10:12,029 --> 00:10:29,904
В конструкторе вызывается метод init_hidden(), создающий матрицу h и заполняющий её нулями.

54
00:10:30,004 --> 00:10:40,542
Матрица h теперь лежит в поле класса self.h и передаётся в параметры метода self.rnn().

55
00:10:40,642 --> 00:10:49,675
Обновлённые активации h заносятся в матрицу self.h строкой self.h = repackage_var(h).

56
00:10:49,775 --> 00:10:53,910
Функция repackage_var(h) — полезный трюк №1.

57
00:10:53,910 --> 00:11:02,550
Допустим, мы не используем её и просто делаем присваивание self.h = h.

58
00:11:02,550 --> 00:11:16,569
Допустим, что размер датасета — миллион символов, тогда в нашей сети миллион оранжевых кругов.

59
00:11:16,669 --> 00:11:38,560
Модель выглядит так — для каждого входного слоя есть свой выходной слой.

60
00:11:38,660 --> 00:11:45,650
При вычислении градиентов используется метод обратного распространения ошибки,

61
00:11:45,750 --> 00:11:56,412
то есть мы идём в обратном направлении и смотрим на ошибку на каждом слое,

62
00:11:56,512 --> 00:12:05,550
а потом меняем веса в соответствии с этими ошибками.

63
00:12:05,550 --> 00:12:30,984
Если в датасете миллион символов, RNN будет состоять из миллиона полносвязных слоёв, вот эта модель.

64
00:12:31,084 --> 00:13:01,680
Такая модель занимает очень много памяти, потому что каждый минибатч мы запоминаем и перемножаем миллион градиентов.

65
00:13:05,399 --> 00:13:25,359
Чтобы этого избежать, мы сохраняем только значения матрицы состояния скрытого слоя, но не градиенты.

66
00:13:25,459 --> 00:13:34,194
Это делает функция repackage_var().

67
00:13:34,294 --> 00:13:47,169
Она возвращает значение матрицы активаций h.data без градиентов,

68
00:13:47,269 --> 00:14:01,149
поэтому метод обратного распространения ошибки останавливаеется на этой переменной.

69
00:14:01,249 --> 00:14:06,266
Функция вызывается при каждом вызове метода forward().

70
00:14:06,366 --> 00:14:13,740
На каждом отрезке 8 символов пройдут через 8 слоёв, на слоях отработает метод обратного распространения ошибки,

71
00:14:13,740 --> 00:14:23,824
а потом сохранятся значения матрицы h без истории проведённых операций.

72
00:14:23,924 --> 00:14:30,055
Это называется метод обратного распространения ошибки во времени (backpropagation through time, BPTT).

73
00:14:30,155 --> 00:14:38,399
В статьях в интернете это выглядит как новый сложнейший алгоритм или какое-то открытие,

74
00:14:38,399 --> 00:14:52,599
а на самом деле — это просто отбрасывание истории состояния скрытого слоя в конце минибатча.

75
00:14:52,699 --> 00:14:59,560
Это полезный трюк №1, функция repackage_var().

76
00:14:59,660 --> 00:15:18,990
Если помните, на четвёртой лекции у нас был параметр bptt — это количество слоёв, после которых история стирается.

77
00:15:18,990 --> 00:15:23,699
Нужно стараться уменьшить количество слоёв обратного распространения ошибки —

78
00:15:23,699 --> 00:15:37,029
модель будет легче обучаться и не так сильно страдать от нестабильности градиентов.

79
00:15:37,029 --> 00:15:43,479
С другой стороны, это количество должно быть достаточно большим,

80
00:15:43,479 --> 00:15:53,109
чтобы модель успевала выучить нужные закономерности.

81
00:15:53,109 --> 00:15:59,699
При создании RNN нужно подбирать значение параметра bptt.

82
00:16:00,389 --> 00:16:13,319
Полезный трюк №2 — как эффективно передавать данные в модель в соответствии с нашей схемой.

83
00:16:13,419 --> 00:16:24,936
Напомню, что мы рассматриваем последовательно каждый отрезок,

84
00:16:25,036 --> 00:16:33,069
но для ускорения вычислений хочется обсчитывать много отрезков одновременно.

85
00:16:35,970 --> 00:17:09,180
Мы будем параллельно обсчитывать группы отрезков.

86
00:17:09,280 --> 00:17:28,520
Различные отрезки будут независимо учитываться в матрице состояния скрытого слоя.

87
00:17:28,520 --> 00:17:46,275
После обучения на первых отрезках мы перейдём к следующим и продолжим обсчитывать их параллельно.

88
00:17:46,375 --> 00:17:56,450
Это должно напомнить вам начало работы с torchtext.

89
00:17:56,450 --> 00:18:15,200
Мы обсуждали процесс создания минибатчей. Полный текст (все сочинения Ницше или все обзоры IMDb)

90
00:18:15,200 --> 00:18:39,990
разбивается на 64 одинаковых части. Не на части длиной 64, а на 64 одинаковых части.

91
00:18:40,090 --> 00:18:55,240
Если полный текст имеет длину 64 миллиона символов, каждая из 64 частей содержит 1 миллион символов.

92
00:18:55,240 --> 00:19:13,410
Полученные части располагаются одна под другой, получается матрица высотой 64 символа и шириной 1 миллион.

93
00:19:13,510 --> 00:19:32,475
Минибатч — идущие подряд bptt столбцов этой матрицы, bptt был около 70.

94
00:19:32,575 --> 00:20:02,580
Для всех строк из минибатча параллельно предсказываются следующие символы, процесс повторяется.

95
00:20:02,680 --> 00:20:17,480
Такое преобразование данных нужно было для того, чтобы параллельно обсчитывать различные отрезки данных.

96
00:20:17,480 --> 00:20:23,820
Символ в начале отрезка в миллион символов может находиться в середине предложения,

97
00:20:23,920 --> 00:20:33,880
но это неважно, потому что такое происходит только один раз на миллион символов.

98
00:20:33,880 --> 00:20:40,740
Вопрос из зала: Вы можете ещё что-то сказать про дополнение данных таких датасетов?

99
00:20:40,840 --> 00:20:52,515
Нет, мне больше нечего сказать на эту тему. Я изучу этот вопрос ко второй части курса.

100
00:20:52,615 --> 00:21:04,820
Недавно начали появляться статьи на эту тему. Например, победитель недавнего соревнования Kaggle

101
00:21:06,830 --> 00:21:17,985
вставлял случайные части строк в свои данные, это может помочь.

102
00:21:18,085 --> 00:21:23,070
Я видел несколько статей, в которых описаны похожие методы,

103
00:21:23,170 --> 00:21:38,259
но именно отдельных статей по NLP на тему дополнения данных — не видел.

104
00:21:38,559 --> 00:21:43,604
Вопрос из зала: Как подобрать значение параметра bptt?

105
00:21:43,704 --> 00:21:48,049
Есть несколько факторов.

106
00:21:48,049 --> 00:22:00,014
Размер обведённой матрицы — (bptt) x (размер минибатча). Для каждого элемента минибатча

107
00:22:00,114 --> 00:22:23,389
есть вектор эмбеддинга, и результирующая трёхмерная матрица должна помещаться в GPU.

108
00:22:23,389 --> 00:22:29,659
Если вы получаете ошибку «CUDA error 'out of memory'», придётся уменьшить одно из этих чисел.

109
00:22:29,659 --> 00:22:37,874
Если модель обучается нестабильно, — например, ошибка резко достигает NaN, — попробуйте уменьшить bptt.

110
00:22:37,974 --> 00:22:43,785
При более коротком обратном распространении ошибки градиентный взрыв не успеет развиться.

111
00:22:43,885 --> 00:22:47,090
Если обучение слишком медленное — попробуйте уменьшить bptt,

112
00:22:47,090 --> 00:22:51,169


113
00:22:51,169 --> 00:22:55,919


114
00:22:56,019 --> 00:23:08,419
Недавно появились квази-рекуррентные нейронные сети (QRNN), они позволяют так делать, но мы пока не можем.

115
00:23:08,419 --> 00:23:13,339
Это главные факторы при выборе параметра bptt — время обучения, занимаемая память и стабильность обучения.

116
00:23:13,439 --> 00:23:21,820
Постарайтесь выбрать максимальное значение параметра bptt с учётом всех этих факторов.

117
00:23:23,100 --> 00:23:40,084
Нарезать данные на отрезки и складывать их вместе довольно муторно, поэтому здесь мы используем torchtext.

118
00:23:40,184 --> 00:23:52,749
При использовании API вроде fastai и torchtext вы можете столкнуться с тем, что

119
00:23:52,749 --> 00:23:58,299
методы API ожидают данные не в том формате, в котором они вам даны изначально.

120
00:23:58,299 --> 00:24:08,919
В таком случае можно преобразовать данные под необходимый формат или написать класс-обёртку.

121
00:24:08,919 --> 00:24:17,749
Я читал форум и заметил, что вы тратите очень много времени на написание классов-обёрток.

122
00:24:17,849 --> 00:24:24,079
Я гораздо ленивее и предпочитаю преобразовывать данные. Оба подхода хороши.

123
00:24:24,179 --> 00:24:38,165
Если вы заметите, что какой-то вид данных появляется часто и для него ещё нет обёртки в fastai,

124
00:24:38,265 --> 00:24:42,860
пожалуйста, напишите эту обёртку и создайте соответствующий pull request на github.com/fastai/fastai.

125
00:24:42,960 --> 00:24:58,009
Я решил преобразовать тексты Ницше в формат, который ожидает torchtext.

126
00:24:58,109 --> 00:25:15,319
В обёртке fastai над torchtext уже доступно разделение данных на обучающую и валидационную выборки.

127
00:25:15,419 --> 00:25:26,224
Я скопировал датасет из папки data/nietzsche/ в папки trn/ и val/.

128
00:25:26,324 --> 00:25:39,190
В папке trn/ я удалил последние 20% данных, а в папке val/ — всё, кроме последних 20%.

129
00:25:39,190 --> 00:25:56,254
Я решил, что это проще, чем писать класс-обёртку, да и валидационная выборка строится разумнее.

130
00:25:56,354 --> 00:26:08,660
На практике приходится обучать модель на одних книгах или авторах, а использовать для других.

131
00:26:08,760 --> 00:26:24,530
Я постарался сделать схожим образом, чтобы лучше проверить качество модели, и разбил корпус на два цельных куска.

132
00:26:24,630 --> 00:26:44,089
Убедитесь, что владеете bash в достаточной степени, чтобы выполнять такие операции.

133
00:26:44,189 --> 00:27:12,404
Обучающая и валидационная выборка обе состоят из одного файла, так строят языковые модели.

134
00:27:12,504 --> 00:27:23,209
Вот данные, вот код, который мы уже видели, давайте его разберём.

135
00:27:23,309 --> 00:27:36,129
Информация о предобработке данных содержится в объекте класса Field.

136
00:27:36,129 --> 00:27:51,300
Здесь я перевожу все символы в нижний регистр и использую функцию list() в качестве функции токенизации.

137
00:27:51,400 --> 00:27:59,924
При предсказывании слов мы использовали функцию, разбивающую текст по пробелам,

138
00:28:00,024 --> 00:28:16,840
а здесь — символы, поэтому используется функция преобразования строки в массив символов.

139
00:28:16,840 --> 00:28:27,620
Видно, как библиотеки вроде torchtext и fastai могут сильно упростить вам жизнь.

140
00:28:30,200 --> 00:28:37,470
Очень часто в качестве параметров методов эти библиотеки принимают функции,

141
00:28:38,809 --> 00:28:44,960
и функции можно использовать любые, в том числе и написанные вами.

142
00:28:44,960 --> 00:28:51,027
Параметр tokenize=list означает, что минибатчи состоят из символов.

143
00:28:51,127 --> 00:29:00,530
Вот остальные параметры. Я использую те же значения, что и в других частях этого Jupyter ноутбука.

144
00:29:00,530 --> 00:29:08,380
Размер минибатча bs=64, длина обратного распространения во времени bptt=8,

145
00:29:08,380 --> 00:29:14,960
длина вектора эмбеддинга n_fac=42 и размер скрытого слоя n_hidden=256.

146
00:29:14,960 --> 00:29:37,870
Размер скрытого слоя — размер матрицы весов, это оранжевые круги на диаграмме.

147
00:29:38,290 --> 00:29:43,865
Это словарь, показывающий, что использовать в качестве обучающей, валидационной и тестовой выборок.

148
00:29:43,965 --> 00:29:49,350
В качестве тестовой выборки я использую валидационную.

149
00:29:49,450 --> 00:29:58,250
После словаря я создаю объект данных модели md методом LanguageModelData.from_text_files().

150
00:29:58,250 --> 00:30:12,740
Все параметры метода нам знакомы. Параметр min_freq=3 здесь бесполезен,

151
00:30:12,740 --> 00:30:22,550
так как вряд ли найдётся символ, использующийся реже трёх раз.

152
00:30:22,550 --> 00:30:43,630
Получилось 963 минибатча, это число должно равняться (количество токенов) / bs / bptt.

153
00:30:45,130 --> 00:30:49,850
На практике это не совсем так.

154
00:30:52,400 --> 00:31:16,755
Нельзя перемешивать датасет, как с изображениями, но можно каждый раз немного менять значение bptt.

155
00:31:16,855 --> 00:31:44,830
PyTorch выдерживает значение bptt=8, но в 5% случаях немного меняет его, сохраняя среднее значение.

156
00:31:45,130 --> 00:31:53,950
Вопрос из зала: Внутри одного батча длина bptt фиксированная?

157
00:31:53,950 --> 00:32:29,640
Да, каждый минибатч происходит умножение матриц, поэтому размер должен быть фиксирован.

158
00:32:33,105 --> 00:32:39,850
Таким образом, количество минибатчей — это длина загрузчика данных.

159
00:32:39,850 --> 00:32:45,130
md.nt — количество уникальных токенов.

160
00:32:45,130 --> 00:33:01,710
После выполнения этой строки в переменной TEXT появляется поле TEXT.vocab,

161
00:33:01,710 --> 00:33:18,950
содержащее словари перевода уникальных символов в индексы и наоборот.

162
00:33:19,050 --> 00:33:34,179
Модель реализована в классе CharSeqStatefulRnn.

163
00:33:34,179 --> 00:33:50,654
Его мы уже обсуждали — последней модификацией был перенос матрицы h в поле self.h.

164
00:33:50,754 --> 00:34:03,880
Я сказал, что размер минибатча фиксирован, но это не совсем так.

165
00:34:03,980 --> 00:34:24,284
Последний минибатч может быть короче, если количество токенов не кратно (bptt * bs).

166
00:34:24,384 --> 00:34:48,438
Поэтому мы проверяем по совпадению размеров, нормальный ли минибатч,

167
00:34:48,538 --> 00:35:01,980
и обнуляем матрицу h, если он слишком маленький — то есть в конце эпохи.

168
00:35:02,080 --> 00:35:10,850
Это полезный трюк №3. В конце каждой эпохи выполняется минибатч меньшего размера,

169
00:35:10,850 --> 00:35:19,190
а к началу следующей эпохи размеры выравниваются.

170
00:35:19,190 --> 00:35:28,580
Поэтому вызов self.init_hidden() есть не только в конструкторе.

171
00:35:31,880 --> 00:35:36,490
Это не очень важная деталь, но полезная.

172
00:35:39,160 --> 00:35:59,650
Последний полезный трюк связан с одним неудобством PyTorch, надеемся, кто-то его исправит.

173
00:35:59,650 --> 00:36:08,230
Функции потерь в PyTorch, например, Softmax, не принимают на вход тензоры третьего ранга.

174
00:36:08,330 --> 00:36:17,240
Напомню, что тензор третьего ранга — просто трёхмерная матрица.

175
00:36:17,240 --> 00:36:22,829
Нет никакой причины, по которой обработка тензоров третьего ранга была бы сложна и неудобна —

176
00:36:22,929 --> 00:36:35,594
можно просто последовательно считать ошибку по первым двум осям.

177
00:36:35,694 --> 00:36:53,159
Почему-то никто это не реализовал, поэтому функции работают только с тензорами второго или четвёртого ранга.

178
00:36:59,519 --> 00:37:34,040
На каждой итерации есть две матрицы размера bs x bptt — предсказания и истинные значения.

179
00:37:34,040 --> 00:37:41,154
Для вычисления ошибки матрицы сверяются поэлементно.

180
00:37:41,254 --> 00:37:57,940
Для передачи в функцию потерь эти матрицы распрямляются до векторов методом .view().

181
00:37:57,940 --> 00:38:11,459
Итоговое количество колонок равно количеству возможных класов self.vocab_size.

182
00:38:11,559 --> 00:38:24,260
Количество строк — сколько получится, здесь будет bs * bptt.

183
00:38:24,260 --> 00:38:34,935
Предсказания нужно приводить к необходимому формату, а целевую переменную torchtext обработал за нас.

184
00:38:35,035 --> 00:38:56,320
torchtext автоматически распрямляет целевую переменную, мы видели это на четвёртой лекции.

185
00:38:56,420 --> 00:39:02,727
Итак, вот полезные трюки — избавляйтесь от истории операций,

186
00:39:02,827 --> 00:39:11,180
обнуляйте состояние скрытого слоя при изменении размера минибатча,

187
00:39:16,970 --> 00:39:23,180
распрямляйте данные и используйте torchtext для создания хороших минибатчей.

188
00:39:23,180 --> 00:39:39,510
После этого можно создать модель, алгоритм оптимизации и обучить модель.

189
00:39:39,610 --> 00:40:02,055
При вызове функции F.log_softmax() необходимо указывать ось dim, по которой считаются потери.

190
00:40:02,155 --> 00:40:11,010
Мы хотим вычислять Softmax по последней оси, поэтому передаём значение dim=-1.

191
00:40:11,110 --> 00:40:26,120
В знаменателе формулы Softmax — суммирование, параметр dim указывает, по какой оси его выполнять.

192
00:40:26,120 --> 00:40:41,625
Последняя ось содержит вероятности, которые в сумме должны давать 1, поэтому суммирование происходит по ней.

193
00:40:41,725 --> 00:41:02,810
Для запуска этого Jupyter ноутбука нужен PyTorch 0.3, вышедший на этой неделе.

194
00:41:02,810 --> 00:41:16,725
PyTorch 0.3 официально не поддерживает Windows, но я смог его установить командой conda install torch,

195
00:41:16,825 --> 00:41:24,140
запустил Jupyter ноутбук первой лекции и всё отработало.

196
00:41:24,140 --> 00:41:40,950
У меня Surface Book 2 15 с видеокартой GTX 1060 и 16 GB GPU, для глубокого обучения подходит отлично.

197
00:41:41,050 --> 00:41:50,595
Он работает примерно втрое медленнее моей машины с видеокартой GTX 1080,

198
00:41:50,695 --> 00:41:59,190
то есть примерно как SP2-инстанс на AWS.

199
00:41:59,290 --> 00:42:07,130
Surface Book также работает как планшет с сенсорным экраном, он тонкий и лёгкий,

200
00:42:07,130 --> 00:42:19,200
у меня никогда не было такого удобного ноутбука. Я установил на нём Linux, там тоже всё работает.

201
00:42:19,300 --> 00:42:24,385
Если вам нужен ноутбук для глубокого обучения, это хороший вариант.

202
00:42:26,560 --> 00:42:36,950
Так, про параметр dim=-1 я сказал, дальше создаём модель, алгоритм оптимизации, и обучаем.

203
00:42:36,950 --> 00:42:43,360
Результаты мало отличаются от того, что было раньше.

204
00:42:43,360 --> 00:42:54,070
Можно ещё немного изменить модель. Рассмотрим класс CharSeqStatefulRnn2.

205
00:42:54,170 --> 00:43:11,680
Этот класс очень похож на CharSeqStatefulRnn, но вместо nn.RNN() теперь используется nn.RNNCell().

206
00:43:14,890 --> 00:43:24,100
Вот код функции RNNCell() в библиотеке PyTorch, его не надо запускать.

207
00:43:24,100 --> 00:43:34,727
Вы уже умеете читать и понимать исходный код PyTorch, мы реализовали похожие классы.

208
00:43:34,827 --> 00:43:47,320
Здесь — умножение входных данных на матрицу весов и прибавление смещений, это делает F.linear().

209
00:43:47,320 --> 00:44:05,230
Здесь входные данные и состояние скрытого слоя не склеиваются, а складываются, особой разницы нет.

210
00:44:05,230 --> 00:44:14,822
Вопрос из зала: Почему в качестве функции активации используется гиперболический тангенс?

211
00:44:14,922 --> 00:44:31,830
Область значений гиперболического тангенса tanh — от -1 до 1, он выглядит так.

212
00:44:36,660 --> 00:44:49,530
Гиперболический тангенс и сигмоида связаны соотношением tanh(x) = 2 * sigmoid(2x) - 1.

213
00:44:49,530 --> 00:45:14,045
Диапазон значений от -1 до 1 позволяет избежать градиентного взрыва, в отличие от неограниченного выпрямителя.

214
00:45:14,145 --> 00:45:28,835
Тем не менее, можно использовать выпрямитель, указав его в параметре nonlinearity.

215
00:45:28,935 --> 00:45:35,750
Обычно используют гиперболический тангенс.

216
00:45:35,850 --> 00:45:45,015
Мы заменили nn.RNN() на nn.RNNCell(), поэтому нужно вернуть цикл.

217
00:45:45,115 --> 00:45:57,530
В каждой итерации цикла полученные результаты склеиваются с предыдущими, в конце получается матрица outp.

218
00:45:57,630 --> 00:46:11,375
Я получил такой же ответ, как и раньше, никакой магии.

219
00:46:11,475 --> 00:46:17,920
На практике модели не пишут так, но кто-нибудь может придумать

220
00:46:17,920 --> 00:46:25,690
хорошую модификацию класса RNNCell, или другой способ отслеживать градиенты, или новый вид регуляризации.

221
00:46:25,690 --> 00:46:43,350
Исходный код fastai выглядит точно так же, чтобы использовать не реализованные в PyTorch методы регуляризации.

222
00:46:43,790 --> 00:47:01,896
На практике RNNCell не спасает от градиентного взрыва и требует низкой скорости обучения

223
00:47:01,996 --> 00:47:13,935
и низкого значения параметра bptt, поэтому этот класс никто не использует.

224
00:47:14,035 --> 00:47:26,890
Вместо этого используется класс GRUCell, управляемый рекуррентный блок.

225
00:47:29,590 --> 00:47:38,425
Он выглядит так и описывается такими уравнениями.

226
00:47:38,525 --> 00:47:44,310
Мы пробежимся по ним, но обсуждать будем во второй части курса.

227
00:47:44,410 --> 00:48:04,640
До этого мы получали предсказания, сложив имеющиеся активации и произведение матрицы весов на входные данные.

228
00:48:06,980 --> 00:48:14,510
Здесь не так. Сначала входные данные умножаются на вспомогательную матрицу h_tilde.
===================

229
00:48:14,510 --> 00:48:18,170


230
00:48:18,170 --> 00:48:24,740


231
00:48:29,450 --> 00:48:33,620


232
00:48:33,620 --> 00:48:40,100


233
00:48:40,100 --> 00:48:46,460


234
00:48:46,460 --> 00:48:53,030


235
00:48:53,030 --> 00:48:56,240


236
00:48:57,650 --> 00:49:00,890


237
00:49:00,890 --> 00:49:05,089


238
00:49:05,089 --> 00:49:09,529


239
00:49:09,529 --> 00:49:14,019


240
00:49:16,839 --> 00:49:28,130


241
00:49:28,130 --> 00:49:30,920


242
00:49:30,920 --> 00:49:35,420


243
00:49:35,420 --> 00:49:39,499


244
00:49:43,099 --> 00:49:47,749


245
00:49:50,329 --> 00:49:56,029


246
00:49:56,029 --> 00:50:00,579


247
00:50:00,579 --> 00:50:05,900


248
00:50:05,900 --> 00:50:09,140


249
00:50:09,140 --> 00:50:12,859


250
00:50:12,859 --> 00:50:17,119


251
00:50:17,119 --> 00:50:22,730


252
00:50:22,730 --> 00:50:28,309


253
00:50:28,309 --> 00:50:34,369


254
00:50:34,369 --> 00:50:39,259


255
00:50:39,259 --> 00:50:45,799


256
00:50:45,799 --> 00:50:49,130


257
00:50:49,130 --> 00:50:53,809


258
00:50:53,809 --> 00:50:57,920


259
00:50:57,920 --> 00:51:02,900


260
00:51:02,900 --> 00:51:08,150


261
00:51:08,150 --> 00:51:11,330


262
00:51:11,330 --> 00:51:16,520


263
00:51:16,520 --> 00:51:20,540


264
00:51:20,540 --> 00:51:30,260


265
00:51:30,260 --> 00:51:35,660


266
00:51:39,650 --> 00:51:46,490


267
00:51:49,880 --> 00:51:55,490


268
00:51:57,230 --> 00:52:03,880


269
00:52:03,880 --> 00:52:07,820


270
00:52:07,820 --> 00:52:11,510


271
00:52:11,510 --> 00:52:15,440


272
00:52:15,440 --> 00:52:21,590


273
00:52:21,590 --> 00:52:27,520


274
00:52:27,520 --> 00:52:32,330


275
00:52:32,330 --> 00:52:39,080


276
00:52:39,080 --> 00:52:47,120


277
00:52:47,120 --> 00:52:53,410


278
00:52:53,410 --> 00:53:02,060


279
00:53:02,060 --> 00:53:05,840


280
00:53:05,840 --> 00:53:09,980


281
00:53:09,980 --> 00:53:18,890


282
00:53:18,890 --> 00:53:24,650


283
00:53:24,650 --> 00:53:29,349


284
00:53:29,349 --> 00:53:36,859


285
00:53:44,930 --> 00:53:50,390


286
00:53:50,390 --> 00:53:55,190


287
00:53:55,190 --> 00:53:59,480


288
00:53:59,480 --> 00:54:02,599


289
00:54:02,599 --> 00:54:06,039


290
00:54:08,319 --> 00:54:15,650


291
00:54:15,650 --> 00:54:18,380


292
00:54:18,380 --> 00:54:24,890


293
00:54:27,109 --> 00:54:31,789


294
00:54:31,789 --> 00:54:37,339


295
00:54:40,130 --> 00:54:44,150


296
00:54:44,150 --> 00:54:50,869


297
00:54:50,869 --> 00:54:53,750


298
00:54:53,750 --> 00:54:58,220


299
00:54:58,220 --> 00:55:05,660


300
00:55:05,660 --> 00:55:09,829


301
00:55:09,829 --> 00:55:13,250


302
00:55:13,250 --> 00:55:19,490


303
00:55:19,490 --> 00:55:29,359


304
00:55:29,359 --> 00:55:35,839


305
00:55:35,839 --> 00:55:44,050


306
00:55:44,050 --> 00:55:49,220


307
00:55:52,130 --> 00:55:56,750


308
00:55:56,750 --> 00:56:01,640


309
00:56:01,640 --> 00:56:06,680


310
00:56:06,680 --> 00:56:16,609


311
00:56:16,609 --> 00:56:25,040


312
00:56:25,040 --> 00:56:31,099


313
00:56:31,099 --> 00:56:36,109


314
00:56:39,109 --> 00:56:43,490


315
00:56:43,490 --> 00:56:49,550


316
00:56:49,550 --> 00:56:54,859


317
00:56:56,930 --> 00:57:02,500


318
00:57:02,500 --> 00:57:08,720


319
00:57:08,720 --> 00:57:18,710


320
00:57:18,710 --> 00:57:21,920


321
00:57:21,920 --> 00:57:25,810


322
00:57:25,810 --> 00:57:34,040


323
00:57:34,040 --> 00:57:40,190


324
00:57:40,190 --> 00:57:45,380


325
00:57:45,380 --> 00:57:50,060


326
00:57:50,060 --> 00:57:52,369


327
00:57:52,369 --> 00:57:59,690


328
00:57:59,690 --> 00:58:02,900


329
00:58:02,900 --> 00:58:06,079


330
00:58:06,079 --> 00:58:10,460


331
00:58:10,460 --> 00:58:18,079


332
00:58:18,079 --> 00:58:22,039


333
00:58:22,039 --> 00:58:25,759


334
00:58:25,759 --> 00:58:31,880


335
00:58:31,880 --> 00:58:37,999


336
00:58:39,829 --> 00:58:44,329


337
00:58:44,329 --> 00:58:48,499


338
00:58:51,499 --> 00:58:57,410


339
00:58:57,410 --> 00:59:02,599


340
00:59:02,599 --> 00:59:06,410


341
00:59:07,819 --> 00:59:10,970


342
00:59:10,970 --> 00:59:17,420


343
00:59:17,420 --> 00:59:23,239


344
00:59:23,239 --> 00:59:28,970


345
00:59:31,579 --> 00:59:36,650


346
00:59:36,650 --> 00:59:43,400


347
00:59:43,400 --> 00:59:49,910


348
00:59:54,109 --> 01:00:01,819


349
01:00:01,819 --> 01:00:08,950


350
01:00:08,950 --> 01:00:14,330


351
01:00:14,330 --> 01:00:20,420


352
01:00:20,420 --> 01:00:24,050


353
01:00:24,050 --> 01:00:29,630


354
01:00:29,630 --> 01:00:36,410


355
01:00:38,570 --> 01:00:44,120


356
01:00:45,470 --> 01:00:53,210


357
01:00:53,210 --> 01:00:59,210


358
01:00:59,210 --> 01:01:03,740


359
01:01:06,830 --> 01:01:12,470


360
01:01:12,470 --> 01:01:15,860


361
01:01:15,860 --> 01:01:21,460


362
01:01:21,460 --> 01:01:27,410


363
01:01:27,410 --> 01:01:31,100


364
01:01:31,100 --> 01:01:34,730


365
01:01:34,730 --> 01:01:38,480


366
01:01:38,480 --> 01:01:42,320


367
01:01:42,320 --> 01:01:45,870


368
01:01:45,970 --> 01:01:48,502


369
01:01:48,602 --> 01:01:53,995
Давайте сделаем перерыв на 5 минут и вернёмся к компьютерному зрению в 7:45.

370
01:02:00,975 --> 01:02:05,040
Итак, мы наконец вернулись к компьютерному зрению.

371
01:02:09,140 --> 01:02:14,070
Соответствующий Jupyter ноутбук называется lesson7-citar.ipynb.

372
01:02:18,320 --> 01:02:34,220
CIFAR-10 - известный датасет изображений, появившийся задолго до ImageNet.

373
01:02:34,320 --> 01:02:47,060
Я считаю, что небольшие датасеты гораздо интереснее ImageNet.

374
01:02:47,060 --> 01:02:56,417
Большинству из вас придётся работать с тысячами изображений, а не с полутора миллионами ImageNet,

375
01:02:56,517 --> 01:03:01,170
поэтому полезно и интересно попрактиковаться.

376
01:03:01,270 --> 01:03:09,320
В большинстве прикладных задач, например, анализе медицинских снимков,

377
01:03:09,320 --> 01:03:17,175
размер изображения редко превышает 32 пикселя.

378
01:03:17,275 --> 01:03:23,997
CIFAR-10 состоит из маленьких изображений, и их немного,

379
01:03:24,097 --> 01:03:31,167
поэтому обучаться на них гораздо сложнее и интереснее, чем на изображениях ImageNet.

380
01:03:31,267 --> 01:03:41,240
Размер CIFAR-10 также позволяет обучаться гораздо быстрее, поэтому он хорош для тестирования алгоритмов.

381
01:03:43,310 --> 01:03:52,670
Многие исследователи жалуются, что у них нет средств на исследование различных версий алгоритма.

382
01:03:52,670 --> 01:04:02,930
Они используют ImageNet, поэтому каждое исследование - это неделя высокомощных вычислений.

383
01:04:02,930 --> 01:04:10,200
Я не понимаю, зачем тестировать алгоритмы на огромных датасетах вроде ImageNet.

384
01:04:10,200 --> 01:04:21,835
На этой неделе эта тема бурно обсуждается в связи с выступлением Ali Rahimi на NIPS 2017.

385
01:04:21,935 --> 01:04:30,060
Он говорил о том, что сейчас в глубоком обучении недостаточно смелых экспериментов.

386
01:04:30,060 --> 01:04:44,730
Мы обсуждали с ним эту тему и ещё не пришли к взаимопонимаю, но у нас общая забота -

387
01:04:44,730 --> 01:04:54,630
люди предпочитают брать побольше GPU и данных и скидывать в кучу, а не обдумывать и ставить эксперименты.

388
01:04:54,630 --> 01:05:09,600
Если вы разрабатываете алгоритм для работы на больших датасетах, используйте маленькие для экспериментов -

389
01:05:09,600 --> 01:05:18,560
попробуйте различные версии алгоритма, уберите некоторые этапы, чтобы посмотреть, на что они влияют, и так далее.

390
01:05:18,560 --> 01:05:27,285
Многие жалуются на датасет MNIST, но его тоже хорошо использовать для того, чтобы понять,

391
01:05:27,385 --> 01:05:34,255
какие составляющие алгоритма имеют значение, а какие нет.

392
01:05:34,355 --> 01:05:39,415
Я думаю, что люди, которые жалуются на MNIST, просто рисуются -

393
01:05:39,615 --> 01:05:52,690
"Посмотрите на меня, я работаю в Google за $100,000 в неделю, у меня есть бесплатные GPU, я крутой".

394
01:05:52,790 --> 01:06:01,420
Кто-то в интернете преобразовал датасет в изображения, их можно скачать по этой ссылке, спасибо ему.

395
01:06:01,520 --> 01:06:13,140
Google перенаправит вас на менее удобный формат, используйте этот, он уже подготовлен для работы.

396
01:06:13,140 --> 01:06:17,725
Вот список классов датасета.

397
01:06:17,825 --> 01:06:46,200
В предобученных моделях fastai функция tfmx_from_model() нормализует данные, вычисляя стандартные отклонения и средние.

398
01:06:48,300 --> 01:07:01,600
Сейчас мы создаём модель с нуля, поэтому стандартные отклонения и средние приходится считать руками.

399
01:07:01,700 --> 01:07:10,015
Я не привожу код, который использовал для их вычисления, думаю, вы и сами справитесь.

400
01:07:10,115 --> 01:07:16,390
Стандартные отклонения и средние нужно считать отдельно для каждого канала.

401
01:07:16,490 --> 01:07:41,660
Нужно выбрать алгоритм дополнения данных. Для CIFAR-10 обычно используют RandomFlipXY(),

402
01:07:41,660 --> 01:07:53,190
добавляют чёрные границы и выбирают случайный квадрат 32x32.

403
01:07:53,190 --> 01:08:01,930
При указании параметра pad в функциях fastai вроде tfms_from_stats() это выполняется автоматически.

404
01:08:02,030 --> 01:08:09,025
Я добавляю границу толщиной 4 пикселя.

405
01:08:09,125 --> 01:08:18,660
После выбора алгоритма дополнения данных я возвращаю объект данных модели методом ImageClassifierData.from_paths().

406
01:08:18,660 --> 01:08:26,590
Размер минибатча bs=256, потому что изображения маленькие и их можно обрабатывать большими пачками.

407
01:08:26,690 --> 01:08:51,009
Вот примеры изображений из датасета - парусник и лягушка.

408
01:08:51,009 --> 01:09:19,060
Kerem Turgutlu написал Jupyter ноутбук, в котором реализовал многие алгоритмы оптимизации с нуля и сравнил их.

409
01:09:19,060 --> 01:09:29,650
Это - версия моего файла Excel про оптимизаторы, написанная на Python, очень круто.

410
01:09:29,649 --> 01:09:43,324
Он написал вспомогательный класс нейронной сети SimpleNet(), мы используем его.

411
01:09:43,424 --> 01:09:49,774
Нейронная сеть состоит из списка полносвязных слоёв.

412
01:09:49,874 --> 01:10:00,589
Массив слоёв нейронной сети в PyTorch нужно оборачивать в nn.ModuleList().

413
01:10:00,689 --> 01:10:18,485
В методе forward() мы выпрямляем данные, пропускаем их через цикл связок линейный слой + выпрямитель и применяем Softmax.

414
01:10:18,585 --> 01:10:35,260
Мы поднимемся на один уровень API fastai выше и вместо вызова функции fit() создадим модель из класса SimpleNet().

415
01:10:35,260 --> 01:10:53,530
Для этого используем метод ConvLearner.from_model_data() и передаём в него модель PyTorch и объект данных модели fastai.

416
01:10:53,530 --> 01:11:01,630
Это проще создания RNN - не нужно указывать алгоритмы оптимизации, имитации отжига и callback-функции.

417
01:11:01,630 --> 01:11:14,440
Созданный таким образом объект модели learn устроен привычным нам образом.

418
01:11:16,090 --> 01:11:22,040
Размер изображений - 32х32, в них три канала, получается 3072 признака.

419
01:11:22,140 --> 01:11:33,115
На выходе из первого слоя у нас 40 признаков, на выходе из второго, последнего - 10, по числу классов CIFAR-10.

420
01:11:35,040 --> 01:11:47,475
Здесь вызываются привычные методы learn.summary(), learn.lr_find(), learn.sched.plot(), learn.fit().

421
01:11:47,575 --> 01:12:10,845
Обычная нейронная сеть с одним скрытым слоем и 122,800 параметрами достигла доли правильных ответом в 47%.

422
01:12:10,945 --> 01:12:16,802
Давайте попробуем улучшить это значение.

423
01:12:16,902 --> 01:12:30,170
В процессе улучшения мы воспроизведём архитектуру ResNet.

424
01:12:30,270 --> 01:12:38,615
Начнём с того, что заменим полносвязную нейронную сеть на свёрточную.

425
01:12:38,715 --> 01:13:10,490
Полносвязный слой - это произведение матрицы активаций на матрицу весов.

426
01:13:10,490 --> 01:13:19,730
Матрица весов содержит вес для каждой активации - поэтому она такая большая.

427
01:13:26,420 --> 01:13:44,600
Модель плохо работает, хотя весов очень много: 3,072 * 40 = 122,880.

428
01:13:44,600 --> 01:13:52,040
Веса используются неэффективно, потому что значение придаётся каждому пикселю.

429
01:13:52,040 --> 01:13:58,260
Разумнее рассматривать группы пикселей 3x3, в которых можно отследить узоры.

430
01:13:58,360 --> 01:14:39,795
Это называется свёртка. Каждая область 3x3 пикселя поэлементно умножается на матрицу свёртки.

431
01:14:39,895 --> 01:14:53,030
Матриц свёртки, или фильтров, может быть несколько, результат работы свёртки - тензор третьего ранга.

432
01:14:55,530 --> 01:15:09,020
Изменения реализованы в классе ConvNet. Мы заменили nn.Linear() на nn.Conv2d().

433
01:15:09,020 --> 01:15:13,120
Мы хотим, чтобы каждый следующий слой был меньше размером.

434
01:15:17,020 --> 01:15:31,010
В таблице Excel после работы свёртки из каждых четырёх чисел выбиралось максимальное.

435
01:15:31,110 --> 01:15:38,720
Сейчас вместо подвыбрки максимумом используется шаг stride=2.

436
01:15:38,820 --> 01:16:08,110
На этапе свёртки участки 3x3 рассматриваются через один - фильтр смещается не на одну строку/столбец, а на две.

437
01:16:08,110 --> 01:16:15,885
Шаг stride=2 и подвыборка максимумом дают один эффект - уменьшение слоя вдвое по каждому измерению.

438
01:16:15,985 --> 01:16:26,230
Шаг указывается параметром stride=2, размер свёрточного фильтра kernel_size=3,

439
01:16:26,230 --> 01:16:34,170
остальные параметры я скопировал из nn.Linear(), это количество входных и выходных признаков.

440
01:16:34,170 --> 01:16:38,175
Мы создаём массив свёрточных слоёв.

441
01:16:38,275 --> 01:17:06,400
На вход подаются изображения 32x32, из первого слоя выходят 15x15, потом 7x7 и 3x3.

442
01:17:06,400 --> 01:17:17,260
Для преобразования матрицы активаций 3x3 в вероятности 10 классов используется адаптивная подвыборка максимумом.

443
01:17:17,260 --> 01:17:35,440
В обычной подвыборке максимумом мы задаём размер области, из которой выбирается максимум - здесь 2x2.

444
01:17:35,440 --> 01:17:57,990
В адаптивной подвыборке максимумом задаётся размер матрицы, которую нужно получить - здесь 14x14 из матрицы 28x28.

445
01:17:58,090 --> 01:18:15,935
В этом примере обычная и адаптивная подвыборки максимумом дают один и тот же результат.

446
01:18:16,035 --> 01:18:28,750
В современных CNN предпоследним слоем обычно ставят адаптивную подвыборку максимумом размера 1,

447
01:18:28,750 --> 01:18:39,100
то есть выбирают максимальную ячейку и используют её в качестве активации.

448
01:18:39,100 --> 01:18:51,155
После этого получается тензор размера (количество признаков) x 1 x 1.

449
01:18:51,255 --> 01:19:03,308
Строка x = x.view(x.size(0), -1) убирает последние две оси этого тензора, преобразуя его в вектор.

450
01:19:03,408 --> 01:19:11,480
Метод forward() обрабатывает минибатчи, поэтому на практике это матрица (размер минибатча) x (количество признаков)

451
01:19:11,580 --> 01:19:23,860
Полученный вектор передаётся в линейный слой для получения необходимого количества вероятностей классов, с=10.

452
01:19:23,860 --> 01:19:32,615
Итак, данные проходят через цикл связок свёртка + выпрямитель, через слой адаптивной подвыборки максимумом,

453
01:19:32,715 --> 01:19:51,700
мы удаляем ненужные оси и передаём их в последний, линейный, слой. Он выдаёт нам вектор длиной 10.

454
01:19:51,700 --> 01:19:59,980
Размер изображения меняется в свёрточных слоях с 3x32x32 до 20x15x15, 40x7x7 и 80x3x3,

455
01:19:59,980 --> 01:20:09,800
адаптивная подвыборка максимумом преобразует тензор 80x3x3 в 80x1x1, мы обрезаем его до 80,

456
01:20:09,900 --> 01:20:15,770
и линейный слой создаёт из 80 активаций 10 вероятностей.

457
01:20:15,870 --> 01:20:30,360
Это базовая архитектура полной свёрточной нейронной сети - все слои свёрточные, кроме последнего.

458
01:20:31,350 --> 01:20:50,650
Алгоритм поиска оптимальной скорости обучения прошёл весь датасет, а ошибка всё продолжала уменьшаться,

459
01:20:50,650 --> 01:21:02,610
поэтому я увеличил максимально возможное значение при поиске end_lr=100 вместо 10 по умолчанию.

460
01:21:02,710 --> 01:21:17,465
Я выбрал значение 1e-1, несколько эпох обучения дали долю правильных ответов в 60%.

461
01:21:17,565 --> 01:21:34,290
Здесь 28,880 параметров, это в четыре раза меньше, чем раньше, а доля правильных ответов поднялась с 47% до 60%.

462
01:21:34,290 --> 01:21:53,140
Время обучения одной эпохи не изменилось - в небольших архитектурах время уходит только на передачу данных.

463
01:21:57,435 --> 01:22:07,780
Я проведу рефакторинг кода, новый класс называется ConvNet2.

464
01:22:07,780 --> 01:22:15,875
Я создам класс ConvLayer, реализующий свёрточный слой, чтобы упростить метод forward().

465
01:22:15,975 --> 01:22:20,210
Размер свёрточного фильтра kernel_size=3, длина шага stride=2.

466
01:22:20,310 --> 01:23:10,880
При свёртке первый слой уменьшается с 32x32 до 15x15, а не 16x16, поэтому я добавлю границу padding=1.

467
01:23:10,980 --> 01:23:25,640
Теперь размер будет уменьшаться точно вдвое и свёртка будет использовать края изображения.

468
01:23:25,740 --> 01:23:41,140
Класс ConvLayer содержит свёрточный слой и выпрямитель. Теперь класс ConvNet2 выглядит аккуратнее,

469
01:23:41,140 --> 01:23:48,080
а класс ConvLayer мы сможем переиспользовать для других архитектур.

470
01:23:48,180 --> 01:23:57,965
Теперь вы можете создавать не только собственные нейронные сети, но и отдельные слои в них.

471
01:23:58,065 --> 01:24:09,700
В PyTorch нейронные сети и отдельные слои реализуются одинаково - конструктор и метод forward().

472
01:24:09,700 --> 01:24:15,755
Поэтому любую нейронную сеть можно использовать в качестве слоя и наоборот, это очень удобно.

473
01:24:15,855 --> 01:24:23,980
Помимо добавления границ я поменял реализацию адаптивной выборки подмаксимумом.

474
01:24:23,980 --> 01:24:42,620
Объект класса nn.AdaptiveMaxPool2d хранится в поле модели, но у него нет весов, поэтому это не обязательно.

475
01:24:42,720 --> 01:24:55,655
Вместо этого я использую функцию F.adaptive_max_pool2d(), модуль F - это torch.nn.functional.

476
01:24:55,755 --> 01:25:10,490
Я обучал эту версию модели недостаточно долго, но результаты должны быть чуть лучше.

477
01:25:10,590 --> 01:25:41,200
Большие свёрточные сети неустойчивы при больших скоростях обучения и обучаются плохо и долго при маленьких.

478
01:25:41,200 --> 01:25:48,255
Чтобы сделать модель более гибкой, я добавлю нормализацию минибатчей.

479
01:25:48,355 --> 01:25:59,705
Нормализация минибатчей появилась пару лет назад и сильно упростила обучение глубоких нейронных сетей.

480
01:25:59,805 --> 01:26:09,230
Следующая модель состоит из пяти свёрточных слоёв и одного полносвязного.

481
01:26:09,330 --> 01:26:17,945
Раньше такие нейронные сети считались довольно глубокими и труднообучаемыми,

482
01:26:18,045 --> 01:26:23,135
нормализация минибатчей сильно упростила их обучение.

483
01:26:23,235 --> 01:26:29,120
Её очень просто использовать, но в образовательных целях мы напишем её с нуля.

484
01:26:29,220 --> 01:26:43,785
Допустим, у нас есть вектор активаций. Представьте, что это минибатч из одного элемента.

485
01:26:43,885 --> 01:26:58,630
Вектор активаций проходит через свёрточный слой. Для простоты заменим его на умножение на матрицу.

486
01:26:58,630 --> 01:27:19,955
Допустим, матрица единичная. Тогда при умножении вектора на эту матрицу много раз ничего не меняется.

487
01:27:20,055 --> 01:27:47,595
Если заменить единицы на двойки, активации будут удваиваться, в глубоких моделях это приведёт к градиентному взрыву.

488
01:27:47,695 --> 01:28:16,960
Чтобы этого избежать, нужно следить за средними значениями матриц весов.

489
01:28:16,960 --> 01:28:23,670
Входные данные перед передачей в модель нормализуются до среднего значения 0 и стандартного отклонения 1,

490
01:28:23,670 --> 01:28:34,089
а хочется нормализовать все слои, не только входной.

491
01:28:34,189 --> 01:28:48,750
Для этого я создал класс BnLayer. Он идентичен классу ConvLayer, но я добавил вычисление

492
01:28:48,750 --> 01:29:02,100
стандартного отклонения и среднего для каждого канала каждого фильтра и нормализацию.

493
01:29:02,100 --> 01:29:19,104
После этого отпала необходимость нормализовать входные данные.

494
01:29:19,204 --> 01:30:03,030
Это не сработает, потому что градиентный спуск будет перечёркивать результаты нормализации каждый минибатч.

495
01:30:03,030 --> 01:30:17,754
Поэтому для каждого канала заводятся параметры self.m (multiplier) и self.a (added value).

496
01:30:17,854 --> 01:30:36,480
Начальные значения параметров self.a - нули, self.m - единицы. Количество фильтров первого слоя nf=3.

497
01:30:36,480 --> 01:30:42,420
Параметры self.m и self.a делают обратные только что введённой нормализации действия.

498
01:30:42,420 --> 01:30:49,685
Оборачивание в класс nn.Parameter() позволяет модели подбирать self.m и self.a вместе с другими весами.

499
01:30:49,785 --> 01:30:59,080
После первого слоя нормализованные данные умножаются на 1 и к результату прибавляется 0.

500
01:30:59,080 --> 01:31:20,625
Теперь, если градиентный спуск захочет увеличить диапазон активаций, он будет менять только параметр self.m.

501
01:31:20,725 --> 01:31:32,540
Если нужно будет сместить веса, пересчитается только параметр self.a, а не вся матрица.

502
01:31:32,640 --> 01:31:48,960
Ali Rahimi на NIPS 2017 упомянул статью о нормализации батчей как исключительно полезный инструмент,

503
01:31:48,960 --> 01:31:56,650
хотя многие не поняли, в чём смысл этого алгоритма.

504
01:31:56,650 --> 01:32:09,570
Если вам кажется, что нормализовать данные, а потом сразу же выполнять обратное действие - это странно,

505
01:32:09,570 --> 01:32:15,160
не бойтесь, вы не одни такие.

506
01:32:15,160 --> 01:32:28,000
Я представляю это так: сначала мы нормализуем данные, а потом можем сместить их в нужный диапазон значений,

507
01:32:28,000 --> 01:32:39,070
используя гораздо меньше параметров, чем задействуется при пересчитывании всех фильтров.

508
01:32:42,040 --> 01:32:57,354
На практике это позволяет увеличить скорость обучения, гибкость модели и количество слоёв.

509
01:32:57,454 --> 01:33:15,420
После замены ConvLayer на BnLayer я смог добавить ещё один слой модели без ущерба обучению.

510
01:33:15,520 --> 01:33:26,155
Вопрос из зала: Нужно ли опасаться деления на очень маленькое число?

511
01:33:26,255 --> 01:33:45,440
Да, я думаю, что в исходном коде PyTorch в знаменателе не self.stds, а self.stds + eps, стоило бы это учесть.

512
01:33:52,790 --> 01:33:58,925
Вопрос из зала: Параметры self.m и self.a обновляются в методе обратного распространения ошибки?

513
01:33:59,025 --> 01:34:10,130
Да, именно так, мы указали это, обернув эти параметры конструктором nn.Parameter().

514
01:34:10,130 --> 01:34:16,219
Нормализация минибатчей также работает как регуляризация,

515
01:34:16,219 --> 01:34:24,445
то есть при её наличии можно уменьшить или совсем убрать дропаут и ограничение весов.

516
01:34:24,545 --> 01:34:34,815
У каждого минибатча своё среднее и своё стандартное отклонение,

517
01:34:34,915 --> 01:34:41,900
поэтому на каждом минибатче значения фильтров слегка отличаются, это добавляет шум.

518
01:34:44,000 --> 01:34:52,920
Шум работает как регуляризация модели.

519
01:34:53,020 --> 01:35:01,570
В классической версии нормализации минибатчей используются не среднее и стандартное отклонение,

520
01:35:01,570 --> 01:35:08,965
а экспоненциально взвешенное скользящее среднее и стандартное отклонение.

521
01:35:09,065 --> 01:35:15,100
Если будет нечем заняться на этой неделе, можете это реализовать.

522
01:35:15,100 --> 01:35:33,340
Выражение if self.traning верно на обучающей выборке и ложно на валидационной,

523
01:35:33,340 --> 01:35:42,010
потому что на валидационной выборке нельзя менять модель.

524
01:35:42,010 --> 01:36:00,550
Работа некоторых видов слоёв зависит от того, на какую выборку сейчас смотрит модель.

525
01:36:00,550 --> 01:36:08,735
Пару недель назад мы заметили баг в Jupyter ноутбуке модели MovieLens -

526
01:36:08,835 --> 01:36:26,945
дропаут не был защищён условием if self.training, поэтому применялся и на валидационной выборке.

527
01:36:27,045 --> 01:36:40,540
Я заменил дропаут в модели на класс nn.Dropout(), который учитывает это автоматически,

528
01:36:40,540 --> 01:36:50,300
а мог бы добавить условие if self.training и получить тот же результат.

529
01:36:50,400 --> 01:37:05,770
Это нужно учитывать только при нормализации минибатчей и дропауте.

530
01:37:05,770 --> 01:37:20,380
В отличных от fastai библиотеках средние и стандартные отклонения обновляются при обучении

531
01:37:20,380 --> 01:37:26,465
во всех слоях, вне зависимости от того, заморожены они или нет.

532
01:37:26,565 --> 01:37:42,380
Такой подход портит предобученные модели с уже посчитанными значениями средних и стандартных отклонений.

533
01:37:42,480 --> 01:38:05,170
В fastai в процессе обучения в замороженных слоях ничего не меняется.

534
01:38:05,170 --> 01:38:17,959
Опыт показывает, что это полезный подход, особенно, если ваш датасет похож на данные,

535
01:38:18,059 --> 01:38:27,515
на которых обучалась предобученная модель.

536
01:38:27,615 --> 01:38:50,480
Вопрос из зала: Все эти махинации не растягивают длину эпохи до бесконечности?

537
01:38:52,719 --> 01:39:16,804
Нет, эти вычисления очень быстрые в сравнении с матричным умножением, которое делает self.conv().

538
01:39:16,904 --> 01:39:24,000
Вопрос из зала: Где в архитектуре сети стоит слой нормализации батчей?

539
01:39:25,320 --> 01:39:34,840
Сейчас к этому перейдём. У нас, как и в оригинальной статье про нормализацию минибатчей,

540
01:39:34,840 --> 01:39:38,849
нормализация минибатчей выполняется после выпрямителя.

541
01:39:41,749 --> 01:39:48,800
Изучение абляции (ablation study) - это анализ работы вашей модели, при котором

542
01:39:48,800 --> 01:39:57,734
вы добавляете и убираете различные части модели, чтобы понять, какие из них нужны.

543
01:39:57,834 --> 01:40:04,239
В оригинальной статье о нормализации минибатчей не было изучения абляции,

544
01:40:06,979 --> 01:40:14,564
поэтому не было информации о том, где лучше ставить этот слой.

545
01:40:14,664 --> 01:40:23,129
Авторы статьи выбрали неудачное место для слоя нормализации минибатчей, и это принесло людям много проблем.

546
01:40:23,229 --> 01:40:35,539
Лучшее место уже найдено, но мне до сих пор говорят - "У вас нормализация минибатчей не там, где надо",

547
01:40:35,539 --> 01:40:46,689
и я каждый раз объясняю им, что я читал статью, но так действительно лучше. Каждый раз неловко.

548
01:40:46,989 --> 01:41:17,530
Вопрос из зала: При создании модели маленькие датасеты вроде CIFAR-10 используются для предобучения?

549
01:41:17,530 --> 01:41:45,559
Думаю, да, если маленький датасет похож на основной и вы хотите посмотреть, масштабируются ли выученные знания.

550
01:41:45,559 --> 01:41:53,239
Я гораздо больше заинтересован в изучении маленьких датасетов как основных, а не вспомогательных,

551
01:41:55,570 --> 01:42:08,034
потому что в реальных датасетах не миллион изображений, а от 2 до 20 тысяч.

552
01:42:08,134 --> 01:42:16,905
Маленькие датасеты ценнее на практике.

553
01:42:17,005 --> 01:42:24,230
Маленький размер изображений нравится мне тем, что, во-первых, такие изображения быстро обрабатываются,

554
01:42:24,770 --> 01:42:34,515
а во вторых, потому что зачастую вся необходимая информация заключается в маленькой части изображения.

555
01:42:38,150 --> 01:42:50,239
Вопрос из зала:

556
01:42:50,239 --> 01:42:57,350


557
01:42:57,350 --> 01:43:01,220


558
01:43:01,220 --> 01:43:04,040


559
01:43:04,040 --> 01:43:08,390


560
01:43:08,390 --> 01:43:13,370


561
01:43:13,370 --> 01:43:17,800


562
01:43:17,800 --> 01:43:24,739


563
01:43:26,360 --> 01:43:31,070


564
01:43:31,070 --> 01:43:35,630


565
01:43:35,630 --> 01:43:38,420


566
01:43:38,420 --> 01:43:43,220


567
01:43:43,220 --> 01:43:45,890


568
01:43:45,890 --> 01:43:49,520


569
01:43:49,520 --> 01:43:52,670


570
01:43:52,670 --> 01:43:59,060


571
01:43:59,060 --> 01:44:02,540


572
01:44:02,540 --> 01:44:06,980


573
01:44:06,980 --> 01:44:09,740


574
01:44:09,740 --> 01:44:16,970


575
01:44:16,970 --> 01:44:21,560


576
01:44:21,560 --> 01:44:25,370


577
01:44:27,080 --> 01:44:31,970


578
01:44:31,970 --> 01:44:35,720


579
01:44:35,720 --> 01:44:41,540


580
01:44:41,540 --> 01:44:46,250


581
01:44:46,250 --> 01:44:50,780


582
01:44:50,780 --> 01:44:57,230


583
01:44:57,230 --> 01:45:01,520


584
01:45:01,520 --> 01:45:08,510


585
01:45:08,510 --> 01:45:11,300


586
01:45:11,300 --> 01:45:16,820


587
01:45:18,620 --> 01:45:25,580


588
01:45:25,580 --> 01:45:30,290


589
01:45:30,290 --> 01:45:35,300


590
01:45:35,300 --> 01:45:39,010


591
01:45:39,110 --> 01:46:06,740
Янет: Дропаут - метод регуляризации, а нормализация минибатчей - гарантия сходимости алгоритма оптимизации?

592
01:46:06,740 --> 01:46:12,880
Да. Я считаю, что нормализацию минибатчей нужно использовать везде.

593
01:46:12,880 --> 01:46:24,360
Есть версии алгоритма, которые не всегда хорошо работают, но люди научились с этим работать.

594
01:46:24,460 --> 01:46:44,610
Я бы советовал всегда искать способ ввести нормализацию минибатчей на каждом слое. С RNN это сложнее, но возможно.

595
01:46:44,710 --> 01:46:52,240
Вопрос из зала: Использование нормализации минибатчей позволяет не нормировать входные данные?

596
01:46:52,340 --> 01:47:12,495
Да, но лучше всё равно нормируйте входные данные, это несложно и делает данные удобнее для использования.

597
01:47:12,595 --> 01:47:25,010
Не все библиотеки хорошо обрабатывают нормализацию минибатчей предобученных моделей,

598
01:47:25,010 --> 01:47:40,170
поэтому кто-то может обучать свою модель поверх вашей, средние значения данных не совпадут, будут проблемы.

599
01:47:40,270 --> 01:47:53,894
В какой-то промежуток времени я не выполнял нормализацию и проблем не было, но так делать не стоит.

600
01:47:53,994 --> 01:48:06,240
Итак, в новом классе ConvBnNet используется BnLayer. Ещё одно изменение - эта строка.

601
01:48:06,340 --> 01:48:14,840
Я вставил свёрточный слой с большими фильтрами и шагом stride=1 в начале модели.

602
01:48:14,840 --> 01:48:23,360
Я сделал это, чтобы на следующий слой подавались более богатые признаками данные.

603
01:48:26,090 --> 01:48:29,630


604
01:48:29,630 --> 01:48:41,810


605
01:48:41,810 --> 01:48:49,520


606
01:48:49,520 --> 01:48:57,590


607
01:48:57,590 --> 01:49:04,730


608
01:49:08,260 --> 01:49:14,389


609
01:49:14,389 --> 01:49:19,429


610
01:49:19,429 --> 01:49:25,219


611
01:49:25,219 --> 01:49:31,400


612
01:49:31,400 --> 01:49:39,380


613
01:49:39,380 --> 01:49:44,599


614
01:49:44,599 --> 01:49:50,119


615
01:49:50,690 --> 01:49:55,099


616
01:49:55,099 --> 01:50:00,380


617
01:50:00,380 --> 01:50:04,550


618
01:50:04,550 --> 01:50:10,550


619
01:50:10,550 --> 01:50:14,690


620
01:50:17,179 --> 01:50:24,080


621
01:50:24,080 --> 01:50:34,040


622
01:50:34,040 --> 01:50:38,510


623
01:50:38,510 --> 01:50:42,680


624
01:50:42,680 --> 01:50:46,310


625
01:50:46,310 --> 01:50:52,880


626
01:50:52,880 --> 01:51:01,430


627
01:51:01,430 --> 01:51:07,220


628
01:51:07,220 --> 01:51:13,070


629
01:51:14,960 --> 01:51:21,650


630
01:51:21,650 --> 01:51:26,090


631
01:51:26,090 --> 01:51:31,580


632
01:51:31,580 --> 01:51:38,960


633
01:51:38,960 --> 01:51:43,340


634
01:51:43,340 --> 01:51:53,960


635
01:51:53,960 --> 01:52:00,890


636
01:52:00,890 --> 01:52:06,050


637
01:52:06,050 --> 01:52:13,460


638
01:52:13,460 --> 01:52:20,270


639
01:52:20,270 --> 01:52:30,860


640
01:52:34,520 --> 01:52:38,120


641
01:52:40,100 --> 01:52:45,410


642
01:52:49,190 --> 01:52:55,160


643
01:52:55,160 --> 01:53:00,710


644
01:53:00,710 --> 01:53:06,910


645
01:53:06,910 --> 01:53:11,120


646
01:53:11,120 --> 01:53:17,739


647
01:53:17,739 --> 01:53:25,070


648
01:53:25,070 --> 01:53:30,699


649
01:53:31,480 --> 01:53:40,100


650
01:53:40,100 --> 01:53:44,690


651
01:53:48,380 --> 01:54:00,880


652
01:54:00,880 --> 01:54:09,650


653
01:54:11,739 --> 01:54:19,760


654
01:54:19,760 --> 01:54:28,550


655
01:54:28,550 --> 01:54:31,120


656
01:54:35,910 --> 01:54:44,890


657
01:54:44,890 --> 01:54:48,700


658
01:54:51,430 --> 01:54:56,230


659
01:54:59,670 --> 01:55:08,760


660
01:55:08,760 --> 01:55:16,900


661
01:55:18,910 --> 01:55:25,180


662
01:55:25,180 --> 01:55:29,440


663
01:55:31,660 --> 01:55:35,710


664
01:55:35,710 --> 01:55:40,900


665
01:55:40,900 --> 01:55:46,510


666
01:55:46,510 --> 01:55:51,760


667
01:55:51,760 --> 01:55:56,470


668
01:55:56,470 --> 01:56:00,430


669
01:56:00,430 --> 01:56:04,450


670
01:56:06,640 --> 01:56:14,440


671
01:56:16,420 --> 01:56:20,580


672
01:56:20,580 --> 01:56:32,910


673
01:56:36,560 --> 01:56:41,900


674
01:56:44,130 --> 01:56:53,390


675
01:56:53,510 --> 01:56:59,909


676
01:57:02,370 --> 01:57:07,170


677
01:57:07,170 --> 01:57:11,790


678
01:57:11,790 --> 01:57:15,600


679
01:57:15,600 --> 01:57:19,860


680
01:57:22,860 --> 01:57:29,489


681
01:57:29,489 --> 01:57:37,560


682
01:57:37,560 --> 01:57:41,699


683
01:57:45,360 --> 01:57:54,050


684
01:57:54,050 --> 01:58:03,510


685
01:58:03,510 --> 01:58:09,540


686
01:58:09,540 --> 01:58:14,760


687
01:58:14,760 --> 01:58:19,380


688
01:58:19,380 --> 01:58:26,520


689
01:58:26,520 --> 01:58:35,550


690
01:58:35,550 --> 01:58:40,040


691
01:58:40,040 --> 01:58:47,959


692
01:58:47,959 --> 01:58:56,900


693
01:58:56,900 --> 01:59:04,249


694
01:59:04,249 --> 01:59:09,860


695
01:59:09,860 --> 01:59:14,840


696
01:59:14,840 --> 01:59:19,099


697
01:59:19,099 --> 01:59:23,959


698
01:59:23,959 --> 01:59:27,260


699
01:59:27,260 --> 01:59:31,820


700
01:59:35,630 --> 01:59:41,229


701
01:59:41,229 --> 01:59:47,570


702
01:59:49,789 --> 01:59:54,979


703
01:59:54,979 --> 02:00:00,110


704
02:00:02,479 --> 02:00:08,090


705
02:00:08,090 --> 02:00:13,189


706
02:00:13,189 --> 02:00:17,900


707
02:00:19,369 --> 02:00:26,389


708
02:00:26,389 --> 02:00:32,420


709
02:00:32,420 --> 02:00:37,849


710
02:00:37,849 --> 02:00:42,380


711
02:00:42,380 --> 02:00:46,309


712
02:00:46,309 --> 02:00:52,489


713
02:00:53,959 --> 02:00:57,979


714
02:00:57,979 --> 02:01:04,000


715
02:01:04,000 --> 02:01:13,780


716
02:01:13,780 --> 02:01:19,120


717
02:01:19,120 --> 02:01:25,840


718
02:01:25,840 --> 02:01:31,210


719
02:01:31,210 --> 02:01:36,040


720
02:01:36,040 --> 02:01:40,840


721
02:01:40,840 --> 02:01:45,520


722
02:01:45,520 --> 02:01:49,510


723
02:01:49,510 --> 02:01:54,280


724
02:01:56,410 --> 02:02:00,040


725
02:02:00,040 --> 02:02:09,730


726
02:02:09,730 --> 02:02:14,710


727
02:02:14,710 --> 02:02:20,590


728
02:02:20,590 --> 02:02:24,490


729
02:02:24,490 --> 02:02:29,680


730
02:02:29,680 --> 02:02:34,900


731
02:02:38,890 --> 02:02:46,330


732
02:02:46,330 --> 02:02:50,260


733
02:02:50,260 --> 02:02:54,070


734
02:02:56,590 --> 02:03:01,780


735
02:03:01,780 --> 02:03:09,370


736
02:03:09,370 --> 02:03:13,080


737
02:03:13,080 --> 02:03:19,170


738
02:03:19,170 --> 02:03:25,650


739
02:03:25,650 --> 02:03:29,699


740
02:03:29,699 --> 02:03:36,960


741
02:03:36,960 --> 02:03:41,760


742
02:03:41,760 --> 02:03:49,770


743
02:03:49,770 --> 02:03:54,960


744
02:03:54,960 --> 02:03:58,890


745
02:03:58,890 --> 02:04:02,160


746
02:04:04,290 --> 02:04:10,080


747
02:04:10,080 --> 02:04:16,170


748
02:04:16,170 --> 02:04:22,560


749
02:04:22,560 --> 02:04:26,820


750
02:04:28,560 --> 02:04:38,100


751
02:04:38,100 --> 02:04:44,390


752
02:04:44,390 --> 02:04:49,530


753
02:04:53,489 --> 02:05:04,110


754
02:05:04,110 --> 02:05:18,270


755
02:05:18,270 --> 02:05:25,050


756
02:05:25,050 --> 02:05:27,960


757
02:05:27,960 --> 02:05:31,290


758
02:05:31,290 --> 02:05:34,710


759
02:05:34,710 --> 02:05:39,270


760
02:05:39,270 --> 02:05:47,250


761
02:05:47,250 --> 02:05:50,849


762
02:05:50,849 --> 02:05:57,199


763
02:06:01,230 --> 02:06:06,329


764
02:06:06,329 --> 02:06:12,449


765
02:06:12,449 --> 02:06:19,679


766
02:06:19,679 --> 02:06:25,980


767
02:06:30,150 --> 02:06:35,550


768
02:06:35,550 --> 02:06:39,869


769
02:06:41,909 --> 02:06:45,449


770
02:06:45,449 --> 02:06:49,619


771
02:06:49,619 --> 02:06:54,869


772
02:06:54,869 --> 02:07:00,239


773
02:07:00,239 --> 02:07:03,750


774
02:07:03,750 --> 02:07:06,210


775
02:07:06,210 --> 02:07:09,900


776
02:07:09,900 --> 02:07:15,420


777
02:07:15,420 --> 02:07:19,619


778
02:07:19,619 --> 02:07:24,090


779
02:07:24,090 --> 02:07:32,550


780
02:07:32,550 --> 02:07:36,599


781
02:07:36,599 --> 02:07:43,320


782
02:07:43,320 --> 02:07:48,180


783
02:07:48,180 --> 02:07:53,280


784
02:07:55,470 --> 02:08:03,900


785
02:08:06,810 --> 02:08:11,610


786
02:08:13,230 --> 02:08:16,920


787
02:08:16,920 --> 02:08:21,240


788
02:08:21,240 --> 02:08:26,160


789
02:08:26,160 --> 02:08:30,840


790
02:08:30,840 --> 02:08:38,450


791
02:08:38,450 --> 02:08:41,970


792
02:08:41,970 --> 02:08:46,440


793
02:08:49,350 --> 02:08:53,700


794
02:08:56,160 --> 02:09:06,120


795
02:09:06,120 --> 02:09:11,850


796
02:09:11,850 --> 02:09:16,320


797
02:09:19,290 --> 02:09:24,750


798
02:09:24,750 --> 02:09:28,980


799
02:09:28,980 --> 02:09:33,990


800
02:09:33,990 --> 02:09:41,190


801
02:09:41,190 --> 02:09:48,450


802
02:09:48,450 --> 02:10:00,210


803
02:10:00,210 --> 02:10:09,420


804
02:10:09,420 --> 02:10:14,940


805
02:10:17,180 --> 02:10:23,430


806
02:10:23,430 --> 02:10:27,180


807
02:10:27,180 --> 02:10:30,650


808
02:10:38,340 --> 02:10:42,780


809
02:10:42,780 --> 02:10:46,770


810
02:10:46,770 --> 02:10:54,570


811
02:10:58,770 --> 02:11:09,650


812
02:11:12,360 --> 02:11:17,790


813
02:11:17,790 --> 02:11:23,720


814
02:11:23,720 --> 02:11:30,630


815
02:11:30,630 --> 02:11:36,120


816
02:11:36,120 --> 02:11:41,100


817
02:11:41,100 --> 02:11:50,010


818
02:11:50,010 --> 02:11:56,550


819
02:11:56,550 --> 02:12:01,590


820
02:12:01,590 --> 02:12:05,630


821
02:12:05,630 --> 02:12:10,730


822
02:12:13,449 --> 02:12:19,969


823
02:12:19,969 --> 02:12:23,449


824
02:12:23,449 --> 02:12:30,800


825
02:12:30,800 --> 02:12:35,659


826
02:12:39,020 --> 02:12:43,520


827
02:12:43,520 --> 02:12:48,860


828
02:12:51,110 --> 02:12:57,020


829
02:12:57,020 --> 02:13:01,850


830
02:13:01,850 --> 02:13:06,530


831
02:13:10,940 --> 02:13:13,969


832
02:13:13,969 --> 02:13:18,199


833
02:13:18,199 --> 02:13:23,780


834
02:13:23,780 --> 02:13:31,810


835
02:13:31,960 --> 02:13:37,370


836
02:13:37,370 --> 02:13:42,980


837
02:13:42,980 --> 02:13:48,880


838
02:13:48,880 --> 02:13:52,250


839
02:13:52,250 --> 02:13:59,060


840
02:13:59,060 --> 02:14:04,159


841
02:14:10,909 --> 02:14:15,980


842
02:14:15,980 --> 02:14:19,340


843
02:14:19,340 --> 02:14:22,760


844
02:14:22,760 --> 02:14:31,699


845
02:14:34,719 --> 02:14:42,050


846
02:14:42,050 --> 02:14:47,360


847
02:14:50,090 --> 02:14:57,260


848
02:14:57,260 --> 02:15:00,710


849
02:15:00,710 --> 02:15:05,810


850
02:15:05,810 --> 02:15:10,849


851
02:15:10,849 --> 02:15:14,210


852
02:15:14,210 --> 02:15:18,800


853
02:15:18,800 --> 02:15:23,750


854
02:15:23,750 --> 02:15:27,980


855
02:15:27,980 --> 02:15:32,179


856
02:15:32,179 --> 02:15:36,980


857
02:15:39,500 --> 02:15:42,590


858
02:15:42,590 --> 02:15:45,770


859
02:15:45,770 --> 02:15:50,810


860
02:15:50,810 --> 02:15:53,630


861
02:15:53,630 --> 02:16:00,020


862
02:16:00,020 --> 02:16:04,159


863
02:16:07,070 --> 02:16:10,659


864
02:16:10,659 --> 02:16:17,030


865
02:16:17,030 --> 02:16:20,719


866
02:16:20,719 --> 02:16:25,849


867
02:16:25,849 --> 02:16:32,381


868
02:16:32,380 --> 02:16:38,679


869
02:16:38,680 --> 02:16:42,251


870
02:16:42,251 --> 02:16:46,720


871
02:16:46,719 --> 02:16:51,460


872
02:16:51,460 --> 02:16:55,689


873
02:16:55,690 --> 02:16:58,600


874
02:16:58,600 --> 02:17:03,581


875
02:17:03,581 --> 02:17:06,671


876
02:17:06,671 --> 02:17:10,001


877
02:17:10,001 --> 02:17:13,990


878
02:17:13,990 --> 02:17:21,190


879
02:17:21,190 --> 02:17:24,190


880
02:17:24,190 --> 02:17:29,270


881
02:17:29,270 --> 02:17:32,379


