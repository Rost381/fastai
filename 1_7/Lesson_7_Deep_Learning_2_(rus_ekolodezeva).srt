1
00:00:00,050 --> 00:00:04,965
Добро пожаловать на последнюю лекцию первой части курса.

2
00:00:05,065 --> 00:00:12,484
Темы первой части курса - классификация и регрессия с использованием глубокого обучения

3
00:00:12,584 --> 00:00:19,439
с уклоном в изучение лучших практик.

4
00:00:19,439 --> 00:00:25,500
Мы начали с использования непонятных трёх строк кода для решения задачи классификации изображений,

5
00:00:25,500 --> 00:00:36,420
потом перешли к NLP, стуктурированным данным и коллаборативной фильтрации,

6
00:00:36,420 --> 00:00:44,450
уточняя необходимые теоретические моменты и обучаясь создавать хорошо работающие модели.

7
00:00:44,450 --> 00:00:51,400
Последние три лекции курса мы посвятили более глубокогу изучению этих тем в другом порядке -

8
00:00:51,500 --> 00:01:02,430
поняли, как что работает, посмотрели на код и реализовали большинство вещей с нуля.

9
00:01:02,430 --> 00:01:17,875
Во второй части курса мы отойдём от задач классификации и регрессии, которые предсказывают несколько чисел,

10
00:01:17,975 --> 00:01:25,259
и перейдём к генеративным моделям, создающим большие объемы данных -

11
00:01:25,259 --> 00:01:34,659
создание связных предложений при переводе, или описании изображений, или ответах на вопросы;

12
00:01:34,759 --> 00:01:42,469
создание изображений при передаче художественного стиля, семантическая сегментация и так далее.

13
00:01:42,569 --> 00:02:18,400
Во второй части курса мы перейдём от испытанных техник к изучению непроверенных методов из свежих статей.

14
00:02:18,500 --> 00:02:33,789
Если вы хотите научиться читать статьи и реализовывать свежие техники, пройдите вторую часть курса.

15
00:02:33,889 --> 00:02:39,325
Вторая часть, как и первая, не требует математики выше школьного уровня,

16
00:02:39,425 --> 00:02:49,869
но предполагается, что у вас есть время и терпение разбираться с необходимыми идеями и воплощать их в коде.

17
00:02:49,969 --> 00:03:00,540
Самое сложное в преподавании RNN - убедить студентов, что рекуррентные нейронные сети

18
00:03:00,540 --> 00:03:08,709
ничем не отличаются от обычных полносвязных нейронных сетей.

19
00:03:08,809 --> 00:03:15,750
Так выглядит полносвязная нейронная сеть.

20
00:03:15,750 --> 00:03:25,984
Напомню, что стрелки показывают операции слоя - как правило, линейный слой с нелинейной функцией активации.

21
00:03:26,084 --> 00:03:33,139
Здесь это умножение матриц и выпрямитель или гиперболический тангенс.

22
00:03:33,239 --> 00:03:40,054
Стрелки одного цвета используют одну матрицу весов.

23
00:03:40,154 --> 00:03:45,369
Единственное отличие от полносвязных сетей, которые мы видели раньше -

24
00:03:45,469 --> 00:03:54,389
есть несколько входных слоёв в разных местах сети.

25
00:03:54,389 --> 00:04:01,180
Мы пробовали прибавлять входные данные к активациям полносвязного слоя, потом - склеивать их вместе,

26
00:04:01,280 --> 00:04:06,540
смысл происходящего не меняется от добавления нескольких входных слоёв.

27
00:04:06,640 --> 00:04:15,030
Эта версия модели реализована в классе Char3Model.

28
00:04:15,030 --> 00:04:19,909
Здесь заданы стрелки разных цветов как три матрицы весов,

29
00:04:19,909 --> 00:04:29,874
конструктор nn.Linear() объединяет матрицу весов и вектор смещений.

30
00:04:29,974 --> 00:04:46,730
Мы создали эмбеддинги, пропустили их через линейные и скрытые слои.

31
00:04:46,830 --> 00:04:57,270
В первый слой не входила оранжевая стрелка, поэтому мы сместили первый входной слой

32
00:04:57,270 --> 00:05:02,009
и добавили пустую матрицу, чтобы все входные слои обрабатывались одинаково.

33
00:05:02,009 --> 00:05:16,389
В классе CharLoopModel мы обернули обработку входных данных в цикл.

34
00:05:16,489 --> 00:05:30,750
Также мы увеличили количество символов с 3 до 8, с циклом это легко сделать.

35
00:05:30,750 --> 00:05:41,380
Класс CharLoopModel - то же самое, что и касс Char3Model, только символов теперь 8, а не 3.

36
00:05:41,480 --> 00:05:51,750
В классе CharRnn мы заменили петлю цикла на единственный вызов конструктора nn.RNN(),

37
00:05:51,750 --> 00:06:04,299
который также сохраняет активации скрытых слоёв в переменной h.

38
00:06:04,399 --> 00:06:12,560
Это тоже был просто рефакторинг, не меняющий принципа работы модели.

39
00:06:14,000 --> 00:06:21,210
После этого мы слегка сократили время обучения.

40
00:06:21,210 --> 00:06:49,760
Мы разрезали датасет отзывов к фильмам на отрезки по 8 символов и предсказывали девятый.

41
00:06:49,860 --> 00:07:13,420
Чтобы использовать все данные, мы использовали для предсказания накладывающиеся отрезки.

42
00:07:13,420 --> 00:07:24,670
Это вычислительно неэффективно, потому что приходится очень много пересчитывать.

43
00:07:24,670 --> 00:07:34,065
Вместо этого мы разделили данные на ненакладывающиеся отрезки

44
00:07:34,165 --> 00:07:53,440
и по каждым 8 символам стали предсказывать 8 следующих символов, а не 1.

45
00:07:53,440 --> 00:08:06,335
Посмотрев на первый символ, мы предсказываем второй; посмотрев на второй - третий и так далее.

46
00:08:06,435 --> 00:08:30,070
Кто-то из вас высказал правильное замечание, что в начале отрезка матрица активаций h обнуляется,

47
00:08:30,070 --> 00:08:48,820
поэтому использующее её предсказание по первому символу отрезка будет очень неточным.

48
00:08:48,920 --> 00:09:08,520
После этого мы решили не выбрасывать h на каждом минибатче, то есть при каждом вызове метода forward().

49
00:09:08,889 --> 00:09:23,520
Матрица h - состояние скрытого слоя, оранжевые круги, они обнулялись после каждого минибатча,

50
00:09:23,520 --> 00:09:34,800
как будто все предыдущие действия не имели значения, это неправильно.

51
00:09:34,800 --> 00:09:58,205
Мы передвинем строку обнуления матрицы h в конструктор и занесём её в поле класса, чтобы отслеживать изменения.

52
00:09:58,305 --> 00:10:12,029
Эта версия модели реализована в классе CharSeqStatefulRnn.

53
00:10:12,029 --> 00:10:29,904
В конструкторе вызывается метод init_hidden(), создающий матрицу h и заполняющий её нулями.

54
00:10:30,004 --> 00:10:40,542
Матрица h теперь лежит в поле класса self.h и передаётся в параметры метода self.rnn().

55
00:10:40,642 --> 00:10:49,675
Обновлённые активации h заносятся в матрицу self.h строкой self.h = repackage_var(h).

56
00:10:49,775 --> 00:10:53,910
Функция repackage_var(h) - полезный трюк №1.

57
00:10:53,910 --> 00:11:02,550
Допустим, мы не используем её и просто делаем присваивание self.h = h.

58
00:11:02,550 --> 00:11:16,569
Допустим, что размер датасета - миллион символов, тогда в нашей сети миллион оранжевых кругов.

59
00:11:16,669 --> 00:11:38,560
Модель выглядит так - для каждого входного слоя есть свой выходной слой.

60
00:11:38,660 --> 00:11:45,650
При вычислении градиентов используется метод обратного распространения ошибки,

61
00:11:45,750 --> 00:11:56,412
то есть мы идём в обратном направлении и смотрим на ошибку на каждом слое,

62
00:11:56,512 --> 00:12:05,550
а потом меняем веса в соответствии с этими ошибками.

63
00:12:05,550 --> 00:12:30,984
Если в датасете миллион символов, RNN будет состоять из миллиона полносвязных слоёв, вот эта модель.

64
00:12:31,084 --> 00:13:01,680
Такая модель занимает очень много памяти, потому что каждый минибатч мы запоминаем и перемножаем миллион градиентов.

65
00:13:05,399 --> 00:13:25,359
Чтобы этого избежать, мы сохраняем только значения матрицы состояния скрытого слоя, но не градиенты.

66
00:13:25,459 --> 00:13:34,194
Это делает функция repackage_var().

67
00:13:34,294 --> 00:13:47,169
Она возвращает значение матрицы активаций h.data без градиентов,

68
00:13:47,269 --> 00:14:01,149
поэтому метод обратного распространения ошибки остановится на этой переменной.

69
00:14:01,249 --> 00:14:06,266
Функция вызывается при каждом вызове метода forward().

70
00:14:06,366 --> 00:14:13,740
На каждом отрезке 8 символов пройдут через 8 слоёв, на слоях отработает метод обратного распространения ошибки,

71
00:14:13,740 --> 00:14:23,824
а потом сохранятся значения матрицы h без истории проведённых операций.

72
00:14:23,924 --> 00:14:30,055
Это называется метод обратного распространения ошибки во времени (backpropagation through time, BPTT).

73
00:14:30,155 --> 00:14:38,399
Когда вы читаете об этом в интернете, это выглядит как новый сложнейший алгоритм или какое-то открытие,

74
00:14:38,399 --> 00:14:52,599
а на самом деле - это просто отбрасывание истории состояния скрытого слоя в конце минибатча.

75
00:14:52,699 --> 00:14:59,560
Это полезный трюк №1, функция repackage_var().

76
00:14:59,660 --> 00:15:18,990
Если помните, на четвёртой лекции у нас был параметр bptt - это количество слоёв, после которых история стирается.

77
00:15:18,990 --> 00:15:23,699
Нужно стараться уменьшить количество слоёв обратного распространения ошибки -

78
00:15:23,699 --> 00:15:37,029
модель будет легче обучаться и не так сильно страдать от нестабильности градиентов.

79
00:15:37,029 --> 00:15:43,479
С другой стороны, это количество должно быть достаточно большим,

80
00:15:43,479 --> 00:15:53,109
чтобы у модели было эффективное количество памяти.

81
00:15:53,109 --> 00:15:59,699
При создании RNN нужно подбирать значение параметра bptt.

82
00:16:00,389 --> 00:16:13,319
Полезный трюк №2 - как эффективно передавать данные в модель в соответствии с нашей схемой.

83
00:16:13,419 --> 00:16:24,936
Напомню, что мы рассматриваем последовательно каждый отрезок,

84
00:16:25,036 --> 00:16:33,069
но для ускорения вычислений хочется обсчитывать много отрезков одновременно.

85
00:16:35,970 --> 00:17:09,180
Мы будем параллельно обсчитывать группы отрезков.

86
00:17:09,280 --> 00:17:28,520
Различные отрезки будут независимо учитываться в матрице состояния скрытого слоя.

87
00:17:28,520 --> 00:17:46,275
После обучения на первых отрезках мы перейдём к следующим и продолжим обсчитывать их параллельно.

88
00:17:46,375 --> 00:17:56,450
Это должно напомнить вам начало работы с torchtext.

89
00:17:56,450 --> 00:18:15,200
Мы обсуждали процесс создания минибатчей. Полный текст (все сочинения Ницше или все обзоры IMDb)

90
00:18:15,200 --> 00:18:39,990
разбивается на 64 одинаковых части. Не на части длиной 64, а на 64 одинаковых части.

91
00:18:40,090 --> 00:18:55,240
Если полный текст имеет длину 64 миллиона символов, каждая из 64 частей содержала бы 1 миллион символов.

92
00:18:55,240 --> 00:19:13,410
Полученные части располагаются одна под другой, получается матрица высотой 64 и шириной 1 миллион.

93
00:19:13,510 --> 00:19:32,475
Минибатч - идущие подряд bptt столбцов этой матрицы, bptt был около 70.

94
00:19:32,575 --> 00:20:02,580
Для всех строк из минибатча параллельно предсказываются следующие символы, процесс повторяется.

95
00:20:02,680 --> 00:20:17,480
Такое преобразование данных нужно было для того, чтобы параллельно обсчитывать различные отрезки данных.

96
00:20:17,480 --> 00:20:23,820
Символ в начале отрезка в миллион символов может находиться в середине предложения,

97
00:20:23,920 --> 00:20:33,880
но это неважно, потому что такое происходит только один раз на миллион символов.

98
00:20:33,880 --> 00:20:40,740
Вопрос из зала: Вы можете ещё что-то сказать про дополнение данных таких датасетов?

99
00:20:40,840 --> 00:20:52,515
Нет, мне больше нечего сказать на эту тему. Я изучу этот вопрос ко второй части курса.

100
00:20:52,615 --> 00:21:04,820
Недавно начали появляться статьи на эту тему. Например, победитель недавнего соревнования Kaggle

101
00:21:06,830 --> 00:21:17,985
вставлял случайные части строк в свои данные, это может быть полезно.

102
00:21:18,085 --> 00:21:23,070
Я видел несколько статей, в которых описаны похожие методы,

103
00:21:23,170 --> 00:21:38,259
но именно отдельных статей по NLP на тему дополнения данных - не видел.

104
00:21:38,559 --> 00:21:43,604
Вопрос из зала: Как подобрать значение параметра bptt?

105
00:21:43,704 --> 00:21:48,049
Есть несколько факторов.

106
00:21:48,049 --> 00:22:00,014
Размер обведённой матрицы - (bptt) x (размер минибатча). Для каждого элемента минибатча

107
00:22:00,114 --> 00:22:23,389
есть вектор эмбеддинга, и результирующая трёхмерная матрица должна помещаться в GPU.

108
00:22:23,389 --> 00:22:29,659
Если вы получаете ошибку "CUDA error 'out of memory'", придётся уменьшить одно из этих чисел.

109
00:22:29,659 --> 00:22:37,874
Если модель обучается нестабильно, - например, ошибка резко достигает NaN, - попробуйте уменьшить bptt.

110
00:22:37,974 --> 00:22:43,785
При более коротком обратном распространении ошибки градиентный взрыв не успеет развиться.

111
00:22:43,885 --> 00:22:47,090
Если обучение слишком медленное - попробуйте уменьшить bptt,

112
00:22:47,090 --> 00:22:51,169


113
00:22:51,169 --> 00:22:55,919


114
00:22:56,019 --> 00:23:08,419
Недавно появилисть квази-рекуррентные нейронные сети (QRNN), они позволяют так делать, но мы пока не можем.

115
00:23:08,419 --> 00:23:13,339
Это главные факторы при выборе параметра bptt - время обучения, занимаемая память и стабильность обучения.

116
00:23:13,439 --> 00:23:21,820
Постарайтесь выбрать максимальное значение параметра bptt с учётом всех этих факторов.

117
00:23:23,100 --> 00:23:40,084
Нарезать данные на отрезки и складывать их вместе довольно муторно, поэтому здесь мы используем torchtext.

118
00:23:40,184 --> 00:23:52,749
При использовании API вроде fastai и torchtext вы можете столкнуться с тем, что

119
00:23:52,749 --> 00:23:58,299
методы API ожидают данные не в том формате, в котором они вам даны изначально.

120
00:23:58,299 --> 00:24:03,549
В таком случае можно преобразовать данные под необходимый формат

121
00:24:03,549 --> 00:24:08,919
или написать класс для оборачивания данных в необходимый формат.

122
00:24:08,919 --> 00:24:17,749
Я читал форум и заметил, что вы тратите очень много времени на написание обёрток данных.

123
00:24:17,849 --> 00:24:24,079
Я гораздо ленивее и предпочитаю менять данные, оба подхода хороши.

124
00:24:24,179 --> 00:24:38,165
Если вы заметите, что какой-то вид данных появляется часто и для него ещё нет обёртки в fastai,

125
00:24:38,265 --> 00:24:42,860
пожалуйста, напишите эту обёртку и создайте соответствующий pull request на github.com/fastai/fastai.

126
00:24:42,960 --> 00:24:58,009
Я решил преобразовать тексты Ницше в формат, который ожидает torchtext.

127
00:24:58,109 --> 00:25:15,319
В обёртке fastai над torchtext уже доступно разделение данных на обучающую и валидационную выборки.

128
00:25:15,419 --> 00:25:26,224
Я скопировал датасет из папки data/nietzsche/ в папки trn/ и val/.

129
00:25:26,324 --> 00:25:39,190
В папке trn/ я удалил последние 20% данных, а в папке val/ - всё, кроме последних 20%.

130
00:25:39,190 --> 00:25:56,254
Я решил, что это проще, чем писать класс-обёртку, да и валидационная выборка строится разумнее.

131
00:25:56,354 --> 00:26:08,660
На практике приходится обучать модель на одних книгах или авторах, а использовать для других.

132
00:26:08,760 --> 00:26:24,530
Я постарался сделать похожее, чтобы лучше проверить качество модели, и разбил корпус на два цельных куска.

133
00:26:24,630 --> 00:26:44,089
Убедитесь, что владеете bash в достаточной степени, чтобы выполнять схожие операции.

134
00:26:44,189 --> 00:27:12,404
Обучающая и валидационная выборка обе состоят из одного файла, так строят языковые модели.

135
00:27:12,504 --> 00:27:23,209
Вот данные, вот код, который мы уже видели, давайте его разберём.

136
00:27:23,309 --> 00:27:36,129
Информация о предобработке данных содержится в объекте класса Field.

137
00:27:36,129 --> 00:27:51,300
Здесь я перевожу все символы в нижний регистр и использую функцию list() в качестве функции токенизации.

138
00:27:51,400 --> 00:27:59,924
При предсказывании слов мы использовали функцию, разбивающую текст по пробелам,

139
00:28:00,024 --> 00:28:16,840
а здесь - символы, поэтому используется функция преобразования строки в массив символов.

140
00:28:16,840 --> 00:28:27,620
Видно, как библиотеки вроде torchtext и fastai могут сильно упростить вам жизнь.

141
00:28:30,200 --> 00:28:37,470
Очень часто в качестве параметров методов эти библиотеки принимают функции,

142
00:28:38,809 --> 00:28:44,960
и функции можно использовать любые, в том числе и написанные руками.

143
00:28:44,960 --> 00:28:51,027
Параметр tokenize=list означает, что минибатчи состоят из символов.

144
00:28:51,127 --> 00:29:00,530
Вот другие параметры. Я использую те же значения, что и в других частях этого Jupyter ноутбука.

145
00:29:00,530 --> 00:29:08,380
Размер минибатча bs=64, размер обратного распространения во времени bptt=8,

146
00:29:08,380 --> 00:29:14,960
длина вектора эмбеддинга n_fac=42 и размер скрытого слоя n_hidden=256.

147
00:29:14,960 --> 00:29:37,870
Размер скрытого слоя - размер матрицы весов, это оранжевые круги на диаграмме.

148
00:29:38,290 --> 00:29:43,865
Это словарь, показывающий, что использовать в качестве обучающей, валидационной и тестовой выборок.

149
00:29:43,965 --> 00:29:49,350
В качестве тестовой выборки я использую валидационную.

150
00:29:49,450 --> 00:29:58,250
После словаря я создаю объект данных модели md методом LanguageModelData.from_text_files().

151
00:29:58,250 --> 00:30:12,740
Все параметры метода знакомы. Параметр min_freq=3 здесь бесполезен,

152
00:30:12,740 --> 00:30:22,550
так как вряд ли найдётся символ, использующийся меньше трёх раз.

153
00:30:22,550 --> 00:30:43,630
Получилось 963 минибатча, это число должно равняться (количество токенов) / bs / bptt.

154
00:30:45,130 --> 00:30:49,850
На практике это не совсем так.

155
00:30:52,400 --> 00:31:16,755
Нельзя перемешивать датасет, как с изображениями, но можно кажый раз немного менять значение bptt.

156
00:31:16,855 --> 00:31:44,830
PyTorch выдерживает значение bptt=8, но в 5% случах он немного меняет его, сохраняя среднее значение.

157
00:31:45,130 --> 00:31:53,950
Вопрос из зала: Внутри одного батча длина bptt фиксированная?

158
00:31:53,950 --> 00:32:29,640
Да, каждый минибатч происходит умножение матриц, поэтому размер должен быть фиксирован.

159
00:32:33,105 --> 00:32:39,850
Таким образом, количество минибатчей - это длина загрузчика данных.

160
00:32:39,850 --> 00:32:45,130
md.nt - количество уникальных токенов.

161
00:32:45,130 --> 00:33:01,710
После выполнения этой строки в переменной TEXT появляется поле TEXT.vocab,

162
00:33:01,710 --> 00:33:18,950
содержащий словари перевода уникальных символов в индексы и наоборот.

163
00:33:19,050 --> 00:33:34,179
Модель реализована в классе CharSeqStatefulRnn.

164
00:33:34,179 --> 00:33:50,654
Его мы уже обсуждали - последней модификацией был перенос матрицы h в поле self.h.

165
00:33:50,754 --> 00:34:03,880
Я сказал, что размер минибатча всегда одинакоы, но это не совсем так.

166
00:34:03,980 --> 00:34:24,284
Последний минибатч может быть короче, если количество токенов не кратно (bptt * bs).

167
00:34:24,384 --> 00:34:48,438
Поэтому мы проверяем по совпадению размеров, нормальный ли минибатч,

168
00:34:48,538 --> 00:35:01,980
и обнуляем матрицу h, если он слишком маленький - то есть в конце эпохи.

169
00:35:02,080 --> 00:35:10,850
Это полезный трюк №3. В конце каждой эпохи выполняется минибатч меньшего размера,

170
00:35:10,850 --> 00:35:19,190
а к началу следующей эпохи размеры выравниваются.

171
00:35:19,190 --> 00:35:28,580
Поэтому вызов self.init_hidden() есть не только в конструкторе.

172
00:35:31,880 --> 00:35:36,490
Это не очень важная деталь, но полезная.

173
00:35:39,160 --> 00:35:59,650
Последний полезный трюк связан с одним неудобством PyTorch, надеемся, кто-то его исправит.

174
00:35:59,650 --> 00:36:08,230
Функции потерь в PyTorch, например, Softmax, не принимают на вход тензоры третьего ранга.

175
00:36:08,330 --> 00:36:17,240
Напомню, что тензор третьего ранга - это просто трёхмерная матрица.

176
00:36:17,240 --> 00:36:22,829
Нет никакой причины, по которой обработка тензоров третьего ранга была бы сложна и неудобна -

177
00:36:22,929 --> 00:36:35,594
можно просто последовательно считать ошибку по первым двум осям.

178
00:36:35,694 --> 00:36:53,159
Почему-то никто это не реализовал, поэтому функции работают только с тензорами второго или четвёртого ранга.

179
00:36:59,519 --> 00:37:34,040
На каждой итерации есть две матрицы размера bs x bptt - предсказания и истинные значения.

180
00:37:34,040 --> 00:37:41,154
Для вычисления ошибки матрицы сверяются поэлементно.

181
00:37:41,254 --> 00:37:57,940
Для передачи в функцию потерь эти матрицы распрямляются до векторов методом .view().

182
00:37:57,940 --> 00:38:11,459
Итоговое количество колонок равно количеству возможных класов self.vocab_size.

183
00:38:11,559 --> 00:38:24,260
Количество строк - сколько получится, здесь будет bs * bptt.

184
00:38:24,260 --> 00:38:34,935
Предсказания нужно приводить к необходимому формату, а целевую переменную torchtext обработал за нас.

185
00:38:35,035 --> 00:38:56,320
torchtext автоматически распрямляет целевую переменную, мы видели это на четвёртой лекции.

186
00:38:56,420 --> 00:39:02,727
Итак, вот полезные трюки - избавляйтесь от истории операций,

187
00:39:02,827 --> 00:39:11,180
обнуляйте состояние скрытого слоя при изменении размера минибатча,

188
00:39:16,970 --> 00:39:23,180
распрямляйте данные и используйте torchtext для создания хороших минибатчей.

189
00:39:23,180 --> 00:39:39,510
После этого можно создать модель, алгоритм оптимизации и обучить модель.

190
00:39:39,610 --> 00:40:02,055
При вызове функции F.log_softmax() необходимо указывать ось dim, по которой считаются потери.

191
00:40:02,155 --> 00:40:11,010
Мы хотим вычислять Softmax по последней оси, поэтому передаём значение dim=-1.

192
00:40:11,110 --> 00:40:26,120
В знаменателе формулы Softmax - суммирование, параметр dim указывает, по какой оси его выполнять.

193
00:40:26,120 --> 00:40:41,625
Последняя ось содержит вероятности, которые в сумме должны давать 1, поэтому суммирование происходит по ней.

194
00:40:41,725 --> 00:41:02,810
Для запуска этого Jupyter ноутбука нужен PyTorch 0.3, он вышел на этой неделе.

195
00:41:02,810 --> 00:41:16,725
PyTorch 0.3 официально не поддерживает Windows, но я смог его установить командой conda install torch,

196
00:41:16,825 --> 00:41:24,140
выполнил Jupyter ноутбук первой лекции и всё отработало.

197
00:41:24,140 --> 00:41:40,950
У меня Surface Book 2 15 с видеокартой GTX 1060 и 16 GB GPU, для глубокого обучения подходит отлично.

198
00:41:41,050 --> 00:41:50,595
Он работает примерно втрое медленнее моей машины с видеокартой GTX 1080,

199
00:41:50,695 --> 00:41:59,190
то есть примерно как SP2-инстанс на AWS.

200
00:41:59,290 --> 00:42:07,130
Surface Book также работает как планшет с сенсорным экраном, он тонкий и лёгкий,

201
00:42:07,130 --> 00:42:19,200
у меня никогда не было такого удобного ноутбука. Я установил на нём Linux, там тоже всё работает.

202
00:42:19,300 --> 00:42:24,385
Если вам нужен ноутбук для глубокого обучения, это хороший вариант.

203
00:42:26,560 --> 00:42:36,950
Так, про параметр dim=-1 я сказал, дальше создаём модель, алгоритм оптимизации, и обучаем.

204
00:42:36,950 --> 00:42:43,360
Результаты мало отличаются от того, что было раньше.

205
00:42:43,360 --> 00:42:54,070
Можно ещё немного изменить модель, рассмотрим класс CharSeqStatefulRnn2.

206
00:42:54,170 --> 00:43:11,680
Этот класс очень похож на CharSeqStatefulRnn, но вместо nn.RNN() теперь используется nn.RNNCell().

207
00:43:14,890 --> 00:43:24,100
Вот код функции RNNCell() в библиотеке PyTorch, его не надо запускать.

208
00:43:24,100 --> 00:43:34,727
Вы уже умеете читать и понимать исходный код PyTorch, мы уже делали похожие вещи.

209
00:43:34,827 --> 00:43:47,320
Здесь - умножение входных данных на матрицу весов и прибавление смещений, это делает F.linear().

210
00:43:47,320 --> 00:44:05,230
Здесь входные данные и состояние скрытого слоя не склеиваются, а складываются, оба подхода хороши.

211
00:44:05,230 --> 00:44:14,822
Вопрос из зала: Почему в качестве функции активации используется гиперболический тангенс?

212
00:44:14,922 --> 00:44:31,830
Область значений гиперболического тангенса tanh - от -1 до 1, он выглядит так.

213
00:44:36,660 --> 00:44:49,530
Гиперболический тангенс и сигмоида связаны соотношением tanh(x) = 2 * sigmoid(2x) - 1.

214
00:44:49,530 --> 00:45:14,045
Диапазон значений от -1 до 1 позволяет избежать градиентного взрыва, в отличие от неограниченного выпрямителя.

215
00:45:14,145 --> 00:45:28,835
Тем не менее, можно использовать выпрямитель, указав его в параметре nonlinearity.

216
00:45:28,935 --> 00:45:35,750
Обычно используют гиперболический тангенс.

217
00:45:35,850 --> 00:45:45,015
Мы заменили nn.RNN() на nn.RNNCell(), поэтому нужно вернуть цикл.

218
00:45:45,115 --> 00:45:57,530
В каждой итерации цикла полученные результаты склеиваются с предыдущими, в конце получается матрица outp.

219
00:45:57,630 --> 00:46:11,375
Я получил такой же ответ, как и раньше, никакой магии.

220
00:46:11,475 --> 00:46:17,920
На практике модели не пишут так, но кто-нибудь может придумать

221
00:46:17,920 --> 00:46:25,690
хорошую модификацию класса RNNCell, или другой способ отслеживать градиенты, или новый вид регуляризации.

222
00:46:25,690 --> 00:46:43,350
Исходный код fastai выглядит точно так же, чтобы использовать не реализованные в PyTorch методы регуляризации.

223
00:46:43,790 --> 00:47:01,896
На практике RNNCell не спасает от градиентного взрыва и требует низкой скорости обучения

224
00:47:01,996 --> 00:47:13,935
и низкого значения параметра bptt, поэтому на практике этот класс никто не использует.

225
00:47:14,035 --> 00:47:26,890
Вместо этого используется класс GRUCell, управляемый рекуррентный блок.

226
00:47:29,590 --> 00:47:38,425
Он выглядит так и описывается такими уравнениями.

227
00:47:38,525 --> 00:47:44,310
Мы пробежимся по ним, но обсуждать будем во второй части курса.

228
00:47:44,410 --> 00:48:04,640
До этого мы получали предсказания, сложив имеющиеся активации и произведение матрицы весов на входные данные.

229
00:48:06,980 --> 00:48:14,510
Здесь не так. Сначала входные данные умножаются на вспомогательную матрицу h_tilde.

230
00:48:14,510 --> 00:48:18,170


231
00:48:18,170 --> 00:48:24,740


232
00:48:29,450 --> 00:48:33,620


233
00:48:33,620 --> 00:48:40,100


234
00:48:40,100 --> 00:48:46,460


235
00:48:46,460 --> 00:48:53,030


236
00:48:53,030 --> 00:48:56,240


237
00:48:57,650 --> 00:49:00,890


238
00:49:00,890 --> 00:49:05,089


239
00:49:05,089 --> 00:49:09,529


240
00:49:09,529 --> 00:49:14,019


241
00:49:16,839 --> 00:49:28,130


242
00:49:28,130 --> 00:49:30,920


243
00:49:30,920 --> 00:49:35,420


244
00:49:35,420 --> 00:49:39,499


245
00:49:43,099 --> 00:49:47,749


246
00:49:50,329 --> 00:49:56,029


247
00:49:56,029 --> 00:50:00,579


248
00:50:00,579 --> 00:50:05,900


249
00:50:05,900 --> 00:50:09,140


250
00:50:09,140 --> 00:50:12,859


251
00:50:12,859 --> 00:50:17,119


252
00:50:17,119 --> 00:50:22,730


253
00:50:22,730 --> 00:50:28,309


254
00:50:28,309 --> 00:50:34,369


255
00:50:34,369 --> 00:50:39,259


256
00:50:39,259 --> 00:50:45,799


257
00:50:45,799 --> 00:50:49,130


258
00:50:49,130 --> 00:50:53,809


259
00:50:53,809 --> 00:50:57,920


260
00:50:57,920 --> 00:51:02,900


261
00:51:02,900 --> 00:51:08,150


262
00:51:08,150 --> 00:51:11,330


263
00:51:11,330 --> 00:51:16,520


264
00:51:16,520 --> 00:51:20,540


265
00:51:20,540 --> 00:51:30,260


266
00:51:30,260 --> 00:51:35,660


267
00:51:39,650 --> 00:51:46,490


268
00:51:49,880 --> 00:51:55,490


269
00:51:57,230 --> 00:52:03,880


270
00:52:03,880 --> 00:52:07,820


271
00:52:07,820 --> 00:52:11,510


272
00:52:11,510 --> 00:52:15,440


273
00:52:15,440 --> 00:52:21,590


274
00:52:21,590 --> 00:52:27,520


275
00:52:27,520 --> 00:52:32,330


276
00:52:32,330 --> 00:52:39,080


277
00:52:39,080 --> 00:52:47,120


278
00:52:47,120 --> 00:52:53,410


279
00:52:53,410 --> 00:53:02,060


280
00:53:02,060 --> 00:53:05,840


281
00:53:05,840 --> 00:53:09,980


282
00:53:09,980 --> 00:53:18,890


283
00:53:18,890 --> 00:53:24,650


284
00:53:24,650 --> 00:53:29,349


285
00:53:29,349 --> 00:53:36,859


286
00:53:44,930 --> 00:53:50,390


287
00:53:50,390 --> 00:53:55,190


288
00:53:55,190 --> 00:53:59,480


289
00:53:59,480 --> 00:54:02,599


290
00:54:02,599 --> 00:54:06,039


291
00:54:08,319 --> 00:54:15,650


292
00:54:15,650 --> 00:54:18,380


293
00:54:18,380 --> 00:54:24,890


294
00:54:27,109 --> 00:54:31,789


295
00:54:31,789 --> 00:54:37,339


296
00:54:40,130 --> 00:54:44,150


297
00:54:44,150 --> 00:54:50,869


298
00:54:50,869 --> 00:54:53,750


299
00:54:53,750 --> 00:54:58,220


300
00:54:58,220 --> 00:55:05,660


301
00:55:05,660 --> 00:55:09,829


302
00:55:09,829 --> 00:55:13,250


303
00:55:13,250 --> 00:55:19,490


304
00:55:19,490 --> 00:55:29,359


305
00:55:29,359 --> 00:55:35,839


306
00:55:35,839 --> 00:55:44,050


307
00:55:44,050 --> 00:55:49,220


308
00:55:52,130 --> 00:55:56,750


309
00:55:56,750 --> 00:56:01,640


310
00:56:01,640 --> 00:56:06,680


311
00:56:06,680 --> 00:56:16,609


312
00:56:16,609 --> 00:56:25,040


313
00:56:25,040 --> 00:56:31,099


314
00:56:31,099 --> 00:56:36,109


315
00:56:39,109 --> 00:56:43,490


316
00:56:43,490 --> 00:56:49,550


317
00:56:49,550 --> 00:56:54,859


318
00:56:56,930 --> 00:57:02,500


319
00:57:02,500 --> 00:57:08,720


320
00:57:08,720 --> 00:57:18,710


321
00:57:18,710 --> 00:57:21,920


322
00:57:21,920 --> 00:57:25,810


323
00:57:25,810 --> 00:57:34,040


324
00:57:34,040 --> 00:57:40,190


325
00:57:40,190 --> 00:57:45,380


326
00:57:45,380 --> 00:57:50,060


327
00:57:50,060 --> 00:57:52,369


328
00:57:52,369 --> 00:57:59,690


329
00:57:59,690 --> 00:58:02,900


330
00:58:02,900 --> 00:58:06,079


331
00:58:06,079 --> 00:58:10,460


332
00:58:10,460 --> 00:58:18,079


333
00:58:18,079 --> 00:58:22,039


334
00:58:22,039 --> 00:58:25,759


335
00:58:25,759 --> 00:58:31,880


336
00:58:31,880 --> 00:58:37,999


337
00:58:39,829 --> 00:58:44,329


338
00:58:44,329 --> 00:58:48,499


339
00:58:51,499 --> 00:58:57,410


340
00:58:57,410 --> 00:59:02,599


341
00:59:02,599 --> 00:59:06,410


342
00:59:07,819 --> 00:59:10,970


343
00:59:10,970 --> 00:59:17,420


344
00:59:17,420 --> 00:59:23,239


345
00:59:23,239 --> 00:59:28,970


346
00:59:31,579 --> 00:59:36,650


347
00:59:36,650 --> 00:59:43,400


348
00:59:43,400 --> 00:59:49,910


349
00:59:54,109 --> 01:00:01,819


350
01:00:01,819 --> 01:00:08,950


351
01:00:08,950 --> 01:00:14,330


352
01:00:14,330 --> 01:00:20,420


353
01:00:20,420 --> 01:00:24,050


354
01:00:24,050 --> 01:00:29,630


355
01:00:29,630 --> 01:00:36,410


356
01:00:38,570 --> 01:00:44,120


357
01:00:45,470 --> 01:00:53,210


358
01:00:53,210 --> 01:00:59,210


359
01:00:59,210 --> 01:01:03,740


360
01:01:06,830 --> 01:01:12,470


361
01:01:12,470 --> 01:01:15,860


362
01:01:15,860 --> 01:01:21,460


363
01:01:21,460 --> 01:01:27,410


364
01:01:27,410 --> 01:01:31,100


365
01:01:31,100 --> 01:01:34,730


366
01:01:34,730 --> 01:01:38,480


367
01:01:38,480 --> 01:01:42,320


368
01:01:42,320 --> 01:01:45,870


369
01:01:45,970 --> 01:01:48,502


370
01:01:48,602 --> 01:01:53,995
Давайте сделаем перерыв на 5 минут и вернёмся к компьютерному зрению в 7:45.

371
01:02:00,975 --> 01:02:05,040
Итак, мы наконец вернулись к компьютерному зрению.

372
01:02:09,140 --> 01:02:14,070
Соответствующий Jupyter ноутбук называется lesson7-citar.ipynb.

373
01:02:18,320 --> 01:02:34,220
CIFAR-10 - известный датасет изображений, появившийся задолго до ImageNet.

374
01:02:34,320 --> 01:02:47,060
Я считаю, что небольшие датасеты гораздо интереснее ImageNet.

375
01:02:47,060 --> 01:02:56,417
Большинству из вас придётся работать с тысячами изображений, а не с полутора миллионами ImageNet,

376
01:02:56,517 --> 01:03:01,170
поэтому полезно и интересно попрактиковаться.

377
01:03:01,270 --> 01:03:09,320
В большинстве прикладных задач, например, анализе медицинских снимков,

378
01:03:09,320 --> 01:03:17,175
размер изображения редко превышает 32 пикселя.

379
01:03:17,275 --> 01:03:23,997
CIFAR-10 состоит из маленьких изображений, и их немного,

380
01:03:24,097 --> 01:03:31,167
поэтому обучаться на них гораздо сложнее и интереснее, чем на изображениях ImageNet.

381
01:03:31,267 --> 01:03:41,240
Размер CIFAR-10 также позволяет обучаться гораздо быстрее, поэтому он хорош для тестирования алгоритмов.

382
01:03:43,310 --> 01:03:52,670
Многие исследователи жалуются, что у них нет средств на исследование различных версий алгоритма.

383
01:03:52,670 --> 01:04:02,930
Они используют ImageNet, поэтому каждое исследование - это неделя высокомощных вычислений.

384
01:04:02,930 --> 01:04:10,200
Я не понимаю, зачем тестировать алгоритмы на огромных датасетах вроде ImageNet.

385
01:04:10,200 --> 01:04:21,835
На этой неделе эта тема бурно обсуждается в связи с выступлением Ali Rahimi на NIPS 2017.

386
01:04:21,935 --> 01:04:30,060
Он говорил о том, что сейчас в глубоком обучении недостаточно смелых экспериментов.

387
01:04:30,060 --> 01:04:44,730
Мы обсуждали с ним эту тему и ещё не пришли к взаимопонимаю, но у нас общая забота -

388
01:04:44,730 --> 01:04:54,630
люди предпочитают брать побольше GPU и данных и скидывать в кучу, а не обдумывать и проектировать эксперименты.

389
01:04:54,630 --> 01:05:09,600
Если вы разрабатываете алгоритм для работы на больших датасетах, используйте маленькие для экспериментов -

390
01:05:09,600 --> 01:05:18,560
попробуйте различные версии алгоритма, уберите некоторые этапы, чтобы посмотреть, на что они влияют, и так далее.

391
01:05:18,560 --> 01:05:27,285
Многие жалуются на датасет MNIST, но его тоже хорошо использовать для того, чтобы понять,

392
01:05:27,385 --> 01:05:34,255
какие составляющие алгоритма имеют значение, а какие нет.

393
01:05:34,355 --> 01:05:39,415
Я думаю, что люди, которые жалуются на MNIST, просто рисуются -

394
01:05:39,615 --> 01:05:52,690
"Посмотрите на меня, я работаю в Google за $100,000 в неделю, у меня есть бесплатные GPU, я крутой".

395
01:05:52,790 --> 01:06:01,420
Кто-то в интернете преобразовал датасет в изображения, их можно скачать по этой ссылке, спасибо ему.

396
01:06:01,520 --> 01:06:07,045
Google перенаправит вас на менее удобный формат, используйте этот, он уже подготовлен для работы.

397
01:06:07,145 --> 01:06:09,330


398
01:06:09,330 --> 01:06:11,185


399
01:06:11,285 --> 01:06:13,140


400
01:06:13,140 --> 01:06:22,410


401
01:06:22,410 --> 01:06:26,430


402
01:06:26,430 --> 01:06:34,710


403
01:06:34,710 --> 01:06:41,070


404
01:06:41,070 --> 01:06:46,200


405
01:06:48,300 --> 01:06:53,040


406
01:06:53,040 --> 01:06:59,070


407
01:06:59,070 --> 01:07:04,230


408
01:07:04,230 --> 01:07:08,160


409
01:07:08,160 --> 01:07:11,970


410
01:07:11,970 --> 01:07:20,910


411
01:07:20,910 --> 01:07:26,990


412
01:07:26,990 --> 01:07:34,350


413
01:07:34,350 --> 01:07:41,660


414
01:07:41,660 --> 01:07:47,250


415
01:07:47,250 --> 01:07:53,190


416
01:07:53,190 --> 01:07:58,710


417
01:07:58,710 --> 01:08:05,250


418
01:08:05,250 --> 01:08:12,900


419
01:08:12,900 --> 01:08:18,660


420
01:08:18,660 --> 01:08:24,630


421
01:08:24,630 --> 01:08:28,650


422
01:08:31,560 --> 01:08:37,420


423
01:08:37,420 --> 01:08:45,580


424
01:08:45,580 --> 01:08:51,009


425
01:08:51,009 --> 01:08:57,969


426
01:08:57,969 --> 01:09:03,779


427
01:09:03,779 --> 01:09:12,040


428
01:09:14,109 --> 01:09:19,060


429
01:09:19,060 --> 01:09:22,900


430
01:09:25,179 --> 01:09:29,650


431
01:09:29,649 --> 01:09:35,499


432
01:09:35,500 --> 01:09:39,400


433
01:09:39,399 --> 01:09:47,349


434
01:09:47,350 --> 01:09:52,299


435
01:09:52,299 --> 01:09:57,040


436
01:09:57,040 --> 01:10:04,239


437
01:10:04,239 --> 01:10:07,179


438
01:10:07,179 --> 01:10:15,250


439
01:10:15,250 --> 01:10:21,820


440
01:10:21,820 --> 01:10:26,199


441
01:10:26,199 --> 01:10:30,429


442
01:10:30,429 --> 01:10:35,260


443
01:10:35,260 --> 01:10:39,280


444
01:10:42,640 --> 01:10:49,300


445
01:10:49,300 --> 01:10:53,530


446
01:10:53,530 --> 01:10:57,160


447
01:10:57,160 --> 01:11:01,630


448
01:11:01,630 --> 01:11:06,310


449
01:11:06,310 --> 01:11:14,440


450
01:11:16,090 --> 01:11:19,540


451
01:11:19,540 --> 01:11:24,640


452
01:11:24,640 --> 01:11:28,660


453
01:11:31,390 --> 01:11:38,590


454
01:11:38,590 --> 01:11:44,950


455
01:11:44,950 --> 01:11:50,100


456
01:11:50,100 --> 01:11:54,880


457
01:11:54,880 --> 01:12:00,940


458
01:12:05,820 --> 01:12:15,970


459
01:12:15,970 --> 01:12:19,600


460
01:12:19,600 --> 01:12:26,320


461
01:12:27,520 --> 01:12:32,920


462
01:12:32,920 --> 01:12:44,410


463
01:12:44,410 --> 01:12:50,230


464
01:12:50,230 --> 01:12:59,470


465
01:12:59,470 --> 01:13:03,800


466
01:13:03,800 --> 01:13:10,490


467
01:13:10,490 --> 01:13:15,560


468
01:13:15,560 --> 01:13:19,730


469
01:13:26,420 --> 01:13:31,520


470
01:13:31,520 --> 01:13:34,490


471
01:13:39,470 --> 01:13:44,600


472
01:13:44,600 --> 01:13:48,170


473
01:13:48,170 --> 01:13:52,040


474
01:13:52,040 --> 01:13:56,270


475
01:13:56,270 --> 01:14:00,350


476
01:14:00,350 --> 01:14:14,780


477
01:14:20,090 --> 01:14:25,130


478
01:14:25,130 --> 01:14:31,280


479
01:14:31,280 --> 01:14:37,220


480
01:14:37,220 --> 01:14:42,470


481
01:14:42,470 --> 01:14:47,420


482
01:14:47,420 --> 01:14:53,030


483
01:14:53,030 --> 01:15:01,010


484
01:15:01,010 --> 01:15:09,020


485
01:15:09,020 --> 01:15:13,120


486
01:15:17,020 --> 01:15:24,580


487
01:15:27,970 --> 01:15:34,150


488
01:15:37,180 --> 01:15:40,360


489
01:15:43,420 --> 01:15:54,210


490
01:15:54,210 --> 01:15:58,450


491
01:15:58,450 --> 01:16:02,980


492
01:16:02,980 --> 01:16:08,110


493
01:16:08,110 --> 01:16:13,390


494
01:16:13,390 --> 01:16:18,480


495
01:16:21,580 --> 01:16:26,230


496
01:16:26,230 --> 01:16:30,190


497
01:16:30,190 --> 01:16:34,170


498
01:16:34,170 --> 01:16:42,280


499
01:16:42,280 --> 01:16:46,600


500
01:16:46,600 --> 01:16:52,510


501
01:16:52,510 --> 01:16:56,950


502
01:16:56,950 --> 01:17:06,400


503
01:17:06,400 --> 01:17:12,340


504
01:17:12,340 --> 01:17:17,260


505
01:17:17,260 --> 01:17:24,730


506
01:17:26,410 --> 01:17:31,900


507
01:17:31,900 --> 01:17:35,440


508
01:17:35,440 --> 01:17:41,200


509
01:17:41,200 --> 01:17:46,870


510
01:17:46,870 --> 01:17:54,540


511
01:17:54,540 --> 01:18:01,540


512
01:18:01,540 --> 01:18:06,060


513
01:18:06,060 --> 01:18:12,760


514
01:18:12,760 --> 01:18:19,210


515
01:18:19,210 --> 01:18:28,750


516
01:18:28,750 --> 01:18:39,100


517
01:18:39,100 --> 01:18:47,500


518
01:18:47,500 --> 01:18:54,910


519
01:18:54,910 --> 01:19:01,420


520
01:19:01,420 --> 01:19:07,360


521
01:19:07,360 --> 01:19:15,700


522
01:19:15,700 --> 01:19:20,410


523
01:19:20,410 --> 01:19:23,860


524
01:19:23,860 --> 01:19:28,780


525
01:19:28,780 --> 01:19:36,550


526
01:19:36,550 --> 01:19:42,190


527
01:19:45,160 --> 01:19:51,700


528
01:19:51,700 --> 01:19:59,980


529
01:19:59,980 --> 01:20:07,120


530
01:20:07,120 --> 01:20:12,580


531
01:20:12,580 --> 01:20:19,060


532
01:20:19,060 --> 01:20:23,620


533
01:20:25,630 --> 01:20:30,360


534
01:20:31,350 --> 01:20:39,489


535
01:20:41,110 --> 01:20:46,660


536
01:20:46,660 --> 01:20:50,650


537
01:20:50,650 --> 01:20:54,430


538
01:20:54,430 --> 01:20:58,390


539
01:20:58,390 --> 01:21:06,930


540
01:21:06,930 --> 01:21:10,510


541
01:21:10,510 --> 01:21:14,260


542
01:21:14,260 --> 01:21:20,770


543
01:21:20,770 --> 01:21:28,510


544
01:21:28,510 --> 01:21:34,290


545
01:21:34,290 --> 01:21:43,180


546
01:21:43,180 --> 01:21:46,239


547
01:21:49,060 --> 01:21:53,140


548
01:21:53,140 --> 01:22:01,630


549
01:22:01,630 --> 01:22:07,780


550
01:22:07,780 --> 01:22:13,570


551
01:22:13,570 --> 01:22:18,280


552
01:22:18,280 --> 01:22:22,240


553
01:22:22,240 --> 01:22:25,870


554
01:22:25,870 --> 01:22:33,340


555
01:22:33,340 --> 01:22:40,170


556
01:22:42,060 --> 01:22:46,990


557
01:22:49,900 --> 01:22:55,420


558
01:22:55,420 --> 01:23:01,030


559
01:23:01,030 --> 01:23:09,070


560
01:23:09,070 --> 01:23:12,790


561
01:23:12,790 --> 01:23:18,760


562
01:23:18,760 --> 01:23:23,680


563
01:23:23,680 --> 01:23:27,700


564
01:23:32,080 --> 01:23:35,860


565
01:23:35,860 --> 01:23:41,140


566
01:23:41,140 --> 01:23:44,530


567
01:23:46,180 --> 01:23:50,080


568
01:23:50,080 --> 01:23:55,480


569
01:23:55,480 --> 01:24:00,550


570
01:24:00,550 --> 01:24:04,960


571
01:24:04,960 --> 01:24:09,700


572
01:24:09,700 --> 01:24:13,360


573
01:24:13,360 --> 01:24:18,250


574
01:24:18,250 --> 01:24:23,980


575
01:24:23,980 --> 01:24:29,350


576
01:24:29,350 --> 01:24:35,920


577
01:24:35,920 --> 01:24:40,030


578
01:24:40,030 --> 01:24:45,310


579
01:24:45,310 --> 01:24:50,440


580
01:24:52,330 --> 01:24:59,080


581
01:24:59,080 --> 01:25:06,130


582
01:25:06,130 --> 01:25:14,950


583
01:25:14,950 --> 01:25:21,150


584
01:25:21,150 --> 01:25:27,490


585
01:25:27,490 --> 01:25:31,750


586
01:25:31,750 --> 01:25:35,470


587
01:25:35,470 --> 01:25:41,200


588
01:25:41,200 --> 01:25:44,910


589
01:25:44,910 --> 01:25:51,700


590
01:25:51,700 --> 01:25:57,040


591
01:25:57,040 --> 01:26:02,470


592
01:26:02,470 --> 01:26:07,000


593
01:26:07,000 --> 01:26:11,560


594
01:26:11,560 --> 01:26:15,460


595
01:26:15,460 --> 01:26:20,530


596
01:26:20,530 --> 01:26:25,840


597
01:26:25,840 --> 01:26:32,500


598
01:26:32,500 --> 01:26:37,990


599
01:26:37,990 --> 01:26:41,260


600
01:26:41,260 --> 01:26:46,410


601
01:26:46,410 --> 01:26:52,360


602
01:26:52,360 --> 01:26:58,630


603
01:26:58,630 --> 01:27:11,200


604
01:27:11,200 --> 01:27:14,710


605
01:27:14,710 --> 01:27:17,620


606
01:27:17,620 --> 01:27:22,390


607
01:27:22,390 --> 01:27:29,680


608
01:27:29,680 --> 01:27:35,860


609
01:27:40,360 --> 01:27:44,620


610
01:27:44,620 --> 01:27:50,670


611
01:27:50,670 --> 01:27:57,610


612
01:27:57,610 --> 01:28:03,370


613
01:28:03,370 --> 01:28:06,460


614
01:28:06,460 --> 01:28:11,440


615
01:28:11,440 --> 01:28:16,960


616
01:28:16,960 --> 01:28:23,670


617
01:28:23,670 --> 01:28:30,810


618
01:28:30,810 --> 01:28:37,469


619
01:28:37,469 --> 01:28:42,179


620
01:28:42,179 --> 01:28:48,750


621
01:28:48,750 --> 01:28:56,610


622
01:28:56,610 --> 01:29:02,100


623
01:29:02,100 --> 01:29:09,150


624
01:29:11,159 --> 01:29:15,570


625
01:29:15,570 --> 01:29:22,739


626
01:29:22,739 --> 01:29:29,760


627
01:29:29,760 --> 01:29:35,880


628
01:29:35,880 --> 01:29:41,090


629
01:29:41,090 --> 01:29:47,250


630
01:29:47,250 --> 01:29:51,150


631
01:29:51,150 --> 01:29:54,540


632
01:29:54,540 --> 01:29:58,650


633
01:29:58,650 --> 01:30:03,030


634
01:30:03,030 --> 01:30:13,949


635
01:30:13,949 --> 01:30:21,659


636
01:30:21,659 --> 01:30:25,949


637
01:30:25,949 --> 01:30:31,409


638
01:30:31,409 --> 01:30:36,480


639
01:30:36,480 --> 01:30:42,420


640
01:30:42,420 --> 01:30:47,290


641
01:30:47,290 --> 01:30:52,180


642
01:30:52,180 --> 01:30:59,080


643
01:30:59,080 --> 01:31:05,500


644
01:31:05,500 --> 01:31:11,500


645
01:31:11,500 --> 01:31:17,400


646
01:31:17,400 --> 01:31:23,950


647
01:31:23,950 --> 01:31:28,780


648
01:31:28,780 --> 01:31:36,400


649
01:31:36,400 --> 01:31:40,600


650
01:31:40,600 --> 01:31:48,960


651
01:31:48,960 --> 01:31:56,650


652
01:31:56,650 --> 01:32:00,790


653
01:32:00,790 --> 01:32:09,570


654
01:32:09,570 --> 01:32:15,160


655
01:32:15,160 --> 01:32:21,790


656
01:32:21,790 --> 01:32:28,000


657
01:32:28,000 --> 01:32:34,270


658
01:32:34,270 --> 01:32:39,070


659
01:32:42,040 --> 01:32:50,740


660
01:32:50,740 --> 01:32:54,670


661
01:32:54,670 --> 01:33:00,139


662
01:33:00,139 --> 01:33:08,690


663
01:33:10,610 --> 01:33:20,330


664
01:33:20,330 --> 01:33:28,130


665
01:33:28,130 --> 01:33:33,080


666
01:33:33,080 --> 01:33:42,260


667
01:33:42,260 --> 01:33:45,440


668
01:33:52,790 --> 01:33:57,800


669
01:33:57,800 --> 01:34:02,600


670
01:34:02,600 --> 01:34:10,130


671
01:34:10,130 --> 01:34:16,219


672
01:34:16,219 --> 01:34:21,170


673
01:34:21,170 --> 01:34:27,820


674
01:34:27,820 --> 01:34:32,420


675
01:34:32,420 --> 01:34:37,310


676
01:34:37,310 --> 01:34:41,900


677
01:34:44,000 --> 01:34:50,000


678
01:34:50,000 --> 01:34:55,940


679
01:34:55,940 --> 01:35:01,570


680
01:35:01,570 --> 01:35:07,280


681
01:35:07,280 --> 01:35:10,750


682
01:35:10,750 --> 01:35:15,100


683
01:35:15,100 --> 01:35:23,080


684
01:35:23,080 --> 01:35:28,300


685
01:35:28,300 --> 01:35:33,340


686
01:35:33,340 --> 01:35:36,790


687
01:35:36,790 --> 01:35:42,010


688
01:35:42,010 --> 01:35:48,310


689
01:35:48,310 --> 01:35:55,300


690
01:35:55,300 --> 01:36:00,550


691
01:36:00,550 --> 01:36:06,250


692
01:36:06,250 --> 01:36:11,320


693
01:36:11,320 --> 01:36:18,850


694
01:36:18,850 --> 01:36:23,620


695
01:36:25,030 --> 01:36:28,960


696
01:36:28,960 --> 01:36:36,190


697
01:36:36,190 --> 01:36:40,540


698
01:36:40,540 --> 01:36:47,650


699
01:36:47,650 --> 01:36:53,050


700
01:36:53,050 --> 01:36:57,640


701
01:36:57,640 --> 01:37:05,770


702
01:37:05,770 --> 01:37:12,880


703
01:37:12,880 --> 01:37:20,380


704
01:37:20,380 --> 01:37:24,130


705
01:37:24,130 --> 01:37:28,900


706
01:37:30,940 --> 01:37:35,139


707
01:37:35,139 --> 01:37:39,340


708
01:37:39,340 --> 01:37:45,520


709
01:37:45,520 --> 01:37:51,099


710
01:37:51,099 --> 01:37:59,679


711
01:37:59,679 --> 01:38:05,170


712
01:38:05,170 --> 01:38:12,099


713
01:38:12,099 --> 01:38:16,329


714
01:38:16,329 --> 01:38:19,690


715
01:38:19,690 --> 01:38:35,440


716
01:38:35,440 --> 01:38:40,630


717
01:38:40,630 --> 01:38:44,260


718
01:38:44,260 --> 01:38:49,480


719
01:38:52,719 --> 01:38:58,389


720
01:38:58,389 --> 01:39:03,699


721
01:39:06,610 --> 01:39:12,309


722
01:39:12,309 --> 01:39:21,400


723
01:39:25,320 --> 01:39:28,929


724
01:39:28,929 --> 01:39:34,840


725
01:39:34,840 --> 01:39:41,749


726
01:39:41,749 --> 01:39:48,800


727
01:39:48,800 --> 01:39:55,729


728
01:39:55,729 --> 01:39:59,840


729
01:39:59,840 --> 01:40:04,239


730
01:40:06,979 --> 01:40:10,039


731
01:40:12,380 --> 01:40:16,849


732
01:40:16,849 --> 01:40:20,510


733
01:40:20,510 --> 01:40:25,849


734
01:40:25,849 --> 01:40:29,150


735
01:40:31,099 --> 01:40:35,539


736
01:40:35,539 --> 01:40:38,539


737
01:40:38,539 --> 01:40:42,709


738
01:40:42,709 --> 01:40:46,689


739
01:40:46,989 --> 01:40:53,300


740
01:40:53,300 --> 01:41:02,749


741
01:41:02,749 --> 01:41:11,599


742
01:41:11,599 --> 01:41:17,530


743
01:41:17,530 --> 01:41:23,570


744
01:41:23,570 --> 01:41:29,539


745
01:41:29,539 --> 01:41:33,199


746
01:41:33,199 --> 01:41:39,530


747
01:41:39,530 --> 01:41:43,219


748
01:41:43,219 --> 01:41:45,559


749
01:41:45,559 --> 01:41:53,239


750
01:41:55,570 --> 01:42:01,489


751
01:42:01,489 --> 01:42:05,180


752
01:42:05,180 --> 01:42:10,989


753
01:42:14,690 --> 01:42:19,220


754
01:42:19,220 --> 01:42:24,230


755
01:42:26,570 --> 01:42:30,980


756
01:42:30,980 --> 01:42:38,150


757
01:42:38,150 --> 01:42:43,550


758
01:42:43,550 --> 01:42:47,030


759
01:42:47,030 --> 01:42:50,239


760
01:42:50,239 --> 01:42:57,350


761
01:42:57,350 --> 01:43:01,220


762
01:43:01,220 --> 01:43:04,040


763
01:43:04,040 --> 01:43:08,390


764
01:43:08,390 --> 01:43:13,370


765
01:43:13,370 --> 01:43:17,800


766
01:43:17,800 --> 01:43:24,739


767
01:43:26,360 --> 01:43:31,070


768
01:43:31,070 --> 01:43:35,630


769
01:43:35,630 --> 01:43:38,420


770
01:43:38,420 --> 01:43:43,220


771
01:43:43,220 --> 01:43:45,890


772
01:43:45,890 --> 01:43:49,520


773
01:43:49,520 --> 01:43:52,670


774
01:43:52,670 --> 01:43:59,060


775
01:43:59,060 --> 01:44:02,540


776
01:44:02,540 --> 01:44:06,980


777
01:44:06,980 --> 01:44:09,740


778
01:44:09,740 --> 01:44:16,970


779
01:44:16,970 --> 01:44:21,560


780
01:44:21,560 --> 01:44:25,370


781
01:44:27,080 --> 01:44:31,970


782
01:44:31,970 --> 01:44:35,720


783
01:44:35,720 --> 01:44:41,540


784
01:44:41,540 --> 01:44:46,250


785
01:44:46,250 --> 01:44:50,780


786
01:44:50,780 --> 01:44:57,230


787
01:44:57,230 --> 01:45:01,520


788
01:45:01,520 --> 01:45:08,510


789
01:45:08,510 --> 01:45:11,300


790
01:45:11,300 --> 01:45:16,820


791
01:45:18,620 --> 01:45:25,580


792
01:45:25,580 --> 01:45:30,290


793
01:45:30,290 --> 01:45:35,300


794
01:45:35,300 --> 01:45:42,820


795
01:45:46,480 --> 01:45:54,320


796
01:45:54,320 --> 01:45:59,690


797
01:45:59,690 --> 01:46:06,740


798
01:46:06,740 --> 01:46:12,880


799
01:46:12,880 --> 01:46:18,199


800
01:46:22,280 --> 01:46:26,540


801
01:46:26,540 --> 01:46:32,989


802
01:46:32,989 --> 01:46:37,340


803
01:46:37,340 --> 01:46:41,780


804
01:46:41,780 --> 01:46:47,540


805
01:46:47,540 --> 01:46:57,040


806
01:46:57,670 --> 01:47:06,199


807
01:47:09,530 --> 01:47:15,560


808
01:47:15,560 --> 01:47:19,850


809
01:47:19,850 --> 01:47:25,010


810
01:47:25,010 --> 01:47:31,489


811
01:47:32,870 --> 01:47:36,920


812
01:47:36,920 --> 01:47:43,520


813
01:47:43,520 --> 01:47:49,219


814
01:47:49,219 --> 01:47:58,670


815
01:47:58,670 --> 01:48:04,460


816
01:48:04,460 --> 01:48:08,120


817
01:48:08,120 --> 01:48:14,840


818
01:48:14,840 --> 01:48:23,360


819
01:48:26,090 --> 01:48:29,630


820
01:48:29,630 --> 01:48:41,810


821
01:48:41,810 --> 01:48:49,520


822
01:48:49,520 --> 01:48:57,590


823
01:48:57,590 --> 01:49:04,730


824
01:49:08,260 --> 01:49:14,389


825
01:49:14,389 --> 01:49:19,429


826
01:49:19,429 --> 01:49:25,219


827
01:49:25,219 --> 01:49:31,400


828
01:49:31,400 --> 01:49:39,380


829
01:49:39,380 --> 01:49:44,599


830
01:49:44,599 --> 01:49:50,119


831
01:49:50,690 --> 01:49:55,099


832
01:49:55,099 --> 01:50:00,380


833
01:50:00,380 --> 01:50:04,550


834
01:50:04,550 --> 01:50:10,550


835
01:50:10,550 --> 01:50:14,690


836
01:50:17,179 --> 01:50:24,080


837
01:50:24,080 --> 01:50:34,040


838
01:50:34,040 --> 01:50:38,510


839
01:50:38,510 --> 01:50:42,680


840
01:50:42,680 --> 01:50:46,310


841
01:50:46,310 --> 01:50:52,880


842
01:50:52,880 --> 01:51:01,430


843
01:51:01,430 --> 01:51:07,220


844
01:51:07,220 --> 01:51:13,070


845
01:51:14,960 --> 01:51:21,650


846
01:51:21,650 --> 01:51:26,090


847
01:51:26,090 --> 01:51:31,580


848
01:51:31,580 --> 01:51:38,960


849
01:51:38,960 --> 01:51:43,340


850
01:51:43,340 --> 01:51:53,960


851
01:51:53,960 --> 01:52:00,890


852
01:52:00,890 --> 01:52:06,050


853
01:52:06,050 --> 01:52:13,460


854
01:52:13,460 --> 01:52:20,270


855
01:52:20,270 --> 01:52:30,860


856
01:52:34,520 --> 01:52:38,120


857
01:52:40,100 --> 01:52:45,410


858
01:52:49,190 --> 01:52:55,160


859
01:52:55,160 --> 01:53:00,710


860
01:53:00,710 --> 01:53:06,910


861
01:53:06,910 --> 01:53:11,120


862
01:53:11,120 --> 01:53:17,739


863
01:53:17,739 --> 01:53:25,070


864
01:53:25,070 --> 01:53:30,699


865
01:53:31,480 --> 01:53:40,100


866
01:53:40,100 --> 01:53:44,690


867
01:53:48,380 --> 01:54:00,880


868
01:54:00,880 --> 01:54:09,650


869
01:54:11,739 --> 01:54:19,760


870
01:54:19,760 --> 01:54:28,550


871
01:54:28,550 --> 01:54:31,120


872
01:54:35,910 --> 01:54:44,890


873
01:54:44,890 --> 01:54:48,700


874
01:54:51,430 --> 01:54:56,230


875
01:54:59,670 --> 01:55:08,760


876
01:55:08,760 --> 01:55:16,900


877
01:55:18,910 --> 01:55:25,180


878
01:55:25,180 --> 01:55:29,440


879
01:55:31,660 --> 01:55:35,710


880
01:55:35,710 --> 01:55:40,900


881
01:55:40,900 --> 01:55:46,510


882
01:55:46,510 --> 01:55:51,760


883
01:55:51,760 --> 01:55:56,470


884
01:55:56,470 --> 01:56:00,430


885
01:56:00,430 --> 01:56:04,450


886
01:56:06,640 --> 01:56:14,440


887
01:56:16,420 --> 01:56:20,580


888
01:56:20,580 --> 01:56:32,910


889
01:56:36,560 --> 01:56:41,900


890
01:56:44,130 --> 01:56:53,390


891
01:56:53,510 --> 01:56:59,909


892
01:57:02,370 --> 01:57:07,170


893
01:57:07,170 --> 01:57:11,790


894
01:57:11,790 --> 01:57:15,600


895
01:57:15,600 --> 01:57:19,860


896
01:57:22,860 --> 01:57:29,489


897
01:57:29,489 --> 01:57:37,560


898
01:57:37,560 --> 01:57:41,699


899
01:57:45,360 --> 01:57:54,050


900
01:57:54,050 --> 01:58:03,510


901
01:58:03,510 --> 01:58:09,540


902
01:58:09,540 --> 01:58:14,760


903
01:58:14,760 --> 01:58:19,380


904
01:58:19,380 --> 01:58:26,520


905
01:58:26,520 --> 01:58:35,550


906
01:58:35,550 --> 01:58:40,040


907
01:58:40,040 --> 01:58:47,959


908
01:58:47,959 --> 01:58:56,900


909
01:58:56,900 --> 01:59:04,249


910
01:59:04,249 --> 01:59:09,860


911
01:59:09,860 --> 01:59:14,840


912
01:59:14,840 --> 01:59:19,099


913
01:59:19,099 --> 01:59:23,959


914
01:59:23,959 --> 01:59:27,260


915
01:59:27,260 --> 01:59:31,820


916
01:59:35,630 --> 01:59:41,229


917
01:59:41,229 --> 01:59:47,570


918
01:59:49,789 --> 01:59:54,979


919
01:59:54,979 --> 02:00:00,110


920
02:00:02,479 --> 02:00:08,090


921
02:00:08,090 --> 02:00:13,189


922
02:00:13,189 --> 02:00:17,900


923
02:00:19,369 --> 02:00:26,389


924
02:00:26,389 --> 02:00:32,420


925
02:00:32,420 --> 02:00:37,849


926
02:00:37,849 --> 02:00:42,380


927
02:00:42,380 --> 02:00:46,309


928
02:00:46,309 --> 02:00:52,489


929
02:00:53,959 --> 02:00:57,979


930
02:00:57,979 --> 02:01:04,000


931
02:01:04,000 --> 02:01:13,780


932
02:01:13,780 --> 02:01:19,120


933
02:01:19,120 --> 02:01:25,840


934
02:01:25,840 --> 02:01:31,210


935
02:01:31,210 --> 02:01:36,040


936
02:01:36,040 --> 02:01:40,840


937
02:01:40,840 --> 02:01:45,520


938
02:01:45,520 --> 02:01:49,510


939
02:01:49,510 --> 02:01:54,280


940
02:01:56,410 --> 02:02:00,040


941
02:02:00,040 --> 02:02:09,730


942
02:02:09,730 --> 02:02:14,710


943
02:02:14,710 --> 02:02:20,590


944
02:02:20,590 --> 02:02:24,490


945
02:02:24,490 --> 02:02:29,680


946
02:02:29,680 --> 02:02:34,900


947
02:02:38,890 --> 02:02:46,330


948
02:02:46,330 --> 02:02:50,260


949
02:02:50,260 --> 02:02:54,070


950
02:02:56,590 --> 02:03:01,780


951
02:03:01,780 --> 02:03:09,370


952
02:03:09,370 --> 02:03:13,080


953
02:03:13,080 --> 02:03:19,170


954
02:03:19,170 --> 02:03:25,650


955
02:03:25,650 --> 02:03:29,699


956
02:03:29,699 --> 02:03:36,960


957
02:03:36,960 --> 02:03:41,760


958
02:03:41,760 --> 02:03:49,770


959
02:03:49,770 --> 02:03:54,960


960
02:03:54,960 --> 02:03:58,890


961
02:03:58,890 --> 02:04:02,160


962
02:04:04,290 --> 02:04:10,080


963
02:04:10,080 --> 02:04:16,170


964
02:04:16,170 --> 02:04:22,560


965
02:04:22,560 --> 02:04:26,820


966
02:04:28,560 --> 02:04:38,100


967
02:04:38,100 --> 02:04:44,390


968
02:04:44,390 --> 02:04:49,530


969
02:04:53,489 --> 02:05:04,110


970
02:05:04,110 --> 02:05:18,270


971
02:05:18,270 --> 02:05:25,050


972
02:05:25,050 --> 02:05:27,960


973
02:05:27,960 --> 02:05:31,290


974
02:05:31,290 --> 02:05:34,710


975
02:05:34,710 --> 02:05:39,270


976
02:05:39,270 --> 02:05:47,250


977
02:05:47,250 --> 02:05:50,849


978
02:05:50,849 --> 02:05:57,199


979
02:06:01,230 --> 02:06:06,329


980
02:06:06,329 --> 02:06:12,449


981
02:06:12,449 --> 02:06:19,679


982
02:06:19,679 --> 02:06:25,980


983
02:06:30,150 --> 02:06:35,550


984
02:06:35,550 --> 02:06:39,869


985
02:06:41,909 --> 02:06:45,449


986
02:06:45,449 --> 02:06:49,619


987
02:06:49,619 --> 02:06:54,869


988
02:06:54,869 --> 02:07:00,239


989
02:07:00,239 --> 02:07:03,750


990
02:07:03,750 --> 02:07:06,210


991
02:07:06,210 --> 02:07:09,900


992
02:07:09,900 --> 02:07:15,420


993
02:07:15,420 --> 02:07:19,619


994
02:07:19,619 --> 02:07:24,090


995
02:07:24,090 --> 02:07:32,550


996
02:07:32,550 --> 02:07:36,599


997
02:07:36,599 --> 02:07:43,320


998
02:07:43,320 --> 02:07:48,180


999
02:07:48,180 --> 02:07:53,280


1000
02:07:55,470 --> 02:08:03,900


1001
02:08:06,810 --> 02:08:11,610


1002
02:08:13,230 --> 02:08:16,920


1003
02:08:16,920 --> 02:08:21,240


1004
02:08:21,240 --> 02:08:26,160


1005
02:08:26,160 --> 02:08:30,840


1006
02:08:30,840 --> 02:08:38,450


1007
02:08:38,450 --> 02:08:41,970


1008
02:08:41,970 --> 02:08:46,440


1009
02:08:49,350 --> 02:08:53,700


1010
02:08:56,160 --> 02:09:06,120


1011
02:09:06,120 --> 02:09:11,850


1012
02:09:11,850 --> 02:09:16,320


1013
02:09:19,290 --> 02:09:24,750


1014
02:09:24,750 --> 02:09:28,980


1015
02:09:28,980 --> 02:09:33,990


1016
02:09:33,990 --> 02:09:41,190


1017
02:09:41,190 --> 02:09:48,450


1018
02:09:48,450 --> 02:10:00,210


1019
02:10:00,210 --> 02:10:09,420


1020
02:10:09,420 --> 02:10:14,940


1021
02:10:17,180 --> 02:10:23,430


1022
02:10:23,430 --> 02:10:27,180


1023
02:10:27,180 --> 02:10:30,650


1024
02:10:38,340 --> 02:10:42,780


1025
02:10:42,780 --> 02:10:46,770


1026
02:10:46,770 --> 02:10:54,570


1027
02:10:58,770 --> 02:11:09,650


1028
02:11:12,360 --> 02:11:17,790


1029
02:11:17,790 --> 02:11:23,720


1030
02:11:23,720 --> 02:11:30,630


1031
02:11:30,630 --> 02:11:36,120


1032
02:11:36,120 --> 02:11:41,100


1033
02:11:41,100 --> 02:11:50,010


1034
02:11:50,010 --> 02:11:56,550


1035
02:11:56,550 --> 02:12:01,590


1036
02:12:01,590 --> 02:12:05,630


1037
02:12:05,630 --> 02:12:10,730


1038
02:12:13,449 --> 02:12:19,969


1039
02:12:19,969 --> 02:12:23,449


1040
02:12:23,449 --> 02:12:30,800


1041
02:12:30,800 --> 02:12:35,659


1042
02:12:39,020 --> 02:12:43,520


1043
02:12:43,520 --> 02:12:48,860


1044
02:12:51,110 --> 02:12:57,020


1045
02:12:57,020 --> 02:13:01,850


1046
02:13:01,850 --> 02:13:06,530


1047
02:13:10,940 --> 02:13:13,969


1048
02:13:13,969 --> 02:13:18,199


1049
02:13:18,199 --> 02:13:23,780


1050
02:13:23,780 --> 02:13:31,810


1051
02:13:31,960 --> 02:13:37,370


1052
02:13:37,370 --> 02:13:42,980


1053
02:13:42,980 --> 02:13:48,880


1054
02:13:48,880 --> 02:13:52,250


1055
02:13:52,250 --> 02:13:59,060


1056
02:13:59,060 --> 02:14:04,159


1057
02:14:10,909 --> 02:14:15,980


1058
02:14:15,980 --> 02:14:19,340


1059
02:14:19,340 --> 02:14:22,760


1060
02:14:22,760 --> 02:14:31,699


1061
02:14:34,719 --> 02:14:42,050


1062
02:14:42,050 --> 02:14:47,360


1063
02:14:50,090 --> 02:14:57,260


1064
02:14:57,260 --> 02:15:00,710


1065
02:15:00,710 --> 02:15:05,810


1066
02:15:05,810 --> 02:15:10,849


1067
02:15:10,849 --> 02:15:14,210


1068
02:15:14,210 --> 02:15:18,800


1069
02:15:18,800 --> 02:15:23,750


1070
02:15:23,750 --> 02:15:27,980


1071
02:15:27,980 --> 02:15:32,179


1072
02:15:32,179 --> 02:15:36,980


1073
02:15:39,500 --> 02:15:42,590


1074
02:15:42,590 --> 02:15:45,770


1075
02:15:45,770 --> 02:15:50,810


1076
02:15:50,810 --> 02:15:53,630


1077
02:15:53,630 --> 02:16:00,020


1078
02:16:00,020 --> 02:16:04,159


1079
02:16:07,070 --> 02:16:10,659


1080
02:16:10,659 --> 02:16:17,030


1081
02:16:17,030 --> 02:16:20,719


1082
02:16:20,719 --> 02:16:25,849


1083
02:16:25,849 --> 02:16:32,381


1084
02:16:32,380 --> 02:16:38,679


1085
02:16:38,680 --> 02:16:42,251


1086
02:16:42,251 --> 02:16:46,720


1087
02:16:46,719 --> 02:16:51,460


1088
02:16:51,460 --> 02:16:55,689


1089
02:16:55,690 --> 02:16:58,600


1090
02:16:58,600 --> 02:17:03,581


1091
02:17:03,581 --> 02:17:06,671


1092
02:17:06,671 --> 02:17:10,001


1093
02:17:10,001 --> 02:17:13,990


1094
02:17:13,990 --> 02:17:21,190


1095
02:17:21,190 --> 02:17:24,190


1096
02:17:24,190 --> 02:17:29,270


1097
02:17:29,270 --> 02:17:32,379


