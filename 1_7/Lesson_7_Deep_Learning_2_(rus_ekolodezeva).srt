1
00:00:00,050 --> 00:00:04,965
Добро пожаловать на последнюю лекцию первой части курса.

2
00:00:05,065 --> 00:00:12,484
Темы первой части курса — классификация и регрессия с использованием глубокого обучения

3
00:00:12,584 --> 00:00:19,439
с уклоном в изучение лучших практик.

4
00:00:19,439 --> 00:00:25,500
Мы начали с использования непонятных трёх строк кода для решения задачи классификации изображений,

5
00:00:25,500 --> 00:00:36,420
потом перешли к NLP, структурированным данным и коллаборативной фильтрации,

6
00:00:36,420 --> 00:00:44,450
уточняя необходимые теоретические моменты и обучаясь создавать хорошо работающие модели.

7
00:00:44,450 --> 00:00:51,400
Последние три лекции курса мы посвятили более глубокому изучению этих тем в другом порядке —

8
00:00:51,500 --> 00:01:02,430
поняли, как всё работает, посмотрели на код и реализовали большинство элементов с нуля.

9
00:01:02,430 --> 00:01:17,875
Во второй части курса мы отойдём от задач классификации и регрессии, которые предсказывают несколько чисел,

10
00:01:17,975 --> 00:01:25,259
и перейдём к генеративным моделям, создающим большие объёмы данных —

11
00:01:25,259 --> 00:01:34,659
связные предложение при переводе, или описании изображений, или ответах на вопросы;

12
00:01:34,759 --> 00:01:42,469
изображения при передаче художественного стиля, семантические сегменты и так далее.

13
00:01:42,569 --> 00:02:18,400
Во второй части курса мы перейдём от испытанных техник к изучению непроверенных методов из свежих статей.

14
00:02:18,500 --> 00:02:33,789
Если вы хотите научиться читать статьи и реализовывать новейшие методы, пройдите вторую часть курса.

15
00:02:33,889 --> 00:02:39,325
Вторая часть, как и первая, не требует математики выше школьного уровня,

16
00:02:39,425 --> 00:02:49,869
но предполагается, что у вас есть время и терпение разбираться с необходимыми идеями и воплощать их в коде.

17
00:02:49,969 --> 00:03:00,540
Самое сложное в преподавании RNN — убедить студентов, что рекуррентные нейронные сети

18
00:03:00,540 --> 00:03:08,709
ничем не отличаются от обычных полносвязных нейронных сетей.

19
00:03:08,809 --> 00:03:15,750
Так выглядит полносвязная нейронная сеть.

20
00:03:15,750 --> 00:03:25,984
Напомню, что стрелки показывают операции слоя — как правило, связку линейный слой + нелинейная функция активации.

21
00:03:26,084 --> 00:03:33,139
Здесь это умножение матриц + выпрямитель или гиперболический тангенс.

22
00:03:33,239 --> 00:03:40,054
Стрелки одного цвета используют одну матрицу весов.

23
00:03:40,154 --> 00:03:45,369
Единственное отличие от полносвязных сетей, которые мы видели раньше —

24
00:03:45,469 --> 00:03:54,389
есть несколько входных слоёв в разных местах сети.

25
00:03:54,389 --> 00:04:01,180
Мы пробовали прибавлять входные данные к активациям полносвязного слоя, потом — склеивать их вместе.

26
00:04:01,280 --> 00:04:06,540
Принцип работы модели не меняется из-за наличия нескольких входных слоёв.

27
00:04:06,640 --> 00:04:15,030
Эта версия модели реализована в классе Char3Model.

28
00:04:15,030 --> 00:04:19,909
Здесь заданы стрелки разных цветов как три матрицы весов,

29
00:04:19,909 --> 00:04:29,874
конструктор nn.Linear() объединяет матрицу весов и вектор смещений.

30
00:04:29,974 --> 00:04:46,730
Мы создали эмбеддинги, пропустили их через линейные и скрытые слои.

31
00:04:46,830 --> 00:04:57,270
В первый слой не входила оранжевая стрелка, поэтому мы сместили первый входной слой

32
00:04:57,270 --> 00:05:02,009
и добавили пустую матрицу, чтобы все входные слои обрабатывались одинаково.

33
00:05:02,009 --> 00:05:16,389
В классе CharLoopModel мы обернули обработку входных данных в цикл.

34
00:05:16,489 --> 00:05:30,750
Также мы увеличили количество символов с 3 до 8, с циклом это легко сделать.

35
00:05:30,750 --> 00:05:41,380
Класс CharLoopModel — то же самое, что и касс Char3Model, только символов теперь 8, а не 3.

36
00:05:41,480 --> 00:05:51,750
В классе CharRnn заменили петлю цикла на единственный вызов конструктора nn.RNN(),

37
00:05:51,750 --> 00:06:04,299
который также сохраняет активации скрытых слоёв в переменной h.

38
00:06:04,399 --> 00:06:12,560
Это тоже был просто рефакторинг, не меняющий принципа работы модели.

39
00:06:14,000 --> 00:06:21,210
После этого слегка сократили время обучения.

40
00:06:21,210 --> 00:06:49,760
Мы разрезали датасет отзывов к фильмам на отрезки по 8 символов и предсказывали девятый.

41
00:06:49,860 --> 00:07:13,420
Чтобы использовать все данные, раньше для предсказания использовались накладывающиеся отрезки.

42
00:07:13,420 --> 00:07:24,670
Это вычислительно неэффективно, потому что приходится очень много пересчитывать.

43
00:07:24,670 --> 00:07:34,065
Мы разделили данные на ненакладывающиеся отрезки

44
00:07:34,165 --> 00:07:53,440
и по каждым 8 символам стали предсказывать 8 следующих символов, а не 1.

45
00:07:53,440 --> 00:08:06,335
Посмотрев на первый символ, предсказываем второй; посмотрев на второй — третий и так далее.

46
00:08:06,435 --> 00:08:30,070
Кто-то из вас высказал правильное замечание, что в начале отрезка матрица активаций h обнуляется,

47
00:08:30,070 --> 00:08:48,820
поэтому использующее её предсказание по первому символу отрезка будет очень неточным.

48
00:08:48,920 --> 00:09:08,520
После этого мы решили не выбрасывать h на каждом минибатче, то есть при каждом вызове метода forward().

49
00:09:08,889 --> 00:09:23,520
Матрица h — состояние скрытого слоя, оранжевые круги, они обнулялись после каждого минибатча,

50
00:09:23,520 --> 00:09:34,800
как будто все предыдущие действия не имели значения, это неправильно.

51
00:09:34,800 --> 00:09:58,205
Мы передвинем строку обнуления матрицы h в конструктор и занесём её в поле класса, чтобы отслеживать изменения.

52
00:09:58,305 --> 00:10:12,029
Эта версия модели реализована в классе CharSeqStatefulRnn.

53
00:10:12,029 --> 00:10:29,904
В конструкторе вызывается метод init_hidden(), создающий матрицу h и заполняющий её нулями.

54
00:10:30,004 --> 00:10:40,542
Матрица h теперь лежит в поле класса self.h и передаётся в параметры метода self.rnn().

55
00:10:40,642 --> 00:10:49,675
Обновлённые активации h заносятся в матрицу self.h строкой self.h = repackage_var(h).

56
00:10:49,775 --> 00:10:53,910
Функция repackage_var(h) — полезный трюк №1.

57
00:10:53,910 --> 00:11:02,550
Допустим, мы не используем её и просто делаем присваивание self.h = h.

58
00:11:02,550 --> 00:11:16,569
Допустим, что размер датасета — миллион символов, тогда в нашей сети миллион оранжевых кругов.

59
00:11:16,669 --> 00:11:38,560
Модель выглядит так — для каждого входного слоя есть свой выходной слой.

60
00:11:38,660 --> 00:11:45,650
При вычислении градиентов используется метод обратного распространения ошибки,

61
00:11:45,750 --> 00:11:56,412
то есть мы идём в обратном направлении и смотрим на ошибку на каждом слое,

62
00:11:56,512 --> 00:12:05,550
а потом меняем веса в соответствии с этими ошибками.

63
00:12:05,550 --> 00:12:30,984
Если в датасете миллион символов, RNN будет состоять из миллиона полносвязных слоёв, вот эта модель.

64
00:12:31,084 --> 00:13:01,680
Такая модель занимает очень много памяти, потому что каждый минибатч мы запоминаем и перемножаем миллион градиентов.

65
00:13:05,399 --> 00:13:25,359
Чтобы этого избежать, мы сохраняем только значения матрицы состояния скрытого слоя, но не градиенты.

66
00:13:25,459 --> 00:13:34,194
Это делает функция repackage_var().

67
00:13:34,294 --> 00:13:47,169
Она возвращает значение матрицы активаций h.data без градиентов,

68
00:13:47,269 --> 00:14:01,149
поэтому метод обратного распространения ошибки останавливаеется на этой переменной.

69
00:14:01,249 --> 00:14:06,266
Функция вызывается при каждом вызове метода forward().

70
00:14:06,366 --> 00:14:13,740
На каждом отрезке 8 символов пройдут через 8 слоёв, на слоях отработает метод обратного распространения ошибки,

71
00:14:13,740 --> 00:14:23,824
а потом сохранятся значения матрицы h без истории проведённых операций.

72
00:14:23,924 --> 00:14:30,055
Это называется метод обратного распространения ошибки во времени (backpropagation through time, BPTT).

73
00:14:30,155 --> 00:14:38,399
В статьях в интернете это выглядит как новый сложнейший алгоритм или какое-то открытие,

74
00:14:38,399 --> 00:14:52,599
а на самом деле — это просто отбрасывание истории состояния скрытого слоя в конце минибатча.

75
00:14:52,699 --> 00:14:59,560
Это полезный трюк №1, функция repackage_var().

76
00:14:59,660 --> 00:15:18,990
Если помните, на четвёртой лекции у нас был параметр bptt — это количество слоёв, после которых история стирается.

77
00:15:18,990 --> 00:15:23,699
Нужно стараться уменьшить количество слоёв обратного распространения ошибки —

78
00:15:23,699 --> 00:15:37,029
модель будет легче обучаться и не так сильно страдать от нестабильности градиентов.

79
00:15:37,029 --> 00:15:43,479
С другой стороны, это количество должно быть достаточно большим,

80
00:15:43,479 --> 00:15:53,109
чтобы модель успевала выучить нужные закономерности.

81
00:15:53,109 --> 00:15:59,699
При создании RNN нужно подбирать значение параметра bptt.

82
00:16:00,389 --> 00:16:13,319
Полезный трюк №2 — как эффективно передавать данные в модель в соответствии с нашей схемой.

83
00:16:13,419 --> 00:16:24,936
Напомню, что мы рассматриваем последовательно каждый отрезок,

84
00:16:25,036 --> 00:16:33,069
но для ускорения вычислений хочется обсчитывать много отрезков одновременно.

85
00:16:35,970 --> 00:17:09,180
Мы будем параллельно обсчитывать группы отрезков.

86
00:17:09,280 --> 00:17:28,520
Различные отрезки будут независимо учитываться в матрице состояния скрытого слоя.

87
00:17:28,520 --> 00:17:46,275
После обучения на первых отрезках мы перейдём к следующим и продолжим обсчитывать их параллельно.

88
00:17:46,375 --> 00:17:56,450
Это должно напомнить вам начало работы с torchtext.

89
00:17:56,450 --> 00:18:15,200
Мы обсуждали процесс создания минибатчей. Полный текст (все сочинения Ницше или все обзоры IMDb)

90
00:18:15,200 --> 00:18:39,990
разбивается на 64 одинаковых части. Не на части длиной 64, а на 64 одинаковых части.

91
00:18:40,090 --> 00:18:55,240
Если полный текст имеет длину 64 миллиона символов, каждая из 64 частей содержит 1 миллион символов.

92
00:18:55,240 --> 00:19:13,410
Полученные части располагаются одна под другой, получается матрица высотой 64 символа и шириной 1 миллион.

93
00:19:13,510 --> 00:19:32,475
Минибатч — идущие подряд bptt столбцов этой матрицы, bptt был около 70.

94
00:19:32,575 --> 00:20:02,580
Для всех строк из минибатча параллельно предсказываются следующие символы, процесс повторяется.

95
00:20:02,680 --> 00:20:17,480
Такое преобразование данных нужно было для того, чтобы параллельно обсчитывать различные отрезки данных.

96
00:20:17,480 --> 00:20:23,820
Символ в начале отрезка в миллион символов может находиться в середине предложения,

97
00:20:23,920 --> 00:20:33,880
но это неважно, потому что такое происходит только один раз на миллион символов.

98
00:20:33,880 --> 00:20:40,740
Вопрос из зала: Вы можете ещё что-то сказать про дополнение данных таких датасетов?

99
00:20:40,840 --> 00:20:52,515
Нет, мне больше нечего сказать на эту тему. Я изучу этот вопрос ко второй части курса.

100
00:20:52,615 --> 00:21:04,820
Недавно начали появляться статьи на эту тему. Например, победитель недавнего соревнования Kaggle

101
00:21:06,830 --> 00:21:17,985
вставлял случайные части строк в свои данные, это может помочь.

102
00:21:18,085 --> 00:21:23,070
Я видел несколько статей, в которых описаны похожие методы,

103
00:21:23,170 --> 00:21:38,259
но именно отдельных статей по NLP на тему дополнения данных — не видел.

104
00:21:38,559 --> 00:21:43,604
Вопрос из зала: Как подобрать значение параметра bptt?

105
00:21:43,704 --> 00:21:48,049
Есть несколько факторов.

106
00:21:48,049 --> 00:22:00,014
Размер обведённой матрицы — (bptt) x (размер минибатча). Для каждого элемента минибатча

107
00:22:00,114 --> 00:22:23,389
есть вектор эмбеддинга, и результирующая трёхмерная матрица должна помещаться в GPU.

108
00:22:23,389 --> 00:22:29,659
Если вы получаете ошибку «CUDA error 'out of memory'», придётся уменьшить одно из этих чисел.

109
00:22:29,659 --> 00:22:37,874
Если модель обучается нестабильно, — например, ошибка резко достигает NaN, — попробуйте уменьшить bptt.

110
00:22:37,974 --> 00:22:43,785
При более коротком обратном распространении ошибки градиентный взрыв не успеет развиться.

111
00:22:43,885 --> 00:22:47,090
Если обучение слишком медленное — попробуйте уменьшить bptt,

112
00:22:47,090 --> 00:22:51,169


113
00:22:51,169 --> 00:22:55,919


114
00:22:56,019 --> 00:23:08,419
Недавно появились квази-рекуррентные нейронные сети (QRNN), они позволяют так делать, но мы пока не можем.

115
00:23:08,419 --> 00:23:13,339
Это главные факторы при выборе параметра bptt — время обучения, занимаемая память и стабильность обучения.

116
00:23:13,439 --> 00:23:21,820
Постарайтесь выбрать максимальное значение параметра bptt с учётом всех этих факторов.

117
00:23:23,100 --> 00:23:40,084
Нарезать данные на отрезки и складывать их вместе довольно муторно, поэтому здесь мы используем torchtext.

118
00:23:40,184 --> 00:23:52,749
При использовании API вроде fastai и torchtext вы можете столкнуться с тем, что

119
00:23:52,749 --> 00:23:58,299
методы API ожидают данные не в том формате, в котором они вам даны изначально.

120
00:23:58,299 --> 00:24:08,919
В таком случае можно преобразовать данные под необходимый формат или написать класс-обёртку.

121
00:24:08,919 --> 00:24:17,749
Я читал форум и заметил, что вы тратите очень много времени на написание классов-обёрток.

122
00:24:17,849 --> 00:24:24,079
Я гораздо ленивее и предпочитаю преобразовывать данные. Оба подхода хороши.

123
00:24:24,179 --> 00:24:38,165
Если вы заметите, что какой-то вид данных появляется часто и для него ещё нет обёртки в fastai,

124
00:24:38,265 --> 00:24:42,860
пожалуйста, напишите эту обёртку и создайте соответствующий pull request на github.com/fastai/fastai.

125
00:24:42,960 --> 00:24:58,009
Я решил преобразовать тексты Ницше в формат, который ожидает torchtext.

126
00:24:58,109 --> 00:25:15,319
В обёртке fastai над torchtext уже доступно разделение данных на обучающую и валидационную выборки.

127
00:25:15,419 --> 00:25:26,224
Я скопировал датасет из папки data/nietzsche/ в папки trn/ и val/.

128
00:25:26,324 --> 00:25:39,190
В папке trn/ я удалил последние 20% данных, а в папке val/ — всё, кроме последних 20%.

129
00:25:39,190 --> 00:25:56,254
Я решил, что это проще, чем писать класс-обёртку, да и валидационная выборка строится разумнее.

130
00:25:56,354 --> 00:26:08,660
На практике приходится обучать модель на одних книгах или авторах, а использовать для других.

131
00:26:08,760 --> 00:26:24,530
Я постарался сделать схожим образом, чтобы лучше проверить качество модели, и разбил корпус на два цельных куска.

132
00:26:24,630 --> 00:26:44,089
Убедитесь, что владеете bash в достаточной степени, чтобы выполнять такие операции.

133
00:26:44,189 --> 00:27:12,404
Обучающая и валидационная выборка обе состоят из одного файла, так строят языковые модели.

134
00:27:12,504 --> 00:27:23,209
Вот данные, вот код, который мы уже видели, давайте его разберём.

135
00:27:23,309 --> 00:27:36,129
Информация о предобработке данных содержится в объекте класса Field.

136
00:27:36,129 --> 00:27:51,300
Здесь я перевожу все символы в нижний регистр и использую функцию list() в качестве функции токенизации.

137
00:27:51,400 --> 00:27:59,924
При предсказывании слов мы использовали функцию, разбивающую текст по пробелам,

138
00:28:00,024 --> 00:28:16,840
а здесь — символы, поэтому используется функция преобразования строки в массив символов.

139
00:28:16,840 --> 00:28:27,620
Видно, как библиотеки вроде torchtext и fastai могут сильно упростить вам жизнь.

140
00:28:30,200 --> 00:28:37,470
Очень часто в качестве параметров методов эти библиотеки принимают функции,

141
00:28:38,809 --> 00:28:44,960
и функции можно использовать любые, в том числе и написанные вами.

142
00:28:44,960 --> 00:28:51,027
Параметр tokenize=list означает, что минибатчи состоят из символов.

143
00:28:51,127 --> 00:29:00,530
Вот остальные параметры. Я использую те же значения, что и в других частях этого Jupyter ноутбука.

144
00:29:00,530 --> 00:29:08,380
Размер минибатча bs=64, длина обратного распространения во времени bptt=8,

145
00:29:08,380 --> 00:29:14,960
длина вектора эмбеддинга n_fac=42 и размер скрытого слоя n_hidden=256.

146
00:29:14,960 --> 00:29:37,870
Размер скрытого слоя — размер матрицы весов, это оранжевые круги на диаграмме.

147
00:29:38,290 --> 00:29:43,865
Это словарь, показывающий, что использовать в качестве обучающей, валидационной и тестовой выборок.

148
00:29:43,965 --> 00:29:49,350
В качестве тестовой выборки я использую валидационную.

149
00:29:49,450 --> 00:29:58,250
После словаря я создаю объект данных модели md методом LanguageModelData.from_text_files().

150
00:29:58,250 --> 00:30:12,740
Все параметры метода нам знакомы. Параметр min_freq=3 здесь бесполезен,

151
00:30:12,740 --> 00:30:22,550
так как вряд ли найдётся символ, использующийся реже трёх раз.

152
00:30:22,550 --> 00:30:43,630
Получилось 963 минибатча, это число должно равняться (количество токенов) / bs / bptt.

153
00:30:45,130 --> 00:30:49,850
На практике это не совсем так.

154
00:30:52,400 --> 00:31:16,755
Нельзя перемешивать датасет, как с изображениями, но можно каждый раз немного менять значение bptt.

155
00:31:16,855 --> 00:31:44,830
PyTorch выдерживает значение bptt=8, но в 5% случаях немного меняет его, сохраняя среднее значение.

156
00:31:45,130 --> 00:31:53,950
Вопрос из зала: Внутри одного батча длина bptt фиксированная?

157
00:31:53,950 --> 00:32:29,640
Да, каждый минибатч происходит умножение матриц, поэтому размер должен быть фиксирован.

158
00:32:33,105 --> 00:32:39,850
Таким образом, количество минибатчей — это длина загрузчика данных.

159
00:32:39,850 --> 00:32:45,130
md.nt — количество уникальных токенов.

160
00:32:45,130 --> 00:33:01,710
После выполнения этой строки в переменной TEXT появляется поле TEXT.vocab,

161
00:33:01,710 --> 00:33:18,950
содержащее словари перевода уникальных символов в индексы и наоборот.

162
00:33:19,050 --> 00:33:34,179
Модель реализована в классе CharSeqStatefulRnn.

163
00:33:34,179 --> 00:33:50,654
Его мы уже обсуждали — последней модификацией был перенос матрицы h в поле self.h.

164
00:33:50,754 --> 00:34:03,880
Я сказал, что размер минибатча фиксирован, но это не совсем так.

165
00:34:03,980 --> 00:34:24,284
Последний минибатч может быть короче, если количество токенов не кратно (bptt * bs).

166
00:34:24,384 --> 00:34:48,438
Поэтому мы проверяем по совпадению размеров, нормальный ли минибатч,

167
00:34:48,538 --> 00:35:01,980
и обнуляем матрицу h, если он слишком маленький — то есть в конце эпохи.

168
00:35:02,080 --> 00:35:10,850
Это полезный трюк №3. В конце каждой эпохи выполняется минибатч меньшего размера,

169
00:35:10,850 --> 00:35:19,190
а к началу следующей эпохи размеры выравниваются.

170
00:35:19,190 --> 00:35:28,580
Поэтому вызов self.init_hidden() есть не только в конструкторе.

171
00:35:31,880 --> 00:35:36,490
Это не очень важная деталь, но полезная.

172
00:35:39,160 --> 00:35:59,650
Последний полезный трюк связан с одним неудобством PyTorch, надеемся, кто-то его исправит.

173
00:35:59,650 --> 00:36:08,230
Функции потерь в PyTorch, например, Softmax, не принимают на вход тензоры третьего ранга.

174
00:36:08,330 --> 00:36:17,240
Напомню, что тензор третьего ранга — просто трёхмерная матрица.

175
00:36:17,240 --> 00:36:22,829
Нет никакой причины, по которой обработка тензоров третьего ранга была бы сложна и неудобна —

176
00:36:22,929 --> 00:36:35,594
можно просто последовательно считать ошибку по первым двум осям.

177
00:36:35,694 --> 00:36:53,159
Почему-то никто это не реализовал, поэтому функции работают только с тензорами второго или четвёртого ранга.

178
00:36:59,519 --> 00:37:34,040
На каждой итерации есть две матрицы размера bs x bptt — предсказания и истинные значения.

179
00:37:34,040 --> 00:37:41,154
Для вычисления ошибки матрицы сверяются поэлементно.

180
00:37:41,254 --> 00:37:57,940
Для передачи в функцию потерь эти матрицы распрямляются до векторов методом .view().

181
00:37:57,940 --> 00:38:11,459
Итоговое количество колонок равно количеству возможных класов self.vocab_size.

182
00:38:11,559 --> 00:38:24,260
Количество строк — сколько получится, здесь будет bs * bptt.

183
00:38:24,260 --> 00:38:34,935
Предсказания нужно приводить к необходимому формату, а целевую переменную torchtext обработал за нас.

184
00:38:35,035 --> 00:38:56,320
torchtext автоматически распрямляет целевую переменную, мы видели это на четвёртой лекции.

185
00:38:56,420 --> 00:39:02,727
Итак, вот полезные трюки — избавляйтесь от истории операций,

186
00:39:02,827 --> 00:39:11,180
обнуляйте состояние скрытого слоя при изменении размера минибатча,

187
00:39:16,970 --> 00:39:23,180
распрямляйте данные и используйте torchtext для создания хороших минибатчей.

188
00:39:23,180 --> 00:39:39,510
После этого можно создать модель, алгоритм оптимизации и обучить модель.

189
00:39:39,610 --> 00:40:02,055
При вызове функции F.log_softmax() необходимо указывать ось dim, по которой считаются потери.

190
00:40:02,155 --> 00:40:11,010
Мы хотим вычислять Softmax по последней оси, поэтому передаём значение dim=-1.

191
00:40:11,110 --> 00:40:26,120
В знаменателе формулы Softmax — суммирование, параметр dim указывает, по какой оси его выполнять.

192
00:40:26,120 --> 00:40:41,625
Последняя ось содержит вероятности, которые в сумме должны давать 1, поэтому суммирование происходит по ней.

193
00:40:41,725 --> 00:41:02,810
Для запуска этого Jupyter ноутбука нужен PyTorch 0.3, вышедший на этой неделе.

194
00:41:02,810 --> 00:41:16,725
PyTorch 0.3 официально не поддерживает Windows, но я смог его установить командой conda install torch,

195
00:41:16,825 --> 00:41:24,140
запустил Jupyter ноутбук первой лекции и всё отработало.

196
00:41:24,140 --> 00:41:40,950
У меня Surface Book 2 15 с видеокартой GTX 1060 и 16 GB GPU, для глубокого обучения подходит отлично.

197
00:41:41,050 --> 00:41:50,595
Он работает примерно втрое медленнее моей машины с видеокартой GTX 1080,

198
00:41:50,695 --> 00:41:59,190
то есть примерно как SP2-инстанс на AWS.

199
00:41:59,290 --> 00:42:07,130
Surface Book также работает как планшет с сенсорным экраном, он тонкий и лёгкий,

200
00:42:07,130 --> 00:42:19,200
у меня никогда не было такого удобного ноутбука. Я установил на нём Linux, там тоже всё работает.

201
00:42:19,300 --> 00:42:24,385
Если вам нужен ноутбук для глубокого обучения, это хороший вариант.

202
00:42:26,560 --> 00:42:36,950
Так, про параметр dim=-1 я сказал, дальше создаём модель, алгоритм оптимизации, и обучаем.

203
00:42:36,950 --> 00:42:43,360
Результаты мало отличаются от того, что было раньше.

204
00:42:43,360 --> 00:42:54,070
Можно ещё немного изменить модель. Рассмотрим класс CharSeqStatefulRnn2.

205
00:42:54,170 --> 00:43:11,680
Этот класс очень похож на CharSeqStatefulRnn, но вместо nn.RNN() теперь используется nn.RNNCell().

206
00:43:14,890 --> 00:43:24,100
Вот код функции RNNCell() в библиотеке PyTorch, его не надо запускать.

207
00:43:24,100 --> 00:43:34,727
Вы уже умеете читать и понимать исходный код PyTorch, мы реализовали похожие классы.

208
00:43:34,827 --> 00:43:47,320
Здесь — умножение входных данных на матрицу весов и прибавление смещений, это делает F.linear().

209
00:43:47,320 --> 00:44:05,230
Здесь входные данные и состояние скрытого слоя не склеиваются, а складываются, особой разницы нет.

210
00:44:05,230 --> 00:44:14,822
Вопрос из зала: Почему в качестве функции активации используется гиперболический тангенс?

211
00:44:14,922 --> 00:44:31,830
Область значений гиперболического тангенса tanh — от -1 до 1, он выглядит так.

212
00:44:36,660 --> 00:44:49,530
Гиперболический тангенс и сигмоида связаны соотношением tanh(x) = 2 * sigmoid(2x) - 1.

213
00:44:49,530 --> 00:45:14,045
Диапазон значений от -1 до 1 позволяет избежать градиентного взрыва, в отличие от неограниченного выпрямителя.

214
00:45:14,145 --> 00:45:28,835
Тем не менее, можно использовать выпрямитель, указав его в параметре nonlinearity.

215
00:45:28,935 --> 00:45:35,750
Обычно используют гиперболический тангенс.

216
00:45:35,850 --> 00:45:45,015
Мы заменили nn.RNN() на nn.RNNCell(), поэтому нужно вернуть цикл.

217
00:45:45,115 --> 00:45:57,530
В каждой итерации цикла полученные результаты склеиваются с предыдущими, в конце получается матрица outp.

218
00:45:57,630 --> 00:46:11,375
Я получил такой же ответ, как и раньше, никакой магии.

219
00:46:11,475 --> 00:46:17,920
На практике модели не пишут так, но кто-нибудь может придумать

220
00:46:17,920 --> 00:46:25,690
хорошую модификацию класса RNNCell, или другой способ отслеживать градиенты, или новый вид регуляризации.

221
00:46:25,690 --> 00:46:43,350
Исходный код fastai выглядит точно так же, чтобы использовать не реализованные в PyTorch методы регуляризации.

222
00:46:43,790 --> 00:47:01,896
На практике RNNCell не спасает от градиентного взрыва и требует низкой скорости обучения

223
00:47:01,996 --> 00:47:13,935
и низкого значения параметра bptt, поэтому этот класс никто не использует.

224
00:47:14,035 --> 00:47:26,890
Вместо этого используется класс GRUCell, управляемый рекуррентный блок.

225
00:47:29,590 --> 00:47:38,425
Он выглядит так и описывается такими уравнениями.

226
00:47:38,525 --> 00:47:44,310
Мы пробежимся по ним, но обсуждать будем во второй части курса.

227
00:47:44,410 --> 00:48:04,640
До этого мы получали предсказания, сложив имеющиеся активации и произведение матрицы весов на входные данные.

228
00:48:06,980 --> 00:48:18,170
Здесь не так. Сначала входные данные умножаются на вспомогательную матрицу h_tilde.
==============

229
00:48:18,170 --> 00:48:28,040
Предыдущие активации умножаются на параметр r (reset).

230
00:48:29,450 --> 00:48:40,100
Параметр r лежит в диапазоне от 0 до 1 и вычисляется как произведение матрицы весов

231
00:48:40,100 --> 00:48:46,460
и комбинации предыдущего состояния скрытого слоя и входных данных.

232
00:48:46,460 --> 00:48:56,240
По сути, это маленькая однослойная нейронная сеть. Результат её работы пропускается через сигмоиду.

233
00:48:57,650 --> 00:49:02,939
Математическая нотация плоха тем, что символы переиспользуются -

234
00:49:03,039 --> 00:49:16,019
сигма как переменная значит стандартное отклоение, а как функция - сигмоиду.

235
00:49:16,839 --> 00:49:33,120
Итак, сначала стоит однослойная нейронная сеть без скрытых слоёв, то есть логистическая регрессия.

236
00:49:33,220 --> 00:49:37,409
Мы обсудим это во второй части, но привыкать нужно уже сейчас.

237
00:49:37,509 --> 00:49:46,561
Идея в том, что нейронная сеть может состоять из других простейших нейронных сетей.

238
00:49:46,661 --> 00:49:58,254
Мы используем эту маленькую нейронную сеть, чтобы понять, какой процент предыдущих активаций оставить.

239
00:49:58,354 --> 00:50:10,149
Может случиться так, что в процессе обучения нужно будет отбросить почти всё уже изученное.

240
00:50:10,349 --> 00:50:14,939
Такое поведение легко реализовать с помощью такой мини-сети.

241
00:50:15,039 --> 00:50:28,309
Результат работы первой мини-сети переходит на следующий этап, здесь вводится параметр z.

242
00:50:28,309 --> 00:50:36,764
Параметр z указывает, с каким весом создать новые активации из предыдущих активаций и входных данных,

243
00:50:36,864 --> 00:50:51,419
то есть в какой мере оставить всё как было, а в какой - использовать только что полученный результат.

244
00:50:51,519 --> 00:50:55,814
Итак, в одном блоке принимаются два решения -

245
00:50:55,914 --> 00:51:00,360
какой процент предыдущих активаций использовать в сочетании с входными данными

246
00:51:00,460 --> 00:51:05,475
и насколько процентов поменять предыдущие активации в соответствии с полученным результатом.

247
00:51:05,575 --> 00:51:32,910
Надеюсь, вы узнали в этой формуле линейную интерполяцию.

248
00:51:33,010 --> 00:51:44,755
Значение параметра z определяет, с каким коэффициентом учесть предыдущие активации, а с каким - новые.

249
00:51:44,855 --> 00:52:03,880
На схеме нарисован переключатель - он может застыть в любом промежуточном положении.

250
00:52:03,880 --> 00:52:24,505
Вот описывающие GRU-блок уравнения - вычисление параметров z_t и r_t и промежуточных активаций.

251
00:52:24,605 --> 00:52:35,655
Вот исходный код GRU-блока в PyTorch. Здесь есть пара оптимизаций, можем обсудить их на форуме, суть такая же.

252
00:52:35,755 --> 00:52:50,215
Класс CharSeqStatefulGRU содержит GRU-блок вместо nn.RNNCell().

253
00:52:50,315 --> 00:53:05,840
Ошибка уменьшилась с 1.542 до 1.369.

254
00:53:05,840 --> 00:53:16,637
GRU-блок очень популярен, как и похожие на него LSTM-сети.

255
00:53:16,737 --> 00:53:29,349
Вопрос из зала: z_t и r_t - скаляры и действуют на матрицы поэлементно?

256
00:53:29,349 --> 00:53:36,859
Да, и так для каждого минибатча.

257
00:53:44,930 --> 00:54:06,039
Вот две хорошие статья про LSTM-сети, будет классно, если вы добавите их в вики курса.

258
00:54:08,319 --> 00:54:15,650
Уберём GRU-блок и реализуем архитектуру LSTM (long short-term memory, долгая краткосрочная память).

259
00:54:15,650 --> 00:54:21,585
Для этого нужно изменить GRU-блок, добавив к нему ещё один элемент.

260
00:54:21,685 --> 00:54:39,239
Метод self.init_hidden() теперь создаёт две матрицы, их размер равен размеру скрытого слоя.

261
00:54:40,130 --> 00:54:55,935
Мы обращаемся с матрицей h так же, как и раньше, технические детали обсудим потом.

262
00:54:56,035 --> 00:55:13,250
Я добавил дропаут и удвоил в соответствии с этим размер скрытого слоя,

263
00:55:13,250 --> 00:55:19,490
это должно увеличить гибкость и обучаемость модели.

264
00:55:19,490 --> 00:55:46,585
Я хотел показать вам класс LayerOptimizer из fastai, он позволяет обойтись без объекта learner.

265
00:55:46,685 --> 00:56:04,110
Для этого нужно создать модель PyTorch привычным образом, а потом вместо присваивания opt = optim.Adam()

266
00:56:04,210 --> 00:56:25,040
создать объект класса LayerOptimizer, передав в него те же параметры - модель, скорость обучения и ограничение весов.

267
00:56:25,040 --> 00:56:33,554
Этот класс реализует дифференциальные скорости обучения и дифференциальные ограничения весов.

268
00:56:33,654 --> 00:56:46,470
Он нужен, если вы собираетесь использовать callback-функции, или градиентный спуск с перезапуском,

269
00:56:46,570 --> 00:57:02,500
или что угодно, где не нужен объект модели learner.

270
00:57:02,500 --> 00:57:23,815
Возвращаемый конструктором объект лежит в переменной lo, алгоритм оптимизации лежит в поле lo.opt.

271
00:57:23,915 --> 00:57:37,065
При обучении в функцию fit() передаётся алгоритм оптимизации lo.opt и callback-функции.

272
00:57:37,165 --> 00:57:47,670
В качестве callback-функции используется имитация отжига по косинусу, для её инициализации нужен объект lo.

273
00:57:47,770 --> 00:58:04,439
Имитация отжига будет менять скорости обучения внутри объекта lo, технические детали не важны.

274
00:58:04,539 --> 00:58:14,219
Итак, callback-функция имитации отжига меняет скорости обучения в объекте lo,

275
00:58:14,319 --> 00:58:28,769
длина эпохи равна количеству минибатчей, то есть длине загрузчика данных модели,

276
00:58:28,869 --> 00:58:33,334
параметр cycle_mult=2 передаётся как обычно,

277
00:58:33,434 --> 00:58:40,879
а параметр on_cycle_end=on_end позволяет сохранять модель в конце каждого цикла,

278
00:58:40,979 --> 00:58:54,404
это параметр, меняющийся при установке параметра cycle_save_name в классе ConvLearner.fit().

279
00:58:54,504 --> 00:59:14,145
callback-функции - полезный инструмент, их можно задавать на старте или в конце обучения, эпохи или минибатча.

280
00:59:14,245 --> 00:59:36,650
callback-функции используются в SGDR, в AdamW, для отрисовки графиков функции потерь от времени и многого другого.

281
00:59:36,650 --> 00:59:57,914
Здесь мы используем callback-функцию для реализации SGDR, это позволяет уменьшить ошибку до 1.25.

282
00:59:58,014 --> 01:00:24,050
Тестируя нашу модель, получаем собственную версию рукописей Ницше.

283
01:00:24,050 --> 01:00:31,250
Текст не такой связный, как у Ницше, но дух мы передали.

284
01:00:31,350 --> 01:01:01,425
Интересно потестировать модель на различных уровнях потерь и понять, как сильно отличаются, например, потери 1.3 и 1.25.

285
01:01:01,525 --> 01:01:18,610
После этого становится понятно, что до очень похожих на Ницше текстов остаётся совсем немного дообучиться.

286
01:01:18,710 --> 01:01:29,205
Если вы строите генеративную модель и она показывает средние результаты, не отчаивайтесь -

287
01:01:29,305 --> 01:01:32,865
вы уже почти дошли до нужной точности.

288
01:01:32,965 --> 01:01:39,365
Разница между моделью, речь которой можно принять за английский только издалека и прищурившись,

289
01:01:39,465 --> 01:01:47,186
и моделью, которая создаёт связные тексты на английском, гораздо меньше, чем кажется.

290
01:01:47,286 --> 01:01:53,995
=========================
Давайте сделаем перерыв на 5 минут и вернёмся к компьютерному зрению в 7:45.

291
01:02:00,975 --> 01:02:05,040
Итак, мы наконец вернулись к началу, к компьютерному зрению.

292
01:02:09,140 --> 01:02:14,070
Соответствующий Jupyter ноутбук называется lesson7-cifar.ipynb.

293
01:02:18,320 --> 01:02:34,220
CIFAR-10 — известный датасет изображений, появившийся задолго до ImageNet.

294
01:02:34,320 --> 01:02:47,060
Я считаю, что небольшие датасеты гораздо интереснее ImageNet.

295
01:02:47,060 --> 01:02:56,417
Большинству из вас придётся работать с тысячами изображений, а не с полутора миллионами изображений ImageNet,

296
01:02:56,517 --> 01:03:01,170
поэтому полезно и интересно попрактиковаться.

297
01:03:01,270 --> 01:03:09,320
В большинстве прикладных задач, например, анализе медицинских снимков,

298
01:03:09,320 --> 01:03:17,175
размер изображения редко превышает 32 пикселя.

299
01:03:17,275 --> 01:03:23,997
CIFAR-10 состоит из маленьких изображений, и их немного,

300
01:03:24,097 --> 01:03:31,167
поэтому обучаться на них гораздо сложнее и интереснее, чем на изображениях ImageNet.

301
01:03:31,267 --> 01:03:41,240
Размер CIFAR-10 также позволяет обучаться гораздо быстрее, поэтому он хорош для тестирования алгоритмов.

302
01:03:43,310 --> 01:03:52,670
Многие исследователи жалуются, что у них нет средств на исследование различных версий алгоритма.

303
01:03:52,670 --> 01:04:02,930
Они используют ImageNet, поэтому каждое исследование — это неделя высокомощных вычислений.

304
01:04:02,930 --> 01:04:10,200
Я не понимаю, зачем тестировать алгоритмы на огромных датасетах вроде ImageNet.

305
01:04:10,200 --> 01:04:21,835
На этой неделе эта тема бурно обсуждается в связи с выступлением Ali Rahimi на NIPS 2017.

306
01:04:21,935 --> 01:04:30,060
Он говорил о том, что сейчас в глубоком обучении недостаточно смелых экспериментов.

307
01:04:30,060 --> 01:04:44,730
Мы обсуждали с ним эту тему и ещё не пришли к взаимопониманию, но у нас общая забота —

308
01:04:44,730 --> 01:04:54,630
люди предпочитают брать побольше GPU и данных и скидывать в кучу, а не обдумывать и ставить эксперименты.

309
01:04:54,630 --> 01:05:09,600
Если вы разрабатываете алгоритм для работы на больших датасетах, используйте маленькие для экспериментов —

310
01:05:09,600 --> 01:05:18,560
попробуйте различные версии алгоритма, уберите некоторые этапы, чтобы посмотреть, на что они влияют, и так далее.

311
01:05:18,560 --> 01:05:27,285
Многие жалуются на датасет MNIST, но его тоже хорошо использовать для того, чтобы понять,

312
01:05:27,385 --> 01:05:34,255
какие составляющие алгоритма имеют значение, а какие нет.

313
01:05:34,355 --> 01:05:39,415
Я думаю, что люди, которые жалуются на MNIST, просто рисуются —

314
01:05:39,615 --> 01:05:52,690
"Посмотрите на меня, я работаю в Google за $100,000 в неделю, у меня есть бесплатные GPU, я крутой".

315
01:05:52,790 --> 01:06:01,420
Кто-то в интернете преобразовал датасет в изображения, их можно скачать по этой ссылке, спасибо ему.

316
01:06:01,520 --> 01:06:13,140
Google перенаправит вас на менее удобный формат, используйте этот, он уже подготовлен для работы.

317
01:06:13,140 --> 01:06:17,725
Вот список классов датасета.

318
01:06:17,825 --> 01:06:46,200
В предобученных моделях fastai функция tfms_from_model() нормализует данные, вычисляя стандартные отклонения и средние.

319
01:06:48,300 --> 01:07:01,600
Сейчас мы создаём модель с нуля, поэтому стандартные отклонения и средние приходится считать руками.

320
01:07:01,700 --> 01:07:10,015
Я не привожу код, который использовал для их вычисления, думаю, вы и сами справитесь.

321
01:07:10,115 --> 01:07:16,390
Стандартные отклонения и средние нужно считать отдельно для каждого канала.

322
01:07:16,490 --> 01:07:41,660
Нужно выбрать алгоритм дополнения данных. Для CIFAR-10 обычно используют RandomFlipXY(),

323
01:07:41,660 --> 01:07:53,190
добавляют чёрные границы и выбирают случайный квадрат 32x32.

324
01:07:53,190 --> 01:08:01,930
При указании параметра pad в функциях fastai вроде tfms_from_stats() это выполняется автоматически.

325
01:08:02,030 --> 01:08:09,025
Я добавляю границу толщиной 4 пикселя.

326
01:08:09,125 --> 01:08:18,660
После выбора алгоритма дополнения данных получаю объект данных модели методом ImageClassifierData.from_paths().

327
01:08:18,660 --> 01:08:26,590
Размер минибатча bs=256, потому что изображения маленькие и их можно обрабатывать большими пачками.

328
01:08:26,690 --> 01:08:51,009
Вот примеры изображений из датасета — парусник и лягушка.

329
01:08:51,009 --> 01:09:19,060
Kerem Turgutlu написал Jupyter ноутбук, в котором реализовал многие алгоритмы оптимизации с нуля и сравнил их.

330
01:09:19,060 --> 01:09:29,650
Получилась версия моего файла Excel про оптимизаторы, написанная на Python, очень круто.

331
01:09:29,649 --> 01:09:43,324
Он написал вспомогательный класс нейронной сети SimpleNet(), мы используем его.

332
01:09:43,424 --> 01:09:49,774
Нейронная сеть состоит из списка полносвязных слоёв.

333
01:09:49,874 --> 01:10:00,589
Массив слоёв нейронной сети в PyTorch нужно оборачивать в nn.ModuleList().

334
01:10:00,689 --> 01:10:18,485
В методе forward() мы выпрямляем данные, пропускаем их через цикл связок линейный слой + выпрямитель и применяем Softmax.

335
01:10:18,585 --> 01:10:35,260
Мы поднимемся на один уровень API fastai выше и вместо вызова функции fit() создадим модель из класса SimpleNet().

336
01:10:35,260 --> 01:10:53,530
Для этого используем метод ConvLearner.from_model_data() и передаём в него модель PyTorch и объект данных модели fastai.

337
01:10:53,530 --> 01:11:01,630
Это проще создания RNN — не нужно указывать алгоритмы оптимизации, имитации отжига и callback-функции.

338
01:11:01,630 --> 01:11:14,440
Созданный таким образом объект модели learn устроен привычным нам образом.

339
01:11:16,090 --> 01:11:22,040
Размер изображений — 32х32, в них три канала, получается 3072 признака.

340
01:11:22,140 --> 01:11:33,115
На выходе из первого слоя у нас 40 признаков, на выходе из второго — 10, по числу классов CIFAR-10.

341
01:11:35,040 --> 01:11:47,475
Здесь вызываются привычные методы learn.summary(), learn.lr_find(), learn.sched.plot(), learn.fit().

342
01:11:47,575 --> 01:12:10,845
Обычная нейронная сеть с одним скрытым слоем и 122,800 параметрами достигла доли правильных ответов в 47%.

343
01:12:10,945 --> 01:12:16,802
Давайте попробуем улучшить это значение.

344
01:12:16,902 --> 01:12:30,170
В процессе улучшения мы воспроизведём архитектуру ResNet.

345
01:12:30,270 --> 01:12:38,615
Начнём с того, что заменим полносвязную нейронную сеть на свёрточную.

346
01:12:38,715 --> 01:13:10,490
Полносвязный слой — это произведение матрицы активаций на матрицу весов.

347
01:13:10,490 --> 01:13:19,730
Матрица весов содержит вес для каждой активации — поэтому она такая большая.

348
01:13:26,420 --> 01:13:44,600
Модель плохо работает, хотя весов очень много: 3,072 * 40 = 122,880.

349
01:13:44,600 --> 01:13:52,040
Веса используются неэффективно, потому что значение придаётся каждому пикселю.

350
01:13:52,040 --> 01:13:58,260
Разумнее рассматривать группы пикселей 3x3, в которых можно отследить узоры.

351
01:13:58,360 --> 01:14:39,795
Это называется свёртка. Каждая область 3x3 пикселя поэлементно умножается на матрицу свёртки.

352
01:14:39,895 --> 01:14:53,030
Матриц свёртки, или фильтров, может быть несколько, результат работы свёртки — тензор третьего ранга.

353
01:14:55,530 --> 01:15:09,020
Изменения реализованы в классе ConvNet. Мы заменили nn.Linear() на nn.Conv2d().

354
01:15:09,020 --> 01:15:13,120
Мы хотим, чтобы каждый следующий слой был меньше в два раза по каждой оси.

355
01:15:17,020 --> 01:15:31,010
В таблице Excel после работы свёртки из каждых четырёх чисел выбиралось максимальное.

356
01:15:31,110 --> 01:15:38,720
Сейчас вместо подвыборки максимумом используется шаг stride=2.

357
01:15:38,820 --> 01:16:08,110
На этапе свёртки участки 3x3 рассматриваются через один — фильтр смещается не на одну строку/столбец, а на две.

358
01:16:08,110 --> 01:16:15,885
Шаг stride=2 и подвыборка максимумом дают один эффект — уменьшение слоя вдвое по каждому измерению.

359
01:16:15,985 --> 01:16:26,230
Шаг задаётся параметром stride=2, размер свёрточного фильтра kernel_size=3,

360
01:16:26,230 --> 01:16:34,170
остальные параметры я скопировал из nn.Linear(), это количество входных и выходных признаков.

361
01:16:34,170 --> 01:16:38,175
Мы создаём массив свёрточных слоёв.

362
01:16:38,275 --> 01:17:06,400
На вход подаются изображения 32x32, из первого слоя выходят изображения размера 15x15, потом 7x7 и 3x3.

363
01:17:06,400 --> 01:17:17,260
Для преобразования матрицы активаций 3x3 в вероятности 10 классов используется адаптивная подвыборка максимумом.

364
01:17:17,260 --> 01:17:35,440
В обычной подвыборке максимумом мы задаём размер области, из которой выбирается максимумальное число — здесь 2x2.

365
01:17:35,440 --> 01:17:57,990
В адаптивной подвыборке максимумом задаётся размер матрицы, которую нужно получить — здесь 14x14 из матрицы 28x28.

366
01:17:58,090 --> 01:18:15,935
В этом примере обычная и адаптивная подвыборки максимумом дают один и тот же результат.

367
01:18:16,035 --> 01:18:28,750
В современных CNN предпоследним слоем обычно ставят адаптивную подвыборку максимумом размера 1,

368
01:18:28,750 --> 01:18:39,100
то есть выбирают максимальную ячейку и используют её в качестве активации.

369
01:18:39,100 --> 01:18:51,155
После этого получается тензор размера (количество признаков) x 1 x 1.

370
01:18:51,255 --> 01:19:03,308
Строка x = x.view(x.size(0), -1) убирает последние две оси этого тензора, преобразуя его в вектор.

371
01:19:03,408 --> 01:19:11,480
Метод forward() обрабатывает минибатчи, поэтому на практике это матрица (размер минибатча) x (количество признаков).

372
01:19:11,580 --> 01:19:23,860
Полученный вектор передаётся в линейный слой для получения необходимого количества вероятностей классов, с=10.

373
01:19:23,860 --> 01:19:32,615
Итак, данные проходят через цикл связок свёртка + выпрямитель, через слой адаптивной подвыборки максимумом,

374
01:19:32,715 --> 01:19:51,700
потом мы удаляем ненужные оси и передаём их в последний, линейный, слой. Он возвращает вектор длиной 10.

375
01:19:51,700 --> 01:19:59,980
Размер изображения меняется в свёрточных слоях с 3x32x32 до 20x15x15, 40x7x7 и 80x3x3,

376
01:19:59,980 --> 01:20:09,800
адаптивная подвыборка максимумом преобразует тензор 80x3x3 в 80x1x1, мы обрезаем его до 80,

377
01:20:09,900 --> 01:20:15,770
и линейный слой создаёт из 80 активаций 10 вероятностей.

378
01:20:15,870 --> 01:20:30,360
Это базовая архитектура полной свёрточной нейронной сети — все слои свёрточные, кроме последнего.

379
01:20:31,350 --> 01:20:50,650
Алгоритм поиска оптимальной скорости обучения прошёл весь датасет, а ошибка не перестала уменьшаться,

380
01:20:50,650 --> 01:21:02,610
поэтому я увеличил максимально возможное значение при поиске end_lr=100 вместо 10 по умолчанию.

381
01:21:02,710 --> 01:21:17,465
Я выбрал значение 1e-1, несколько эпох обучения дали долю правильных ответов в 60%.

382
01:21:17,565 --> 01:21:34,290
Здесь 28,880 параметров, это в четыре раза меньше, чем раньше, а доля правильных ответов поднялась с 47% до 60%.

383
01:21:34,290 --> 01:21:53,140
Время обучения одной эпохи не изменилось — в небольших архитектурах время уходит только на передачу данных.

384
01:21:57,435 --> 01:22:07,780
Я проведу рефакторинг кода, новый класс называется ConvNet2.

385
01:22:07,780 --> 01:22:15,875
Я создам класс ConvLayer, реализующий свёрточный слой, чтобы упростить метод forward().

386
01:22:15,975 --> 01:22:20,210
Размер свёрточного фильтра kernel_size=3, длина шага stride=2.

387
01:22:20,310 --> 01:23:10,880
При свёртке первый слой уменьшается с 32x32 до 15x15, а не 16x16, поэтому я добавлю границу padding=1.

388
01:23:10,980 --> 01:23:25,640
Теперь размер будет уменьшаться ровно вдвое и свёртка будет использовать края изображения.

389
01:23:25,740 --> 01:23:41,140
Класс ConvLayer содержит свёрточный слой и выпрямитель. Теперь класс ConvNet2 выглядит аккуратнее,

390
01:23:41,140 --> 01:23:48,080
а класс ConvLayer мы сможем переиспользовать для других архитектур.

391
01:23:48,180 --> 01:23:57,965
Теперь вы можете создавать не только собственные нейронные сети, но и отдельные слои в них.

392
01:23:58,065 --> 01:24:09,700
В PyTorch нейронные сети и отдельные слои реализуются одинаково — конструктор и метод forward().

393
01:24:09,700 --> 01:24:15,755
Поэтому любую нейронную сеть можно использовать в качестве слоя и наоборот, это очень удобно.

394
01:24:15,855 --> 01:24:23,980
Помимо добавления границ я поменял реализацию адаптивной подвыборки максимумом.

395
01:24:23,980 --> 01:24:42,620
Объект класса nn.AdaptiveMaxPool2d хранится в поле модели, но у него нет весов, поэтому это не обязательно.

396
01:24:42,720 --> 01:24:55,655
Вместо этого я использую функцию F.adaptive_max_pool2d(), модуль F — это torch.nn.functional.

397
01:24:55,755 --> 01:25:10,490
Я обучал эту версию модели недостаточно долго, но результаты должны быть чуть лучше.

398
01:25:10,590 --> 01:25:41,200
Большие свёрточные сети неустойчивы при больших скоростях обучения и обучаются плохо и долго при маленьких.

399
01:25:41,200 --> 01:25:48,255
Чтобы сделать модель более гибкой, я добавлю нормализацию минибатчей.

400
01:25:48,355 --> 01:25:59,705
Нормализация минибатчей появилась пару лет назад и сильно упростила обучение глубоких нейронных сетей.

401
01:25:59,805 --> 01:26:09,230
Следующая модель состоит из пяти свёрточных слоёв и одного полносвязного.

402
01:26:09,330 --> 01:26:17,945
Раньше такие нейронные сети считались очень глубокими и поэтому труднообучаемыми,

403
01:26:18,045 --> 01:26:23,135
а нормализация минибатчей сильно упростила их обучение.

404
01:26:23,235 --> 01:26:29,120
Её очень просто использовать, но в образовательных целях мы напишем её с нуля.

405
01:26:29,220 --> 01:26:43,785
Допустим, у нас есть вектор активаций. Представьте, что это минибатч из одного элемента.

406
01:26:43,885 --> 01:26:58,630
Вектор активаций проходит через свёрточный слой. Для простоты заменим его на умножение на матрицу.

407
01:26:58,630 --> 01:27:19,955
Допустим, матрица единичная. Тогда при многократном умножении вектора на эту матрицу ничего не меняется.

408
01:27:20,055 --> 01:27:47,595
Если заменить единицы на двойки, активации будут удваиваться, в глубоких моделях это приведёт к градиентному взрыву.

409
01:27:47,695 --> 01:28:16,960
Чтобы этого избежать, нужно следить за средними значениями матриц весов.

410
01:28:16,960 --> 01:28:23,670
Входные данные перед передачей в модель нормализуются до среднего 0 и стандартного отклонения 1,

411
01:28:23,670 --> 01:28:34,089
а хочется нормализовать все слои, не только входной.

412
01:28:34,189 --> 01:28:48,750
Для этого я создал класс BnLayer. Он идентичен классу ConvLayer, но содержит вычисление

413
01:28:48,750 --> 01:29:02,100
стандартного отклонения и среднего для каждого канала каждого фильтра и нормализацию.

414
01:29:02,100 --> 01:29:19,104
После этого отпала бы необходимость нормализовать входные данные.

415
01:29:19,204 --> 01:30:03,030
Ксожалению, это не работает, потому что градиентный спуск будет перечёркивать результаты нормализации каждый минибатч.

416
01:30:03,030 --> 01:30:17,754
Поэтому для каждого канала заводятся параметры self.m (multiplier) и self.a (added value).

417
01:30:17,854 --> 01:30:36,480
Начальные значения параметров self.a — нули, self.m — единицы. Количество фильтров первого слоя nf=3.

418
01:30:36,480 --> 01:30:42,420
Параметры self.m и self.a делают обратные только что введённой нормализации действия.

419
01:30:42,420 --> 01:30:49,685
Оборачивание в класс nn.Parameter() позволяет модели подбирать self.m и self.a вместе с другими весами.

420
01:30:49,785 --> 01:30:59,080
После первого слоя нормализованные данные умножаются на 1 и к результату прибавляется 0.

421
01:30:59,080 --> 01:31:20,625
Теперь, если градиентный спуск захочет увеличить диапазон активаций, он будет менять только параметр self.m.

422
01:31:20,725 --> 01:31:32,540
Если нужно будет сместить веса, пересчитается только параметр self.a, а не вся матрица.

423
01:31:32,640 --> 01:31:48,960
Ali Rahimi на NIPS 2017 упомянул статью о нормализации минибатчей как исключительно полезный инструмент,

424
01:31:48,960 --> 01:31:56,650
хотя многие не поняли, в чём смысл этого алгоритма.

425
01:31:56,650 --> 01:32:09,570
Если вам кажется, что нормализовать данные, а потом сразу же выполнять обратное действие — это странно,

426
01:32:09,570 --> 01:32:15,160
не бойтесь, вы не одни такие.

427
01:32:15,160 --> 01:32:28,000
Я представляю это так: сначала мы нормализуем данные, а потом можем сместить их в нужный диапазон значений,

428
01:32:28,000 --> 01:32:39,070
используя гораздо меньше параметров, чем задействуется при пересчитывании всех фильтров.

429
01:32:42,040 --> 01:32:57,354
На практике это позволяет увеличить скорость обучения, гибкость модели и количество слоёв.

430
01:32:57,454 --> 01:33:15,420
После замены ConvLayer на BnLayer я смог добавить ещё один слой модели без ущерба обучению.

431
01:33:15,520 --> 01:33:26,155
Вопрос из зала: Нужно ли опасаться деления на очень маленькое число?

432
01:33:26,255 --> 01:33:45,440
Да, я думаю, что в исходном коде PyTorch в знаменателе не self.stds, а self.stds + eps, стоило бы это учесть.

433
01:33:52,790 --> 01:33:58,925
Вопрос из зала: Параметры self.m и self.a обновляются в методе обратного распространения ошибки?

434
01:33:59,025 --> 01:34:10,130
Да, мы указали это, обернув эти параметры конструктором nn.Parameter().

435
01:34:10,130 --> 01:34:16,219
Нормализация минибатчей также работает как регуляризация,

436
01:34:16,219 --> 01:34:24,445
то есть при её наличии можно уменьшить или совсем убрать дропаут и ограничение весов.

437
01:34:24,545 --> 01:34:34,815
У каждого минибатча своё среднее и своё стандартное отклонение,

438
01:34:34,915 --> 01:34:41,900
поэтому на каждом минибатче значения фильтров слегка отличаются, это добавляет шум.

439
01:34:44,000 --> 01:34:52,920
Шум работает как регуляризация модели.

440
01:34:53,020 --> 01:35:01,570
В классической версии нормализации минибатчей используются не среднее и стандартное отклонение,

441
01:35:01,570 --> 01:35:08,965
а их экспоненциально взвешенные скользящие аналоги.

442
01:35:09,065 --> 01:35:15,100
Если будет нечем заняться на этой неделе, можете это реализовать.

443
01:35:15,100 --> 01:35:33,340
Выражение (if self.training) верно на обучающей выборке и ложно на валидационной,

444
01:35:33,340 --> 01:35:42,010
мы ввели его, потому что на валидационной выборке нельзя менять модель.

445
01:35:42,010 --> 01:36:00,550
Работа некоторых видов слоёв зависит от того, на какую выборку смотрит модель.

446
01:36:00,550 --> 01:36:08,735
Пару недель назад мы заметили баг в Jupyter ноутбуке модели MovieLens —

447
01:36:08,835 --> 01:36:26,945
дропаут не был защищён условием (if self.training), поэтому применялся и на валидационной выборке.

448
01:36:27,045 --> 01:36:40,540
Я заменил дропаут в модели на класс nn.Dropout(), который учитывает это автоматически,

449
01:36:40,540 --> 01:36:50,300
а мог бы добавить условие (if self.training) и получить тот же результат.

450
01:36:50,400 --> 01:37:05,770
Это нужно учитывать только при нормализации минибатчей и дропауте.

451
01:37:05,770 --> 01:37:20,380
В отличных от fastai библиотеках средние и стандартные отклонения обновляются при обучении

452
01:37:20,380 --> 01:37:26,465
во всех слоях, вне зависимости от того, заморожены они или нет.

453
01:37:26,565 --> 01:37:42,380
Такой подход портит предобученные модели с уже посчитанными значениями средних и стандартных отклонений.

454
01:37:42,480 --> 01:38:05,170
В fastai в процессе обучения в замороженных слоях ничего не меняется.

455
01:38:05,170 --> 01:38:17,959
Опыт показывает, что это полезный подход, особенно, если ваш датасет похож на данные,

456
01:38:18,059 --> 01:38:27,515
на которых обучалась предобученная модель.

457
01:38:27,615 --> 01:38:50,480
Вопрос из зала: Все эти махинации не растягивают длину эпохи до бесконечности?

458
01:38:52,719 --> 01:39:16,804
Нет, эти вычисления очень быстрые в сравнении с матричным умножением, которое делает self.conv().

459
01:39:16,904 --> 01:39:24,000
Вопрос из зала: Где в архитектуре сети стоит слой нормализации батчей?

460
01:39:25,320 --> 01:39:34,840
Сейчас к этому перейдём. У нас, как и в оригинальной статье про нормализацию минибатчей,

461
01:39:34,840 --> 01:39:38,849
нормализация минибатчей выполняется после выпрямителя.

462
01:39:41,749 --> 01:39:48,800
Изучение абляции (ablation study) — это анализ работы вашей модели, при котором

463
01:39:48,800 --> 01:39:57,734
вы добавляете и убираете различные части модели, чтобы понять, какие из них нужны.

464
01:39:57,834 --> 01:40:04,239
В оригинальной статье о нормализации минибатчей не было изучения абляции,

465
01:40:06,979 --> 01:40:14,564
поэтому не было информации о том, где лучше ставить этот слой.

466
01:40:14,664 --> 01:40:23,129
Авторы статьи выбрали неудачное место для слоя нормализации минибатчей, и это принесло людям много проблем.

467
01:40:23,229 --> 01:40:35,539
Лучшее место уже найдено, но мне до сих пор говорят — "У вас нормализация минибатчей не там, где надо",

468
01:40:35,539 --> 01:40:46,689
и я каждый раз объясняю им, что я читал статью, но так действительно лучше. Каждый раз неловко.

469
01:40:46,989 --> 01:41:17,530
Вопрос из зала: При создании модели маленькие датасеты вроде CIFAR-10 используются для предобучения?

470
01:41:17,530 --> 01:41:45,559
Думаю, да, если маленький датасет похож на основной и вы хотите посмотреть, масштабируются ли выученные знания.

471
01:41:45,559 --> 01:41:53,239
Я гораздо больше заинтересован в изучении маленьких датасетов как основных, а не вспомогательных,

472
01:41:55,570 --> 01:42:08,034
потому что в реальных датасетах не миллион изображений, а от 2 до 20 тысяч.

473
01:42:08,134 --> 01:42:16,905
Маленькие датасеты ценнее на практике.

474
01:42:17,005 --> 01:42:24,230
Маленький размер изображений нравится мне тем, что, во-первых, такие изображения быстро обрабатываются,

475
01:42:24,770 --> 01:42:34,515
а во вторых, потому что зачастую вся необходимая информация заключается в маленькой части изображения.

476
01:42:38,150 --> 01:42:50,239
Вопрос из зала: У меня два вопроса. Первый — что вы можете сказать по поводу стартапа Vicarious?

477
01:42:50,239 --> 01:42:57,350
Вопрос из зала: Второй вопрос связан с выступлением Ali Rahimi на NIPS 2017.

478
01:42:57,350 --> 01:43:10,317
Вопрос из зала: Али назвал машинное обучение алхимией и призвал внести больше ясности в алгоритмы,

479
01:43:10,417 --> 01:43:23,385
Вопрос из зала: которым мы доверяем важные задачи. Ян Лекун негативно отреагировал на это выступление

480
01:43:23,485 --> 01:43:36,452
Вопрос из зала: и сказал, что в истории человечества всегда сначала появлялись рабочие методы,

481
01:43:36,552 --> 01:43:49,520
Вопрос из зала: а уже потом учёные реагировали на спрос. Что вы скажете по этому поводу?

482
01:43:49,520 --> 01:43:55,815
Я не могу ничего сказать про Vicarious, потому что не читал их статьи.

483
01:43:55,915 --> 01:44:04,710
Я не помню, достигли ли они каких-то успехов, но за последние двадцать месяцев могли сильно продвинуться.

484
01:44:04,810 --> 01:44:13,305
Я следил за дискуссией Али и Яна Лекуна. Это очень интересно, потому что они оба умны и им есть что сказать.

485
01:44:13,405 --> 01:44:23,415
Многие люди неправильно поняли Али, и, возможно, он высказался недостаточно ясно,

486
01:44:23,515 --> 01:44:38,580
но позже уточнил, что хотел бы видеть больше экспериментов, а не теоретических статей.

487
01:44:38,680 --> 01:44:46,250
Али также жалеет, что использовал слово строгость — и я тоже жалею, потому что в нём нет смысла.

488
01:44:46,250 --> 01:44:50,780
Всем и так понятно, что, когда он говорит "Исследования должны быть строгими", он имеет в виду

489
01:44:50,780 --> 01:45:08,510
"Все должны заниматься тем же, чем и я, и на таком же высоком уровне".

490
01:45:08,510 --> 01:45:14,010
Я собирался встретиться с ним в январе и обсудить это, может, что-то прояснится.

491
01:45:14,110 --> 01:45:22,050
Я думаю, и Али, и Ян Лекун согласились бы с тем, что продуманные эксперименты — это важно,

492
01:45:22,150 --> 01:45:30,290
а бездумное использование огромного количества GPU — скучно и бессмысленно,

493
01:45:30,290 --> 01:45:39,010
и нужно стараться придумывать эксперименты, которые дадут нам больше информации о работе моделей.

494
01:45:39,110 --> 01:46:06,740
Янет: Дропаут — метод регуляризации, а нормализация минибатчей — гарантия сходимости алгоритма оптимизации?

495
01:46:06,740 --> 01:46:12,880
Да. Я считаю, что нормализацию минибатчей нужно использовать везде.

496
01:46:12,880 --> 01:46:24,360
Есть версии алгоритма, которые не всегда хорошо работают, но люди научились с этим работать.

497
01:46:24,460 --> 01:46:44,610
Я бы советовал всегда искать способ ввести нормализацию минибатчей на каждом слое. С RNN это сложнее, но возможно.

498
01:46:44,710 --> 01:46:52,240
Вопрос из зала: Использование нормализации минибатчей позволяет не нормировать входные данные?

499
01:46:52,340 --> 01:47:12,495
Да, но лучше всё равно нормируйте входные данные, это несложно и делает данные удобнее для использования.

500
01:47:12,595 --> 01:47:25,010
Не все библиотеки хорошо обрабатывают нормализацию минибатчей предобученных моделей,

501
01:47:25,010 --> 01:47:40,170
поэтому кто-то может обучать свою модель поверх вашей, и средние значения данных не совпадут, будут проблемы.

502
01:47:40,270 --> 01:47:53,894
В какой-то промежуток времени я не выполнял нормализацию и проблем не было, но так делать не стоит.

503
01:47:53,994 --> 01:48:06,240
Итак, в новом классе ConvBnNet используется BnLayer. Ещё одно изменение — эта строка.

504
01:48:06,340 --> 01:48:14,840
Я вставил свёрточный слой с большими фильтрами и шагом stride=1 в начале модели.

505
01:48:14,840 --> 01:48:23,360
Я сделал это, чтобы на следующий слой подавались более богатые признаками данные.

506
01:48:26,090 --> 01:48:29,630


507
01:48:29,630 --> 01:48:41,810


508
01:48:41,810 --> 01:48:49,520


509
01:48:49,520 --> 01:48:57,590


510
01:48:57,590 --> 01:49:04,730


511
01:49:08,260 --> 01:49:14,389


512
01:49:14,389 --> 01:49:19,429


513
01:49:19,429 --> 01:49:25,219


514
01:49:25,219 --> 01:49:31,400


515
01:49:31,400 --> 01:49:39,380


516
01:49:39,380 --> 01:49:44,599


517
01:49:44,599 --> 01:49:50,119


518
01:49:50,690 --> 01:49:55,099


519
01:49:55,099 --> 01:50:00,380


520
01:50:00,380 --> 01:50:04,550


521
01:50:04,550 --> 01:50:10,550
Для этого я добавил дополнительный свёрточный слой в начале.

522
01:50:10,550 --> 01:50:20,579
После него идут другие свёрточные слои, слой адаптивной подвыборки максимумом и линейный слой классификации.

523
01:50:20,679 --> 01:50:38,510
Последние изменения позволили увеличить долю правильных ответов с 60% до 68%.

524
01:50:38,510 --> 01:50:44,445
Мы добавили нормализацию минибатчей и дополнительный свёрточный слой.

525
01:50:44,545 --> 01:50:52,880
Доля правильных ответов всё ещё растёт, это обнадёживает.

526
01:50:52,880 --> 01:51:04,275
Раз всё идёт хорошо, можно попробовать увеличить глубину модели.

527
01:51:04,375 --> 01:51:10,095
Я не могу просто добавить больше свёрточных слоёв с шагом stride=2, потому что

528
01:51:10,195 --> 01:51:18,255
такие слои уменьшают размер изображения, а у меня уже 2x2.

529
01:51:18,355 --> 01:51:41,100
Вместо этого после каждого слоя с шагом stride=2 вставлю слой с шагом stride=1, он не меняет размер.

530
01:51:41,200 --> 01:51:57,375
Нейронная сеть стала в два раза глубже, а размер последнего слоя всё ещё 2x2.

531
01:51:57,475 --> 01:52:16,815
Это не помогло, потому что нормализация минибатчей не справляется с такой глубокой сетью —

532
01:52:16,915 --> 01:52:30,860
десять обычных свёрточных слоёв, один свёрточный слой в начале и линейный слой в конце, всего 12.

533
01:52:34,520 --> 01:52:45,410
Свёрточную сеть с двенадцатью слоями сложно обучать. Вместо этого используем архитектуру ResNet.

534
01:52:49,190 --> 01:52:55,160
ResNet — финальная стадия разработки нашей модели компьютерного зрения.

535
01:52:55,160 --> 01:53:06,910
Класс ResnetLayer() наследуется от класса BnLayer и переопределяет метод forward().

536
01:53:06,910 --> 01:53:17,739
Нейронная сеть глубже в три раза и обучается превосходно, всё из-за этой строки.

537
01:53:17,739 --> 01:53:42,345
Почему это работает? y = x + f(x), где y — предсказания, x — входные данные, f(x) — какая-то функция, здесь свёртки.

538
01:53:42,445 --> 01:54:09,650
После очевидного преобразования получаем f(x) = y - x.

539
01:54:11,739 --> 01:54:24,105
Блок ResNet пытается подобрать функцию f(x) к значению разности (y - x).

540
01:54:24,205 --> 01:54:31,120
Эта разность называется остаток (residual).

541
01:54:35,910 --> 01:54:48,700
Если y — целевая переменная, а x — активации предыдущего слоя, то их разница — это ошибка.

542
01:54:51,430 --> 01:55:08,760
Мы подбираем веса свёрточных фильтров так, чтобы компенсировать эту ошибку.

543
01:55:08,760 --> 01:55:43,655
Мы последовательно предсказываем и прибавляем разницу между текущим и желаемым результатами.

544
01:55:43,755 --> 01:55:56,470
Таким образом, каждый раз при наличии ошибки мы создаём модель, которая её предсказывает,

545
01:55:56,470 --> 01:56:04,450
и добавляем модель к предыдущей, и так много раз, пока предсказания не приблизятся к истинным значениям.

546
01:56:06,640 --> 01:56:18,450
Это основано на теории бустинга. Бустинг уже знаком тем, кто раньше занимался машинным обучением.

547
01:56:18,550 --> 01:56:53,390
Здесь бустинг реализуется очень просто — этой строкой, очень круто.

548
01:56:53,510 --> 01:57:15,600
После каждого слоя с нормализацией минибатчей и шагом stride=2 стоят два слоя ResNet с шагом stride=1.

549
01:57:15,600 --> 01:57:26,124
Нейронная сеть стала глубже в три раза, я сгруппировал слои по три.

550
01:57:26,224 --> 01:57:31,431
Свёрточный слой в начале и линейный в конце не меняются.

551
01:57:31,531 --> 01:57:49,655
Сеть может очень долго обучаться, на доле правильных ответов в 82% мне стало скучно.

552
01:57:49,755 --> 01:58:03,510
С появлением ResNet стало возможным создавать очень глубокие нейронные сети.

553
01:58:03,510 --> 01:58:43,949
В изначальной версии архитектуры в методе forward выполняется две свёртки, а у нас одна.

554
01:58:44,049 --> 01:59:07,004
Один из трёх слоёв в группе — обычный свёрточный слой с шагом stride=2, он называется узкий слой (bottleneck layer).

555
01:59:07,104 --> 01:59:21,839
Это не блок ResNet, а обычный слой. В ResNet используется специальный блок для узкого слоя,

556
01:59:21,939 --> 01:59:27,260
мы обсудим его во второй части курса.

557
01:59:27,260 --> 01:59:32,620
Как видите, даже упрощённая версия ResNet работает отлично.

558
01:59:34,830 --> 01:59:44,349
В классе Resnet2 я увеличил размеры слоёв и добавил дропаут.

559
01:59:44,449 --> 01:59:57,494
Этот класс — не просто упрощённая версия ResNet, а хорошая отправная точка для построения современной архитектуры.

560
01:59:57,594 --> 02:00:03,806
Итак, я увеличил размеры и добавил дропаут 20%.

561
02:00:03,906 --> 02:00:13,189
После обучения и применения тестового дополнения данных доля правильных ответов дошла до 85%.

562
02:00:13,189 --> 02:00:17,900
Я написал этот Jupyter ноутбук за три часа,

563
02:00:19,369 --> 02:00:29,354
а в 2013-2014 годах такая модель была бы среди лучших достигнутых результатов.

564
02:00:29,454 --> 02:00:40,064
Модель хорошая. Сейчас лучшие результаты достигают 97%.

565
02:00:40,164 --> 02:00:46,309
Нашей модели ещё есть куда расти, но все лучшие результаты основаны на тех же техниках.

566
02:00:46,309 --> 02:00:57,979
Во второй части курса вы узнаете, что есть лучшие алгоритмы дополнения данных, регуляризации,

567
02:00:57,979 --> 02:01:04,000
есть пара мелких трюков, но архитектура ResNet не меняется.

568
02:01:06,470 --> 02:01:19,120
Вопрос из зала: Можно ли применять остаточное обучение к задачам не с изображениями?

569
02:01:19,120 --> 02:01:40,840
Да, но этого никто не делает. В NLP недавно появилась схожая архитектура Transformer.

570
02:01:40,840 --> 02:02:04,835
Больше применений не видел, хотя ResNet легко обобщается на другие виды данных, это перспективное направление.

571
02:02:04,935 --> 02:02:22,490
Последнее, что я покажу — один трюк из PyTorch, затравка ко второй части курса.

572
02:02:22,590 --> 02:02:29,680
Я покажу его на примере датасета с кошками и собаками с первой лекции.

573
02:02:29,680 --> 02:02:35,213
Мы реализуем архитектуру ResNet34.

574
02:02:35,313 --> 02:02:52,115
Числа 34, 50, 101 и так далее в ResNet означают размер группы слоёв и количество групп,

575
02:02:52,215 --> 02:03:09,370
это можно посмотреть в исходном коде PyTorch.

576
02:03:09,370 --> 02:03:22,360
Мы используем архитектуру ResNet34, но кое-что напишем сами. При заданной архитектуре модель можно получить так.

577
02:03:22,460 --> 02:03:33,279
Параметр pretrained=True значит, что я хочу использовать предобученную модель, то есть предвычисленные на ImageNet веса.

578
02:03:33,379 --> 02:03:39,310
Модель содержится в переменной m.

579
02:03:39,410 --> 02:04:00,475
Вот свёрточный слой conv1, размер его фильтров — 7x7, шаг stride=(2,2)и границы padding=(3, 3) (по двум осям).

580
02:04:00,575 --> 02:04:07,135
Затем идут слои нормализации минибатчей bn1 и выпрямителя relu.

581
02:04:07,235 --> 02:04:33,280
Слой layer1 содержит три блока ResNet.

582
02:04:33,380 --> 02:04:46,910
Некоторые слои содержат шаг stride=(2,2), это узкие слои.

583
02:04:47,010 --> 02:05:29,575
В слое BnLayer выпрямитель стоит перед нормализацией минибатчей, а здесь наоборот.

584
02:05:29,675 --> 02:05:52,361
Есть несколько версий с различным порядком этих слоёв, лучшая — PreAct ResNet, посмотрите сами.

585
02:05:52,461 --> 02:06:00,599
Итак, у нас классическая архитектура ResNet34, предобученная на изображениях ImageNet.

586
02:06:01,230 --> 02:06:16,014
В ImageNet тысяча классов, а нам нужно два, поэтому последний слой нужно удалить.

587
02:06:16,114 --> 02:06:24,379
При использовании класса ConvLearner fastai удаляет последние два слоя.

588
02:06:24,479 --> 02:06:47,484
Предпоследний слой — адаптивная подвыборка средним,

589
02:06:47,584 --> 02:06:57,504
в fastai она заменяется на комбинацию адаптивной подвыборки средним и адаптивной подвыборки максимумом.

590
02:06:57,604 --> 02:07:03,750
Одновременно с тем, как мы придумали эту технику, кто-то написал об этом статью,

591
02:07:03,750 --> 02:07:12,610
поэтому мы не успели заявить о своём открытии, но в других библиотеках такого нет, тем более по умолчанию.

592
02:07:12,710 --> 02:07:24,090
В образовательных целях мы сделаем это руками — удалим последние два слоя и добавим новые:

593
02:07:24,090 --> 02:07:39,909
свёрточный слой с двумя выходными фильтрами, адаптивную подвыборку средним и Softmax.

594
02:07:40,009 --> 02:07:59,635
У нас был полносвязный слой, а теперь есть свёрточный с двумя фильтрами, то есть матрицей 2x7x7.

595
02:07:59,735 --> 02:08:11,610
После слоя подвыборки средним получим два числа и используем их для получения вероятностей.

596
02:08:13,230 --> 02:08:19,030
Потом станет видно, почему мы не добавили просто полносвязный слой.

597
02:08:19,130 --> 02:08:23,650
Для создания модели используются обычные методы и функции fastai —

598
02:08:23,750 --> 02:08:34,595
tfms_from_model(), ImageClassifierData.from_paths(), ConvLearner.from_model_data().

599
02:08:34,695 --> 02:08:41,970
Для обучения замораживаю все слои, кроме последнего.

600
02:08:41,970 --> 02:08:51,475
Доля правильных ответов — 99%, подход хороший.

601
02:08:51,575 --> 02:09:01,090
Теперь можно выполнить алгоритм CAM (class activation maps).

602
02:09:01,190 --> 02:09:21,970
Алгоритм покажет, какие части изображения важнее всего для классификации.

603
02:09:22,070 --> 02:09:28,980
Вот пример работы алгоритма.

604
02:09:28,980 --> 02:09:48,450
Алгоритм создаёт матрицу чисел, по которым потом рисуется тепловая карта.

605
02:09:48,450 --> 02:10:27,972
Матрица получается произведением матрицы feat на вектор py, где py — предсказания,

606
02:10:28,072 --> 02:11:01,415
а feat — результат работы последнего свёрточного слоя, та самая матрица 2x7x7.

607
02:11:01,515 --> 02:11:20,705
При умножении матрицы feat на вектор [1, 0] останется только первый канал,

608
02:11:20,805 --> 02:11:30,630
то есть те числа, которые определяют, что на изображении кошка.

609
02:11:30,630 --> 02:11:45,505
Первый канал матрицы feat отвечает за кошек, второй — за собак, при умножении будет такой результат.

610
02:11:45,605 --> 02:11:53,230
Полученная матрица показывает, какие части изображения больше всего похожи на кошку.

611
02:11:53,330 --> 02:12:01,590
После последнего свёрточного слоя стоит только слой подвыборки средним,

612
02:12:01,590 --> 02:12:05,630
который вычисляет среднее значение каждого фильтра 7x7,

613
02:12:05,630 --> 02:12:16,659
то есть насколько изображение в среднем показывает кошку, а насколько — собаку.

614
02:12:16,759 --> 02:12:33,179
Я могу взять матрицу, по которой считалось среднее, растянуть её до размеров изображения и наложить на него.

615
02:12:33,279 --> 02:12:46,140
Если у вас большие изображения, посчитайте эту матрицу сначала на маленьких размерах,

616
02:12:46,240 --> 02:13:01,850
найдите самый активный участок, приблизьте изображение так, чтобы он был виден лучше, и так несколько раз.

617
02:13:01,850 --> 02:13:16,034
У нас кончилось время, поэтому я только затронул эту тему. Подробнее расскажу во второй части, обсудим на форуме.

618
02:13:16,134 --> 02:13:27,745
Я не рассказал, как получить матрицу последнего слоя, дам материалы почитать.

619
02:13:27,845 --> 02:13:40,125
Для этого есть специальная техника — петля (hook), она вызывается в классе SaveFeatures().

620
02:13:40,225 --> 02:13:52,250
Конструктор класса вызывается после выполнения каждого слоя, это похоже на callback-функцию.

621
02:13:52,250 --> 02:14:15,980
С помощью петли я и сохранил выходные данные нужного мне слоя.

622
02:14:15,980 --> 02:14:27,179
Это описано в документации PyTorch.

623
02:14:27,279 --> 02:14:43,302
Янет: Расскажите, как вы пришли к глубокому обучению и что нам нужно делать, чтобы оставаться в курсе дела.

624
02:14:43,402 --> 02:14:49,860
Сконцентрируюсь на втором вопросе.

625
02:14:50,090 --> 02:15:03,210
Если вас заинтересовало глубокое обучение, советую пройти вторую часть курса. Кто хочет? Отлично, почти все.

626
02:15:03,310 --> 02:15:12,479
Во второй части я буду ожидать от вас полного понимания материалов первой части.

627
02:15:12,579 --> 02:15:27,980
У вас ещё есть время наверстать, но темп работы будет такой же, как и сейчас.

628
02:15:27,980 --> 02:15:34,529
Большинство закончивших прошлый запуск курса студентов посмотрели каждое видео в среднем три раза,

629
02:15:34,629 --> 02:15:44,130
некоторые даже случайно выучили отдельные части наизусть. Видеозаписи лекций помогают.

630
02:15:44,230 --> 02:15:50,810
Важно понимать всё написанное в Jupyter ноутбуках, не сверяясь с лекциями.

631
02:15:50,810 --> 02:15:56,775
Чтобы сделать процесс обучения более интересным, повторяйте изученное на различных датасетах.

632
02:15:56,875 --> 02:16:17,030
Читайте наш форум и выкладывайте свежие статьи, со временем научитесь их читать.

633
02:16:17,030 --> 02:16:23,234
Главное — не сдавайтесь. Всегда что-то будет непонятно, но вы обучаетесь —

634
02:16:23,334 --> 02:16:32,381
первые две лекции нашего курса сейчас покажутся вам очевидной болтовнёй.

635
02:16:32,380 --> 02:16:38,579
Надеюсь, что этот курс помог вам узнать что-то новое.

636
02:16:38,680 --> 02:16:44,335
Как показывает опыт, преуспевают те, кто продолжает работу несмотря ни на что.

637
02:16:44,435 --> 02:16:49,039
Пропуская эти лекции по понедельникам, вы стопорите своё обучение.

638
02:16:49,139 --> 02:16:54,026
Я замечал, что по понедельникам в пять вечера активность на форуме резко возрастает —

639
02:16:54,126 --> 02:16:58,600
все спешат задать свои вопросы перед началом очередной лекции.

640
02:16:58,600 --> 02:17:05,076
Лекции кончились, постарайтесь найти новый стимул для обучения —

641
02:17:05,176 --> 02:17:10,001
например, скажите своему партнёру, что будете делать что-то новое каждую субботу следующие четыре недели,

642
02:17:10,001 --> 02:17:15,715
или рассказывайте про то, то собираетесь прочитать новую статью, или ещё что-нибудь.

643
02:17:15,815 --> 02:17:24,190
Надеюсь увидеть вас снова в марте. В любом случае — мне очень понравилось с вами работать,

644
02:17:24,190 --> 02:17:29,270
надеюсь, что активность на форуме продолжится. Спасибо вам всем.

