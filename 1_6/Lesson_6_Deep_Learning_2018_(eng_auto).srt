1
00:00:00,060 --> 00:00:09,719
welcome back less than six so this is
our penultimate lesson and believe it or
not a couple of weeks ago in Lesson four

2
00:00:15,929 --> 00:00:19,980
I mentioned I was going to share that
lesson with this terrific you know P

3
00:00:19,980 --> 00:00:25,529
researcher Sebastian Reuter which I did
and he he said he loved it and he's gone

4
00:00:25,529 --> 00:00:31,050
on to yesterday released this new post
he called optimization for deep learning

5
00:00:31,050 --> 00:00:36,510
highlights in 2017 in which he covered
basically everything that we talked

6
00:00:36,510 --> 00:00:42,090
about in that lesson and with some very
nice shout outs to some of the work that

7
00:00:42,090 --> 00:00:46,920
some of the students here have done
including when he talked about this

8
00:00:46,920 --> 00:00:58,020
separation of the separation of weight
decay from the momentum term and so he

9
00:00:58,020 --> 00:01:04,199
actually mentions here the opportunities
in terms of improved kind of software

10
00:01:04,199 --> 00:01:11,909
decoupling this allows and actually
links to the commits from an answer hah

11
00:01:11,909 --> 00:01:16,710
actually showing how to implement this
in fast AI so first a eyes code is
actually being used as a bit of a role

12
00:01:18,060 --> 00:01:25,170
model now he then covers some of these
learning rate tuning techniques that

13
00:01:25,170 --> 00:01:33,720
we've talked about and this is the SGD
our schedule it looks a bit different to

14
00:01:33,720 --> 00:01:36,780
what you're used to seeing this is on a
log curve this is the way that they show

15
00:01:36,780 --> 00:01:44,270
it on the paper and for more information
again links to two blog posts one from

16
00:01:44,270 --> 00:01:53,430
vitaly about this topic and and again
ananza ha is blog post on this topic so

17
00:01:53,430 --> 00:01:58,280
it's great to see that some of the work
from faster our students is already

18
00:01:58,280 --> 00:02:02,490
getting noticed and picked up and shared
and this blog post went on to get on the

19
00:02:02,490 --> 00:02:09,750
front page of hacker news so that's
pretty cool and hopefully more and more

20
00:02:09,750 --> 00:02:12,740
of this work or be picked up on sisters
released

21
00:02:12,740 --> 00:02:21,620
publicly so last week we were kind of
doing a deep dive into collaborative

22
00:02:21,620 --> 00:02:31,090
filtering and let's remind ourselves of
kind of what our final model looked like

23
00:02:34,300 --> 00:02:42,200
so in the end we kind of ended up
rebuilding the model that's actually in
the first a a library where we had an

24
00:02:47,630 --> 00:02:51,500
embedding so we had this little get
embedding function that grabbed an

25
00:02:51,500 --> 00:02:59,360
embedding and randomly initialize the
weights for the users and for the items

26
00:02:59,360 --> 00:03:02,480
that's the kind of generic term in our
case the items are movies
and the bias for the users the bias for

27
00:03:04,610 --> 00:03:11,540
the items and we had n factors embedding
size for each for each one of course the
biases just had a single one and then we

28
00:03:14,210 --> 00:03:18,980
grabbed the users and item in weddings
multiply them together summed it up each
row and add it on the bias terms pop

29
00:03:22,730 --> 00:03:27,430
that through a sigmoid to put it into
the range that we wanted so that was our

30
00:03:27,430 --> 00:03:35,420
model and one of you asked if we can
kind of interpret this information in
some way and I promised this week we

31
00:03:37,490 --> 00:03:42,340
would see how to do that so let's take a
look so we're going to start with the

32
00:03:42,340 --> 00:03:46,550
model we built here where we just used
that fast AI library

33
00:03:46,550 --> 00:03:52,850
collaborative data set from CSP and then
that get learner and then we fitted it
in three epochs 19 seconds we've got a

34
00:03:58,430 --> 00:04:09,770
pretty good result so what we can now do
is to analyze that model so you may

35
00:04:09,770 --> 00:04:15,650
remember right back when we started we
read in the movies CSV file but that's

36
00:04:15,650 --> 00:04:19,880
just a mapping from the ID of the movie
to the name of the movie and so we're

37
00:04:19,880 --> 00:04:24,370
just going to use that for display
purposes so we can see what we're doing

38
00:04:24,640 --> 00:04:27,639
because
not all of us have watched every movie

39
00:04:27,639 --> 00:04:33,520
I'm just going to limit this to the top
500 most populous or 3,000 most popular

40
00:04:33,520 --> 00:04:36,660
movies so we might have more chance of
recognizing the movies we're looking at

41
00:04:36,660 --> 00:04:43,270
and then I'll go ahead and change it
from the movie IDs from movie lens to
those unique IDs that we're using the

42
00:04:46,330 --> 00:04:57,340
contiguous IDs because that's what a
model has alright so inside the learn

43
00:04:57,340 --> 00:05:03,130
object that we create inside alona we
can always grab the PI torch model

44
00:05:03,130 --> 00:05:09,790
itself just by saying learn model okay
and like I'm going to kind of show you

45
00:05:09,790 --> 00:05:14,680
more and more of the code at the moment
so let's take a look at the definition

46
00:05:14,680 --> 00:05:21,700
of model and so a model is a property so
if you haven't seen a property before a

47
00:05:21,700 --> 00:05:26,979
property is just something in Python
which looks like a method when you

48
00:05:26,979 --> 00:05:32,800
define it that you can call it without
parentheses as we do here alright and so

49
00:05:32,800 --> 00:05:36,520
it kind of looks when you call it like
it's a regular attribute but it looks
like when you define it like it's a

50
00:05:37,960 --> 00:05:42,669
method so every time you call it it
actually runs this code okay and so in

51
00:05:42,669 --> 00:05:48,789
this case it's just a shortcut to grab
something called dot models model so you

52
00:05:48,789 --> 00:05:50,460
may be interested to know what that
looks like

53
00:05:50,460 --> 00:05:59,979
learn about models and so this is
there's a fast AI model type is a very

54
00:05:59,979 --> 00:06:06,160
thin wrapper for pite watch models so we
could take a look at this code filter

55
00:06:06,160 --> 00:06:19,720
model and see what that is it's only one
line of code okay and yeah we'll talk

56
00:06:19,720 --> 00:06:23,770
more about these in part two right but
basically that there's this very thin
wrapper and the main thing one of the

57
00:06:25,210 --> 00:06:28,690
main things that fast i out does is we
have this concept of layer groups where

58
00:06:28,690 --> 00:06:31,840
basically when you say here though
different learning rates and they're

59
00:06:31,840 --> 00:06:35,020
going to apply two different sets of
layers and that's something that's not

60
00:06:35,020 --> 00:06:39,070
in paid watch so when you say I want to
use this PI torch model

61
00:06:39,070 --> 00:06:42,820
all this with one thing we have to do
which is to say like okay one hour later

62
00:06:42,820 --> 00:06:47,260
groups yeah so the details aren't
terribly important but in general if you

63
00:06:47,260 --> 00:06:51,400
want to create a little wrapper for some
other pipe watch model you could just

64
00:06:51,400 --> 00:06:58,600
write something like this so to get to
get inside that to grab the actual PI

65
00:06:58,600 --> 00:07:04,720
torch model itself its models dot model
that's the PI torch model and then the

66
00:07:04,720 --> 00:07:11,020
learn object has a shortcut to that okay
so we're going to set m to be the PI
torch model and so when you print out a

67
00:07:14,350 --> 00:07:19,000
pipe watch model it prints it out
basically by listing out all of the

68
00:07:19,000 --> 00:07:24,730
layers that you created in the
constructor it's quite it's quite nifty

69
00:07:24,730 --> 00:07:28,900
actually when you kind of think about
the way this works thanks to kind of
some very handy stuff in Python we're

70
00:07:32,560 --> 00:07:39,460
actually able to use standard - oh wow
to kind of define these modules in these

71
00:07:39,460 --> 00:07:44,110
layers and they basically automatically
kind of register themselves with pipe

72
00:07:44,110 --> 00:07:49,480
which so back in our embedding bias we
just had a bunch of things where we said

73
00:07:49,480 --> 00:07:54,850
okay each of these things are equal to
these things and then it automatically

74
00:07:54,850 --> 00:08:00,640
knows how to represent that so you can
see there's the name is you and so the
name is just literally whatever we

75
00:08:02,170 --> 00:08:07,960
called it yeah you
and then the definition is it's this
kind of layer okay so that's our height

76
00:08:12,910 --> 00:08:21,490
watch model so we can look inside that
basically use that so if we say m dot I

77
00:08:21,490 --> 00:08:28,660
be then that's referring to the
embedding layer for an item which is the

78
00:08:28,660 --> 00:08:34,000
bias layer so an item bias in this case
is the movie bias so each move either a

79
00:08:34,000 --> 00:08:40,990
9000 of them has a single bias element
okay now the really nice thing about

80
00:08:40,990 --> 00:08:48,130
high torch layers and models is that
they all look the same they basically

81
00:08:48,130 --> 00:08:50,710
got to use them you call them as if they
were

82
00:08:50,710 --> 00:08:57,340
action so we can go m.i.b parenthesis
right and that basically says I want you

83
00:08:57,340 --> 00:09:03,160
to return the value of that layer and
that layer could be a full-on model
right so to actually get a prediction

84
00:09:06,220 --> 00:09:12,520
from a play torch model you just I would
go m and pass in my variable okay and so

85
00:09:12,520 --> 00:09:22,570
in this case my B and pass in my top
movie indexes now models remember layers

86
00:09:22,570 --> 00:09:29,140
they require variables not tensors
because it needs to keep track of the
derivatives okay and so we use this

87
00:09:31,900 --> 00:09:37,870
capital V to turn the tensor into a
variable and was just announced this

88
00:09:37,870 --> 00:09:44,260
week that PI torch 0.4 which is the
version after the one that's just about

89
00:09:44,260 --> 00:09:49,330
to be released is going to get rid of
variables and will actually be able to

90
00:09:49,330 --> 00:09:53,170
use tensors directly to keep track of
derivatives so if you're watching this
on the MOOC and you're looking at point

91
00:09:55,270 --> 00:09:58,540
four then you'll probably notice that
the code doesn't have this V unit

92
00:09:58,540 --> 00:10:02,410
anymore
and so that would be pretty exciting

93
00:10:02,410 --> 00:10:05,050
when that happens but for now we have to
remember if we're going to pass

94
00:10:05,050 --> 00:10:09,100
something into a model to turn it into a
variable first and remember a variable

95
00:10:09,100 --> 00:10:14,110
has a strict superset of the API of a
tensor so anything you can do to a

96
00:10:14,110 --> 00:10:20,140
tensor you can do to a variable and it
up will take its log or whatever okay so

97
00:10:20,140 --> 00:10:24,730
that's going to return a variable which
consists of going through each of these

98
00:10:24,730 --> 00:10:30,910
movie IDs putting it through this
embedding layer to get its bias okay and

99
00:10:30,910 --> 00:10:38,820
that's going to return a variable let's
take a look

100
00:10:41,790 --> 00:10:47,680
so before I press shift down to here you
can have a think about what I'm going to

101
00:10:47,680 --> 00:10:53,200
have I've got a list of 3,000 movies
going in turning into variable putting

102
00:10:53,200 --> 00:10:56,950
it through this embedding layer so just
have a think about what we expect to

103
00:10:56,950 --> 00:11:03,580
come out okay and we have a variable of
size 3,000 by one hopefully that doesn't

104
00:11:03,580 --> 00:11:06,550
surprise you
we had 3000 movies that we are looking
up each one hadn't had a one long

105
00:11:08,830 --> 00:11:12,940
embedding okay so there's our three
thousand one you'll notice it's a

106
00:11:12,940 --> 00:11:16,510
variable just not surprising because we
fed it a variable so we've got a
variable back and it's a variable that's

107
00:11:18,640 --> 00:11:26,230
on the GPU right doc CUDA okay so we
have a little shortcut in fast AI

108
00:11:26,230 --> 00:11:31,000
because we we very often when I take
variables turn them into tensors and

109
00:11:31,000 --> 00:11:33,910
move them back to the CPU so we can play
with them more easily

110
00:11:33,910 --> 00:11:39,670
so two NP is is two numpy okay and that
does all of those things and it works

111
00:11:39,670 --> 00:11:43,450
regardless of whether it's a tensor or a
variable it works regardless of whether

112
00:11:43,450 --> 00:11:49,150
it's on the CPU or GPU it'll end up
giving you a a numpy array from that
okay so if we do that that gives us

113
00:11:53,140 --> 00:11:58,930
exactly the same thing as we just looked
at but now in numpy form okay so that's

114
00:11:58,930 --> 00:12:04,180
a super handy thing to use when you're
playing around with pi torch my approach

115
00:12:04,180 --> 00:12:12,940
to things is I try to use numpy for
everything except when I explicit and

116
00:12:12,940 --> 00:12:17,170
you need something to run on the GPU or
I need its derivatives right in which
case I use PI torch because like none

117
00:12:19,330 --> 00:12:24,220
part like I kind of find none PI's often
easier to work with it's been around

118
00:12:24,220 --> 00:12:32,620
many years longer than PI torch so you
know and lots of things like the Python

119
00:12:32,620 --> 00:12:39,700
imaging library OpenCV and lots and lots
of stuff like pandas it works with numpy

120
00:12:39,700 --> 00:12:45,460
so my approach is kind of like do as
much as I can in num pile and finally

121
00:12:45,460 --> 00:12:49,120
when I'm ready to do something on the
GPU or take its derivative to PI torch

122
00:12:49,120 --> 00:12:53,410
and then as soon as I can I put it back
in vampire and you'll see that the first

123
00:12:53,410 --> 00:12:57,970
AI library really works this way like
all the transformations and stuff happen

124
00:12:57,970 --> 00:13:03,430
in lamb pie which is different to most
high torch computer vision libraries
which tend to do it all as much as

125
00:13:05,830 --> 00:13:13,780
possible in pi torch I try to do as much
as possible in non pipe so let's say we
wanted to transfer build a model in the

126
00:13:15,910 --> 00:13:21,070
GPU with the GPU and train it
then we want to bring this to production

127
00:13:21,070 --> 00:13:26,500
so would we call to numpy on the model
itself or would we have to iterate

128
00:13:26,500 --> 00:13:31,660
through all the different layers and
then call to NP yeah good question so

129
00:13:31,660 --> 00:13:36,310
it's very likely that you want to do
inference on a cpu rather than a GPU

130
00:13:36,310 --> 00:13:39,970
it's it's more scalable you don't have
to worry about putting things in batches

131
00:13:39,970 --> 00:13:48,220
you know and so forth so you can move a
model onto the cpu just by typing m dot

132
00:13:48,220 --> 00:13:55,660
CPU and that model is now on the cpu and
so therefore you can also then put your
variable on the CPU by doing exactly the

133
00:13:59,170 --> 00:14:08,170
same thing so you can say like so now
having said that if you're if you'll

134
00:14:08,170 --> 00:14:12,880
serve it doesn't have a GPU or CUDA GPU
you don't have to do this because it

135
00:14:12,880 --> 00:14:19,390
won't put it on the GPU at all so if for
inferencing on the server if you're

136
00:14:19,390 --> 00:14:24,460
running it on you know some t2 instance
or something it'll work fine and will

137
00:14:24,460 --> 00:14:30,760
run on the on the cpu automatically
quick follow-up and if we train the

138
00:14:30,760 --> 00:14:37,840
model on the GPU and then we save those
embeddings and the weights would we have

139
00:14:37,840 --> 00:14:45,040
to do anything special to load you know
you won't we have something well it kind
of depends how much of faster I you're

140
00:14:46,990 --> 00:14:51,990
using so I'll show you how you can do
that in case you have to do it manually

141
00:14:51,990 --> 00:15:02,170
one of the students figure this out
which is really handy when we there's a

142
00:15:02,170 --> 00:15:07,570
load model function and you'll see what
it does but it does torch dot load is it

143
00:15:07,570 --> 00:15:12,250
basically this is like some magic
incantation that like normally it has to

144
00:15:12,250 --> 00:15:16,300
load it onto the same GPU or saved on
but this will like load it into what it

145
00:15:16,300 --> 00:15:24,160
was what it is available so there's a
Andy discovery thanks for the great

146
00:15:24,160 --> 00:15:27,960
questions and

147
00:15:28,819 --> 00:15:36,619
to put that back on the GPU I'll need to
say doc CUDA and now there we go I can

148
00:15:36,619 --> 00:15:44,299
run it again okay so it's really
important to know about the zip function

149
00:15:44,299 --> 00:15:50,509
in Python which iterates through a
number of lists at the same time so in

150
00:15:50,509 --> 00:15:56,179
this case I want to grab each movie
along with its bias term so that I can

151
00:15:56,179 --> 00:16:00,259
just pop it into our list of tuples so
if I just go zip like that that's going

152
00:16:00,259 --> 00:16:06,319
to iterate through each movie ID and
each bias term and so then I can use

153
00:16:06,319 --> 00:16:11,289
that in a list comprehension to grab the
name of each movie along with its place

154
00:16:11,289 --> 00:16:20,239
okay so having done that I can then sort
and so here are I told you that John

155
00:16:20,239 --> 00:16:26,720
John Travolta Scientology movie at the
most negative of the quiet by a lot if

156
00:16:26,720 --> 00:16:30,799
this was a cable competition Battlefield
Earth would have like won by miles or

157
00:16:30,799 --> 00:16:36,379
this seven seven seven ninety six so
here's the worst movie of all time

158
00:16:36,379 --> 00:16:40,939
according to IMDB and like it's
interesting when you think about what

159
00:16:40,939 --> 00:16:44,329
this means right because this is like a
much more authentic way to find out how

160
00:16:44,329 --> 00:16:49,459
bad this movie is because like some
people are just more negative about

161
00:16:49,459 --> 00:16:53,869
movies right and like it more of them
watch your movie like you know highly

162
00:16:53,869 --> 00:16:57,319
critical audience they're gonna read it
badly so if you take an average it's not
quite fair right and so what this is you

163
00:17:02,149 --> 00:17:06,470
know what this is doing is saying once
we you know remove the fact that

164
00:17:06,470 --> 00:17:10,399
different people have different overall
positive or negative experiences and
different people watch different kinds

165
00:17:11,689 --> 00:17:17,269
of movies and we correct for all that
this is the worst movie of all time so

166
00:17:17,269 --> 00:17:20,169
that's a good thing to know

167
00:17:21,638 --> 00:17:29,360
so this is how we can yeah look inside
our our model and and interpret the bias

168
00:17:29,360 --> 00:17:36,559
vectors you'll see here I've sorted by
the zeroth element of each tuple by

169
00:17:36,559 --> 00:17:42,680
using a lambda originally I used this
special item ghetto this is part

170
00:17:42,680 --> 00:17:47,450
of pythons operator library and this
creates a function that returns the

171
00:17:47,450 --> 00:17:52,550
zeroth element of something in order to
save time and then I actually realize

172
00:17:52,550 --> 00:17:57,350
that the lambda is only one more
character to write then the item get us

173
00:17:57,350 --> 00:18:01,790
so maybe we don't need to know this
after all so yeah really useful to make

174
00:18:01,790 --> 00:18:06,290
sure you know how to write lambdas in
Python so this is this is a function

175
00:18:06,290 --> 00:18:11,540
okay and so sort the sort is going to
call this function every time it decides

176
00:18:11,540 --> 00:18:15,560
like is this thing higher or lower than
that other thing and this fact this is

177
00:18:15,560 --> 00:18:21,410
going to return the zeroth element okay
so here's the same thing and item get a

178
00:18:21,410 --> 00:18:27,890
format and here is the reverse and
Shawshank Redemption right at the top

179
00:18:27,890 --> 00:18:31,340
I'll definitely agree with that
Godfather usual suspects yeah these are
all pretty great movies twelve Angry Men

180
00:18:35,120 --> 00:18:43,100
absolutely so there you go there's how
we can look at the base so then the

181
00:18:43,100 --> 00:18:47,330
second piece to look at would be the the
embeddings how can we look at the

182
00:18:47,330 --> 00:18:53,180
embeddings so we can do the same thing
so remember I was the item embeddings

183
00:18:53,180 --> 00:18:58,030
rather than IV with the item bias we can
pass in our list of movies as a variable

184
00:18:58,030 --> 00:19:04,700
turn it into numpy and here's our movie
embedding so for each of the 3,000 most

185
00:19:04,700 --> 00:19:12,740
popular movies here are its 50
embeddings so it's very hard unless

186
00:19:12,740 --> 00:19:17,330
you're Geoffrey Hinton to visualize a 50
dimensional space so what we'll do is

187
00:19:17,330 --> 00:19:23,480
we'll turn it into a three dimensional
space so we can compress high

188
00:19:23,480 --> 00:19:26,690
dimensional spaces down into lower
dimensional spaces using lots of

189
00:19:26,690 --> 00:19:31,720
different techniques perhaps one of the
most common and popular is called PCA

190
00:19:31,720 --> 00:19:38,380
PCA stands for principle components
analysis it's a linear technique but

191
00:19:38,380 --> 00:19:44,870
when your techniques generally work fine
for this kind of embedding I'm not going
to teach you about PCA now but I will

192
00:19:46,460 --> 00:19:50,810
say in Rachel's computation or linear
algebra class which you can get to you

193
00:19:50,810 --> 00:19:58,310
from first at AI we cover PCA in
detail and it's a really important

194
00:19:58,310 --> 00:20:02,030
technique it actually it turns out to be
almost identical to something called

195
00:20:02,030 --> 00:20:06,790
singular value decomposition which is a
type of matrix decomposition which

196
00:20:06,790 --> 00:20:13,880
actually does turn up in deep learning a
little bit from time to time it's kind

197
00:20:13,880 --> 00:20:18,740
of somewhat worth knowing if you were
going to dig more into linear algebra

198
00:20:18,740 --> 00:20:24,440
you know SPD and PCA along with
eigenvalues and eigenvectors which are

199
00:20:24,440 --> 00:20:28,100
all slightly different versions is this
kind of the same thing or all worth
knowing but for now just know that you

200
00:20:30,770 --> 00:20:36,830
can grab PCA from SK learn to calm
position say how much you want to reduce

201
00:20:36,830 --> 00:20:41,540
the dimensionality too so I want to find
three components and what this is going

202
00:20:41,540 --> 00:20:48,380
to do is it's going to find three linear
combinations of the 50 dimensions which

203
00:20:48,380 --> 00:20:52,280
capture as much as the variation as
possible Badar is different to each
other as possible

204
00:20:53,690 --> 00:21:02,270
okay so we would call this a lower rank
approximation of our matrix all right

205
00:21:02,270 --> 00:21:06,490
so then we can grab the components so
that's going to be their three

206
00:21:06,490 --> 00:21:11,620
dimensions and so once we've done that
we've now got three by three thousand
and so we can now take a look at the

207
00:21:15,320 --> 00:21:18,980
first of them and we'll do the same
thing of using zip to look at each one

208
00:21:18,980 --> 00:21:24,350
along with its movie and so here's the
thing right we we don't know ahead of

209
00:21:24,350 --> 00:21:33,650
time what this PCA thing is it's just
it's just a bunch of latent factors you

210
00:21:33,650 --> 00:21:39,650
know it's it's kind of the the main axis
in this space of latent factors and so

211
00:21:39,650 --> 00:21:45,290
what we can do is we can look at it and
see if we can figure out what it's about
right so given that police academy for

212
00:21:49,550 --> 00:21:54,800
is high up here along with water world
where else Fargo Pulp Fiction and God

213
00:21:54,800 --> 00:22:00,200
further a high up here I'm gonna guess
that a high value is not going to

214
00:22:00,200 --> 00:22:06,200
represent like critically acclaimed
movies or serious watching so I kind of

215
00:22:06,200 --> 00:22:09,280
like all this yeah okay I call this easy
what she

216
00:22:09,280 --> 00:22:13,450
is serious all right but like this is
kind of how you have to interpret your

217
00:22:13,450 --> 00:22:18,910
embeddings is like take a look at what
they seem to be showing and decide what

218
00:22:18,910 --> 00:22:25,060
you think it means so this is the kind
of the the principal axis in this set of
embedding so we can look at the next one

219
00:22:27,910 --> 00:22:34,000
so do the same thing and look at the the
first index one embedding this one's a

220
00:22:34,000 --> 00:22:37,030
little bit harder to kind of figure out
what's going on but with things like

221
00:22:37,030 --> 00:22:42,310
Mulholland Drive and Purple Rose of
Cairo these look more kind of dialog II

222
00:22:42,310 --> 00:22:46,480
kind of ones or else things like Lord of
the Rings in the Latin and Star Wars

223
00:22:46,480 --> 00:22:50,260
these book more like kind of modern CGI
II kind of ones so you could kind of

224
00:22:50,260 --> 00:22:57,000
imagine that on that pair of dimensions
it probably represents a lot of you know

225
00:22:57,000 --> 00:23:02,440
differences between how people read
movies you know some people like you

226
00:23:02,440 --> 00:23:09,070
know purple rise of Cairo
type movies you know Woody Allen kind of
classic and some people like these you

227
00:23:11,140 --> 00:23:17,530
know big Hollywood spectacles some
people presumably like police academy
for more than they like Fargo so yeah so

228
00:23:22,810 --> 00:23:26,380
I'm like you can kind of get the idea of
what's happened it's it's done a you

229
00:23:26,380 --> 00:23:36,730
know through a model which was you know
for a model which was literally multiply

230
00:23:36,730 --> 00:23:43,990
two things together and Adam hop it's
learnt quite a lot you know which is

231
00:23:43,990 --> 00:23:53,110
kind of cool so that's what we can do
with with that and then we could we

232
00:23:53,110 --> 00:24:00,370
could plot them if we wanted to I just
grabbed a small subset to plot on those

233
00:24:00,370 --> 00:24:09,010
first two asses all right so that's that
so I wanted to next kind of dig in a

234
00:24:09,010 --> 00:24:18,300
layer deeper into what actually happens
when we say fit alright so when we said

235
00:24:18,300 --> 00:24:23,700
learn fit what's it doing

236
00:24:24,850 --> 00:24:30,790
for something like the store model is it
a way to interpret the embeddings for

237
00:24:30,790 --> 00:24:35,230
something like this the rustman one yes
yeah we'll see that in a moment well

238
00:24:35,230 --> 00:24:51,310
let's jump straight there what the hell
okay so so for the rustman how much are

239
00:24:51,310 --> 00:25:04,270
we going to sell at each store on each
date model we this is from the paper

240
00:25:04,270 --> 00:25:09,340
gore and burke on it so it's a great
paper by the way well worth you know

241
00:25:09,340 --> 00:25:14,950
like pretty accessible I think any of
you would at this point be able to at

242
00:25:14,950 --> 00:25:18,790
least get the gist of it if you know and
much of the detail as well particularly

243
00:25:18,790 --> 00:25:23,260
as you've also done the machine learning
course and they actually make this point

244
00:25:23,260 --> 00:25:28,090
in the paper this is in the paper that
the equivalent of what they call entity

245
00:25:28,090 --> 00:25:33,370
embedding layers so an embedding of a
categorical variable is identical to a

246
00:25:33,370 --> 00:25:39,550
one hot encoding followed by a matrix
multiply that's why they're basically

247
00:25:39,550 --> 00:25:44,140
saying if you've got three embeddings
that's the same as doing three one hot

248
00:25:44,140 --> 00:25:48,370
encodings putting each through one
through a matrix multiply and then put
that through a a dense layer

249
00:25:50,680 --> 00:25:56,040
well what pi torch would call a linear
oh yeah right

250
00:25:56,040 --> 00:26:00,220
one of the nice things here is because
this is kind of like well they thought

251
00:26:00,220 --> 00:26:03,730
it was the first paper is actually the
second I think paper to show the idea of

252
00:26:03,730 --> 00:26:08,500
using categorical embeddings for this
kind of data set they really go to clean

253
00:26:08,500 --> 00:26:13,510
too quite a lot of detail to you know
right back to the the detailed stuff

254
00:26:13,510 --> 00:26:15,520
that we learnt about so it's kind of a
second

255
00:26:15,520 --> 00:26:21,610
you know a second cat of thinking about
what embeddings are doing so one of the

256
00:26:21,610 --> 00:26:25,420
interesting things that they did was
they said okay after we've trained a

257
00:26:25,420 --> 00:26:34,930
neural net with these embeddings what
else could we do with it so

258
00:26:34,930 --> 00:26:41,170
they got a winning result with a neural
network where the entity meetings but

259
00:26:41,170 --> 00:26:45,070
then they said hey you know what
we could take those empty embeddings and
replace each categorical variable with

260
00:26:48,070 --> 00:26:54,310
the learnt entity embeddings and then
feed that into a GBM right so in other

261
00:26:54,310 --> 00:26:58,840
words like rather than passing into the
GBM a one modern coded version or an

262
00:26:58,840 --> 00:27:04,540
ordinal version let's actually replace
the categorical variable with its

263
00:27:04,540 --> 00:27:11,400
embedding for the appropriate level for
that row right so it's actually a way of

264
00:27:11,400 --> 00:27:18,250
create you know feature engineering and
so the main average percent error

265
00:27:18,250 --> 00:27:25,960
without that for gbms I'm using just 100
codings was 0.15 but with that it was

266
00:27:25,960 --> 00:27:34,180
0.11 that random forests without that
was point one six with that 0.108 nearly

267
00:27:34,180 --> 00:27:39,220
as good as the neural net right so this
is kind of an interesting technique

268
00:27:39,220 --> 00:27:45,190
because what it means is in your
organization you can train a neural net

269
00:27:45,190 --> 00:27:50,050
that has an embedding of stores and an
embedding of product types and an

270
00:27:50,050 --> 00:27:54,340
embedding of I don't know whatever kind
of high cardinality or even medium

271
00:27:54,340 --> 00:27:58,150
cardinality categorical variables you
have and then everybody else in the

272
00:27:58,150 --> 00:28:03,580
organization can now like chuck those
into their you know JVM or random forest
or whatever and I'm use them and what

273
00:28:07,210 --> 00:28:11,670
this is saying is they won't get in fact
you can even use K nearest neighbors

274
00:28:11,670 --> 00:28:17,770
with this technique and get nearly as
good a result right so this is a good

275
00:28:17,770 --> 00:28:21,370
way of kind of giving the power of
neural nets to everybody in your

276
00:28:21,370 --> 00:28:26,470
organization without having them do the
faster idea of learning course first you

277
00:28:26,470 --> 00:28:30,730
know they can just use whatever SK learn
or R or whatever that they're used to

278
00:28:30,730 --> 00:28:35,740
and like those those embeddings could
literally be in a database table because

279
00:28:35,740 --> 00:28:40,210
if you think about an embedding is just
an index lookup right which is the same

280
00:28:40,210 --> 00:28:45,940
as an inner join in SQL right so if
you've got a table on each product along

281
00:28:45,940 --> 00:28:48,830
with its embedding vector then you can
literally do
in a joint and now you have every row in

282
00:28:51,830 --> 00:28:55,880
your table along with its product
embedding vector so that's a really this

283
00:28:55,880 --> 00:29:02,930
is this is a really useful idea and
gbm's and random forests learn a lot

284
00:29:02,930 --> 00:29:07,400
quicker than neural nets do all right so
that's like even if you do know how to

285
00:29:07,400 --> 00:29:12,530
train your on its this is still
potentially quite handy so here's what
happened when they took the various

286
00:29:14,120 --> 00:29:19,400
different states of Germany and plotted
the first two principal components of

287
00:29:19,400 --> 00:29:23,420
their embedding vectors and they
basically here is where they were in

288
00:29:23,420 --> 00:29:30,260
that 2d space and wacken lee enough i've
circled in red three cities and i've

289
00:29:30,260 --> 00:29:34,790
circled here the three cities in Germany
and here I've circled in purple so blue

290
00:29:34,790 --> 00:29:41,600
here at the blue here's the green here's
the green so it's actually drawn a map

291
00:29:41,600 --> 00:29:47,870
of Germany even though it never was told
anything about how far these states are

292
00:29:47,870 --> 00:29:52,460
away from each other or the very concept
of geography didn't exist so that's

293
00:29:52,460 --> 00:30:01,610
pretty crazy so that was from there
paper so I went ahead and looked well
here's another thing I think this is

294
00:30:03,290 --> 00:30:10,190
also from their paper they took every
pair of places and they looked at how

295
00:30:10,190 --> 00:30:15,980
far away they are on a map versus how
far away are they in embedding space and

296
00:30:15,980 --> 00:30:23,240
they've got this beautiful correlation
alright so again it kind of apparently

297
00:30:23,240 --> 00:30:30,340
you know it's doors that are near by
each other physically have similar

298
00:30:30,340 --> 00:30:36,050
characteristics in terms of when people
buy more or less stuff from them so I

299
00:30:36,050 --> 00:30:41,060
looked at the same thing four days of
the week right so here's an embedding of

300
00:30:41,060 --> 00:30:45,260
the days of the week from our model and
I just kind of joined up Monday Tuesday

301
00:30:45,260 --> 00:30:48,170
Wednesday Tuesday Thursday Friday
Saturday Sunday I did the same thing for

302
00:30:48,170 --> 00:30:52,850
the months of the year all right again
you can see you know here's here's
winter here's summer so yeah I think

303
00:30:58,640 --> 00:31:03,100
like visualize
embeddings can be interesting like it's
good to like first of all check you can

304
00:31:06,070 --> 00:31:10,720
see things you would expect to see you
know and then you could like try and see

305
00:31:10,720 --> 00:31:14,280
like maybe things you didn't expect to
see so you could try all kinds of

306
00:31:14,280 --> 00:31:23,559
clusterings or or whatever and this is
not something which has been widely

307
00:31:23,559 --> 00:31:27,100
studied at all right so I'm not going to
tell you what the limitations are of

308
00:31:27,100 --> 00:31:34,210
this technique or whatever oh yes so
I've heard of other ways to generate

309
00:31:34,210 --> 00:31:40,059
embeddings like skip grams uh-huh
wondering if you could say is there one

310
00:31:40,059 --> 00:31:44,830
better than the other using your own
Network sir skip grams so screwed grams

311
00:31:44,830 --> 00:31:49,679
is quite specific to NLP right so like

312
00:31:49,860 --> 00:31:56,410
I'm not sure if we'll cover it in this
course but basically the the approach to

313
00:31:56,410 --> 00:32:01,600
original kind of word to vac approach to
generating embeddings was to say you
know what we actually don't have we

314
00:32:07,300 --> 00:32:12,400
don't actually have our labelled data
set you know they said all we have is

315
00:32:12,400 --> 00:32:16,720
like google books and so they have an
unsupervised learning problem

316
00:32:16,720 --> 00:32:21,190
unlabeled problem and so the best way in
my opinion to turn an unlabeled problem

317
00:32:21,190 --> 00:32:25,570
into a labelled problem is to kind of
invent some labels and so what they did
in the word to vet case was they said

318
00:32:27,700 --> 00:32:33,490
okay here's a sentence with 11 words in
it right and then they said okay let's

319
00:32:33,490 --> 00:32:44,500
delete the middle word and replace it
for the random word and so you know

320
00:32:44,500 --> 00:32:53,500
originally it said cat and they say no
let's replace that with justice all

321
00:32:53,500 --> 00:32:58,929
right so before it said the cute little
cat sat on the fuzzy mat and now it says

322
00:32:58,929 --> 00:33:01,840
the cute little justice sat on the fuzzy
man
right and what they do is they do that

323
00:33:03,970 --> 00:33:10,170
so they have one sentence where they
keep exactly as is

324
00:33:12,120 --> 00:33:16,710
and then they make a copy of it and they
do the replacement and so then they have

325
00:33:16,710 --> 00:33:22,680
a label where they say it's a one if it
was unchanged it was the original and

326
00:33:22,680 --> 00:33:29,280
zero otherwise okay and so basically
then you now have something you can

327
00:33:29,280 --> 00:33:33,330
build a machine learning model on and so
they went and build a machine learning

328
00:33:33,330 --> 00:33:40,800
model on this so the model was like try
and find the effect sentences not

329
00:33:40,800 --> 00:33:44,910
because they were interested in a fake
sentence binder but because as a result

330
00:33:44,910 --> 00:33:48,360
they now have embeddings that just like
we discussed you can now use for other
purposes and that became word to vet now

331
00:33:50,720 --> 00:33:58,290
it turns out that if you do this as just
a kind of a effectively like a single

332
00:33:58,290 --> 00:34:01,860
matrix multiply rather than make it a
deep neural net you can train this super

333
00:34:01,860 --> 00:34:06,720
quickly and so that's basically what
they did with they'd met there though

334
00:34:06,720 --> 00:34:11,700
they kind of decided we're going to make
a pretty crappy model like a shallow

335
00:34:11,699 --> 00:34:15,989
learning model rather than a deep model
you know with the downside it's a less

336
00:34:15,989 --> 00:34:20,159
powerful model but a number of upsides
the first thing we can train it on a

337
00:34:20,159 --> 00:34:24,479
really large data set and then also
really importantly we're going to end up

338
00:34:24,480 --> 00:34:30,150
with embeddings which have really very
linear characteristics so we can like

339
00:34:30,150 --> 00:34:37,380
add them together and subtract them and
stuff like that okay so that so there's

340
00:34:37,380 --> 00:34:41,610
a lot of stuff we can learn about there
from like for other types of embedding

341
00:34:41,610 --> 00:34:45,930
like categorical embeddings and
specifically if we want categorical
embeddings which we can kind of draw

342
00:34:48,810 --> 00:34:52,710
nicely and expect them to us to be able
to add and subtract them and behave

343
00:34:52,710 --> 00:34:57,810
linearly you know probably if we want to
use them in k-nearest neighbors and

344
00:34:57,810 --> 00:35:03,990
stuff we should probably use shallow
learning if we want something that's

345
00:35:03,990 --> 00:35:09,300
going to be more predictive we probably
want to use a neural net and so actually

346
00:35:09,300 --> 00:35:17,130
an NLP I'm really pushing the idea that
we need to move past word to backhand

347
00:35:17,130 --> 00:35:21,960
glove these linear based methods because
it turns out that those embeddings are

348
00:35:21,960 --> 00:35:25,260
way less predictive than embeddings
learnt from

349
00:35:25,260 --> 00:35:29,220
models and so the language model that we
learned about which ended up getting a
state-of-the-art on sentiment analysis

350
00:35:30,750 --> 00:35:35,790
didn't used a lot more work to vet that
instead we pre trained a deep recurrent

351
00:35:35,790 --> 00:35:40,470
neural network and we ended up with not
just a pre trained word vectors but a

352
00:35:40,470 --> 00:35:48,870
for pre-trained model so it looks like
Duke creates embeddings for entities we

353
00:35:48,870 --> 00:35:53,250
need like a dummy task not necessarily a
dummy task like in this case we had a

354
00:35:53,250 --> 00:35:56,820
real task right so we created the
embeddings for Rossmann by trying to

355
00:35:56,820 --> 00:36:02,900
predict store sales you only need this
isn't just in this isn't just for

356
00:36:02,900 --> 00:36:09,870
learning embeddings for learning any
kind of feature space you either need

357
00:36:09,870 --> 00:36:16,200
label data or you need to invent some
kind of fake task

358
00:36:16,200 --> 00:36:20,640
so does that task matter like if I
choose a task and train and lettings if

359
00:36:20,640 --> 00:36:26,550
I choose another task and train and
lettings like which one is it's a great

360
00:36:26,550 --> 00:36:31,050
question and it's not something that's
been studied nearly enough right I'm not

361
00:36:31,050 --> 00:36:34,850
sure that many people even quite
understand that when they say
unsupervised learning now about nowadays

362
00:36:37,500 --> 00:36:45,030
they almost nearly always mean fake
tasks labeled learning and so the idea

363
00:36:45,030 --> 00:36:49,650
of like what makes a good fake task I
don't know that I've seen a paper on

364
00:36:49,650 --> 00:36:56,780
that right that intuitively you know we
need something where the kinds of

365
00:36:56,780 --> 00:37:01,500
relationships it's going to learn likely
to be the kinds of relationships that

366
00:37:01,500 --> 00:37:09,930
you probably care about right so for
example in in computer vision one kind

367
00:37:09,930 --> 00:37:16,770
of fake task people use is to say like
let's take some images and use some kind

368
00:37:16,770 --> 00:37:23,190
of like unreal and unreasonable data
augmentation like like recolor them too

369
00:37:23,190 --> 00:37:27,390
much or whatever and then we'll ask the
neural net to like predict which one was

370
00:37:27,390 --> 00:37:37,200
the Augmented which one was not you
admitted yeah so it's I think it's a

371
00:37:37,200 --> 00:37:41,100
fascinating area
one which you know would be really

372
00:37:41,100 --> 00:37:44,130
interesting for people to you know maybe
some of the students here they're

373
00:37:44,130 --> 00:37:47,310
looking to further it's like take some
interesting semi-supervised tour

374
00:37:47,310 --> 00:37:54,150
unsupervised datasets and try and come
up with some like more clever fake tasks

375
00:37:54,150 --> 00:37:59,640
and see like does it matter you know how
much does it matter in general like if

376
00:37:59,640 --> 00:38:04,100
you can't come up with a fake task that
you think seems great I would say use it

377
00:38:04,100 --> 00:38:09,869
use the best you can it's an often
surprising how how little you need like

378
00:38:09,869 --> 00:38:15,119
the ultimately crappy fake task is
called the auto encoder and the auto

379
00:38:15,119 --> 00:38:20,940
encoder is the thing which which one the
claims prediction competition that just

380
00:38:20,940 --> 00:38:27,090
finished on cattle they had lots of
examples of insurance policies where we

381
00:38:27,090 --> 00:38:30,360
knew this was how much was claimed and
then lots of examples of insurance

382
00:38:30,360 --> 00:38:34,560
policies where I guess there must have
been still still open we didn't yet know

383
00:38:34,560 --> 00:38:39,720
how much they claimed right and so what
they did was they said okay so for all

384
00:38:39,720 --> 00:38:44,130
of the ones so let's basically start off
by grabbing every policy right and we'll

385
00:38:44,130 --> 00:38:50,940
take a single policy and we'll put it
through a neural net right and we'll try

386
00:38:50,940 --> 00:38:57,990
and have it reconstruct itself but in
these intermediate layers and at least
one of those intermediate layers will

387
00:38:59,580 --> 00:39:03,690
make sure there's less activations and
there were inputs so let's say if there

388
00:39:03,690 --> 00:39:08,580
was a hundred variables on the insurance
policy you know we'll have something in

389
00:39:08,580 --> 00:39:13,590
the middle that only has like twenty
activations all right and so when you

390
00:39:13,590 --> 00:39:18,090
basically are saying hey reconstruct
your own input like it's not a different

391
00:39:18,090 --> 00:39:23,970
kind of model doesn't require any
special code it's literally just passing
you can use any standard pipe torch or

392
00:39:25,619 --> 00:39:31,710
fast AI learner you just say my output
equals my input right and that's that's

393
00:39:31,710 --> 00:39:37,650
like the the most uncreated you know
invented task you can create and that's

394
00:39:37,650 --> 00:39:41,430
called an autoencoder
and it works surprisingly well in fact
to the point that it literally just won

395
00:39:43,590 --> 00:39:47,100
a cackle competition they took the
features that it learnt and chucked it
into another neural net and

396
00:39:49,980 --> 00:39:56,430
yeah and one you know maybe if we have
enough students taking an interest in

397
00:39:56,430 --> 00:40:01,560
this then you know we'll be able to
cover covered unsupervised learning in

398
00:40:01,560 --> 00:40:10,589
more detail in in part two specially
given this cattle have a win I think

399
00:40:10,589 --> 00:40:15,740
this may be related to the previous
question when training language models

400
00:40:15,740 --> 00:40:19,980
is the language model example trained on
the archive data is that useful at all

401
00:40:19,980 --> 00:40:29,640
in the movie great question you know I
was just talking to Sebastian about this

402
00:40:29,640 --> 00:40:32,670
question read about this this week and
we thought would try and do some

403
00:40:32,670 --> 00:40:36,839
research on this in January it's it's
again it's not well done

404
00:40:36,839 --> 00:40:42,900
we know that in computer vision it's
shockingly effective to train on cats

405
00:40:42,900 --> 00:40:47,970
and dogs and use that fruit train
network to do lung cancer diagnosis and
CT scans in the NLP world nobody much

406
00:40:52,230 --> 00:40:56,490
seems to have tried this the NLP
research as I've spoken to other than
Sebastian about this assume that it

407
00:40:58,950 --> 00:41:01,680
wouldn't work and they generally haven't
bother trying I think it would work

408
00:41:01,680 --> 00:41:12,420
great so so since we're talking about
ruspin I just mentioned during the week

409
00:41:12,420 --> 00:41:17,810
I was interested to see like how good
this solution actually actually was

410
00:41:17,810 --> 00:41:21,150
because I noticed that on the public
leader board it didn't look like it was

411
00:41:21,150 --> 00:41:25,770
going to be that great and I also
thought it'd be good to see like what

412
00:41:25,770 --> 00:41:30,690
does it actually take to use a test set
properly with this kind of structured

413
00:41:30,690 --> 00:41:34,380
data so if you have a look at ruspin now
I've pushed some changes that actually

414
00:41:34,380 --> 00:41:38,430
run the test set through as well and so
you can get a sense of how to do this
so you'll see basically every line

415
00:41:40,440 --> 00:41:46,650
appears twice one for tests and one-foot
one for train when we get there yeah

416
00:41:46,650 --> 00:41:50,609
test train test trains history obviously
you could do this on a lot fewer lines

417
00:41:50,609 --> 00:41:55,410
of code by putting all of the steps into
a method and then pass either the train

418
00:41:55,410 --> 00:42:00,380
data set well the test data set up
dataframe to it in this case i wanted to

419
00:42:00,380 --> 00:42:03,750
kind of put for teaching purposes you'd
be able to see

420
00:42:03,750 --> 00:42:08,010
step and to experiment to see what each
step looks like but you could certainly

421
00:42:08,010 --> 00:42:16,470
simplify this code so yeah so we do this
for every data frame and then some of
these you can see I kind of lived

422
00:42:17,580 --> 00:42:24,480
through the data frame in joined and the
joint test right training just this

423
00:42:24,480 --> 00:42:30,150
whole thing about the durations I
basically put two lines here one that

424
00:42:30,150 --> 00:42:34,140
said data frame equals train columns one
that says data frame equals test columns

425
00:42:34,140 --> 00:42:39,060
and so my you know basically ideas you'd
run this line first and then you would

426
00:42:39,060 --> 00:42:42,600
skip the next one and you'd run
everything beneath it and then you'd go

427
00:42:42,600 --> 00:42:45,900
back and run this line and then run
everything believe it

428
00:42:45,900 --> 00:42:49,950
so some people on the forum were asking
how come this code wasn't working this
week which is a good reminder that the

429
00:42:52,260 --> 00:42:56,340
code is not designed to be code that you
always run top to bottom without

430
00:42:56,340 --> 00:43:00,180
thinking right you're meant to like
think like what is this code here should
I be running it right now okay and so

431
00:43:03,960 --> 00:43:07,560
like the early lessons I tried to make
it so you can run it top to bottom but

432
00:43:07,560 --> 00:43:10,800
increasingly as we go along I kind of
make it more and more that like you

433
00:43:10,800 --> 00:43:17,340
actually have to think about what's
going on so Jimmy you're talking about

434
00:43:17,340 --> 00:43:22,620
shadow learning and deep learning could
you define that a bit better by sure I'm
learning I think I just mean anything

435
00:43:24,000 --> 00:43:28,260
that doesn't have a hidden layer so
something that's like a dot product

436
00:43:28,260 --> 00:43:43,590
matrix multiplier basically okay so so
we end up with a training and a test

437
00:43:43,590 --> 00:43:50,040
version and then everything else is
basically the same one thing to note on

438
00:43:50,040 --> 00:43:53,190
a lot of these details of this we cover
in the machine learning course by the

439
00:43:53,190 --> 00:43:55,650
way because it's not really deep
learning specific so check that out if

440
00:43:55,650 --> 00:44:00,450
you're just in the details
I should mention you know we use apply
cats rather than train cats to make sure

441
00:44:02,970 --> 00:44:09,540
the test set and the training set have
the same categorical codes and that they

442
00:44:09,540 --> 00:44:15,990
join too we also need to make sure that
we keep track of the mapper this is the

443
00:44:15,990 --> 00:44:18,500
thing which basically says
what's the mean and standard deviation

444
00:44:18,500 --> 00:44:27,440
of each continuous column and then apply
that same method test set and so when we

445
00:44:27,440 --> 00:44:30,950
do all that that's basically it then the
rest is easy we just have to pass you in
the test data frame in the usual way

446
00:44:33,609 --> 00:44:42,050
when we create our model data object and
there's no changes through all here we

447
00:44:42,050 --> 00:44:51,380
trained it in the same way and then once
we finish training it we can then call

448
00:44:51,380 --> 00:44:57,710
predict as per usual passing in true to
say this is the test set rather than the

449
00:44:57,710 --> 00:45:03,650
validation set and pass that off to
cattle and so it was really interesting

450
00:45:03,650 --> 00:45:14,839
because this was my submission it got a
public score of 103 which would put us

451
00:45:14,839 --> 00:45:25,339
in about 300 and some things place which
looks awful right and our private score

452
00:45:25,339 --> 00:45:38,890
of 107 need a board private here's about
fifth

453
00:45:38,890 --> 00:45:45,799
right so like if you're competing in a
cable competition and you don't haven't

454
00:45:45,799 --> 00:45:49,819
thoughtfully created a validation set of
your own and you're relying on publicly

455
00:45:49,819 --> 00:45:53,510
the board feedback this could totally
happen to you but the other way around

456
00:45:53,510 --> 00:45:56,420
you'll be like oh I'm in the top ten I'm
doing great

457
00:45:56,420 --> 00:46:01,970
and then oh for example at the moment
the ice Berg's competition recognizing
icebergs a very large percentage of the

458
00:46:04,940 --> 00:46:10,640
public leaderboard set is synthetically
generated data augmentation data like

459
00:46:10,640 --> 00:46:15,290
totally meaningless and so your
validation set is going to be much more

460
00:46:15,290 --> 00:46:21,859
helpful and the public leaderboard
feedback right so yeah be very careful

461
00:46:21,859 --> 00:46:26,270
so our final score here is kind of
within statistical noise of the actual

462
00:46:26,270 --> 00:46:31,040
third-place getters so I'm pretty
confident that we've we've captured

463
00:46:31,040 --> 00:46:40,700
their approach and so that's that's
pretty interesting something to mention

464
00:46:40,700 --> 00:46:45,020
there's a nice kernel about the rustman
I quite a few nice kernels actually but

465
00:46:45,020 --> 00:46:47,150
you can go back and see like
particularly if you're doing the

466
00:46:47,150 --> 00:46:50,180
groceries competition go and have a look
at the Rossmann kernels because actually
quite a few of them a higher quality

467
00:46:51,890 --> 00:46:56,090
than the ones for the Ecuadorian
groceries competition one of them for

468
00:46:56,090 --> 00:47:02,480
example showed how on four particular
stores like straw eighty five the sales

469
00:47:02,480 --> 00:47:08,330
for non Sundays and the sale for
Sunday's looked very different where

470
00:47:08,330 --> 00:47:11,870
else there are some other stores where
the sales on Sunday don't look any

471
00:47:11,870 --> 00:47:14,720
different and it can kind of like get a
sense of why you need these kind of

472
00:47:14,720 --> 00:47:18,290
interactions the one I particularly
wanted to point out is the one I think I

473
00:47:18,290 --> 00:47:22,850
briefly mentioned that the third-place
winners whose approach we used they

474
00:47:22,850 --> 00:47:29,990
didn't notice is this one and here's a
really cool visualization here you can

475
00:47:29,990 --> 00:47:37,280
see that the store this store is closed
right and just after oh my god we run a

476
00:47:37,280 --> 00:47:41,540
we run out of eggs
and just before oh my god go and get the
milk before the store closes alright

477
00:47:44,870 --> 00:47:52,190
and here again closed bang right so this
third-place winner actually deleted all

478
00:47:52,190 --> 00:47:57,230
of the closed store rows before they
started doing any analysis right so

479
00:47:57,230 --> 00:48:02,380
remember how we talked about like don't
touch your data unless you first of all

480
00:48:02,380 --> 00:48:08,720
analyze to see whether that thing you're
doing is actually okay no assumptions
right so in this case I am sure like I

481
00:48:11,780 --> 00:48:15,260
haven't tried it but I'm sure they would
have one otherwise right because like

482
00:48:15,260 --> 00:48:19,670
well though there weren't actually any
store closures to my knowledge in the

483
00:48:19,670 --> 00:48:25,100
test set period the problem is that
their model was trying to fit to these

484
00:48:25,100 --> 00:48:28,550
like really extreme things and so and
because it wasn't able to do it very

485
00:48:28,550 --> 00:48:32,720
well it was gonna end up getting a
little bit confused it's not gonna break
the model but it's definitely gonna harm

486
00:48:34,880 --> 00:48:38,480
it because it's kind of trying to do
computations to fit something which it

487
00:48:38,480 --> 00:48:42,869
literally doesn't have the data for your
neck can you pass that back there

488
00:48:42,869 --> 00:48:52,779
all right so that Russman model again
like it's nice to kind of look inside to

489
00:48:52,779 --> 00:49:02,559
see what's actually going on right and
so that Russman model I want to make

490
00:49:02,559 --> 00:49:05,170
sure you kind of know how to find your
way around the code so you can answer

491
00:49:05,170 --> 00:49:12,309
these questions for yourself so it's
inside columnar model data now um we

492
00:49:12,309 --> 00:49:15,190
started out by kind of saying hey if you
want to look at the code for something

493
00:49:15,190 --> 00:49:21,999
you couldn't like a question mark
question mark like this and oh okay I
need to I haven't got this reading but

494
00:49:23,829 --> 00:49:27,999
you can use question mark question mark
to get the source code for something
right but obviously like that's not

495
00:49:31,259 --> 00:49:35,229
really a great way because often you
look at that source code and it turns

496
00:49:35,229 --> 00:49:38,170
out you need to look at something else
right and so for those of you that

497
00:49:38,170 --> 00:49:43,349
haven't done much coding you might not
be aware that almost certainly the

498
00:49:43,349 --> 00:49:48,219
editor you're using probably has the
ability to both open up stuff directly

499
00:49:48,219 --> 00:49:53,229
off SSH and to navigate through it so
you can jump straight from place to
place right so want to show you what I

500
00:49:55,029 --> 00:50:00,130
mean so if I were to find columnar model
data and I have to be using vim here I

501
00:50:00,130 --> 00:50:05,710
can basically say tag columnar model
data and it will jump straight to the

502
00:50:05,710 --> 00:50:10,960
definition of that plus right and so
then I notice here that like oh it's

503
00:50:10,960 --> 00:50:15,069
actually building up a data loader
that's interesting if I get control

504
00:50:15,069 --> 00:50:19,569
right square bracket it'll jump to the
definition of the thing that was under

505
00:50:19,569 --> 00:50:22,450
my cursor and after I finished reading
it for a while

506
00:50:22,450 --> 00:50:26,170
I can hit ctrl T to jump back up to
where I came from

507
00:50:26,170 --> 00:50:30,729
right and you kind of get the idea right
or if I want to find it for usage of
this in this file of columnar model data

508
00:50:34,710 --> 00:50:41,140
I can hit star to jump to the next place
it's new used you know and so forth

509
00:50:41,140 --> 00:50:49,739
alright so in this case get learner was
the thing which actually got the model

510
00:50:49,739 --> 00:50:55,880
and we want to find out what kind of
model it is and apparently it uses a

511
00:50:55,880 --> 00:50:59,420
I'm not using collaborative filtering
are we were using columnar model data

512
00:50:59,420 --> 00:51:10,970
sorry columnar model data okay learner
which users and so here you can see
mixed input model is the PI torch model

513
00:51:13,549 --> 00:51:20,990
and then it wraps it in the structured
learner which is the the first day I

514
00:51:20,990 --> 00:51:25,369
learn a type which wraps the data and
the model together so if we want to see

515
00:51:25,369 --> 00:51:30,200
the definition of this actual PI torch
model I can go to control right square
bracket to see it right and so here is

516
00:51:34,519 --> 00:51:42,890
the model right and nearly all of this
we can now understand right so we got

517
00:51:42,890 --> 00:52:04,819
past we got past a list of embedding
sizes in the mixed model that we saw

518
00:52:04,819 --> 00:52:15,410
does it always expect categorical and
continuous together yes it does

519
00:52:15,410 --> 00:52:21,859
and the the model data behind the scenes
if there are no none of the other type

520
00:52:21,859 --> 00:52:30,079
it creates a column of ones or zeros or
something okay so if it is null it can

521
00:52:30,079 --> 00:52:36,950
still work yeah yeah yeah it's kind of
ugly and hacky and will you know
hopefully improve it but yeah you can

522
00:52:39,289 --> 00:52:43,759
pass in an empty list of categorical or
continuous variables to the model data

523
00:52:43,759 --> 00:52:49,720
and it will basically yeah it'll
basically pass an unused column of zeros

524
00:52:49,720 --> 00:52:56,599
to avoid things breaking and I'm I'm
leaving fixing some of these slightly

525
00:52:56,599 --> 00:53:01,970
hacky edge cases because height or 0.4
as well as you're getting rid of
variables they're going to also add rank

526
00:53:04,789 --> 00:53:08,839
0 tensors which is to say if you grab a
single
thing out of like a rent 110 sir rather

527
00:53:11,479 --> 00:53:16,819
than getting back at a number which is
like qualitatively different you're

528
00:53:16,819 --> 00:53:20,509
actually going to get back like a tensor
that just happens to have no rank now it

529
00:53:20,509 --> 00:53:23,210
turns out that a lot of this kind of
codes gonna be actually easier to write

530
00:53:23,210 --> 00:53:29,019
then so and for now it's it's a little
bit more happier than it needs to be

531
00:53:30,400 --> 00:53:35,269
Jeremy you talk about this a little bit
before where maybe it's a good time at
some points talk about how can we write

532
00:53:38,960 --> 00:53:44,749
something that is slightly different for
worries in the library yeah I think
we'll cover that a little bit next week

533
00:53:47,329 --> 00:53:53,779
that I'm mainly going to do that in part
to like Pat who's going to cover quite a

534
00:53:53,779 --> 00:53:57,079
lot of stuff one of the main things were
cover in part two is what it called

535
00:53:57,079 --> 00:54:00,710
generative models so things where the
output is a whole sentence or a whole

536
00:54:00,710 --> 00:54:08,930
image but you know I also dig into like
powder really either customize the first

537
00:54:08,930 --> 00:54:16,249
day I library or use it on more custom
models but if we have time we'll touch

538
00:54:16,249 --> 00:54:25,069
on it a little bit next week okay so the
the learner we were passing in a list of

539
00:54:25,069 --> 00:54:28,999
embedding sizes and as you can see that
embedding sizes list was literally just

540
00:54:28,999 --> 00:54:32,569
the number of rows and the number of
columns in each embedding right and the

541
00:54:32,569 --> 00:54:38,809
number of code rose was just coming from
literally how many stores are there in

542
00:54:38,809 --> 00:54:44,299
the store category for example and the
number of columns was just a quarter

543
00:54:44,299 --> 00:54:49,309
that divided by two and a maximum of 50
so that thing that list of tuples was

544
00:54:49,309 --> 00:54:53,210
coming in and so you can see here how we
use it right we go through each of those

545
00:54:53,210 --> 00:55:00,200
tuples grab the number of categories and
the size of the embedding and construct

546
00:55:00,200 --> 00:55:05,869
an embedding all right and so that's a
that's a list right one minor thing

547
00:55:05,869 --> 00:55:11,269
height or specific thing we haven't
talked about before is for it to be able
to like register remember how we kind of

548
00:55:13,759 --> 00:55:18,109
said like it registers your parameters
it registers your your layers like

549
00:55:18,109 --> 00:55:21,200
someone we like listed the model it
actually printed out the Novation

550
00:55:21,200 --> 00:55:27,020
varying an age bias it can't do that if
they're hidden inside a list right they

551
00:55:27,020 --> 00:55:33,470
have to be like a there have to be a an
actual n n dot module subclass so

552
00:55:33,470 --> 00:55:38,120
there's a special thing called an N n
dot module list which takes a list and

553
00:55:38,120 --> 00:55:42,650
it basically says I want you to register
everything in here has been part of this

554
00:55:42,650 --> 00:55:49,370
model okay so it's just a minor tweak so
yeah so our mixed input model has a list

555
00:55:49,370 --> 00:55:56,960
of embeddings and then I do the same
thing for a list of linear layers right

556
00:55:56,960 --> 00:56:06,320
so when I said here 1000 comma 500 this
was saying how many activations I wanted

557
00:56:06,320 --> 00:56:13,400
featured my lineal is okay and so here I
just go through that list and create a

558
00:56:13,400 --> 00:56:19,610
linear layer that goes from this size to
the next size okay so you can see like

559
00:56:19,610 --> 00:56:24,710
how easy it is to kind of construct your
own not just your own model but a kind

560
00:56:24,710 --> 00:56:28,220
of a model which you can pass parameters
to have a constructed on the fly
dynamically and that's normal talk about

561
00:56:30,830 --> 00:56:36,380
next week this is initialization we've
mentioned climbing her initialization

562
00:56:36,380 --> 00:56:44,750
before and we mentioned it last week and
then drop out same thing right we have

563
00:56:44,750 --> 00:56:48,920
here a list of how much drop out to
apply to each layer right so again here

564
00:56:48,920 --> 00:56:52,970
it's just like go through each thing in
that list and create a drop out layer
for it okay so this constructor we

565
00:56:55,910 --> 00:57:00,470
understand everything in it except for
batch norm which we don't have to worry

566
00:57:00,470 --> 00:57:08,180
about for now so that's the constructor
and so then the forward also you know

567
00:57:08,180 --> 00:57:11,900
all stuff we're aware of go through each
of those embedding layers that we just

568
00:57:11,900 --> 00:57:16,550
saw and remember we've just treated like
as a function so call it with the ithe

569
00:57:16,550 --> 00:57:22,250
categorical variable and then
concatenate them all together put that

570
00:57:22,250 --> 00:57:29,240
through drop out and then go through
each one of our linear layers and call

571
00:57:29,240 --> 00:57:33,940
it apply relia to it
apply dropout

572
00:57:33,940 --> 00:57:40,160
and then finally apply the final linear
layer and the final linear layer has

573
00:57:40,160 --> 00:57:49,880
this as its size which is here right
size one there's a single unit sales

574
00:57:49,880 --> 00:57:55,010
okay so we're kind of getting to the
point where oh and then of course at the

575
00:57:55,010 --> 00:57:59,960
end if this I mentioned would come back
to this if you passed in a Y underscore

576
00:57:59,960 --> 00:58:03,260
range parameter then we're going to do
the thing we just learned about last

577
00:58:03,260 --> 00:58:07,730
week which is to use a sigmoid right and
this is a cool little trick to make

578
00:58:07,730 --> 00:58:11,120
you're not just to make your
collaborative filtering better but in

579
00:58:11,120 --> 00:58:17,890
this case my basic idea was you know
sales are going to be greater than zero

580
00:58:17,890 --> 00:58:25,730
and probably less than the largest sale
they've ever had so I just pass in that

581
00:58:25,730 --> 00:58:30,950
as Y range and so we do a sigmoid and
multiply with the sigmoid by the range

582
00:58:30,950 --> 00:58:37,580
that I passed it all right and so
hopefully we can find that here yeah

583
00:58:37,580 --> 00:58:43,250
here it is right so I actually said hey
maybe the range is between zero and you

584
00:58:43,250 --> 00:58:47,960
know the highest x one point two you
know cuz maybe maybe the next two weeks

585
00:58:47,960 --> 00:58:51,200
we have one bigger but this is kind of
like again try to make it a little bit

586
00:58:51,200 --> 00:58:56,630
easier for it to give us the kind of
results that it thinks is right so like

587
00:58:56,630 --> 00:59:04,210
increasingly you know I'd love your wall
to kind of try to not treat these

588
00:59:04,210 --> 00:59:09,320
learners and models as black boxes but
to feel like you now have the

589
00:59:09,320 --> 00:59:12,500
information you need to look inside them
and remember you could then copy and

590
00:59:12,500 --> 00:59:19,400
paste this plus paste it into a cell in
duple notebook and start fiddling with

591
00:59:19,400 --> 00:59:27,580
it to create your own versions okay

592
00:59:30,640 --> 00:59:35,319
I think what I might do is we might take
a bit of a early break because we've got

593
00:59:35,319 --> 00:59:42,160
a lot to cover and I want to do it all
in one big go so let's take a let's take

594
00:59:42,160 --> 00:59:47,650
a break until 7:45 and then we're going
to come back and talk about recurrent

595
00:59:47,650 --> 00:59:56,160
neural networks all right

596
00:59:56,670 --> 01:00:02,079
so we're going to talk about Aaron ends
before we do we've got to kind of dig a

597
01:00:02,079 --> 01:00:06,760
little bit deeper into SGD because I
just want to make sure everybody's

598
01:00:06,760 --> 01:00:13,390
totally comfortable with with SGD and so
what we're going to look at is we're
going to look at lesson six SGD notebook

599
01:00:17,549 --> 01:00:26,500
and we're going to look at a really
simple example of using SGD to learn y

600
01:00:26,500 --> 01:00:32,160
equals ax plus B and so what we're going
to do here is we're going to create like

601
01:00:32,160 --> 01:00:38,710
the simplest possible model y equals ax
plus B okay and then we're going to

602
01:00:38,710 --> 01:00:46,750
generate some random data that looks
like so so here's our X and here's our Y

603
01:00:46,750 --> 01:00:56,019
we want to predict Y from X and we
passed in 3 & 8 as our a and B so we're

604
01:00:56,019 --> 01:01:00,609
going to kind of try and recover that
right and so the idea is that if we can

605
01:01:00,609 --> 01:01:05,500
solve something like this which has two
parameters we can use the same technique

606
01:01:05,500 --> 01:01:10,990
to solve we can use the same technique
to solve something with a hundred

607
01:01:10,990 --> 01:01:22,000
million parameters right without any
changes at all so in order to find a and

608
01:01:22,000 --> 01:01:25,990
a B that fits this we need a loss
function and this is a regression

609
01:01:25,990 --> 01:01:30,130
problem because we have a continuous
output so for continuous output

610
01:01:30,130 --> 01:01:33,849
regression we tend to use mean squared
error all right and obviously all of

611
01:01:33,849 --> 01:01:36,339
this stuff there's there's
implementations in non pious

612
01:01:36,339 --> 01:01:39,970
implementations in flight or we're just
doing stuff by hand so you can see all

613
01:01:39,970 --> 01:01:44,020
the steps right so there's MSE okay
y hat is

614
01:01:44,020 --> 01:01:48,250
we often call our predictions Y hat
mitis y squared mean there's I meant

615
01:01:48,250 --> 01:01:54,160
whatever okay so for example if we had
ten and five where a and B then there's

616
01:01:54,160 --> 01:01:59,110
our mean square R squared error three
point two five okay so if we've got an A

617
01:01:59,110 --> 01:02:02,500
and a B and we've got an x and a y then
our mean square error loss is just the

618
01:02:02,500 --> 01:02:07,420
mean squared error of our linear that's
our predictions and our way okay so

619
01:02:07,420 --> 01:02:13,510
there's a last four ten five X Y all
right so that's a loss function right

620
01:02:13,510 --> 01:02:21,520
and so when we talk about combining
linear layers and loss functions and
optionally nonlinear layers this is all

621
01:02:24,520 --> 01:02:29,560
we're doing right is we're putting a
function inside a function yeah that's

622
01:02:29,560 --> 01:02:35,350
that's all like I know people draw these
clever looking dots and lines all over

623
01:02:35,350 --> 01:02:38,110
the screen when they're saying this is
what a neural network is but it's just

624
01:02:38,110 --> 01:02:41,260
it's just a function of a function of a
function okay so here we've got a

625
01:02:41,260 --> 01:02:45,700
prediction function being a linear layer
followed by a loss function being MSE
and now we can say like oh well let's

626
01:02:47,440 --> 01:02:51,970
just define this as MSA Lost's and we'll
use that in the future okay so there's
our loss function which incorporates our

627
01:02:53,980 --> 01:03:00,550
prediction function okay so let's
generate 10,000 items or thick data and

628
01:03:00,550 --> 01:03:03,940
let's show them in two variables so we
can use them with PI torch because

629
01:03:03,940 --> 01:03:07,140
Jeremy doesn't like taking derivatives
so we're going to use PI torch for that

630
01:03:07,140 --> 01:03:14,530
and let's create random wait for a and B
so a single random number and we want

631
01:03:14,530 --> 01:03:19,060
the gradients of these to be calculated
as we start computing with them because
these are the actual things we need to

632
01:03:20,560 --> 01:03:31,090
update in our SGD okay so here's our a
and B 0.029 0.111 all right so let's

633
01:03:31,090 --> 01:03:40,600
pick a learning rate okay and let let's
do 10,000 epochs of SGD in fact this
isn't really SGD it's not stochastic

634
01:03:42,520 --> 01:03:45,570
gradient it said this is actually full
gradient descent we're going to each

635
01:03:45,570 --> 01:03:52,390
each loop is going to look at all of the
data okay stochastic gradient descent
would be looking at a subset each time

636
01:03:56,330 --> 01:04:01,040
so to do gradient descent we basically
calculate loss right so remember we've

637
01:04:01,040 --> 01:04:05,420
started out with a random a and B okay
and so this is going to compute some

638
01:04:05,420 --> 01:04:10,160
amount of loss and then it's nice from
time to time so one way of saying from

639
01:04:10,160 --> 01:04:15,950
time to time is if the epoch number mod
a thousand is zero right so every

640
01:04:15,950 --> 01:04:21,050
thousand epochs just print out the loss
so you have it do it okay

641
01:04:21,050 --> 01:04:26,090
so now that we've computed the loss we
can compute our gradients right and so

642
01:04:26,090 --> 01:04:32,180
you just remember this thing here is
both a number a single number that is
our lost something we can print but it's

643
01:04:34,130 --> 01:04:38,210
also a variable because we passed
variables into it and therefore it also

644
01:04:38,210 --> 01:04:43,790
has a method type backward which means
calculate the gradients of everything

645
01:04:43,790 --> 01:04:48,830
that we asked it to everything where we
said requires radical is true okay so at

646
01:04:48,830 --> 01:04:57,620
this point we now have a dot grad
property inside a and inside P and here

647
01:04:57,620 --> 01:05:02,390
they are here is that grant grad
property okay so now that we've

648
01:05:02,390 --> 01:05:07,880
calculated the gradients for a and B we
can update them by saying a is equal to

649
01:05:07,880 --> 01:05:14,530
whatever it used to be - the learning
rate times the gradient okay dot data

650
01:05:14,530 --> 01:05:21,380
because a is a variable and a variable
contains a tensor and it's dot data

651
01:05:21,380 --> 01:05:26,750
property and we again this is going to
disappear in height which point four but

652
01:05:26,750 --> 01:05:31,340
for now it's actually the ten so that we
need to update okay so update the tensor

653
01:05:31,340 --> 01:05:37,030
inside here with whatever it used to be
- the learning rate times the gradient

654
01:05:37,030 --> 01:05:43,220
okay and that's basically it
all right that's basically all gradient
descent is okay so it's it's as simple

655
01:05:45,980 --> 01:05:51,950
as we claimed there's one extra step in
pi torch which is that you might have

656
01:05:51,950 --> 01:05:56,560
like multiple different loss functions
or like lots of lots of output layers

657
01:05:56,560 --> 01:06:00,830
all contributing to the gradient and you
like to have to add them all together

658
01:06:00,830 --> 01:06:05,960
and so if you've got multiple loss
functions you could be calling loss stop

659
01:06:05,960 --> 01:06:08,430
backward on each of them and what it
does is an ad

660
01:06:08,430 --> 01:06:12,510
sit to the gradients right and so you
have to tell it when to set the

661
01:06:12,510 --> 01:06:18,860
gradients back to zero okay so that's
where you just go okay set a to zero and

662
01:06:18,860 --> 01:06:25,710
gradients in set B gradients to zero
okay and so this is wrapped up inside

663
01:06:25,710 --> 01:06:34,260
the you know op TMS JD class right so
when we say up Tim dot SGD and we just

664
01:06:34,260 --> 01:06:38,850
say you know dot step it's just doing
these for us so when we say dot zero
gradients is just doing this force and

665
01:06:41,850 --> 01:06:48,600
this underscore here every pretty much
every function that applies to a tensor

666
01:06:48,600 --> 01:06:53,430
in pi torch if you stick an underscore
on the end it means do it in place okay

667
01:06:53,430 --> 01:06:56,730
so this is actually going to not return
a bunch of zeros but it's going to

668
01:06:56,730 --> 01:07:04,320
change this in place to be a bunch of
zeros so that's basically it we can look

669
01:07:04,320 --> 01:07:10,740
at the same thing without PI torch which
means we actually do have to do some
calculus so if we generate some fake

670
01:07:12,960 --> 01:07:18,060
data again we're just going to create 50
data points this time just to make this

671
01:07:18,060 --> 01:07:23,970
fast and easy to look at and so let's
create a function called update right

672
01:07:23,970 --> 01:07:28,230
we're just going to use numpy no pi
torch okay so our predictions is equal

673
01:07:28,230 --> 01:07:32,760
to again linear and in this case we
actually gonna calculate the derivatives

674
01:07:32,760 --> 01:07:37,920
so the derivative of the square of the
loss is just two times and then the

675
01:07:37,920 --> 01:07:41,400
derivative is the vector a is just that
you can confirm that yourself if you

676
01:07:41,400 --> 01:07:46,260
want to and so here our we're going to
update a minus equals learning rate

677
01:07:46,260 --> 01:07:52,770
times the derivative of loss with
respect to a and for B it's learning
rate times derivative with respect to B

678
01:07:54,810 --> 01:08:02,670
okay and so what we can do let's just
run all this so just for fun

679
01:08:02,670 --> 01:08:07,590
rather than looping through manually we
can use the map flop matplotlib func

680
01:08:07,590 --> 01:08:14,820
animation command to run the animate
function a bunch of times and the

681
01:08:14,820 --> 01:08:20,370
animate function is going to run 30
epochs and at the end of each epoch it's

682
01:08:20,370 --> 01:08:24,029
going to print out
on the plot where the line currently is

683
01:08:24,029 --> 01:08:30,119
and that creates this at all movie okay
so you can actually see that the line
moving at a place right so if you want

684
01:08:33,238 --> 01:08:40,249
to play around with like understanding
how high torque gradients actually work

685
01:08:40,250 --> 01:08:46,739
step-by-step here's like the world's
simplest at all example okay and you

686
01:08:46,738 --> 01:08:52,198
know it's kind of like it's kind of
weird to say like that's that's it like

687
01:08:52,198 --> 01:08:55,408
when you're optimizing a hundred million
parameters in a neural net it's doing

688
01:08:55,408 --> 01:08:59,218
the same thing but it it actually is
alright you can actually look at the PI

689
01:08:59,219 --> 01:09:04,710
torch code and see it's this is it right
there's no trick

690
01:09:04,710 --> 01:09:09,779
well we load a couple of minor tricks
last time which was like momentum and

691
01:09:09,779 --> 01:09:15,839
atom right that if you could do it in
Excel you can do it invite them so okay
so let's do talk about our lens so we're

692
01:09:19,439 --> 01:09:29,629
now in less than six hour and in
notebook and we're going to study

693
01:09:29,630 --> 01:09:34,520
Nietzsche as you should

694
01:09:34,549 --> 01:09:41,960
so Nietzsche says supposing that truth
is a woman what then I love this

695
01:09:41,960 --> 01:09:47,339
apparently all philosophers have failed
to understand women

696
01:09:47,339 --> 01:09:49,799
so apparently at the point that
Nietzsche was alive there was no female

697
01:09:49,799 --> 01:09:54,599
philosophers or at least those that were
around didn't understand women either so

698
01:09:54,599 --> 01:09:59,909
anyway so this is the philosopher
apparently we've chosen to study it
leech is actually much less worse than

699
01:10:02,159 --> 01:10:07,800
people think he is but it's a different
era I guess alright so we're going to

700
01:10:07,800 --> 01:10:16,110
learn to write philosophy like Nietzsche
and so we're going to do it one
character at a time so this is like the

701
01:10:18,119 --> 01:10:21,570
language model that we did in Lesson
four where we did it a word at the time

702
01:10:21,570 --> 01:10:26,760
but this time we're going to do a
character at a time and so the main

703
01:10:26,760 --> 01:10:31,469
thing I'm going to try and convince you
is an RNN is no different to anything

704
01:10:31,469 --> 01:10:35,700
you've already learned okay and so to
show you that

705
01:10:35,700 --> 01:10:41,130
going to build it from plain PI torch
layers all of which are extremely

706
01:10:41,130 --> 01:10:44,370
familiar already okay and eventually
we're going to use something really

707
01:10:44,370 --> 01:10:48,780
complex which is a for loop okay so
that's when we're going to make a really

708
01:10:48,780 --> 01:10:55,640
sophisticated so the basic idea of our n
ends is that you want to keep track of

709
01:10:55,640 --> 01:10:59,790
the main thing is you want to keep track
of kind of state over long term

710
01:10:59,790 --> 01:11:04,620
dependencies so for example if you're
trying to model something like this kind

711
01:11:04,620 --> 01:11:10,620
of template language right then at the
end of your percent comment blue percent

712
01:11:10,620 --> 01:11:15,630
you need a percent common end percent
right and so somehow your model needs to

713
01:11:15,630 --> 01:11:19,710
keep track of the fact that it's like
inside a comment over all of these

714
01:11:19,710 --> 01:11:23,550
different characters right and so this
is this idea of state it's kind of

715
01:11:23,550 --> 01:11:29,670
memory right and this is quite a
difficult thing to do with like just a

716
01:11:29,670 --> 01:11:34,890
calm confident it turns out actually to
be possible but it's it's you know a

717
01:11:34,890 --> 01:11:38,520
little bit tricky
where elsewhere as an iron in it turns

718
01:11:38,520 --> 01:11:42,030
out to be pretty straightforward all
right so these are the basic ideas if
you want the stateful representation

719
01:11:43,530 --> 01:11:47,370
where you kind of keeping track of like
where are we now have memory have long

720
01:11:47,370 --> 01:11:53,310
term dependencies and potentially even
have variable length sequences these are

721
01:11:53,310 --> 01:11:57,030
all difficult things to do with
confidence they're very straightforward
with arid ends so for example SwiftKey a

722
01:12:02,250 --> 01:12:06,840
year or so ago did a blog post about how
they had a new language model where they

723
01:12:06,840 --> 01:12:10,950
basically this is from their blog post
we basically said like of course this is

724
01:12:10,950 --> 01:12:15,780
what their neural net looks like somehow
they always look like this on the

725
01:12:15,780 --> 01:12:19,530
internet you know you've got a bunch of
words and it's basically going to take

726
01:12:19,530 --> 01:12:23,190
your particular words in their
particular orders and try and figure out

727
01:12:23,190 --> 01:12:27,180
what the next words going to be which is
to say they built a language model they

728
01:12:27,180 --> 01:12:30,600
actually have a pretty good language
model if you've used SwiftKey they seem

729
01:12:30,600 --> 01:12:35,310
to do better predictions than anybody
else still another cool example was

730
01:12:35,310 --> 01:12:39,630
andre capaci a couple of years ago
showed that he could use character level

731
01:12:39,630 --> 01:12:45,420
are a 10 to actually create an entire
latex document so he didn't actually

732
01:12:45,420 --> 01:12:49,550
tell it in any way what life looks like
he just passed the

733
01:12:49,550 --> 01:12:54,600
some may tech text like this and said
generate more low text text and it

734
01:12:54,600 --> 01:12:58,890
literally started writing something
which means about as much to me as most

735
01:12:58,890 --> 01:13:07,890
math papers do this okay so we're gonna
start with something that's not an RN

736
01:13:07,890 --> 01:13:15,330
and I'm going to introduce Jeremy's
patented neural network notation

737
01:13:15,330 --> 01:13:23,969
involving boxes circles and triangles so
let me explain what's going on as a

738
01:13:23,969 --> 01:13:35,489
rectangle is an input an arrow is a
layer as a circle in fact every square
is a bunch of activate so every shape is

739
01:13:38,670 --> 01:13:43,980
a bunch of activations right the
rectangle is the input activations the
circle is a hidden activations and a

740
01:13:47,960 --> 01:13:56,340
triangle is an output activations and
arrow is a layer operation right or

741
01:13:56,340 --> 01:14:02,340
possibly more than one all right so here
my rectangle is an input of number of

742
01:14:02,340 --> 01:14:06,660
rows equal a batch size and number of
columns equal to the number of number of

743
01:14:06,660 --> 01:14:11,969
inputs number of variables all right and
so my first arrow my first operation is

744
01:14:11,969 --> 01:14:17,040
going to represent a matrix product
followed by our Lu and that's going to

745
01:14:17,040 --> 01:14:23,940
generate a set of activation remember
activations like an activation is a

746
01:14:23,940 --> 01:14:29,670
number that an activation is a number a
number that's being calculated by a

747
01:14:29,670 --> 01:14:35,760
value or a matrix product or whatever
it's a number right so this circle here

748
01:14:35,760 --> 01:14:41,250
represents a matrix of activations all
of the numbers that come out when we

749
01:14:41,250 --> 01:14:45,120
take the inputs we do a matrix product
followed by a value so we started with

750
01:14:45,120 --> 01:14:50,190
batch size byte number of inputs and so
after we do this matrix operation we now

751
01:14:50,190 --> 01:14:56,940
have batch size by you know whatever the
number of columns in our matrix product

752
01:14:56,940 --> 01:15:02,520
was by number of hidden units okay and
so if we now take these activations

753
01:15:02,520 --> 01:15:06,540
but it's the matrix and we put it
through another operation in this case
another matrix product and the softmax

754
01:15:08,450 --> 01:15:12,540
we get a triangle that's our output
activations another matrix of

755
01:15:12,540 --> 01:15:17,520
activations and again number of roses
batch size number of columns number is

756
01:15:17,520 --> 01:15:21,300
equal to the number of classes again
however many columns our matrix in this

757
01:15:21,300 --> 01:15:29,520
matrix product head so that's a that's a
neuro net right that's our basic kind of

758
01:15:29,520 --> 01:15:35,910
one hidden layer neural net and if you
haven't written one of these from
scratch try it you know and in fact in

759
01:15:39,720 --> 01:15:43,950
lessons nine ten and eleven of the
machine learning course we do this right

760
01:15:43,950 --> 01:15:47,910
we create one of these from scratch so
if you're not quite sure how to do it

761
01:15:47,910 --> 01:15:51,900
you can check out the machine learning
costs yeah in general the machine

762
01:15:51,900 --> 01:15:56,070
learning cost is much more like building
stuff up from the foundations where else

763
01:15:56,070 --> 01:16:02,880
this course is much more like best
practices kind of top-down all right so

764
01:16:02,880 --> 01:16:08,460
if we were doing like a cognate with a
single dense hidden layer our input

765
01:16:08,460 --> 01:16:13,380
would be equal to actually number yeah
that's very implied watch number of

766
01:16:13,380 --> 01:16:18,720
channels by height by width right and
notice that here batch size appeared

767
01:16:18,720 --> 01:16:21,570
every time so I'm not gonna I'm not
gonna write it anymore
okay so I've removed the batch size also

768
01:16:26,070 --> 01:16:30,780
the activation function it's always
basically value or something similar for

769
01:16:30,780 --> 01:16:34,620
all the hidden layers and softmax at the
end for classification so I'm not going

770
01:16:34,620 --> 01:16:38,670
to write that either okay so I'm kind of
edge picture I'm going to simplify it a

771
01:16:38,670 --> 01:16:42,720
little bit alright so I'm not gonna
mention batch size it's still there

772
01:16:42,720 --> 01:16:45,960
we're not going to mention real you or
softmax but it's still there so here's

773
01:16:45,960 --> 01:16:50,430
our input and so in this case rather
than a matrix product will do a

774
01:16:50,430 --> 01:16:56,100
convolution let's drive to convolution
so we'll skip over every second one or

775
01:16:56,100 --> 01:17:01,320
could be a convolution followed by a mac
spool in either case we end up with
something which is replaced number of

776
01:17:03,390 --> 01:17:08,070
channels with number of filters right
and we have now height divided by two
and width divided by 2

777
01:17:09,270 --> 01:17:15,420
okay and then we can flatten that out
somehow we'll talk next week about the

778
01:17:15,420 --> 01:17:17,580
main way
we do that nowadays which is basically

779
01:17:17,580 --> 01:17:21,810
to do something called an adaptive max
pooling where we basically get an

780
01:17:21,810 --> 01:17:28,020
average across the height and the width
and turn that into a vector anyway

781
01:17:28,020 --> 01:17:33,240
somehow we flatten it out into a vector
we can do a matrix product or a couple
of matrix products we actually tend to

782
01:17:34,920 --> 01:17:40,770
do in fast AI so that'll be our fully
connected layer with some number of

783
01:17:40,770 --> 01:17:46,260
activations final matrix product give us
some number of classes okay so this is

784
01:17:46,260 --> 01:17:51,800
our basic component remembering
rectangles input circle is hidden

785
01:17:51,800 --> 01:17:59,190
triangle is output all other shapes
represent a tensor of activations all of
the arrows represent a operation or lay

786
01:18:02,220 --> 01:18:06,360
operation all right
so now that's going to jump to the one
the first one that we're going to

787
01:18:07,500 --> 01:18:14,250
actually try to try to create for NLP
and we're going to basically do exactly

788
01:18:14,250 --> 01:18:18,930
the same thing as here right and we're
going to try and predict the third

789
01:18:18,930 --> 01:18:25,640
character in a three character sequence
based on the previous two characters so

790
01:18:25,640 --> 01:18:33,810
our input and again remember we've
removed the batch size dimension we're

791
01:18:33,810 --> 01:18:39,390
not saying that we're still here okay
and also here I've removed the names of

792
01:18:39,390 --> 01:18:43,680
the layer operations entirely
okay just keeping simplifying things so

793
01:18:43,680 --> 01:18:50,700
for example our first import would be
the first character of each string in
our mini batch okay and assuming this is

794
01:18:54,660 --> 01:18:59,490
one hot encoded then the width is just
however many items there are in the

795
01:18:59,490 --> 01:19:04,020
vocabulary how many unique characters
could we have okay we probably won't

796
01:19:04,020 --> 01:19:08,400
really one hot encoder will feed it in
as an integer and pretend it's one hot
encoded by using an embedding layer

797
01:19:09,720 --> 01:19:15,000
which is mathematically identical okay
and then we that's going to give us some

798
01:19:15,000 --> 01:19:23,880
activations which we can stick through a
fully connected layer okay so we we put

799
01:19:23,880 --> 01:19:27,030
that through if we click through a fully
connected layer to get some activations

800
01:19:27,030 --> 01:19:31,670
we can then put that
another fully connected layer and now

801
01:19:31,670 --> 01:19:36,410
we're going to bring in the input of
character to alright so the character to

802
01:19:36,410 --> 01:19:39,410
input will be exactly the same
dimensionality as the character one

803
01:19:39,410 --> 01:19:45,050
input and we now need to somehow combine
these two arrows together so we could

804
01:19:45,050 --> 01:19:50,230
just add them up for instance right
because remember this arrow here
represents a matrix product so this

805
01:19:53,600 --> 01:19:56,600
matrix product is going to spit out the
same dimensionality as this matrix

806
01:19:56,600 --> 01:20:02,960
product so we could just add them up to
create these activations and so now we

807
01:20:02,960 --> 01:20:05,690
can put that through another matrix
product and of course remember all these

808
01:20:05,690 --> 01:20:10,520
metrics products have a RAL you as well
and this final one will have a softmax

809
01:20:10,520 --> 01:20:18,350
instead to create our predicted set of
characters right so it's a standard you

810
01:20:18,350 --> 01:20:23,090
know two hidden layer
I guess it's actually three matrix

811
01:20:23,090 --> 01:20:29,240
products neural net this first one is
coming through an embedding layer the

812
01:20:29,240 --> 01:20:34,760
only difference is that we're also got a
second input coming in here that we're

813
01:20:34,760 --> 01:20:39,530
just adding in right but it's kind of
conceptually identical so let's let's
implement that for Nietzsche all right

814
01:20:46,160 --> 01:20:51,050
so I'm not going to use torch text I'm
gonna try not to use almost any fast AI

815
01:20:51,050 --> 01:20:56,030
so we can see it all kind of again from
raw right so here's the first 400

816
01:20:56,030 --> 01:21:02,390
characters of the collected works let's
grab a set of all of the letters that we

817
01:21:02,390 --> 01:21:08,090
see there and sort them okay and so a
set creates all the unique letters so

818
01:21:08,090 --> 01:21:15,110
we've got 85 unique letters in our vocab
let's pop up it's nice to put an empty
kind of a null or some some kind of

819
01:21:17,210 --> 01:21:20,180
padding character in there for padding
so we're gonna put a parenting character

820
01:21:20,180 --> 01:21:29,030
at the start right and so here is what
our vocab looks like okay so so Kars is
our bouquet so as per usual we want some

821
01:21:33,350 --> 01:21:41,200
way to map every character to a unique
ID and every unique ID to a character
and

822
01:21:42,600 --> 01:21:48,390
so now we can just go through our
collected works of niche and grab the

823
01:21:48,390 --> 01:21:54,720
index of each one of those characters so
now we've just turned it into this right

824
01:21:54,720 --> 01:22:08,010
so rather than quote PR e we now have 40
42 29 okay so so that's basically the

825
01:22:08,010 --> 01:22:13,560
first step and just to confirm we can
now take each of those indexes and turn

826
01:22:13,560 --> 01:22:20,060
them back into characters and join them
together and yeah there it is okay so

827
01:22:20,060 --> 01:22:23,430
from now on we're just going to work
with this IDX

828
01:22:23,430 --> 01:22:30,330
list the list of character members in
the connected works of Nietzsche yes so

829
01:22:30,330 --> 01:22:36,540
Jeremy why are we doing like a model of
characters and not a model of words I

830
01:22:36,540 --> 01:22:42,180
just thought it seemed simpler you know
with a vocab of 80-ish items we can kind

831
01:22:42,180 --> 01:22:50,100
of see it better character level models
turn out to be potentially quite useful

832
01:22:50,100 --> 01:22:54,390
in a number of situations but we'll
cover that in part two the short answer

833
01:22:54,390 --> 01:22:58,560
is like you generally want to combine
both the word level model and a connect

834
01:22:58,560 --> 01:23:02,250
character level model like if you're
doing say translation it's a great way

835
01:23:02,250 --> 01:23:05,970
to deal with unknown like unusual words
rather than treating it as unknown

836
01:23:05,970 --> 01:23:09,390
anytime you see a word you haven't seen
before you could use a character level

837
01:23:09,390 --> 01:23:13,200
model for that and there's actually
something in between the two quarter
byte pair and coding vpe which basically

838
01:23:15,810 --> 01:23:22,830
looks at at all engrams of characters
but we'll cover all that in part two if
you want to look at it right now

839
01:23:24,020 --> 01:23:30,540
then part two of the existing course
already has this stuff taught and part
two of the version 1 of this course

840
01:23:33,170 --> 01:23:38,430
although the NLP stuff is in flight
which by the way so you'll understand it

841
01:23:38,430 --> 01:23:43,110
straight away it was actually the thing
that inspired us to move to piped watch
because trying to do it in chaos turned

842
01:23:44,940 --> 01:23:52,650
out to be a nightmare all right so let's
create the inputs to this we're actually

843
01:23:52,650 --> 01:23:55,679
going to do something slightly different
what I said we're actually going to

844
01:23:55,679 --> 01:24:00,659
I predict the fourth character that
actually this the fifth character using

845
01:24:00,659 --> 01:24:04,559
the first four so the index four
character using the index zero one two

846
01:24:04,559 --> 01:24:08,459
and three okay so it was exactly the
same thing but with just a couple more

847
01:24:08,459 --> 01:24:17,909
layers so that means that we need a list
of the zeroth first second and third

848
01:24:17,909 --> 01:24:21,929
characters that's why I'm just cutting
every character from the start from the

849
01:24:21,929 --> 01:24:28,559
one from two from three skipping over
three at a time okay

850
01:24:28,559 --> 01:24:36,840
so hmm
this is I I said this wrong so we're

851
01:24:36,840 --> 01:24:41,070
going to predict the third character the
fourth character from the third for the

852
01:24:41,070 --> 01:24:46,110
first story okay
the fourth character is history

853
01:24:46,110 --> 01:24:53,909
all right so our inputs will be these
three lists right so we can just use n P

854
01:24:53,909 --> 01:25:01,639
dot stack to pop them together all right
so here's the zero one and two

855
01:25:01,639 --> 01:25:05,519
characters that are going to feed into a
model and then here is the next

856
01:25:05,519 --> 01:25:23,249
character in the list so for example X 1
X 2 X 3 and Y all right so you can see

857
01:25:23,249 --> 01:25:33,749
for example we start off the first the
very first item would be 40 42 and 29

858
01:25:33,749 --> 01:25:40,949
right so that's characters naught 1 and
2 and then we'd be predicting 30 that's

859
01:25:40,949 --> 01:25:46,289
the fourth character which is the start
of the next row

860
01:25:46,289 --> 01:25:52,739
all right so then 30 25 27 we need to
predict 29 which is the start of next

861
01:25:52,739 --> 01:25:59,030
row and so forth so we're always using
three characters to predict the fourth

862
01:25:59,030 --> 01:26:08,550
so there are 200,000 of these that we're
going to try and model right so we're

863
01:26:08,550 --> 01:26:11,159
going to build this
which means we need to decide how many
activations so I'm going to use 256 okay

864
01:26:18,299 --> 01:26:21,899
and we need to decide how big our
embeddings are going to be and so I

865
01:26:21,899 --> 01:26:27,449
decided to use 42 so about half the
number of characters I have and you can
play around these so you can come up

866
01:26:28,709 --> 01:26:32,729
with better numbers it's just a kind of
experimental and now we're going to

867
01:26:32,729 --> 01:26:38,579
build our model now I'm gonna change my
model slightly and so here is the the
full version so predicting character for

868
01:26:41,249 --> 01:26:45,869
using characters 1 2 & 3 as you can see
it's the same picture as a previous page

869
01:26:45,869 --> 01:26:51,299
but I put some very important coloured
arrows here all the arrows of the same

870
01:26:51,299 --> 01:26:58,319
color are going to use the same matrix
the same weight matrix right so all of

871
01:26:58,319 --> 01:27:05,609
our input embeddings are going to use
the same matrix all of our layers that

872
01:27:05,609 --> 01:27:09,689
go from one layer to the next they're
going to use the same orange arrow

873
01:27:09,689 --> 01:27:15,359
weight matrix and then our output will
have its own matrix so we're going to

874
01:27:15,359 --> 01:27:22,499
have one two three weight matrices right
and the idea here is the reason I'm not
gonna have a separate one but every

875
01:27:24,089 --> 01:27:29,369
everything here is that like why would
kind of semantically a carrot to have a

876
01:27:29,369 --> 01:27:33,029
different meaning depending if it's the
first or the second or the third item in

877
01:27:33,029 --> 01:27:36,599
a sequence like it's not like we're even
starting every sequence at the start of

878
01:27:36,599 --> 01:27:39,959
a sentence we're just arbitrarily
chopped it into groups of three right so

879
01:27:39,959 --> 01:27:44,189
you would expect these to all have the
same kind of conceptual mapping and

880
01:27:44,189 --> 01:27:48,659
ditto like when we're moving from
claritin or character one you know to
kind of say build up some state here why

881
01:27:51,149 --> 01:27:53,819
would that be any different kind of
operation to moving from character

882
01:27:53,819 --> 01:28:00,599
wonder character to so that's the basic
idea so let's create a three character

883
01:28:00,599 --> 01:28:06,899
model and so we're going to create one
linear layer for our Green Arrow one

884
01:28:06,899 --> 01:28:11,819
linear layer fat orange arrow and one
linear layer for our blue arrow and then

885
01:28:11,819 --> 01:28:18,959
also one embedding okay so the embedding
is going to bring in something with size
whatever it was 84

886
01:28:20,609 --> 01:28:23,010
I think vocab size and spit out
something with an

887
01:28:23,010 --> 01:28:29,519
factors in the embedding well then put
that through a linear layer and then

888
01:28:29,519 --> 01:28:34,650
we've got our hidden layers before the
output layer so when we call forward
they're going to be passing in one two

889
01:28:37,110 --> 01:28:42,480
three characters so if each one will
stick it through an embedding we'll

890
01:28:42,480 --> 01:28:46,769
stick it through a linear layer and
we'll stick it through a value just to

891
01:28:46,769 --> 01:28:51,920
do it the character one character - and
character three okay

892
01:28:51,920 --> 01:29:07,769
then I'm going to create this circle of
activations here okay and that matrix

893
01:29:07,769 --> 01:29:12,539
I'm going to call H right and so it's
going to be equal to my input
activations okay after going through the

894
01:29:17,250 --> 01:29:21,510
value and the linear layer and the
embedding right and then I'm going to

895
01:29:21,510 --> 01:29:30,239
apply this l hidden so the orange arrow
and that's going to get me to here okay
so that's what this layer here does and

896
01:29:32,489 --> 01:29:36,960
then to get to the next one I need to
reply the same thing and it apply the

897
01:29:36,960 --> 01:29:43,710
orange arrow to that okay but I also
have to add in this second input right

898
01:29:43,710 --> 01:29:52,619
so take my second input and add in okay
my previous layer your neck could you

899
01:29:52,619 --> 01:30:01,980
pass it back three rows I don't really
see how these dimensions are the same

900
01:30:01,980 --> 01:30:07,980
from eight and in2 from literature which
from yeah okay let's go through so let's

901
01:30:07,980 --> 01:30:16,079
figure out the dimensions together so
self dot E is gonna be of length 42 okay

902
01:30:16,079 --> 01:30:24,000
and then it's gonna go through L in I'm
just gonna make it of size n hidden okay

903
01:30:24,000 --> 01:30:30,869
and so then we're going to pass that
which is now size n hidden through this

904
01:30:30,869 --> 01:30:35,970
which is also going to return something
of size n hidden

905
01:30:35,970 --> 01:30:39,180
okay so it's a really important to
notice that this is square this is a

906
01:30:39,180 --> 01:30:46,290
square weight matrix okay so we now know
that this is of size n hidden into it's

907
01:30:46,290 --> 01:30:50,460
going to be exactly the same size as in
one was which is n hidden so we can now

908
01:30:50,460 --> 01:30:56,400
sum together two sets of activations
both the size n hidden passing it into

909
01:30:56,400 --> 01:31:01,770
here and again it returns something of
size n hidden so basically the trick was

910
01:31:01,770 --> 01:31:05,250
to make this a square matrix and to make
sure that it's square matrix was the

911
01:31:05,250 --> 01:31:09,330
same size as the output of this hidden
well thanks for the great question can

912
01:31:09,330 --> 01:31:19,920
you pass that out to you now Jeremy is
summing the only thing people can do in

913
01:31:19,920 --> 01:31:25,500
these cases I'll come back to that in a
moment that's great point okay um I

914
01:31:25,500 --> 01:31:29,790
don't like it when I have like three
bits of code that look identical and

915
01:31:29,790 --> 01:31:32,970
then three bits of codes that look
nearly identical but aren't quiet

916
01:31:32,970 --> 01:31:40,200
because it's harder to refactor so I'm
going to put a make H into a bunch of

917
01:31:40,200 --> 01:31:49,110
zeros so that I can then put H here and
these are now identical okay so that the

918
01:31:49,110 --> 01:31:54,360
hugely complex trick that we're going to
do very shortly is to replace these

919
01:31:54,360 --> 01:32:00,810
three things with a for loop okay and
it's going to loop through one two and

920
01:32:00,810 --> 01:32:05,130
three that's that's going to be the for
loop or actually zero one two okay at

921
01:32:05,130 --> 01:32:09,000
that point we'll be able to call it a
recurrent neural network okay so just to

922
01:32:09,000 --> 01:32:16,470
skip ahead a little bit alright so we
create that that model make sure I've

923
01:32:16,470 --> 01:32:24,870
run all these so we can actually run
this thing okay so we can now just use

924
01:32:24,870 --> 01:32:28,710
the same columnar model data class that
we've used before and if we use from

925
01:32:28,710 --> 01:32:33,570
arrays then it's basically it's going to
spit back the exact arrays we gave it

926
01:32:33,570 --> 01:32:38,490
right so if we pass if we stack together
those three arrays then it's going to

927
01:32:38,490 --> 01:32:43,430
feed us those three things back to our
forward method so if you want to like

928
01:32:43,430 --> 01:32:49,889
play around with training models using
like you know as roar

929
01:32:49,889 --> 01:32:53,579
approach as possible but without writing
lots of boilerplate this is kind of how

930
01:32:53,579 --> 01:32:58,349
to do it here's column Namit model data
from arrays and then if you pass in

931
01:32:58,349 --> 01:33:06,959
whatever you pass in here right you're
going to get back here okay so I've

932
01:33:06,959 --> 01:33:11,369
passed in three things which means I'm
going to get sent three things okay so

933
01:33:11,369 --> 01:33:14,059
that's how that works

934
01:33:14,189 --> 01:33:18,209
batch size 512 because this is you know
this data is tiny so I can use a bigger
batch size so I'm not using really much

935
01:33:23,309 --> 01:33:27,059
faster i stuff at all I'm using fast AI
stuff just to save me fiddling around

936
01:33:27,059 --> 01:33:30,239
with data loaders and data sets and
stuff but I'm actually going to create a
standard ply torch model I'm not going

937
01:33:33,209 --> 01:33:36,929
to create a loner okay so this is a
standard paper model and because I'm

938
01:33:36,929 --> 01:33:40,619
using ply towards that means I have to
remember to write CUDA okay let's tick

939
01:33:40,619 --> 01:33:50,939
it on the GPU so here is how we can look
inside at what's going on right so we

940
01:33:50,939 --> 01:33:56,429
can say it er MD train data loader to
grab the iterator to iterate through the

941
01:33:56,429 --> 01:34:01,530
training set we can then call next on
that to grab a mini batch and that's

942
01:34:01,530 --> 01:34:08,280
going to return all of our X's and why
tensor and so we can then take a look at

943
01:34:08,280 --> 01:34:16,169
you know here's our X's for example all
right and so you would expect have a

944
01:34:16,169 --> 01:34:22,050
think about what you would expect for
this length three not surprisingly

945
01:34:22,050 --> 01:34:32,459
because these are the three things okay
and so then XS 0 not surprisingly okay

946
01:34:32,459 --> 01:34:38,579
is of length 512 and it's not actually
one hot encoded because we use an

947
01:34:38,579 --> 01:34:42,959
embedding to pretend it is okay and so
then we can use a model as if it's a

948
01:34:42,959 --> 01:34:49,079
function okay by passing to it
the variable eyes version of our tensors

949
01:34:49,079 --> 01:34:56,280
and so have a think about what you would
expect to be returned here okay so not

950
01:34:56,280 --> 01:35:01,439
surprisingly we had a mini batch of 512
so we still have 5 12 and then 85 is the

951
01:35:01,439 --> 01:35:05,739
probability of each of the possible
vocab items and of course we've got the
log of them because that's kind of what

952
01:35:07,480 --> 01:35:12,250
we do in pi torch okay you can see here
the softmax alright so that's how you

953
01:35:12,250 --> 01:35:15,880
can look inside alright so you can see
here how to do everything really very

954
01:35:15,880 --> 01:35:22,270
much by hand so we can create an
optimizer again using standard pipe

955
01:35:22,270 --> 01:35:27,160
torch so with PI torch when you use a
plate or optimizer you have to pass in a
list of the things to optimize and so if

956
01:35:29,739 --> 01:35:34,719
you call m dot parameters that will
return that list for you and then we can

957
01:35:34,719 --> 01:35:43,900
fit and there it goes
okay and so we don't have learning rate

958
01:35:43,900 --> 01:35:47,469
finders and sttr and all that stuff
because we're not using a learner so

959
01:35:47,469 --> 01:35:50,530
we'll have to manually do learning rate
annealing so set the learning rate a

960
01:35:50,530 --> 01:36:01,150
little bit lower and fit again okay and
so now we can write a little function to

961
01:36:01,150 --> 01:36:10,630
to test this thing out okay
so here's something called getnext where

962
01:36:10,630 --> 01:36:17,980
we can pass in three characters like why
full top space right and so I can then
go through and turn that into a tensor

963
01:36:19,750 --> 01:36:26,590
with capital T of an array of the
character index for each character in

964
01:36:26,590 --> 01:36:28,420
that list
so basically turn those into the

965
01:36:28,420 --> 01:36:35,080
integers turn those into variables pass
that to our model right and then we can

966
01:36:35,080 --> 01:36:40,719
do an Arg max on that to grab which
character number is it and in order to

967
01:36:40,719 --> 01:36:45,730
do stuff in none pile and I use two NP
to turn that variable into a lumpy array
right and then I can return that

968
01:36:47,320 --> 01:36:51,310
character and so for example a capital T
because what it thinks would be

969
01:36:51,310 --> 01:36:56,200
reasonable after seeing why . space that
seems like a very reasonable way to
start a sentence if it was ppl a that

970
01:37:00,219 --> 01:37:05,110
sounds reasonable space th a that's
bouncer e small a and D space that

971
01:37:05,110 --> 01:37:08,820
sounds reasonable
so it seems to reflect created something

972
01:37:08,820 --> 01:37:15,000
sensible alright so you know the
important thing to note here is our

973
01:37:15,000 --> 01:37:21,610
character model
is a totally standard fully connected

974
01:37:21,610 --> 01:37:27,190
model right the only slightly
interesting thing we did was to kind of

975
01:37:27,190 --> 01:37:36,540
do this addition of each of the inputs
one at a time okay but there's nothing

976
01:37:36,540 --> 01:37:42,510
new conceptually here we're training it
in the usual way

977
01:37:42,960 --> 01:37:59,440
all right let's now create an errand in
so an iron in is when we do exactly the

978
01:37:59,440 --> 01:38:05,800
same thing that we did here all right
but I could draw this more simply by

979
01:38:05,800 --> 01:38:10,210
saying you know what if we've got a
green arrow going to a circle let's not

980
01:38:10,210 --> 01:38:14,710
draw a green arrow go into a circle
again and again and again so let's just

981
01:38:14,710 --> 01:38:18,550
draw it like this green arrow going to a
circle right and rather than drawing an

982
01:38:18,550 --> 01:38:24,280
orange arrow going to a circle let's
just draw it like this okay so this is

983
01:38:24,280 --> 01:38:32,530
the same picture exactly the same
picture as this one right and so you

984
01:38:32,530 --> 01:38:35,950
just have to say how many times to go
around this circle right so in this case

985
01:38:35,950 --> 01:38:39,790
if we were to predict character number n
from characters one through n minus one

986
01:38:39,790 --> 01:38:44,710
then we can take the character one input
get some activations feed that to some

987
01:38:44,710 --> 01:38:49,270
new activations that go through remember
orange is the hidden to hidden weight

988
01:38:49,270 --> 01:38:53,560
matrix right and each time we'll also
bring in the next character of input

989
01:38:53,560 --> 01:39:01,480
through its embeddings okay so that
picture and that picture I have two ways
of writing the same thing but this one

990
01:39:04,210 --> 01:39:07,930
is more flexible because rather than me
having to say hey let's do it for H I

991
01:39:07,930 --> 01:39:16,330
don't have to draw eight circles right I
can just say I'll just repeat this so I

992
01:39:16,330 --> 01:39:21,670
could simplify this a little bit further
by saying you know what rather than

993
01:39:21,670 --> 01:39:26,170
having this thing as a special case
let's actually start out with a bunch of

994
01:39:26,170 --> 01:39:30,699
zeros right and then let's have all of
our characters
inside here yes yeah so I was wondering

995
01:39:39,550 --> 01:39:45,909
if you can explain it be better why are
you reusing those why you think oh
they're the same yeah where are you you

996
01:39:47,800 --> 01:39:52,929
kind of seem to be reusing the same same
weight matrices weight matrices yeah

997
01:39:52,929 --> 01:39:57,369
maybe this is kind of similar to what we
did in convolution your Nets like if

998
01:39:57,369 --> 01:40:02,679
somehow no I don't think so
at least not that I can see so the idea

999
01:40:02,679 --> 01:40:12,579
is just kind of semantically speaking
like this arrow here this this arrow

1000
01:40:12,579 --> 01:40:21,999
here is saying take a character of
import and represented as some says some

1001
01:40:21,999 --> 01:40:26,019
set of features right and this arrow is
saying the same thing take some

1002
01:40:26,019 --> 01:40:30,179
character and represent as a set of
features and so is this one okay so like

1003
01:40:30,179 --> 01:40:34,300
why would the three be represented with
different weight matrices because it's

1004
01:40:34,300 --> 01:40:40,900
all doing the same thing right and this
orange arrow is saying kind of
transition from character 0 state to

1005
01:40:44,019 --> 01:40:49,539
character 1 state 2 characters to state
again it's it's the same thing it's like
why would the transition from character

1006
01:40:51,550 --> 01:40:55,630
0 to 1 be different to character from
transition from one or two so the idea

1007
01:40:55,630 --> 01:41:02,949
is like but is to like say hey if if
it's doing the same conceptual thing
let's use the exact same white matrix my

1008
01:41:07,360 --> 01:41:12,159
comment on convolution neural networks
is that a filter or so this apply to

1009
01:41:12,159 --> 01:41:18,699
multiple places I think something like a
convolution is almost like a kind of a

1010
01:41:18,699 --> 01:41:22,630
special dot product with shared weights
yeah no that's okay

1011
01:41:22,630 --> 01:41:27,340
that's very good point and in fact one
of our students actually wrote a good

1012
01:41:27,340 --> 01:41:31,300
blog post about that last year we should
dig that up okay I totally see where

1013
01:41:31,300 --> 01:41:37,150
you're coming from and I totally agree
with you all right so let's let's
implement this version so this time

1014
01:41:41,499 --> 01:41:48,760
we're going to do eight
as eight sees okay and so let's create a

1015
01:41:48,760 --> 01:41:55,119
list of every eighth character from zero
through seven and then our outputs will
be the next character and so we can

1016
01:41:57,400 --> 01:42:05,290
stack them together and so now we've got
six hundred thousand by eight so here's

1017
01:42:05,290 --> 01:42:15,699
an example so for example after this
series of eight characters right so this

1018
01:42:15,699 --> 01:42:20,050
is characters north through eight
this is characters one through nine this

1019
01:42:20,050 --> 01:42:25,119
is two through ten these are all
overlapping okay so after characters one
north through eight this is going to be

1020
01:42:27,190 --> 01:42:33,130
the next one okay and then after these
characters this will be the next one all

1021
01:42:33,130 --> 01:42:39,070
right so you can see that this one here
has 43 is its Y value right because

1022
01:42:39,070 --> 01:42:46,380
after those the next one will be 43 okay
so so this is the first eight characters

1023
01:42:46,380 --> 01:42:51,280
this is two through nine three through
ten and so forth right so these are

1024
01:42:51,280 --> 01:42:57,270
overlapping groups of eight characters
and then this is the the next one okay

1025
01:42:57,270 --> 01:43:09,130
so let's create that model okay so again
we use from arrays to create a model

1026
01:43:09,130 --> 01:43:14,110
data class and so you'll see here we
have exactly the same code as we had

1027
01:43:14,110 --> 01:43:18,219
before
there's our embedding Linea hidden

1028
01:43:18,219 --> 01:43:25,210
output these are literally identical
okay and then we've replaced our value

1029
01:43:25,210 --> 01:43:30,489
of the linear input of the embedding
with something that's inside a loop okay
and then we've replaced the cell hidden

1030
01:43:33,310 --> 01:43:38,730
thing okay
also inside the loop

1031
01:43:43,940 --> 01:43:48,699
I just realize didn't mentioned last
time the use of the hyperbolic tan

1032
01:43:48,699 --> 01:43:59,719
hyperbolic tan looks like this okay so
it's just a sigmoid that's offset right

1033
01:43:59,719 --> 01:44:05,270
and it's very common to use a hyperbolic
tan inside this trend this state to

1034
01:44:05,270 --> 01:44:09,800
state transition because it kind of
stops it from flying off too high or too

1035
01:44:09,800 --> 01:44:15,710
low you know it's nicely controlled back
in the old days we used to use

1036
01:44:15,710 --> 01:44:22,969
hyperbolic tanh or the equivalent
sigmoid a lot as most of our activation

1037
01:44:22,969 --> 01:44:28,430
functions nowadays we tend to use value
but in these hidden state to here in the

1038
01:44:28,430 --> 01:44:33,949
hidden state transition weight matrices
we still tend to use hyperbolic tanh
quite a lot so you'll see I've done that

1039
01:44:36,650 --> 01:44:43,550
also yeah hyperbolic tanh okay so this
is exactly the same as before but I've

1040
01:44:43,550 --> 01:44:50,719
just replaced it with a Pollard and then
here's my output yes you know so a does

1041
01:44:50,719 --> 01:44:57,500
he have to do anything with convergence
these networks yeah we'll talk about

1042
01:44:57,500 --> 01:45:03,140
that a little bit over time let's let's
let's come back to that though for now

1043
01:45:03,140 --> 01:45:06,980
we're not really going to do anything
special at all you know recognizing this
is just a standard fully connected

1044
01:45:09,050 --> 01:45:16,010
Network you know interestingly it's
quite a deep one right like because this

1045
01:45:16,010 --> 01:45:22,160
is actually this that we've got eight of
these things now we've now got a deep

1046
01:45:22,160 --> 01:45:26,630
eight layer Network which is why units
starting suggest we should be concerned

1047
01:45:26,630 --> 01:45:29,989
as you know as we get deeper and deeper
networks they can be harder and harder
to train but let's try training this

1048
01:45:37,530 --> 01:45:45,030
all right so when it goes as before
we've got a batch size of 512 we're

1049
01:45:45,030 --> 01:45:51,300
using Adam and where it goes so we will
sit there watching it so we can then set

1050
01:45:51,300 --> 01:45:58,710
the learning rate down back to 20 neg 3
we can fit it again and yeah it's

1051
01:45:58,710 --> 01:46:05,490
actually it seems to be training fun
okay but we're gonna try something else

1052
01:46:05,490 --> 01:46:09,510
which is we're going to use this a trick
that your net rather hinted at before

1053
01:46:09,510 --> 01:46:14,790
which is maybe we shouldn't be adding
these things together and so the reason

1054
01:46:14,790 --> 01:46:18,030
you might want to be feeling a little
uncomfortable about adding these things

1055
01:46:18,030 --> 01:46:26,130
together is that the input state and the
hidden state are kind of qualitatively

1056
01:46:26,130 --> 01:46:31,260
different kinds of things right the
input state is the is the encoding of

1057
01:46:31,260 --> 01:46:36,900
this character for us H represents the
encoding of the series of characters so
far and so adding them together is kind

1058
01:46:39,780 --> 01:46:44,790
of potentially going to lose information
so I think what your net was going to

1059
01:46:44,790 --> 01:46:47,940
prefer that we might do is maybe to
concatenate these instead of adding them

1060
01:46:47,940 --> 01:46:53,760
so it sound good to you you know she's
not it okay so let's now make a copy of

1061
01:46:53,760 --> 01:46:59,870
the previous cell all the same right
rather than using plus let's use cat

1062
01:46:59,870 --> 01:47:06,650
okay now if we can cat then we need to
make sure now that our input layer is

1063
01:47:06,650 --> 01:47:12,630
not from n fac-2 hidden which is what we
had before but because we're

1064
01:47:12,630 --> 01:47:19,590
concatenated it needs to be in fact plus
and hidden to end hidden okay and so now

1065
01:47:19,590 --> 01:47:26,160
that's going to make all the dimensions
work nicely so this now is of size n

1066
01:47:26,160 --> 01:47:33,270
fact plus and hidden this now makes it
back to size n hidden again okay and

1067
01:47:33,270 --> 01:47:37,170
then this is putting it through the same
square matrix as before so it's still a

1068
01:47:37,170 --> 01:47:45,150
size n here okay so this is like a good
design heuristic if you're designing an

1069
01:47:45,150 --> 01:47:49,440
architecture is if you've got different
types of information that you want to

1070
01:47:49,440 --> 01:47:54,550
combine you generally want
concatenate it okay you know adding

1071
01:47:54,550 --> 01:48:00,850
things together even if they're the same
shape is losing information okay and so

1072
01:48:00,850 --> 01:48:05,460
once you've concatenated things together
you can always convert it back down to a

1073
01:48:05,460 --> 01:48:11,560
fixed size by just tracking it through a
matrix product okay so that's what we've

1074
01:48:11,560 --> 01:48:17,469
done here again it's the same thing but
now we're concatenating instead and so

1075
01:48:17,469 --> 01:48:23,620
we can fit that and so last time we got
one point seven two this time you go at

1076
01:48:23,620 --> 01:48:26,440
one point six six so it's not setting
the world on fire but it's an

1077
01:48:26,440 --> 01:48:29,650
improvement and the improvements of it
okay

1078
01:48:29,650 --> 01:48:34,719
so we can now test that with get next
and so now we can pass in eight things

1079
01:48:34,719 --> 01:48:41,760
right so it's no before those let's go
to a part of that sounds good as well so

1080
01:48:41,760 --> 01:48:49,440
Queens and that sounds good too all
right so great so that's enough
manual hackery let's see if pi torch

1081
01:48:53,860 --> 01:48:58,840
couldn't do some of this for us and so
basically what pi torch will do for us

1082
01:48:58,840 --> 01:49:05,290
is it will write this loop automatically
okay and it will create these linear

1083
01:49:05,290 --> 01:49:11,680
input layers automatically okay and so
to ask it to do that we can use the n n

1084
01:49:11,680 --> 01:49:19,180
dot R and n plus so here's the exact
same thing in less code by taking

1085
01:49:19,180 --> 01:49:23,830
advantage of height choice and again I'm
not using a conceptual analogy to say

1086
01:49:23,830 --> 01:49:28,030
player torches doing something like it
I'm saying play torch is doing it now

1087
01:49:28,030 --> 01:49:32,500
this is just the code you just saw
wrapped up a little bit

1088
01:49:32,500 --> 01:49:36,340
reflect it a little bit for your
convenience right so where we say we now

1089
01:49:36,340 --> 01:49:43,660
want to create an era ten call our it n
then what this does is it does that for

1090
01:49:43,660 --> 01:49:51,550
live now notice that our for loop needed
a starting point you remember why right

1091
01:49:51,550 --> 01:49:54,850
because otherwise our for loop didn't
quite work we couldn't quite refactor it

1092
01:49:54,850 --> 01:49:59,739
out and because this is exactly the same
this needs our starting point to and so
let's give it a starting point and so

1093
01:50:01,480 --> 01:50:05,139
you have to pass in your initial hidden
State

1094
01:50:05,139 --> 01:50:14,080
for reasons that will become apparent
later on it turns out to be quite useful

1095
01:50:14,080 --> 01:50:20,409
to be able to get back that here in the
state at the end and just like we could

1096
01:50:20,409 --> 01:50:25,540
here we could actually keep track of the
hidden state we get back to things we

1097
01:50:25,540 --> 01:50:30,520
get back both the output and the hidden
state right so we pass in the input in

1098
01:50:30,520 --> 01:50:36,340
the hidden State when we get back the
output and the hidden state yes could

1099
01:50:36,340 --> 01:50:42,940
you remind us what the hint state
represents the hidden state is H so it's
the it's the orange circle ellipse of

1100
01:50:49,960 --> 01:51:06,010
activations okay and so it is of size
256 okay all right so we can okay

1101
01:51:06,010 --> 01:51:11,980
there's one other thing too to know
which is in our case we were replacing H
with a new hidden state the one minor

1102
01:51:16,360 --> 01:51:22,330
difference in pi torch is they append
the new hidden state to a list or to a

1103
01:51:22,330 --> 01:51:25,929
tensor which gets bigger and bigger so
they actually give you back all of the

1104
01:51:25,929 --> 01:51:29,400
hidden states so in other words rather
than just giving you back the final

1105
01:51:29,400 --> 01:51:33,310
ellipse they give you back all the
ellipses stacked on top of each other

1106
01:51:33,310 --> 01:51:38,110
and so because we just want the final
one I was got indexed into it with minus
one here okay other than that this is

1107
01:51:40,989 --> 01:51:46,300
the same code as before put that through
our output layer to get the correct

1108
01:51:46,300 --> 01:51:50,940
vocab size and then we can train that

1109
01:51:57,630 --> 01:52:01,239
alright so you can see here I can do it
manually I can create some hidden state
I can pass it to that area and I can see

1110
01:52:03,520 --> 01:52:12,429
the stuff I get back you'll see that the
dimensionality of H it's actually a rank

1111
01:52:12,429 --> 01:52:18,239
3 tensor where else in my version it was
a

1112
01:52:19,020 --> 01:52:25,180
let's see it was a rank two tensor okay
and the difference is here we've got

1113
01:52:25,180 --> 01:52:30,280
just a unit axis at the front we'll
learn more about why that is later but
basically it turns out you can have a

1114
01:52:32,440 --> 01:52:36,940
second R and n that goes backwards
alright one that goes forwards one that

1115
01:52:36,940 --> 01:52:40,530
goes backwards from the idea is neck and
then it's going to be better at finding

1116
01:52:40,530 --> 01:52:44,920
relationships that kind of go backwards
that's quite a bi-directional eridan

1117
01:52:44,920 --> 01:52:48,730
also it turns out you can have an error
in feed to an iron in that's got a
multi-layer eridan so basically if you

1118
01:52:51,580 --> 01:52:56,170
have those things you need an additional
access on your tensor to keep track of

1119
01:52:56,170 --> 01:53:01,690
those additional layers of hidden state
but for now we'll always have a one yeah
and we'll always also get back a one at

1120
01:53:05,650 --> 01:53:13,930
the end okay so if we go ahead and fit
this now let's actually trade it for a

1121
01:53:13,930 --> 01:53:17,950
bit longer
okay so last time we only kind of did a
couple of epochs this time we're due for

1122
01:53:20,230 --> 01:53:24,780
a pox
what have we sit at one in egg three and

1123
01:53:24,780 --> 01:53:31,090
then we'll do another to epochs at one
in egg four and so we've now got our

1124
01:53:31,090 --> 01:53:39,610
lost down to one point five so getting
better and better so here's our get next
again okay and you know let's just it

1125
01:53:43,060 --> 01:53:49,090
was the same thing so what we can now do
is we can look through like forty times

1126
01:53:49,090 --> 01:53:54,460
calling get next each time and then each
time will replace our input by removing

1127
01:53:54,460 --> 01:53:58,780
the first character and adding the thing
that we just predicted and so that way

1128
01:53:58,780 --> 01:54:02,610
we can like feed in a new set of eight
characters that get them again and again

1129
01:54:02,610 --> 01:54:09,010
and so that way we'll call that get next
in so here are 40 characters that we've

1130
01:54:09,010 --> 01:54:14,890
generated so we started out with four th
OS so we got four those of the same -

1131
01:54:14,890 --> 01:54:18,190
the same - the same you can probably
guess what happens if you can't
predicting the same - the same all right

1132
01:54:20,620 --> 01:54:31,420
so it's you know it's doing okay we we
now have something which you know

1133
01:54:31,420 --> 01:54:37,240
we've basically built from scratch and
then we've said here's how high torture

1134
01:54:37,240 --> 01:54:41,260
effected it for us so if you want to
like have an interesting little homework

1135
01:54:41,260 --> 01:54:48,160
assignment this week try to write your
own version of an RNN plus all right

1136
01:54:48,160 --> 01:54:53,560
like try to like literally like create
your like you know Jeremy's aren't in

1137
01:54:53,560 --> 01:54:58,690
and then like type in here
Jeremy's aren't in or in your case maybe
your name's not Jeremy which is okay too

1138
01:55:00,810 --> 01:55:06,100
and then get it to run writing your
implementation that's fast from scratch

1139
01:55:06,100 --> 01:55:10,210
without looking at the piped water
source code you know like basically it's

1140
01:55:10,210 --> 01:55:14,680
just a case of like going up and seeing
what we did back here right and like

1141
01:55:14,680 --> 01:55:19,120
make sure you get the same answers and
confirm that you do so that's kind of a

1142
01:55:19,120 --> 01:55:24,130
good little test simply simple at all
assignment but I think you'll feel

1143
01:55:24,130 --> 01:55:29,910
really good when you seem like oh I've
just reimplemented an end alone in

1144
01:55:30,390 --> 01:55:37,750
alright so I'm going to do one other
thing when I switched from this one when

1145
01:55:37,750 --> 01:55:41,530
I've moved the car one input inside the
dotted line right this dotted rectangle

1146
01:55:41,530 --> 01:55:47,580
represents the thing I'm repeating I
also watch the triangle the output I

1147
01:55:47,580 --> 01:55:51,930
moved that inside as well now that's a
big difference

1148
01:55:51,930 --> 01:55:59,140
because now what I've actually done is
I'm actually saying spit out an output

1149
01:55:59,140 --> 01:56:07,200
after every one of these circles so spit
out an output here and here and here
alright so in other words if I have a

1150
01:56:09,730 --> 01:56:13,780
three character input I'm going to spit
out a three character output I'm saying

1151
01:56:13,780 --> 01:56:17,470
half the character 1 this will be next
after character to this be next after

1152
01:56:17,470 --> 01:56:24,240
character 3 this will be next
so again nothing different
and again this you know if you want to

1153
01:56:26,470 --> 01:56:31,450
go a bit further with the assignment you
could write this by hand as well but

1154
01:56:31,450 --> 01:56:37,590
basically what we're saying is in the
for loop would be saying like you know

1155
01:56:37,590 --> 01:56:42,730
results equals some empty list right and
then would be going through and rather
than returning that

1156
01:56:44,960 --> 01:56:55,400
we're instead be saying you know results
dot append that right and then like

1157
01:56:55,400 --> 01:57:02,480
return whatever torch dot stat something
like that right that it made me right in

1158
01:57:02,480 --> 01:57:09,610
my question so now you know we now have
like every step we've created an output

1159
01:57:09,610 --> 01:57:16,610
okay so which is basically this picture
and so the reason was lots of reasons

1160
01:57:16,610 --> 01:57:21,560
that's interesting but I think the main
reason right now that's interesting is

1161
01:57:21,560 --> 01:57:32,060
that you probably noticed this this
approach to dealing with our data seems

1162
01:57:32,060 --> 01:57:38,660
terribly inefficient like we're grabbing
the first eight right but then this next
set all but one of them overlap the

1163
01:57:41,960 --> 01:57:48,230
previous one right so we're kind of like
recalculating the exact set of
embeddings seven out of eight of them

1164
01:57:50,210 --> 01:57:55,790
are going to be exact same embeddings
right exact same transitions it kind of

1165
01:57:55,790 --> 01:58:00,740
seems weird to like do all this
calculation to just predict one thing

1166
01:58:00,740 --> 01:58:04,220
and then go back and recalculate seven
out of eight of them and add one more to

1167
01:58:04,220 --> 01:58:06,980
the end to calculate the next thing all
right
so the basic idea then is to say well

1168
01:58:09,590 --> 01:58:18,080
let's not do it that way instead let's
taking non overlapping sets of

1169
01:58:18,080 --> 01:58:25,460
characters all right so like so here is
our first eight characters here is the

1170
01:58:25,460 --> 01:58:29,860
next day characters here are the next
day characters so like if you read this

1171
01:58:29,860 --> 01:58:38,240
top left to bottom right that would be
the whole nature right and so then if

1172
01:58:38,240 --> 01:58:43,940
these are the first eight characters
then offset this by one starting here

1173
01:58:43,940 --> 01:58:51,110
that's a list of outputs right so after
we see characters zero through seven

1174
01:58:51,110 --> 01:58:58,440
we should predict characters 1 through 8
the XS so after 40 should come 42

1175
01:58:58,440 --> 01:59:05,670
as it did after 42 should come 29 as it
did okay and so now that can be our

1176
01:59:05,670 --> 01:59:14,489
inputs and labels for that model and so
it shouldn't be any more or less
accurate it should just be the same

1177
01:59:17,250 --> 01:59:27,390
right pretty much but it should allow us
to do it more efficiently so let's try

1178
01:59:27,390 --> 01:59:39,800
that all right
so I mentioned last time that we had a

1179
01:59:40,489 --> 01:59:48,239
minus 1 index here because we just
wanted to grab the last triangle okay so

1180
01:59:48,239 --> 01:59:52,410
in this case we're going to grab all the
triangles so this this is actually the

1181
01:59:52,410 --> 01:59:58,350
way it end on RNN creates things we we
only kept the last one but this time

1182
01:59:58,350 --> 02:00:07,890
we're going to keep all of them so we've
made one change which is to remove that

1183
02:00:07,890 --> 02:00:22,739
minus one other than that this is the
exact same code as before okay so but
there's nothing much to show you here I

1184
02:00:24,120 --> 02:00:31,260
mean except of course at this time if we
look at the labels it's now 512 by eight

1185
02:00:31,260 --> 02:00:38,670
factors we're trying to predict eight
things every time through so there is

1186
02:00:38,670 --> 02:00:46,670
one complexity here which is that we
want to use the negative log likelihood

1187
02:00:46,670 --> 02:00:53,400
loss function as before right but the
ligand if lost likelihood loss function

1188
02:00:53,400 --> 02:00:59,910
just like our MSE expects to receive to
rank one tensors actually with the

1189
02:00:59,910 --> 02:01:05,550
mini-batch access to rank two tensors
all right so two to mini-batches of

1190
02:01:05,550 --> 02:01:11,960
vectors problem is that we've got

1191
02:01:12,330 --> 02:01:18,190
eight-time steps you know it characters
in an RNN we call it a time step right

1192
02:01:18,190 --> 02:01:24,370
we have eight time steps and then for
each one we have 84 probabilities we

1193
02:01:24,370 --> 02:01:30,940
have the probability for every single
one of those eight times deaths and then

1194
02:01:30,940 --> 02:01:37,389
we have that for each of our 512 items
in the mini batch so we have a rank 3

1195
02:01:37,389 --> 02:01:44,230
tensor not a rank two tensor um so that
means that the negative log likelihood
loss function is going to spit out an

1196
02:01:46,179 --> 02:01:50,230
error now frankly I think this is kind
of dumb you know I think it would be
better if PI torch had written the loss

1197
02:01:54,429 --> 02:01:59,110
functions in such a way that they didn't
care at all about rank and they just

1198
02:01:59,110 --> 02:02:05,260
applied it to whatever rank you gave it
but for now at least it does care about

1199
02:02:05,260 --> 02:02:09,310
rick but the nice thing is I get to show
you how to write a custom loss function

1200
02:02:09,310 --> 02:02:14,530
okay so we're going to create a special
negative log likelihood loss function
for sequences okay and so it's going to

1201
02:02:17,110 --> 02:02:21,550
take an input in the target and it's got
a call f dot negative log likelihood

1202
02:02:21,550 --> 02:02:28,000
lost so the pipe launched one all right
but what we're going to do is we're

1203
02:02:28,000 --> 02:02:37,290
going to flatten our input and we're
going to flatten our targets right and

1204
02:02:37,290 --> 02:02:45,340
so and it turns out these are going to
be the first two axes that I have to be

1205
02:02:45,340 --> 02:02:54,070
transposed so the way PI torch handles
are and end data by default is the first

1206
02:02:54,070 --> 02:02:59,199
axis is the sequence length in this case
eight right so the sequence length of an

1207
02:02:59,199 --> 02:03:04,449
R and n is how many times deaths so we
have eight characters so a sequence

1208
02:03:04,449 --> 02:03:10,090
length of eight the second axis is the
batch size and then as would expect the

1209
02:03:10,090 --> 02:03:15,989
third axis is the actual hidden state
itself okay so this is going to be eight
by 512 by n hidden which I think was 256

1210
02:03:22,830 --> 02:03:28,550
yeah
okay so we can grab the size and unpack

1211
02:03:28,550 --> 02:03:39,550
it into each of these sequence length
batch size and I'm hidden now target

1212
02:03:39,790 --> 02:03:53,270
mighty dot size is 512 by 8 where else
this one here was 8 by 512 so to make

1213
02:03:53,270 --> 02:04:00,360
them match we're going to have to
transpose the first two axis okay
[Music]

1214
02:04:01,540 --> 02:04:06,500
hi torch when you do something like
transpose doesn't generally actually

1215
02:04:06,500 --> 02:04:11,530
shuffle the memory order but instead it
just kind of keeps some internal

1216
02:04:11,530 --> 02:04:17,830
metadata to say like hey you should
treat this as if it's transposed and

1217
02:04:17,830 --> 02:04:23,270
some things in pi torch will give you an
error if you try and use it when it has

1218
02:04:23,270 --> 02:04:30,949
these like this internal state and I
basically say error this tensor is not
contiguous if you ever see that error at

1219
02:04:33,260 --> 02:04:38,150
the word contiguous after it and it goes
away so I don't know they can't do that
for you apparently so in this particular

1220
02:04:39,739 --> 02:04:44,239
case I got that error so I wrote the
code contiguous after it okay and so

1221
02:04:44,239 --> 02:04:49,130
then finally we need to flatten it out
into a single vector and so we can just

1222
02:04:49,130 --> 02:04:53,719
go a dot view which is the same as non
PI dot reshape and minus one means as

1223
02:04:53,719 --> 02:05:02,120
long as it needs to be okay and then the
input again we also reshape that right

1224
02:05:02,120 --> 02:05:09,920
but remember the input sorry the the the
predictions also have this axis of

1225
02:05:09,920 --> 02:05:15,850
length 84 all of the predicted
probabilities okay so so here's a custom

1226
02:05:15,850 --> 02:05:20,420
these are custom lost function that's it
right so if you ever want to play around

1227
02:05:20,420 --> 02:05:27,230
with your own loss functions you can
just do that like so and then pass that
to fit okay so it's important to

1228
02:05:30,710 --> 02:05:37,480
remember that Fitch is this like lowest
level fast AI abstraction
that's--it's that this is the thing that

1229
02:05:39,790 --> 02:05:45,970
implements the training look okay and so
like you're the stuff you pass it in is

1230
02:05:45,970 --> 02:05:53,290
all standard pi torch stuff except for
this this is our model data object this

1231
02:05:53,290 --> 02:05:58,600
is the thing that wraps up the test set
the training set and the validation set

1232
02:05:58,600 --> 02:06:06,460
to get that okay your neck could you
pass that back so when we pull the

1233
02:06:06,460 --> 02:06:13,480
triangle into the replicator structure
right so the the first n minus one
iterations of the sequence length we

1234
02:06:15,910 --> 02:06:20,500
don't see the whole sequence length yeah
so does that mean that the batch size

1235
02:06:20,500 --> 02:06:26,080
should be much bigger so that be careful
you don't mean that size you main

1236
02:06:26,080 --> 02:06:31,930
sequence length right because the batch
size is like some firing yeah okay so

1237
02:06:31,930 --> 02:06:37,120
yes yes if you have a short sequence
length like eight yeah

1238
02:06:37,120 --> 02:06:46,480
the first character has nothing to go on
it starts with an empty hidden state of

1239
02:06:46,480 --> 02:06:51,310
zeros okay so what we're going to start
with next week

1240
02:06:51,310 --> 02:06:55,390
is we're going to learn how to avoid
that problem right and so it's a really

1241
02:06:55,390 --> 02:07:01,900
insightful question or concern right and
but if you think about it the basic idea
is why should we reset this to zero

1242
02:07:05,910 --> 02:07:13,750
every time you know like if we can kind
of line up these mini batches somehow so

1243
02:07:13,750 --> 02:07:18,640
that the next mini batch joins up
correctly it represents like the next

1244
02:07:18,640 --> 02:07:26,880
letter in leaches works then we'd want
to move this up into the constructor

1245
02:07:26,880 --> 02:07:38,020
right and then like pass that here and
then store it here right and now we're

1246
02:07:38,020 --> 02:07:43,000
not resetting the hidden state each time
we're actually we're actually keeping

1247
02:07:43,000 --> 02:07:48,340
the hidden state from call to call and
so the only time that it would be

1248
02:07:48,340 --> 02:07:53,109
failing to benefit from
learning state would be like literally

1249
02:07:53,109 --> 02:07:56,559
at the very start of the document so
that's where but that's where we're

1250
02:07:56,559 --> 02:08:09,069
going to try and ahead next week
I feel like this lesson every time I've

1251
02:08:09,069 --> 02:08:13,149
got a punch line coming somebody asks me
a question where I have to like do the

1252
02:08:13,149 --> 02:08:21,339
punch line ahead of time okay so we can
fit that and we can fit that and I want

1253
02:08:21,339 --> 02:08:25,869
to show you something interesting and
this is coming to the punch line that

1254
02:08:25,869 --> 02:08:33,909
another punch line that you net try to
spoil which is when we're you know

1255
02:08:33,909 --> 02:08:37,689
remember this is just doing a loop right
applying the same matrix multiply again

1256
02:08:37,689 --> 02:08:46,419
and again if that matrix multiply tends
to increase the activations each time

1257
02:08:46,419 --> 02:08:50,439
then effectively we're doing that to the
power of eight right so it's going to

1258
02:08:50,439 --> 02:08:54,869
like to shoot off really high or if it's
decreasing it a little bit each time
that's going to shoot off really low so

1259
02:08:57,760 --> 02:09:01,659
this is what we call a gradient
explosion right and so we really want to
make sure that the initial H naught H

1260
02:09:07,209 --> 02:09:16,359
the initial but if we call it the
initial L hidden that we create is is

1261
02:09:16,359 --> 02:09:22,149
like oversize that's not going to cause
our activations on average to increase

1262
02:09:22,149 --> 02:09:29,939
or decrease right and there's actually a
very nice matrix that does exactly that

1263
02:09:29,939 --> 02:09:35,589
called the identity matrix so the
identity matrix for those that don't

1264
02:09:35,589 --> 02:09:42,309
quite remember their linear algebra is
this this would be a size 3 identity

1265
02:09:42,309 --> 02:09:49,839
matrix all right and so the trick about
an identity matrix is anything times an

1266
02:09:49,839 --> 02:09:54,519
identity matrix is itself right and so
therefore you could multiply it by this

1267
02:09:54,519 --> 02:09:59,289
again and again and again and again and
still end up with itself right so
there's no gradient explosion so what we

1268
02:10:03,280 --> 02:10:08,080
could do is instead
of using whatever the default random in

1269
02:10:08,080 --> 02:10:14,350
it is for this matrix we could instead
after we create our errand in is we can

1270
02:10:14,350 --> 02:10:17,410
go into that
Erol in right and notice this right we
can go m dot RN n right and if we now go

1271
02:10:22,770 --> 02:10:29,110
like so we can get the docs for m dot R
and M right and as well as the arguments
for constructing it it also tells you

1272
02:10:31,930 --> 02:10:35,770
the inputs and outputs for calling the
layer and it also tells you the

1273
02:10:35,770 --> 02:10:39,610
attributes and so it tells you there's
something called weight
H H and these are the learn about hidden

1274
02:10:42,070 --> 02:10:46,390
to hidden weights that's that square
matrix right so after we've constructed

1275
02:10:46,390 --> 02:10:55,030
our M we can just go in and say all
right m dot R and n dot weight h HL dot
data that's the tensor dot copy

1276
02:10:58,680 --> 02:11:07,920
underscore in place torch I that is I
for identity in case you are wondering

1277
02:11:07,920 --> 02:11:14,949
so this is an identity matrix of size n
hidden so this both puts into this
weight matrix and returns the identity

1278
02:11:18,310 --> 02:11:29,110
matrix and so this was like actually a
Geoffrey Hinton paper was like hey you
know after it was it's 2015

1279
02:11:31,449 --> 02:11:38,190
so after recurrent neural Nets have been
around for decades here's like hey gang

1280
02:11:38,190 --> 02:11:44,440
maybe we should just use the identity
matrix to initialize this and like it

1281
02:11:44,440 --> 02:11:49,870
actually turns out to work really well
and so that was a 2015 paper believe it

1282
02:11:49,870 --> 02:11:54,550
or not from the father of neural
networks and so here is the here is our

1283
02:11:54,550 --> 02:11:58,090
implementation of his paper and this is
an important thing to know right when

1284
02:11:58,090 --> 02:12:02,620
very famous people like Geoffrey Hinton
write a paper sometimes in entire

1285
02:12:02,620 --> 02:12:07,890
implementation of that paper looks like
one line of code okay so let's do it

1286
02:12:07,890 --> 02:12:12,969
before we got point six one two five
seven we'll fit it with exactly the same

1287
02:12:12,969 --> 02:12:16,900
parameters and now we get 0.5 1 and in
fact

1288
02:12:16,900 --> 02:12:22,360
can keep training 0.50 so like this
tweak really really really helped okay

1289
02:12:22,360 --> 02:12:26,739
now one of the nice things about this
tweak was before I could only use a

1290
02:12:26,739 --> 02:12:32,620
learning rate of one in x3 before it
started going crazy but after identity

1291
02:12:32,620 --> 02:12:36,429
matrix I found I could use one in egg
too because it's you know it's better

1292
02:12:36,429 --> 02:12:41,890
behaved weight initialization I found I
could use a higher learning rate okay

1293
02:12:41,890 --> 02:12:49,179
and honestly these things you know
increasingly we're trying to incorporate

1294
02:12:49,179 --> 02:12:53,650
into the defaults in first day I you
know you don't necessarily personally

1295
02:12:53,650 --> 02:12:59,140
need to actually know them but you know
at this point we're still at a point

1296
02:12:59,140 --> 02:13:02,290
where you know most things in most
libraries most of the time don't have

1297
02:13:02,290 --> 02:13:05,679
great defaults it's good to know all
these little tricks it's also nice to

1298
02:13:05,679 --> 02:13:09,310
know if you want to improve something
what kind of tricks people have used

1299
02:13:09,310 --> 02:13:15,250
elsewhere because you can often borrow
them yourself all right well that's the

1300
02:13:15,250 --> 02:13:20,560
end of the lesson today and so next week
we will look at this idea of a stateful

1301
02:13:20,560 --> 02:13:23,830
RNN that's going to keep this hidden
state around and then we're going to go
back to looking at language models again

1302
02:13:27,070 --> 02:13:30,070
and then finally we're going to go all
the way back to computer vision and

1303
02:13:30,070 --> 02:13:35,350
learn about things like rez nets and
batch norm and all the tricks that were

1304
02:13:35,350 --> 02:13:40,679
in figured out in cats versus dogs see
you then

1305
02:13:41,040 --> 02:13:43,960
[Applause]