1
00:00:00,060 --> 00:00:14,991
welcome back less than six so this is
our penultimate lesson and believe it or
not a couple of weeks ago in Lesson four
I mentioned I was going to share that
lesson with this terrific you know P

2
00:00:15,091 --> 00:00:22,704
researcher Sebastian Reuter which I did

3
00:00:22,804 --> 00:00:39,250
and he he said he loved it and he's gone
on to yesterday released this new post
he called optimization for deep learning
highlights in 2017 in which he covered
basically everything that we talked
about in that lesson and with some very

4
00:00:39,350 --> 00:00:44,455
nice shout outs to some of the work that
some of the students here have done

5
00:00:44,555 --> 00:00:58,020
including when he talked about this
separation of the separation of weight
decay from the momentum term and so he

6
00:00:58,020 --> 00:01:13,034
actually mentions here the opportunities
in terms of improved kind of software
decoupling this allows and actually
links to the commits from an answer hah
actually showing how to implement this

7
00:01:13,134 --> 00:01:16,710
in fast AI so first a eyes code is
actually being used as a bit of a role

8
00:01:18,060 --> 00:01:25,170
model now he then covers some of these
learning rate tuning techniques that

9
00:01:25,170 --> 00:01:36,780
we've talked about and this is the SGD
our schedule it looks a bit different to
what you're used to seeing this is on a
log curve this is the way that they show

10
00:01:36,780 --> 00:01:53,430
it on the paper and for more information
again links to two blog posts one from
vitaly about this topic and and again
ananza ha is blog post on this topic so

11
00:01:53,430 --> 00:02:00,335
it's great to see that some of the work
from faster our students is already
getting noticed and picked up and shared

12
00:02:00,435 --> 00:02:06,070
and this blog post went on to get on the
front page of hacker news so that's

13
00:02:06,170 --> 00:02:14,885
pretty cool and hopefully more and more
of this work or be picked up on sisters
released
publicly so last week we were kind of

14
00:02:14,985 --> 00:02:31,090
doing a deep dive into collaborative
filtering and let's remind ourselves of
kind of what our final model looked like

15
00:02:34,300 --> 00:02:42,200
so in the end we kind of ended up
rebuilding the model that's actually in
the first a a library where we had an

16
00:02:47,630 --> 00:02:56,370
embedding so we had this little get
embedding function that grabbed an
embedding and randomly initialize the
weights for the users and for the items

17
00:02:56,470 --> 00:03:00,870
that's the kind of generic term in our
case the items are movies

18
00:03:00,970 --> 00:03:11,540
and the bias for the users the bias for
the items and we had n factors embedding
size for each for each one of course the
biases just had a single one and then we

19
00:03:14,210 --> 00:03:22,180
grabbed the users and item in weddings
multiply them together summed it up each
row and add it on the bias terms pop

20
00:03:22,730 --> 00:03:27,430
that through a sigmoid to put it into
the range that we wanted so that was our

21
00:03:27,430 --> 00:03:39,865
model and one of you asked if we can
kind of interpret this information in
some way and I promised this week we
would see how to do that so let's take a

22
00:03:39,965 --> 00:04:04,050
look so we're going to start with the
model we built here where we just used
that fast AI library
collaborative data set from CSP and then
that get learner and then we fitted it
in three epochs 19 seconds we've got a
pretty good result so what we can now do

23
00:04:04,150 --> 00:04:09,770
is to analyze that model so you may

24
00:04:09,770 --> 00:04:24,370
remember right back when we started we
read in the movies CSV file but that's
just a mapping from the ID of the movie
to the name of the movie and so we're
just going to use that for display
purposes so we can see what we're doing

25
00:04:24,640 --> 00:04:36,660
because
not all of us have watched every movie
I'm just going to limit this to the top
500 most populous or 3,000 most popular
movies so we might have more chance of
recognizing the movies we're looking at

26
00:04:36,660 --> 00:04:51,785
and then I'll go ahead and change it
from the movie IDs from movie lens to
those unique IDs that we're using the
contiguous IDs because that's what a

27
00:04:51,885 --> 00:05:06,410
model has alright so inside the learn
object that we create inside alona we
can always grab the PI torch model
itself just by saying learn model okay

28
00:05:06,510 --> 00:05:18,140
and like I'm going to kind of show you
more and more of the code at the moment
so let's take a look at the definition
of model and so a model is a property so

29
00:05:18,240 --> 00:05:32,800
if you haven't seen a property before a
property is just something in Python
which looks like a method when you
define it that you can call it without
parentheses as we do here alright and so

30
00:05:32,800 --> 00:05:42,669
it kind of looks when you call it like
it's a regular attribute but it looks
like when you define it like it's a
method so every time you call it it
actually runs this code okay and so in

31
00:05:42,669 --> 00:05:48,789
this case it's just a shortcut to grab
something called dot models model so you

32
00:05:48,789 --> 00:06:03,019
may be interested to know what that
looks like
learn about models and so this is
there's a fast AI model type is a very
thin wrapper for pite watch models so we

33
00:06:03,119 --> 00:06:19,720
could take a look at this code filter
model and see what that is it's only one
line of code okay and yeah we'll talk

34
00:06:19,720 --> 00:06:33,380
more about these in part two right but
basically that there's this very thin
wrapper and the main thing one of the
main things that fast i out does is we
have this concept of layer groups where
basically when you say here though
different learning rates and they're
going to apply two different sets of

35
00:06:33,480 --> 00:06:42,820
layers and that's something that's not
in paid watch so when you say I want to
use this PI torch model
all this with one thing we have to do
which is to say like okay one hour later

36
00:06:42,820 --> 00:06:54,950
groups yeah so the details aren't
terribly important but in general if you
want to create a little wrapper for some
other pipe watch model you could just
write something like this so to get to

37
00:06:55,050 --> 00:07:07,820
get inside that to grab the actual PI
torch model itself its models dot model
that's the PI torch model and then the
learn object has a shortcut to that okay
so we're going to set m to be the PI

38
00:07:07,920 --> 00:07:21,815
torch model and so when you print out a
pipe watch model it prints it out
basically by listing out all of the
layers that you created in the

39
00:07:21,915 --> 00:07:39,460
constructor it's quite it's quite nifty
actually when you kind of think about
the way this works thanks to kind of
some very handy stuff in Python we're
actually able to use standard - oh wow
to kind of define these modules in these

40
00:07:39,460 --> 00:08:07,960
layers and they basically automatically
kind of register themselves with pipe
which so back in our embedding bias we
just had a bunch of things where we said
okay each of these things are equal to
these things and then it automatically
knows how to represent that so you can
see there's the name is you and so the
name is just literally whatever we
called it yeah you
and then the definition is it's this
kind of layer okay so that's our height

41
00:08:12,910 --> 00:08:28,660
watch model so we can look inside that
basically use that so if we say m dot I
be then that's referring to the
embedding layer for an item which is the

42
00:08:28,660 --> 00:08:34,000
bias layer so an item bias in this case
is the movie bias so each move either a

43
00:08:34,000 --> 00:08:40,990
9000 of them has a single bias element
okay now the really nice thing about

44
00:08:40,990 --> 00:08:50,710
high torch layers and models is that
they all look the same they basically
got to use them you call them as if they
were

45
00:08:50,710 --> 00:09:12,520
action so we can go m.i.b parenthesis
right and that basically says I want you
to return the value of that layer and
that layer could be a full-on model
right so to actually get a prediction
from a play torch model you just I would
go m and pass in my variable okay and so

46
00:09:12,520 --> 00:09:22,570
in this case my B and pass in my top
movie indexes now models remember layers

47
00:09:22,570 --> 00:09:34,835
they require variables not tensors
because it needs to keep track of the
derivatives okay and so we use this
capital V to turn the tensor into a

48
00:09:34,935 --> 00:10:00,425
variable and was just announced this
week that PI torch 0.4 which is the
version after the one that's just about
to be released is going to get rid of
variables and will actually be able to
use tensors directly to keep track of
derivatives so if you're watching this
on the MOOC and you're looking at point
four then you'll probably notice that
the code doesn't have this V unit
anymore

49
00:10:00,525 --> 00:10:07,025
and so that would be pretty exciting
when that happens but for now we have to
remember if we're going to pass
something into a model to turn it into a

50
00:10:07,125 --> 00:10:20,140
variable first and remember a variable
has a strict superset of the API of a
tensor so anything you can do to a
tensor you can do to a variable and it
up will take its log or whatever okay so

51
00:10:20,140 --> 00:10:38,820
that's going to return a variable which
consists of going through each of these
movie IDs putting it through this
embedding layer to get its bias okay and
that's going to return a variable let's
take a look

52
00:10:41,790 --> 00:11:06,550
so before I press shift down to here you
can have a think about what I'm going to
have I've got a list of 3,000 movies
going in turning into variable putting
it through this embedding layer so just
have a think about what we expect to
come out okay and we have a variable of
size 3,000 by one hopefully that doesn't
surprise you
we had 3000 movies that we are looking
up each one hadn't had a one long

53
00:11:08,830 --> 00:11:14,675
embedding okay so there's our three
thousand one you'll notice it's a
variable just not surprising because we
fed it a variable so we've got a

54
00:11:14,775 --> 00:11:22,385
variable back and it's a variable that's
on the GPU right doc CUDA okay so we

55
00:11:22,485 --> 00:11:36,740
have a little shortcut in fast AI
because we we very often when I take
variables turn them into tensors and
move them back to the CPU so we can play
with them more easily
so two NP is is two numpy okay and that

56
00:11:36,840 --> 00:11:58,930
does all of those things and it works
regardless of whether it's a tensor or a
variable it works regardless of whether
it's on the CPU or GPU it'll end up
giving you a a numpy array from that
okay so if we do that that gives us
exactly the same thing as we just looked
at but now in numpy form okay so that's

57
00:11:58,930 --> 00:12:18,970
a super handy thing to use when you're
playing around with pi torch my approach
to things is I try to use numpy for
everything except when I explicit and
you need something to run on the GPU or
I need its derivatives right in which
case I use PI torch because like none

58
00:12:19,330 --> 00:12:39,700
part like I kind of find none PI's often
easier to work with it's been around
many years longer than PI torch so you
know and lots of things like the Python
imaging library OpenCV and lots and lots
of stuff like pandas it works with numpy

59
00:12:39,700 --> 00:12:45,460
so my approach is kind of like do as
much as I can in num pile and finally

60
00:12:45,460 --> 00:12:53,410
when I'm ready to do something on the
GPU or take its derivative to PI torch
and then as soon as I can I put it back
in vampire and you'll see that the first

61
00:12:53,410 --> 00:13:03,430
AI library really works this way like
all the transformations and stuff happen
in lamb pie which is different to most
high torch computer vision libraries
which tend to do it all as much as

62
00:13:05,830 --> 00:13:11,767
possible in pi torch I try to do as much
as possible in non pipe so let's say we
wanted to transfer build a model in the

63
00:13:11,867 --> 00:13:31,660
GPU with the GPU and train it
then we want to bring this to production
so would we call to numpy on the model
itself or would we have to iterate
through all the different layers and
then call to NP yeah good question so

64
00:13:31,660 --> 00:13:38,090
it's very likely that you want to do
inference on a cpu rather than a GPU
it's it's more scalable you don't have

65
00:13:38,190 --> 00:13:44,045
to worry about putting things in batches
you know and so forth so you can move a

66
00:13:44,145 --> 00:14:03,620
model onto the cpu just by typing m dot
CPU and that model is now on the cpu and
so therefore you can also then put your
variable on the CPU by doing exactly the
same thing so you can say like so now

67
00:14:03,720 --> 00:14:27,560
having said that if you're if you'll
serve it doesn't have a GPU or CUDA GPU
you don't have to do this because it
won't put it on the GPU at all so if for
inferencing on the server if you're
running it on you know some t2 instance
or something it'll work fine and will
run on the on the cpu automatically

68
00:14:27,660 --> 00:14:41,390
quick follow-up and if we train the
model on the GPU and then we save those
embeddings and the weights would we have
to do anything special to load you know
you won't we have something well it kind

69
00:14:41,490 --> 00:14:51,990
of depends how much of faster I you're
using so I'll show you how you can do
that in case you have to do it manually

70
00:14:51,990 --> 00:15:08,350
one of the students figure this out
which is really handy when we there's a
load model function and you'll see what
it does but it does torch dot load is it
basically this is like some magic
incantation that like normally it has to

71
00:15:09,050 --> 00:15:24,160
load it onto the same GPU or saved on
but this will like load it into what it
was what it is available so there's a
Andy discovery thanks for the great

72
00:15:28,819 --> 00:15:36,619
to put that back on the GPU I'll need to
say doc CUDA and now there we go I can

73
00:15:36,619 --> 00:15:50,509
run it again okay so it's really
important to know about the zip function
in Python which iterates through a
number of lists at the same time so in

74
00:15:50,509 --> 00:16:00,259
this case I want to grab each movie
along with its bias term so that I can
just pop it into our list of tuples so
if I just go zip like that that's going

75
00:16:00,259 --> 00:16:11,289
to iterate through each movie ID and
each bias term and so then I can use
that in a list comprehension to grab the
name of each movie along with its place

76
00:16:11,289 --> 00:16:15,714
okay so having done that I can then sort

77
00:16:15,814 --> 00:16:33,539
and so here are I told you that John
John Travolta Scientology movie at the
most negative of the quiet by a lot if
this was a cable competition Battlefield
Earth would have like won by miles or
this seven seven seven ninety six so

78
00:16:33,639 --> 00:16:46,844
here's the worst movie of all time
according to IMDB and like it's
interesting when you think about what
this means right because this is like a
much more authentic way to find out how
bad this movie is because like some

79
00:16:46,944 --> 00:16:57,319
people are just more negative about
movies right and like it more of them
watch your movie like you know highly
critical audience they're gonna read it
badly so if you take an average it's not
quite fair right and so what this is you

80
00:17:02,149 --> 00:17:10,399
know what this is doing is saying once
we you know remove the fact that
different people have different overall
positive or negative experiences and
different people watch different kinds

81
00:17:11,689 --> 00:17:20,169
of movies and we correct for all that
this is the worst movie of all time so
that's a good thing to know

82
00:17:21,638 --> 00:17:29,360
so this is how we can yeah look inside
our our model and and interpret the bias

83
00:17:29,360 --> 00:17:59,520
vectors you'll see here I've sorted by
the zeroth element of each tuple by
using a lambda originally I used this
special item ghetto this is part
of pythons operator library and this
creates a function that returns the
zeroth element of something in order to
save time and then I actually realize
that the lambda is only one more
character to write then the item get us
so maybe we don't need to know this

84
00:17:59,620 --> 00:18:06,290
after all so yeah really useful to make
sure you know how to write lambdas in
Python so this is this is a function

85
00:18:06,290 --> 00:18:18,435
okay and so sort the sort is going to
call this function every time it decides
like is this thing higher or lower than
that other thing and this fact this is
going to return the zeroth element okay

86
00:18:18,535 --> 00:18:31,340
so here's the same thing and item get a
format and here is the reverse and
Shawshank Redemption right at the top
I'll definitely agree with that
Godfather usual suspects yeah these are
all pretty great movies twelve Angry Men

87
00:18:35,120 --> 00:18:43,100
absolutely so there you go there's how
we can look at the base so then the

88
00:18:43,100 --> 00:18:50,205
second piece to look at would be the the
embeddings how can we look at the
embeddings so we can do the same thing

89
00:18:50,305 --> 00:19:01,315
so remember I was the item embeddings
rather than IV with the item bias we can
pass in our list of movies as a variable
turn it into numpy and here's our movie

90
00:19:01,415 --> 00:19:08,670
embedding so for each of the 3,000 most
popular movies here are its 50

91
00:19:08,770 --> 00:19:20,355
embeddings so it's very hard unless
you're Geoffrey Hinton to visualize a 50
dimensional space so what we'll do is
we'll turn it into a three dimensional

92
00:19:20,455 --> 00:19:29,155
space so we can compress high
dimensional spaces down into lower
dimensional spaces using lots of
different techniques perhaps one of the

93
00:19:29,255 --> 00:19:36,073
most common and popular is called PCA
PCA stands for principle components
analysis it's a linear technique but

94
00:19:36,173 --> 00:19:41,575
when your techniques generally work fine
for this kind of embedding I'm not going

95
00:19:41,675 --> 00:19:54,510
to teach you about PCA now but I will
say in Rachel's computation or linear
algebra class which you can get to you
from first at AI we cover PCA in

96
00:19:54,610 --> 00:20:04,360
detail and it's a really important
technique it actually it turns out to be
almost identical to something called
singular value decomposition which is a

97
00:20:04,460 --> 00:20:18,740
type of matrix decomposition which
actually does turn up in deep learning a
little bit from time to time it's kind
of somewhat worth knowing if you were
going to dig more into linear algebra

98
00:20:18,740 --> 00:20:26,220
you know SPD and PCA along with
eigenvalues and eigenvectors which are
all slightly different versions is this
kind of the same thing or all worth

99
00:20:26,320 --> 00:20:33,750
knowing but for now just know that you
can grab PCA from SK learn to calm

100
00:20:33,850 --> 00:20:41,540
position say how much you want to reduce
the dimensionality too so I want to find
three components and what this is going

101
00:20:41,540 --> 00:20:48,380
to do is it's going to find three linear
combinations of the 50 dimensions which

102
00:20:48,380 --> 00:20:53,180
capture as much as the variation as
possible Badar is different to each
other as possible

103
00:20:53,690 --> 00:21:02,270
okay so we would call this a lower rank
approximation of our matrix all right

104
00:21:02,270 --> 00:21:09,005
so then we can grab the components so
that's going to be their three
dimensions and so once we've done that
we've now got three by three thousand

105
00:21:09,105 --> 00:21:21,615
and so we can now take a look at the
first of them and we'll do the same
thing of using zip to look at each one
along with its movie and so here's the

106
00:21:21,715 --> 00:21:42,420
thing right we we don't know ahead of
time what this PCA thing is it's just
it's just a bunch of latent factors you
know it's it's kind of the the main axis
in this space of latent factors and so
what we can do is we can look at it and
see if we can figure out what it's about

107
00:21:42,520 --> 00:22:11,315
right so given that police academy for
is high up here along with water world
where else Fargo Pulp Fiction and God
further a high up here I'm gonna guess
that a high value is not going to
represent like critically acclaimed
movies or serious watching so I kind of
like all this yeah okay I call this easy
what she
is serious all right but like this is

108
00:22:11,415 --> 00:22:25,060
kind of how you have to interpret your
embeddings is like take a look at what
they seem to be showing and decide what
you think it means so this is the kind
of the the principal axis in this set of
embedding so we can look at the next one

109
00:22:27,910 --> 00:22:35,465
so do the same thing and look at the the
first index one embedding this one's a
little bit harder to kind of figure out

110
00:22:35,565 --> 00:22:48,320
what's going on but with things like
Mulholland Drive and Purple Rose of
Cairo these look more kind of dialog II
kind of ones or else things like Lord of
the Rings in the Latin and Star Wars
these book more like kind of modern CGI

111
00:22:48,420 --> 00:23:13,574
II kind of ones so you could kind of
imagine that on that pair of dimensions
it probably represents a lot of you know
differences between how people read
movies you know some people like you
know purple rise of Cairo
type movies you know Woody Allen kind of
classic and some people like these you
know big Hollywood spectacles some
people presumably like police academy

112
00:23:13,674 --> 00:23:21,930
for more than they like Fargo so yeah so

113
00:23:22,810 --> 00:23:28,892
I'm like you can kind of get the idea of
what's happened it's it's done a you
know through a model which was you know

114
00:23:28,992 --> 00:23:43,990
for a model which was literally multiply
two things together and Adam hop it's
learnt quite a lot you know which is

115
00:23:48,600 --> 00:24:04,640
with with that and then we could we
could plot them if we wanted to I just
grabbed a small subset to plot on those
first two asses all right so that's that

116
00:24:04,740 --> 00:24:23,700
so I wanted to next kind of dig in a
layer deeper into what actually happens
when we say fit alright so when we said
learn fit what's it doing

117
00:24:24,850 --> 00:24:30,790
for something like the store model is it
a way to interpret the embeddings for

118
00:24:30,790 --> 00:24:43,220
something like this the rustman one yes
yeah we'll see that in a moment well
let's jump straight there what the hell

119
00:24:57,840 --> 00:25:18,790
date model we this is from the paper
gore and burke on it so it's a great
paper by the way well worth you know
like pretty accessible I think any of
you would at this point be able to at
least get the gist of it if you know and
much of the detail as well particularly

120
00:25:18,790 --> 00:25:23,260
as you've also done the machine learning
course and they actually make this point

121
00:25:23,260 --> 00:25:39,550
in the paper this is in the paper that
the equivalent of what they call entity
embedding layers so an embedding of a
categorical variable is identical to a
one hot encoding followed by a matrix
multiply that's why they're basically

122
00:25:39,550 --> 00:25:45,122
saying if you've got three embeddings
that's the same as doing three one hot
encodings putting each through one

123
00:25:45,222 --> 00:25:56,040
through a matrix multiply and then put
that through a a dense layer
well what pi torch would call a linear
oh yeah right

124
00:25:56,040 --> 00:26:21,610
one of the nice things here is because
this is kind of like well they thought
it was the first paper is actually the
second I think paper to show the idea of
using categorical embeddings for this
kind of data set they really go to clean
too quite a lot of detail to you know
right back to the the detailed stuff
that we learnt about so it's kind of a
second
you know a second cat of thinking about
what embeddings are doing so one of the

125
00:26:21,610 --> 00:26:34,930
interesting things that they did was
they said okay after we've trained a
neural net with these embeddings what
else could we do with it so

126
00:26:34,930 --> 00:26:45,070
they got a winning result with a neural
network where the entity meetings but
then they said hey you know what
we could take those empty embeddings and
replace each categorical variable with

127
00:26:48,070 --> 00:27:07,920
the learnt entity embeddings and then
feed that into a GBM right so in other
words like rather than passing into the
GBM a one modern coded version or an
ordinal version let's actually replace
the categorical variable with its
embedding for the appropriate level for

128
00:27:08,020 --> 00:27:14,775
that row right so it's actually a way of
create you know feature engineering and

129
00:27:14,875 --> 00:27:25,960
so the main average percent error
without that for gbms I'm using just 100
codings was 0.15 but with that it was

130
00:27:25,960 --> 00:27:36,650
0.11 that random forests without that
was point one six with that 0.108 nearly
as good as the neural net right so this

131
00:27:36,750 --> 00:27:56,195
is kind of an interesting technique
because what it means is in your
organization you can train a neural net
that has an embedding of stores and an
embedding of product types and an
embedding of I don't know whatever kind
of high cardinality or even medium
cardinality categorical variables you

132
00:27:56,295 --> 00:28:03,580
have and then everybody else in the
organization can now like chuck those
into their you know JVM or random forest
or whatever and I'm use them and what

133
00:28:07,210 --> 00:28:14,670
this is saying is they won't get in fact
you can even use K nearest neighbors
with this technique and get nearly as

134
00:28:14,770 --> 00:28:30,730
good a result right so this is a good
way of kind of giving the power of
neural nets to everybody in your
organization without having them do the
faster idea of learning course first you
know they can just use whatever SK learn
or R or whatever that they're used to

135
00:28:30,730 --> 00:28:59,355
and like those those embeddings could
literally be in a database table because
if you think about an embedding is just
an index lookup right which is the same
as an inner join in SQL right so if
you've got a table on each product along
with its embedding vector then you can
literally do
in a joint and now you have every row in
your table along with its product
embedding vector so that's a really this
is this is a really useful idea and

136
00:28:59,455 --> 00:29:05,115
gbm's and random forests learn a lot
quicker than neural nets do all right so

137
00:29:05,215 --> 00:29:09,915
that's like even if you do know how to
train your on its this is still
potentially quite handy so here's what

138
00:29:10,015 --> 00:29:26,790
happened when they took the various
different states of Germany and plotted
the first two principal components of
their embedding vectors and they
basically here is where they were in
that 2d space and wacken lee enough i've

139
00:29:26,890 --> 00:29:41,600
circled in red three cities and i've
circled here the three cities in Germany
and here I've circled in purple so blue
here at the blue here's the green here's
the green so it's actually drawn a map

140
00:29:41,600 --> 00:29:54,672
of Germany even though it never was told
anything about how far these states are
away from each other or the very concept
of geography didn't exist so that's
pretty crazy so that was from there

141
00:29:54,772 --> 00:30:10,190
paper so I went ahead and looked well
here's another thing I think this is
also from their paper they took every
pair of places and they looked at how

142
00:30:10,190 --> 00:30:15,980
far away they are on a map versus how
far away are they in embedding space and

143
00:30:15,980 --> 00:30:36,050
they've got this beautiful correlation
alright so again it kind of apparently
you know it's doors that are near by
each other physically have similar
characteristics in terms of when people
buy more or less stuff from them so I

144
00:30:36,050 --> 00:30:48,170
looked at the same thing four days of
the week right so here's an embedding of
the days of the week from our model and
I just kind of joined up Monday Tuesday
Wednesday Tuesday Thursday Friday
Saturday Sunday I did the same thing for

145
00:30:48,170 --> 00:30:52,850
the months of the year all right again
you can see you know here's here's
winter here's summer so yeah I think

146
00:30:58,640 --> 00:31:08,345
like visualize
embeddings can be interesting like it's
good to like first of all check you can
see things you would expect to see you

147
00:31:08,445 --> 00:31:18,869
know and then you could like try and see
like maybe things you didn't expect to
see so you could try all kinds of
clusterings or or whatever and this is

148
00:31:18,969 --> 00:31:30,605
not something which has been widely
studied at all right so I'm not going to
tell you what the limitations are of
this technique or whatever oh yes so

149
00:31:30,705 --> 00:31:44,830
I've heard of other ways to generate
embeddings like skip grams uh-huh
wondering if you could say is there one
better than the other using your own
Network sir skip grams so screwed grams

150
00:31:44,830 --> 00:31:49,679
is quite specific to NLP right so like

151
00:31:49,860 --> 00:32:16,720
I'm not sure if we'll cover it in this
course but basically the the approach to
original kind of word to vac approach to
generating embeddings was to say you
know what we actually don't have we
don't actually have our labelled data
set you know they said all we have is
like google books and so they have an
unsupervised learning problem

152
00:32:16,720 --> 00:32:22,210
unlabeled problem and so the best way in
my opinion to turn an unlabeled problem
into a labelled problem is to kind of

153
00:32:22,310 --> 00:32:34,645
invent some labels and so what they did
in the word to vet case was they said
okay here's a sentence with 11 words in

154
00:32:34,945 --> 00:32:53,500
it right and then they said okay let's
delete the middle word and replace it
for the random word and so you know
originally it said cat and they say no
let's replace that with justice all

155
00:32:53,500 --> 00:33:00,334
right so before it said the cute little
cat sat on the fuzzy mat and now it says
the cute little justice sat on the fuzzy
man

156
00:33:00,434 --> 00:33:14,365
right and what they do is they do that
so they have one sentence where they
keep exactly as is
and then they make a copy of it and they

157
00:33:14,465 --> 00:33:25,930
do the replacement and so then they have
a label where they say it's a one if it
was unchanged it was the original and
zero otherwise okay and so basically

158
00:33:26,030 --> 00:33:40,800
then you now have something you can
build a machine learning model on and so
they went and build a machine learning
model on this so the model was like try
and find the effect sentences not

159
00:33:40,800 --> 00:33:45,113
because they were interested in a fake
sentence binder but because as a result
they now have embeddings that just like

160
00:33:45,213 --> 00:33:50,360
we discussed you can now use for other
purposes and that became word to vet now

161
00:33:50,720 --> 00:34:01,860
it turns out that if you do this as just
a kind of a effectively like a single
matrix multiply rather than make it a
deep neural net you can train this super

162
00:34:01,860 --> 00:34:15,989
quickly and so that's basically what
they did with they'd met there though
they kind of decided we're going to make
a pretty crappy model like a shallow
learning model rather than a deep model
you know with the downside it's a less

163
00:34:15,989 --> 00:34:27,265
powerful model but a number of upsides
the first thing we can train it on a
really large data set and then also
really importantly we're going to end up
with embeddings which have really very

164
00:34:27,365 --> 00:34:33,715
linear characteristics so we can like
add them together and subtract them and

165
00:34:33,815 --> 00:34:52,710
stuff like that okay so that so there's
a lot of stuff we can learn about there
from like for other types of embedding
like categorical embeddings and
specifically if we want categorical
embeddings which we can kind of draw
nicely and expect them to us to be able
to add and subtract them and behave

166
00:34:52,710 --> 00:34:57,810
linearly you know probably if we want to
use them in k-nearest neighbors and

167
00:34:57,810 --> 00:35:03,990
stuff we should probably use shallow
learning if we want something that's

168
00:35:03,990 --> 00:35:09,300
going to be more predictive we probably
want to use a neural net and so actually

169
00:35:09,300 --> 00:35:19,495
an NLP I'm really pushing the idea that
we need to move past word to backhand
glove these linear based methods because

170
00:35:19,595 --> 00:35:25,260
it turns out that those embeddings are
way less predictive than embeddings
learnt from

171
00:35:25,260 --> 00:35:30,520
models and so the language model that we
learned about which ended up getting a
state-of-the-art on sentiment analysis

172
00:35:30,750 --> 00:35:35,790
didn't used a lot more work to vet that
instead we pre trained a deep recurrent

173
00:35:35,790 --> 00:35:44,620
neural network and we ended up with not
just a pre trained word vectors but a
for pre-trained model so it looks like

174
00:35:44,720 --> 00:35:51,010
Duke creates embeddings for entities we
need like a dummy task not necessarily a

175
00:35:51,110 --> 00:35:59,810
dummy task like in this case we had a
real task right so we created the
embeddings for Rossmann by trying to
predict store sales you only need this

176
00:35:59,910 --> 00:36:06,335
isn't just in this isn't just for
learning embeddings for learning any

177
00:36:06,435 --> 00:36:16,200
kind of feature space you either need
label data or you need to invent some
kind of fake task

178
00:36:16,200 --> 00:36:26,550
so does that task matter like if I
choose a task and train and lettings if
I choose another task and train and
lettings like which one is it's a great

179
00:36:26,550 --> 00:36:31,050
question and it's not something that's
been studied nearly enough right I'm not

180
00:36:31,050 --> 00:36:45,030
sure that many people even quite
understand that when they say
unsupervised learning now about nowadays
they almost nearly always mean fake
tasks labeled learning and so the idea

181
00:36:45,030 --> 00:36:49,650
of like what makes a good fake task I
don't know that I've seen a paper on

182
00:36:49,650 --> 00:37:05,665
that right that intuitively you know we
need something where the kinds of
relationships it's going to learn likely
to be the kinds of relationships that
you probably care about right so for

183
00:37:05,765 --> 00:37:19,930
example in in computer vision one kind
of fake task people use is to say like
let's take some images and use some kind
of like unreal and unreasonable data

184
00:37:20,030 --> 00:37:37,200
augmentation like like recolor them too
much or whatever and then we'll ask the
neural net to like predict which one was
the Augmented which one was not you
admitted yeah so it's I think it's a

185
00:37:37,200 --> 00:37:42,565
fascinating area
one which you know would be really
interesting for people to you know maybe

186
00:37:42,665 --> 00:37:47,310
some of the students here they're
looking to further it's like take some
interesting semi-supervised tour

187
00:37:47,310 --> 00:37:59,640
unsupervised datasets and try and come
up with some like more clever fake tasks
and see like does it matter you know how
much does it matter in general like if

188
00:37:59,640 --> 00:38:09,869
you can't come up with a fake task that
you think seems great I would say use it
use the best you can it's an often
surprising how how little you need like

189
00:38:09,869 --> 00:38:15,119
the ultimately crappy fake task is
called the auto encoder and the auto

190
00:38:15,119 --> 00:38:23,965
encoder is the thing which which one the
claims prediction competition that just
finished on cattle they had lots of

191
00:38:24,065 --> 00:38:28,675
examples of insurance policies where we
knew this was how much was claimed and

192
00:38:28,775 --> 00:38:37,090
then lots of examples of insurance
policies where I guess there must have
been still still open we didn't yet know
how much they claimed right and so what

193
00:38:37,190 --> 00:38:44,880
they did was they said okay so for all
of the ones so let's basically start off
by grabbing every policy right and we'll

194
00:38:44,980 --> 00:38:52,733
take a single policy and we'll put it
through a neural net right and we'll try
and have it reconstruct itself but in

195
00:38:52,833 --> 00:39:01,585
these intermediate layers and at least
one of those intermediate layers will
make sure there's less activations and

196
00:39:01,685 --> 00:39:13,590
there were inputs so let's say if there
was a hundred variables on the insurance
policy you know we'll have something in
the middle that only has like twenty
activations all right and so when you

197
00:39:13,590 --> 00:39:23,970
basically are saying hey reconstruct
your own input like it's not a different
kind of model doesn't require any
special code it's literally just passing
you can use any standard pipe torch or

198
00:39:25,619 --> 00:39:31,710
fast AI learner you just say my output
equals my input right and that's that's

199
00:39:31,710 --> 00:39:39,490
like the the most uncreated you know
invented task you can create and that's
called an autoencoder
and it works surprisingly well in fact

200
00:39:39,590 --> 00:39:44,392
to the point that it literally just won
a cackle competition they took the

201
00:39:44,492 --> 00:39:53,155
features that it learnt and chucked it
into another neural net and
yeah and one you know maybe if we have

202
00:39:53,255 --> 00:40:06,024
enough students taking an interest in
this then you know we'll be able to
cover covered unsupervised learning in
more detail in in part two specially

203
00:40:09,424 --> 00:40:24,760
given this cattle have a win I think
this may be related to the previous
question when training language models
is the language model example trained on
the archive data is that useful at all
in the movie great question you know I

204
00:40:24,860 --> 00:40:36,839
was just talking to Sebastian about this
question read about this this week and
we thought would try and do some
research on this in January it's it's
again it's not well done

205
00:40:36,839 --> 00:40:47,877
we know that in computer vision it's
shockingly effective to train on cats
and dogs and use that fruit train
network to do lung cancer diagnosis and
CT scans in the NLP world nobody much

206
00:40:48,577 --> 00:40:53,220
seems to have tried this the NLP

207
00:40:53,320 --> 00:41:01,680
research as I've spoken to other than
Sebastian about this assume that it
wouldn't work and they generally haven't
bother trying I think it would work

208
00:41:04,390 --> 00:41:17,810
ruspin I just mentioned during the week
I was interested to see like how good
this solution actually actually was

209
00:41:17,810 --> 00:41:23,410
because I noticed that on the public
leader board it didn't look like it was
going to be that great and I also

210
00:41:23,510 --> 00:41:30,690
thought it'd be good to see like what
does it actually take to use a test set
properly with this kind of structured

211
00:41:30,690 --> 00:41:46,650
data so if you have a look at ruspin now
I've pushed some changes that actually
run the test set through as well and so
you can get a sense of how to do this
so you'll see basically every line
appears twice one for tests and one-foot
one for train when we get there yeah

212
00:41:46,650 --> 00:42:00,380
test train test trains history obviously
you could do this on a lot fewer lines
of code by putting all of the steps into
a method and then pass either the train
data set well the test data set up
dataframe to it in this case i wanted to

213
00:42:00,380 --> 00:42:08,010
kind of put for teaching purposes you'd
be able to see
step and to experiment to see what each
step looks like but you could certainly

214
00:42:08,010 --> 00:42:24,480
simplify this code so yeah so we do this
for every data frame and then some of
these you can see I kind of lived
through the data frame in joined and the
joint test right training just this

215
00:42:24,480 --> 00:42:34,140
whole thing about the durations I
basically put two lines here one that
said data frame equals train columns one
that says data frame equals test columns

216
00:42:34,140 --> 00:42:45,900
and so my you know basically ideas you'd
run this line first and then you would
skip the next one and you'd run
everything beneath it and then you'd go
back and run this line and then run
everything believe it

217
00:42:45,900 --> 00:42:49,950
so some people on the forum were asking
how come this code wasn't working this
week which is a good reminder that the

218
00:42:52,260 --> 00:43:00,180
code is not designed to be code that you
always run top to bottom without
thinking right you're meant to like
think like what is this code here should
I be running it right now okay and so

219
00:43:03,960 --> 00:43:15,680
like the early lessons I tried to make
it so you can run it top to bottom but
increasingly as we go along I kind of
make it more and more that like you
actually have to think about what's
going on so Jimmy you're talking about

220
00:43:15,780 --> 00:43:21,275
shadow learning and deep learning could
you define that a bit better by sure I'm
learning I think I just mean anything

221
00:43:21,375 --> 00:43:32,975
that doesn't have a hidden layer so
something that's like a dot product
matrix multiplier basically okay so so

222
00:43:35,975 --> 00:43:50,040
we end up with a training and a test
version and then everything else is
basically the same one thing to note on

223
00:43:50,040 --> 00:43:58,000
a lot of these details of this we cover
in the machine learning course by the
way because it's not really deep
learning specific so check that out if
you're just in the details
I should mention you know we use apply

224
00:43:58,100 --> 00:44:10,040
cats rather than train cats to make sure
the test set and the training set have
the same categorical codes and that they

225
00:44:12,815 --> 00:44:27,440
we keep track of the mapper this is the
thing which basically says
what's the mean and standard deviation
of each continuous column and then apply
that same method test set and so when we

226
00:44:27,440 --> 00:44:46,665
do all that that's basically it then the
rest is easy we just have to pass you in
the test data frame in the usual way
when we create our model data object and
there's no changes through all here we
trained it in the same way and then once

227
00:44:46,765 --> 00:44:57,710
we finish training it we can then call
predict as per usual passing in true to
say this is the test set rather than the

228
00:44:57,710 --> 00:45:03,650
validation set and pass that off to
cattle and so it was really interesting

229
00:45:03,650 --> 00:45:38,890
because this was my submission it got a
public score of 103 which would put us
in about 300 and some things place which
looks awful right and our private score
of 107 need a board private here's about
fifth

230
00:45:38,890 --> 00:45:56,420
right so like if you're competing in a
cable competition and you don't haven't
thoughtfully created a validation set of
your own and you're relying on publicly
the board feedback this could totally
happen to you but the other way around
you'll be like oh I'm in the top ten I'm
doing great

231
00:45:56,420 --> 00:46:01,970
and then oh for example at the moment
the ice Berg's competition recognizing
icebergs a very large percentage of the

232
00:46:04,940 --> 00:46:10,640
public leaderboard set is synthetically
generated data augmentation data like

233
00:46:10,640 --> 00:46:21,859
totally meaningless and so your
validation set is going to be much more
helpful and the public leaderboard
feedback right so yeah be very careful

234
00:46:21,859 --> 00:46:35,820
so our final score here is kind of
within statistical noise of the actual
third-place getters so I'm pretty
confident that we've we've captured
their approach and so that's that's

235
00:46:40,700 --> 00:46:53,940
there's a nice kernel about the rustman
I quite a few nice kernels actually but
you can go back and see like
particularly if you're doing the
groceries competition go and have a look
at the Rossmann kernels because actually
quite a few of them a higher quality
than the ones for the Ecuadorian

236
00:46:54,040 --> 00:47:02,480
groceries competition one of them for
example showed how on four particular
stores like straw eighty five the sales

237
00:47:02,480 --> 00:47:11,870
for non Sundays and the sale for
Sunday's looked very different where
else there are some other stores where
the sales on Sunday don't look any

238
00:47:11,870 --> 00:47:16,455
different and it can kind of like get a
sense of why you need these kind of
interactions the one I particularly

239
00:47:16,555 --> 00:47:29,990
wanted to point out is the one I think I
briefly mentioned that the third-place
winners whose approach we used they
didn't notice is this one and here's a
really cool visualization here you can

240
00:47:29,990 --> 00:47:48,480
see that the store this store is closed
right and just after oh my god we run a
we run out of eggs
and just before oh my god go and get the
milk before the store closes alright
and here again closed bang right so this

241
00:47:48,580 --> 00:47:57,230
third-place winner actually deleted all
of the closed store rows before they
started doing any analysis right so

242
00:47:57,230 --> 00:48:05,500
remember how we talked about like don't
touch your data unless you first of all
analyze to see whether that thing you're
doing is actually okay no assumptions

243
00:48:05,600 --> 00:48:15,260
right so in this case I am sure like I
haven't tried it but I'm sure they would
have one otherwise right because like

244
00:48:15,260 --> 00:48:22,335
well though there weren't actually any
store closures to my knowledge in the
test set period the problem is that

245
00:48:22,435 --> 00:48:42,869
their model was trying to fit to these
like really extreme things and so and
because it wasn't able to do it very
well it was gonna end up getting a
little bit confused it's not gonna break
the model but it's definitely gonna harm
it because it's kind of trying to do
computations to fit something which it
literally doesn't have the data for your
neck can you pass that back there

246
00:48:42,869 --> 00:48:55,149
all right so that Russman model again
like it's nice to kind of look inside to
see what's actually going on right and

247
00:49:02,559 --> 00:49:08,689
sure you kind of know how to find your
way around the code so you can answer
these questions for yourself so it's

248
00:49:08,789 --> 00:49:27,999
inside columnar model data now um we
started out by kind of saying hey if you
want to look at the code for something
you couldn't like a question mark
question mark like this and oh okay I
need to I haven't got this reading but
you can use question mark question mark
to get the source code for something
right but obviously like that's not

249
00:49:31,259 --> 00:49:36,649
really a great way because often you
look at that source code and it turns
out you need to look at something else

250
00:49:36,749 --> 00:49:50,674
right and so for those of you that
haven't done much coding you might not
be aware that almost certainly the
editor you're using probably has the
ability to both open up stuff directly
off SSH and to navigate through it so
you can jump straight from place to

251
00:49:50,774 --> 00:50:08,285
place right so want to show you what I
mean so if I were to find columnar model
data and I have to be using vim here I
can basically say tag columnar model
data and it will jump straight to the
definition of that plus right and so

252
00:50:08,385 --> 00:50:19,569
then I notice here that like oh it's
actually building up a data loader
that's interesting if I get control
right square bracket it'll jump to the
definition of the thing that was under

253
00:50:19,569 --> 00:50:26,170
my cursor and after I finished reading
it for a while
I can hit ctrl T to jump back up to
where I came from

254
00:50:26,170 --> 00:50:41,140
right and you kind of get the idea right
or if I want to find it for usage of
this in this file of columnar model data
I can hit star to jump to the next place
it's new used you know and so forth

255
00:50:41,140 --> 00:50:59,420
alright so in this case get learner was
the thing which actually got the model
and we want to find out what kind of
model it is and apparently it uses a
I'm not using collaborative filtering
are we were using columnar model data

256
00:50:59,420 --> 00:51:23,129
sorry columnar model data okay learner
which users and so here you can see
mixed input model is the PI torch model
and then it wraps it in the structured
learner which is the the first day I
learn a type which wraps the data and

257
00:51:23,229 --> 00:51:30,200
the model together so if we want to see
the definition of this actual PI torch
model I can go to control right square
bracket to see it right and so here is

258
00:51:34,519 --> 00:51:42,890
the model right and nearly all of this
we can now understand right so we got

259
00:51:42,890 --> 00:52:02,919
past we got past a list of embedding
sizes in the mixed model that we saw

260
00:52:03,119 --> 00:52:15,710
does it always expect categorical and
continuous together yes it does

261
00:52:15,410 --> 00:52:30,079
and the the model data behind the scenes
if there are no none of the other type
it creates a column of ones or zeros or
something okay so if it is null it can

262
00:52:30,079 --> 00:52:49,720
still work yeah yeah yeah it's kind of
ugly and hacky and will you know
hopefully improve it but yeah you can
pass in an empty list of categorical or
continuous variables to the model data
and it will basically yeah it'll
basically pass an unused column of zeros

263
00:52:49,720 --> 00:52:59,234
to avoid things breaking and I'm I'm
leaving fixing some of these slightly
hacky edge cases because height or 0.4
as well as you're getting rid of

264
00:52:59,334 --> 00:53:20,509
variables they're going to also add rank
0 tensors which is to say if you grab a
single
thing out of like a rent 110 sir rather
than getting back at a number which is
like qualitatively different you're
actually going to get back like a tensor
that just happens to have no rank now it

265
00:53:20,509 --> 00:53:29,019
turns out that a lot of this kind of
codes gonna be actually easier to write
then so and for now it's it's a little
bit more happier than it needs to be

266
00:53:30,400 --> 00:53:42,549
Jeremy you talk about this a little bit
before where maybe it's a good time at
some points talk about how can we write
something that is slightly different for
worries in the library yeah I think
we'll cover that a little bit next week

267
00:53:43,229 --> 00:53:53,779
that I'm mainly going to do that in part
to like Pat who's going to cover quite a

268
00:53:53,779 --> 00:54:00,710
lot of stuff one of the main things were
cover in part two is what it called
generative models so things where the
output is a whole sentence or a whole

269
00:54:00,710 --> 00:54:12,539
image but you know I also dig into like
powder really either customize the first
day I library or use it on more custom

270
00:54:12,639 --> 00:54:20,609
models but if we have time we'll touch
on it a little bit next week okay so the

271
00:54:20,709 --> 00:54:26,984
the learner we were passing in a list of
embedding sizes and as you can see that

272
00:54:27,084 --> 00:54:32,569
embedding sizes list was literally just
the number of rows and the number of
columns in each embedding right and the

273
00:54:32,569 --> 00:54:46,754
number of code rose was just coming from
literally how many stores are there in
the store category for example and the
number of columns was just a quarter
that divided by two and a maximum of 50

274
00:54:46,854 --> 00:55:05,869
so that thing that list of tuples was
coming in and so you can see here how we
use it right we go through each of those
tuples grab the number of categories and
the size of the embedding and construct
an embedding all right and so that's a
that's a list right one minor thing

275
00:55:05,869 --> 00:55:45,960
height or specific thing we haven't
talked about before is for it to be able
to like register remember how we kind of
said like it registers your parameters
it registers your your layers like
someone we like listed the model it
actually printed out the Novation
varying an age bias it can't do that if
they're hidden inside a list right they
have to be like a there have to be a an
actual n n dot module subclass so
there's a special thing called an N n
dot module list which takes a list and
it basically says I want you to register
everything in here has been part of this
model okay so it's just a minor tweak so

276
00:55:46,060 --> 00:55:56,960
yeah so our mixed input model has a list
of embeddings and then I do the same
thing for a list of linear layers right

277
00:55:56,960 --> 00:56:16,455
so when I said here 1000 comma 500 this
was saying how many activations I wanted
featured my lineal is okay and so here I
just go through that list and create a
linear layer that goes from this size to

278
00:56:16,555 --> 00:56:26,415
the next size okay so you can see like
how easy it is to kind of construct your
own not just your own model but a kind
of a model which you can pass parameters
to have a constructed on the fly

279
00:56:26,515 --> 00:56:40,515
dynamically and that's normal talk about
next week this is initialization we've
mentioned climbing her initialization
before and we mentioned it last week and

280
00:56:40,615 --> 00:56:50,895
then drop out same thing right we have
here a list of how much drop out to
apply to each layer right so again here
it's just like go through each thing in
that list and create a drop out layer

281
00:56:50,995 --> 00:57:04,275
for it okay so this constructor we
understand everything in it except for
batch norm which we don't have to worry
about for now so that's the constructor

282
00:57:04,375 --> 00:57:22,250
and so then the forward also you know
all stuff we're aware of go through each
of those embedding layers that we just
saw and remember we've just treated like
as a function so call it with the ithe
categorical variable and then
concatenate them all together put that

283
00:57:22,250 --> 00:57:33,940
through drop out and then go through
each one of our linear layers and call
it apply relia to it
apply dropout

284
00:57:33,940 --> 00:57:49,880
and then finally apply the final linear
layer and the final linear layer has
this as its size which is here right
size one there's a single unit sales

285
00:57:49,880 --> 00:58:11,120
okay so we're kind of getting to the
point where oh and then of course at the
end if this I mentioned would come back
to this if you passed in a Y underscore
range parameter then we're going to do
the thing we just learned about last
week which is to use a sigmoid right and
this is a cool little trick to make
you're not just to make your
collaborative filtering better but in

286
00:58:11,120 --> 00:58:21,760
this case my basic idea was you know
sales are going to be greater than zero
and probably less than the largest sale

287
00:58:21,860 --> 00:58:45,555
they've ever had so I just pass in that
as Y range and so we do a sigmoid and
multiply with the sigmoid by the range
that I passed it all right and so
hopefully we can find that here yeah
here it is right so I actually said hey
maybe the range is between zero and you
know the highest x one point two you

288
00:58:45,655 --> 00:58:56,630
know cuz maybe maybe the next two weeks
we have one bigger but this is kind of
like again try to make it a little bit
easier for it to give us the kind of
results that it thinks is right so like

289
00:58:56,630 --> 00:59:06,715
increasingly you know I'd love your wall
to kind of try to not treat these
learners and models as black boxes but

290
00:59:06,815 --> 00:59:27,580
to feel like you now have the
information you need to look inside them
and remember you could then copy and
paste this plus paste it into a cell in
duple notebook and start fiddling with
it to create your own versions okay

291
00:59:30,640 --> 00:59:51,855
I think what I might do is we might take
a bit of a early break because we've got
a lot to cover and I want to do it all
in one big go so let's take a let's take
a break until 7:45 and then we're going
to come back and talk about recurrent
neural networks all right

292
00:59:56,670 --> 01:00:10,025
so we're going to talk about Aaron ends
before we do we've got to kind of dig a
little bit deeper into SGD because I
just want to make sure everybody's
totally comfortable with with SGD and so
what we're going to look at is we're

293
01:00:10,125 --> 01:00:29,280
going to look at lesson six SGD notebook
and we're going to look at a really
simple example of using SGD to learn y
equals ax plus B and so what we're going

294
01:00:29,380 --> 01:00:46,750
to do here is we're going to create like
the simplest possible model y equals ax
plus B okay and then we're going to
generate some random data that looks
like so so here's our X and here's our Y

295
01:00:46,750 --> 01:00:58,264
we want to predict Y from X and we
passed in 3 & 8 as our a and B so we're
going to kind of try and recover that

296
01:00:58,364 --> 01:01:16,445
right and so the idea is that if we can
solve something like this which has two
parameters we can use the same technique
to solve we can use the same technique
to solve something with a hundred
million parameters right without any

297
01:01:16,545 --> 01:01:30,130
changes at all so in order to find a and
a B that fits this we need a loss
function and this is a regression
problem because we have a continuous
output so for continuous output

298
01:01:30,130 --> 01:01:35,044
regression we tend to use mean squared
error all right and obviously all of
this stuff there's there's

299
01:01:35,144 --> 01:01:41,945
implementations in non pious
implementations in flight or we're just
doing stuff by hand so you can see all
the steps right so there's MSE okay

300
01:01:42,045 --> 01:01:56,585
y hat is
we often call our predictions Y hat
mitis y squared mean there's I meant
whatever okay so for example if we had
ten and five where a and B then there's
our mean square R squared error three

301
01:01:56,685 --> 01:02:13,510
point two five okay so if we've got an A
and a B and we've got an x and a y then
our mean square error loss is just the
mean squared error of our linear that's
our predictions and our way okay so
there's a last four ten five X Y all
right so that's a loss function right

302
01:02:13,510 --> 01:02:29,560
and so when we talk about combining
linear layers and loss functions and
optionally nonlinear layers this is all
we're doing right is we're putting a
function inside a function yeah that's

303
01:02:29,560 --> 01:02:39,635
that's all like I know people draw these
clever looking dots and lines all over
the screen when they're saying this is
what a neural network is but it's just
it's just a function of a function of a

304
01:02:39,735 --> 01:02:51,970
function okay so here we've got a
prediction function being a linear layer
followed by a loss function being MSE
and now we can say like oh well let's
just define this as MSA Lost's and we'll
use that in the future okay so there's
our loss function which incorporates our

305
01:02:53,980 --> 01:03:07,140
prediction function okay so let's
generate 10,000 items or thick data and
let's show them in two variables so we
can use them with PI torch because
Jeremy doesn't like taking derivatives
so we're going to use PI torch for that

306
01:03:07,140 --> 01:03:19,060
and let's create random wait for a and B
so a single random number and we want
the gradients of these to be calculated
as we start computing with them because
these are the actual things we need to

307
01:03:20,560 --> 01:03:31,090
update in our SGD okay so here's our a
and B 0.029 0.111 all right so let's

308
01:03:31,090 --> 01:03:35,795
pick a learning rate okay and let let's
do 10,000 epochs of SGD in fact this

309
01:03:35,895 --> 01:03:43,995
isn't really SGD it's not stochastic
gradient it said this is actually full

310
01:03:44,095 --> 01:03:54,890
gradient descent we're going to each
each loop is going to look at all of the
data okay stochastic gradient descent
would be looking at a subset each time

311
01:03:56,330 --> 01:04:21,050
so to do gradient descent we basically
calculate loss right so remember we've
started out with a random a and B okay
and so this is going to compute some
amount of loss and then it's nice from
time to time so one way of saying from
time to time is if the epoch number mod
a thousand is zero right so every
thousand epochs just print out the loss
so you have it do it okay

312
01:04:21,050 --> 01:04:29,085
so now that we've computed the loss we
can compute our gradients right and so
you just remember this thing here is
both a number a single number that is

313
01:04:29,185 --> 01:04:38,210
our lost something we can print but it's
also a variable because we passed
variables into it and therefore it also

314
01:04:38,210 --> 01:04:48,830
has a method type backward which means
calculate the gradients of everything
that we asked it to everything where we
said requires radical is true okay so at

315
01:04:48,830 --> 01:04:59,955
this point we now have a dot grad
property inside a and inside P and here
they are here is that grant grad

316
01:05:00,055 --> 01:05:14,530
property okay so now that we've
calculated the gradients for a and B we
can update them by saying a is equal to
whatever it used to be - the learning
rate times the gradient okay dot data

317
01:05:14,530 --> 01:05:24,015
because a is a variable and a variable
contains a tensor and it's dot data
property and we again this is going to

318
01:05:24,115 --> 01:05:28,995
disappear in height which point four but
for now it's actually the ten so that we

319
01:05:29,095 --> 01:05:37,030
need to update okay so update the tensor
inside here with whatever it used to be
- the learning rate times the gradient

320
01:05:37,030 --> 01:05:43,220
okay and that's basically it
all right that's basically all gradient
descent is okay so it's it's as simple

321
01:05:45,980 --> 01:06:00,830
as we claimed there's one extra step in
pi torch which is that you might have
like multiple different loss functions
or like lots of lots of output layers
all contributing to the gradient and you
like to have to add them all together

322
01:06:00,830 --> 01:06:10,420
and so if you've got multiple loss
functions you could be calling loss stop
backward on each of them and what it
does is an ad
sit to the gradients right and so you

323
01:06:10,520 --> 01:06:22,235
have to tell it when to set the
gradients back to zero okay so that's
where you just go okay set a to zero and
gradients in set B gradients to zero

324
01:06:22,335 --> 01:06:36,505
okay and so this is wrapped up inside
the you know op TMS JD class right so
when we say up Tim dot SGD and we just
say you know dot step it's just doing
these for us so when we say dot zero

325
01:06:36,605 --> 01:06:50,965
gradients is just doing this force and
this underscore here every pretty much
every function that applies to a tensor
in pi torch if you stick an underscore

326
01:06:51,065 --> 01:07:00,475
on the end it means do it in place okay
so this is actually going to not return
a bunch of zeros but it's going to
change this in place to be a bunch of

327
01:07:02,475 --> 01:07:07,580
zeros so that's basically it we can look
at the same thing without PI torch which
means we actually do have to do some

328
01:07:07,580 --> 01:07:20,965
calculus so if we generate some fake
data again we're just going to create 50
data points this time just to make this
fast and easy to look at and so let's

329
01:07:21,065 --> 01:07:26,050
create a function called update right
we're just going to use numpy no pi

330
01:07:26,150 --> 01:07:30,445
torch okay so our predictions is equal
to again linear and in this case we

331
01:07:30,545 --> 01:07:41,400
actually gonna calculate the derivatives
so the derivative of the square of the
loss is just two times and then the
derivative is the vector a is just that
you can confirm that yourself if you

332
01:07:41,400 --> 01:07:52,770
want to and so here our we're going to
update a minus equals learning rate
times the derivative of loss with
respect to a and for B it's learning
rate times derivative with respect to B

333
01:07:54,810 --> 01:08:07,590
okay and so what we can do let's just
run all this so just for fun
rather than looping through manually we
can use the map flop matplotlib func

334
01:08:07,590 --> 01:08:30,119
animation command to run the animate
function a bunch of times and the
animate function is going to run 30
epochs and at the end of each epoch it's
going to print out
on the plot where the line currently is
and that creates this at all movie okay
so you can actually see that the line
moving at a place right so if you want

335
01:08:33,238 --> 01:08:46,739
to play around with like understanding
how high torque gradients actually work
step-by-step here's like the world's
simplest at all example okay and you

336
01:08:46,738 --> 01:08:55,408
know it's kind of like it's kind of
weird to say like that's that's it like
when you're optimizing a hundred million
parameters in a neural net it's doing

337
01:08:55,408 --> 01:09:04,710
the same thing but it it actually is
alright you can actually look at the PI
torch code and see it's this is it right
there's no trick

338
01:09:04,710 --> 01:09:12,759
well we load a couple of minor tricks
last time which was like momentum and
atom right that if you could do it in
Excel you can do it invite them so okay

339
01:09:16,739 --> 01:09:23,134
now in less than six hour and in

340
01:09:27,730 --> 01:09:38,204
Nietzsche as you should
so Nietzsche says supposing that truth

341
01:09:38,304 --> 01:09:54,599
is a woman what then I love this
apparently all philosophers have failed
to understand women
so apparently at the point that
Nietzsche was alive there was no female
philosophers or at least those that were
around didn't understand women either so

342
01:09:54,599 --> 01:10:04,929
anyway so this is the philosopher
apparently we've chosen to study it
leech is actually much less worse than
people think he is but it's a different

343
01:10:05,029 --> 01:10:16,110
era I guess alright so we're going to
learn to write philosophy like Nietzsche
and so we're going to do it one
character at a time so this is like the

344
01:10:16,719 --> 01:10:24,115
language model that we did in Lesson
four where we did it a word at the time
but this time we're going to do a

345
01:10:24,215 --> 01:10:33,534
character at a time and so the main
thing I'm going to try and convince you
is an RNN is no different to anything
you've already learned okay and so to

346
01:10:33,634 --> 01:10:42,700
show you that
going to build it from plain PI torch
layers all of which are extremely
familiar already okay and eventually

347
01:10:42,800 --> 01:10:48,780
we're going to use something really
complex which is a for loop okay so
that's when we're going to make a really

348
01:10:48,780 --> 01:10:59,790
sophisticated so the basic idea of our n
ends is that you want to keep track of
the main thing is you want to keep track
of kind of state over long term

349
01:10:59,790 --> 01:11:07,570
dependencies so for example if you're
trying to model something like this kind
of template language right then at the

350
01:11:07,670 --> 01:11:21,580
end of your percent comment blue percent
you need a percent common end percent
right and so somehow your model needs to
keep track of the fact that it's like
inside a comment over all of these
different characters right and so this

351
01:11:21,680 --> 01:11:40,225
is this idea of state it's kind of
memory right and this is quite a
difficult thing to do with like just a
calm confident it turns out actually to
be possible but it's it's you know a
little bit tricky
where elsewhere as an iron in it turns
out to be pretty straightforward all
right so these are the basic ideas if

352
01:11:40,325 --> 01:11:45,400
you want the stateful representation
where you kind of keeping track of like

353
01:11:45,500 --> 01:11:55,120
where are we now have memory have long
term dependencies and potentially even
have variable length sequences these are
all difficult things to do with
confidence they're very straightforward

354
01:11:55,220 --> 01:12:15,780
with arid ends so for example SwiftKey a
year or so ago did a blog post about how
they had a new language model where they
basically this is from their blog post
we basically said like of course this is
what their neural net looks like somehow
they always look like this on the

355
01:12:15,780 --> 01:12:27,180
internet you know you've got a bunch of
words and it's basically going to take
your particular words in their
particular orders and try and figure out
what the next words going to be which is
to say they built a language model they

356
01:12:27,180 --> 01:12:32,905
actually have a pretty good language
model if you've used SwiftKey they seem
to do better predictions than anybody

357
01:12:33,005 --> 01:12:42,475
else still another cool example was
andre capaci a couple of years ago
showed that he could use character level
are a 10 to actually create an entire

358
01:12:42,575 --> 01:12:54,600
latex document so he didn't actually
tell it in any way what life looks like
he just passed the
some may tech text like this and said
generate more low text text and it

359
01:12:54,600 --> 01:13:03,340
literally started writing something
which means about as much to me as most
math papers do this okay so we're gonna

360
01:13:03,440 --> 01:13:23,969
start with something that's not an RN
and I'm going to introduce Jeremy's
patented neural network notation
involving boxes circles and triangles so
let me explain what's going on as a

361
01:13:23,969 --> 01:13:35,489
rectangle is an input an arrow is a
layer as a circle in fact every square
is a bunch of activate so every shape is

362
01:13:38,670 --> 01:13:56,340
a bunch of activations right the
rectangle is the input activations the
circle is a hidden activations and a
triangle is an output activations and
arrow is a layer operation right or

363
01:13:56,340 --> 01:14:09,264
possibly more than one all right so here
my rectangle is an input of number of
rows equal a batch size and number of
columns equal to the number of number of
inputs number of variables all right and

364
01:14:09,364 --> 01:14:17,040
so my first arrow my first operation is
going to represent a matrix product
followed by our Lu and that's going to

365
01:14:17,040 --> 01:14:32,665
generate a set of activation remember
activations like an activation is a
number that an activation is a number a
number that's being calculated by a
value or a matrix product or whatever

366
01:14:32,765 --> 01:14:45,120
it's a number right so this circle here
represents a matrix of activations all
of the numbers that come out when we
take the inputs we do a matrix product
followed by a value so we started with

367
01:14:45,120 --> 01:14:59,680
batch size byte number of inputs and so
after we do this matrix operation we now
have batch size by you know whatever the
number of columns in our matrix product
was by number of hidden units okay and

368
01:14:59,780 --> 01:15:06,540
so if we now take these activations
but it's the matrix and we put it
through another operation in this case
another matrix product and the softmax

369
01:15:08,450 --> 01:15:12,540
we get a triangle that's our output
activations another matrix of

370
01:15:12,540 --> 01:15:21,300
activations and again number of roses
batch size number of columns number is
equal to the number of classes again
however many columns our matrix in this

371
01:15:21,300 --> 01:15:35,910
matrix product head so that's a that's a
neuro net right that's our basic kind of
one hidden layer neural net and if you
haven't written one of these from
scratch try it you know and in fact in

372
01:15:39,720 --> 01:15:49,855
lessons nine ten and eleven of the
machine learning course we do this right
we create one of these from scratch so
if you're not quite sure how to do it
you can check out the machine learning

373
01:15:49,955 --> 01:16:02,880
costs yeah in general the machine
learning cost is much more like building
stuff up from the foundations where else
this course is much more like best
practices kind of top-down all right so

374
01:16:02,880 --> 01:16:08,460
if we were doing like a cognate with a
single dense hidden layer our input

375
01:16:08,460 --> 01:16:16,000
would be equal to actually number yeah
that's very implied watch number of
channels by height by width right and

376
01:16:16,100 --> 01:16:21,570
notice that here batch size appeared
every time so I'm not gonna I'm not
gonna write it anymore
okay so I've removed the batch size also

377
01:16:26,070 --> 01:16:36,595
the activation function it's always
basically value or something similar for
all the hidden layers and softmax at the
end for classification so I'm not going
to write that either okay so I'm kind of

378
01:16:36,695 --> 01:16:45,960
edge picture I'm going to simplify it a
little bit alright so I'm not gonna
mention batch size it's still there
we're not going to mention real you or
softmax but it's still there so here's

379
01:16:45,960 --> 01:16:58,660
our input and so in this case rather
than a matrix product will do a
convolution let's drive to convolution
so we'll skip over every second one or
could be a convolution followed by a mac
spool in either case we end up with

380
01:16:58,760 --> 01:17:08,370
something which is replaced number of
channels with number of filters right
and we have now height divided by two
and width divided by 2

381
01:17:09,270 --> 01:17:17,580
okay and then we can flatten that out
somehow we'll talk next week about the
main way
we do that nowadays which is basically

382
01:17:17,580 --> 01:17:21,810
to do something called an adaptive max
pooling where we basically get an

383
01:17:21,810 --> 01:17:30,580
average across the height and the width
and turn that into a vector anyway
somehow we flatten it out into a vector
we can do a matrix product or a couple

384
01:17:30,680 --> 01:17:35,995
of matrix products we actually tend to
do in fast AI so that'll be our fully

385
01:17:36,195 --> 01:17:40,770
connected layer with some number of

386
01:17:40,770 --> 01:17:46,260
activations final matrix product give us
some number of classes okay so this is

387
01:17:46,260 --> 01:17:59,842
our basic component remembering
rectangles input circle is hidden
triangle is output all other shapes
represent a tensor of activations all of
the arrows represent a operation or lay
operation all right

388
01:17:59,942 --> 01:18:25,640
so now that's going to jump to the one
the first one that we're going to
actually try to try to create for NLP
and we're going to basically do exactly
the same thing as here right and we're
going to try and predict the third
character in a three character sequence
based on the previous two characters so

389
01:18:25,640 --> 01:18:43,680
our input and again remember we've
removed the batch size dimension we're
not saying that we're still here okay
and also here I've removed the names of
the layer operations entirely
okay just keeping simplifying things so

390
01:18:43,680 --> 01:18:50,700
for example our first import would be
the first character of each string in
our mini batch okay and assuming this is

391
01:18:54,660 --> 01:19:01,705
one hot encoded then the width is just
however many items there are in the
vocabulary how many unique characters

392
01:19:01,805 --> 01:19:12,310
could we have okay we probably won't
really one hot encoder will feed it in
as an integer and pretend it's one hot
encoded by using an embedding layer
which is mathematically identical okay

393
01:19:12,410 --> 01:19:27,030
and then we that's going to give us some
activations which we can stick through a
fully connected layer okay so we we put
that through if we click through a fully
connected layer to get some activations

394
01:19:27,030 --> 01:19:33,990
we can then put that
another fully connected layer and now
we're going to bring in the input of

395
01:19:34,090 --> 01:19:39,410
character to alright so the character to
input will be exactly the same
dimensionality as the character one

396
01:19:39,410 --> 01:19:50,230
input and we now need to somehow combine
these two arrows together so we could
just add them up for instance right
because remember this arrow here
represents a matrix product so this

397
01:19:53,600 --> 01:19:59,730
matrix product is going to spit out the
same dimensionality as this matrix
product so we could just add them up to

398
01:19:59,830 --> 01:20:04,275
create these activations and so now we
can put that through another matrix

399
01:20:04,375 --> 01:20:14,385
product and of course remember all these
metrics products have a RAL you as well
and this final one will have a softmax
instead to create our predicted set of

400
01:20:14,485 --> 01:20:29,240
characters right so it's a standard you
know two hidden layer
I guess it's actually three matrix
products neural net this first one is
coming through an embedding layer the

401
01:20:29,240 --> 01:20:36,295
only difference is that we're also got a
second input coming in here that we're
just adding in right but it's kind of
conceptually identical so let's let's

402
01:20:36,395 --> 01:20:40,630
implement that for Nietzsche all right

403
01:20:46,160 --> 01:20:53,490
so I'm not going to use torch text I'm
gonna try not to use almost any fast AI
so we can see it all kind of again from

404
01:20:53,590 --> 01:20:59,160
raw right so here's the first 400
characters of the collected works let's

405
01:20:59,260 --> 01:21:08,090
grab a set of all of the letters that we
see there and sort them okay and so a
set creates all the unique letters so

406
01:21:08,090 --> 01:21:20,180
we've got 85 unique letters in our vocab
let's pop up it's nice to put an empty
kind of a null or some some kind of
padding character in there for padding
so we're gonna put a parenting character

407
01:21:20,180 --> 01:21:29,030
at the start right and so here is what
our vocab looks like okay so so Kars is
our bouquet so as per usual we want some

408
01:21:30,750 --> 01:21:41,200
way to map every character to a unique
ID and every unique ID to a character
and

409
01:21:42,600 --> 01:22:01,315
so now we can just go through our
collected works of niche and grab the
index of each one of those characters so
now we've just turned it into this right
so rather than quote PR e we now have 40

410
01:22:01,415 --> 01:22:20,060
42 29 okay so so that's basically the
first step and just to confirm we can
now take each of those indexes and turn
them back into characters and join them
together and yeah there it is okay so

411
01:22:20,060 --> 01:22:30,330
from now on we're just going to work
with this IDX
list the list of character members in
the connected works of Nietzsche yes so

412
01:22:30,330 --> 01:22:36,540
Jeremy why are we doing like a model of
characters and not a model of words I

413
01:22:36,540 --> 01:22:42,180
just thought it seemed simpler you know
with a vocab of 80-ish items we can kind

414
01:22:42,180 --> 01:22:54,390
of see it better character level models
turn out to be potentially quite useful
in a number of situations but we'll
cover that in part two the short answer

415
01:22:54,390 --> 01:23:00,355
is like you generally want to combine
both the word level model and a connect
character level model like if you're

416
01:23:00,455 --> 01:23:05,970
doing say translation it's a great way
to deal with unknown like unusual words
rather than treating it as unknown

417
01:23:05,970 --> 01:23:09,390
anytime you see a word you haven't seen
before you could use a character level

418
01:23:09,390 --> 01:23:19,270
model for that and there's actually
something in between the two quarter
byte pair and coding vpe which basically
looks at at all engrams of characters
but we'll cover all that in part two if

419
01:23:19,370 --> 01:23:38,430
you want to look at it right now
then part two of the existing course
already has this stuff taught and part
two of the version 1 of this course
although the NLP stuff is in flight
which by the way so you'll understand it

420
01:23:38,430 --> 01:23:48,745
straight away it was actually the thing
that inspired us to move to piped watch
because trying to do it in chaos turned
out to be a nightmare all right so let's

421
01:23:48,845 --> 01:24:04,559
create the inputs to this we're actually
going to do something slightly different
what I said we're actually going to
I predict the fourth character that
actually this the fifth character using
the first four so the index four
character using the index zero one two

422
01:24:04,559 --> 01:24:28,559
and three okay so it was exactly the
same thing but with just a couple more
layers so that means that we need a list
of the zeroth first second and third
characters that's why I'm just cutting
every character from the start from the
one from two from three skipping over
three at a time okay

423
01:24:28,559 --> 01:24:46,110
so hmm
this is I I said this wrong so we're
going to predict the third character the
fourth character from the third for the
first story okay
the fourth character is history

424
01:24:46,110 --> 01:25:14,334
all right so our inputs will be these
three lists right so we can just use n P
dot stack to pop them together all right
so here's the zero one and two
characters that are going to feed into a
model and then here is the next
character in the list so for example X 1

425
01:25:14,434 --> 01:25:23,249
X 2 X 3 and Y all right so you can see

426
01:25:23,249 --> 01:25:40,949
for example we start off the first the
very first item would be 40 42 and 29
right so that's characters naught 1 and
2 and then we'd be predicting 30 that's

427
01:25:40,949 --> 01:25:46,289
the fourth character which is the start
of the next row

428
01:25:46,289 --> 01:25:59,030
all right so then 30 25 27 we need to
predict 29 which is the start of next
row and so forth so we're always using
three characters to predict the fourth

429
01:25:59,030 --> 01:26:08,550
so there are 200,000 of these that we're
going to try and model right so we're

430
01:26:08,550 --> 01:26:24,624
going to build this
which means we need to decide how many
activations so I'm going to use 256 okay
and we need to decide how big our
embeddings are going to be and so I
decided to use 42 so about half the
number of characters I have and you can

431
01:26:24,724 --> 01:26:32,729
play around these so you can come up
with better numbers it's just a kind of
experimental and now we're going to

432
01:26:32,729 --> 01:26:43,509
build our model now I'm gonna change my
model slightly and so here is the the
full version so predicting character for
using characters 1 2 & 3 as you can see

433
01:26:43,609 --> 01:26:58,319
it's the same picture as a previous page
but I put some very important coloured
arrows here all the arrows of the same
color are going to use the same matrix
the same weight matrix right so all of

434
01:26:58,319 --> 01:27:12,474
our input embeddings are going to use
the same matrix all of our layers that
go from one layer to the next they're
going to use the same orange arrow
weight matrix and then our output will

435
01:27:12,574 --> 01:27:17,069
have its own matrix so we're going to
have one two three weight matrices right

436
01:27:17,169 --> 01:27:33,029
and the idea here is the reason I'm not
gonna have a separate one but every
everything here is that like why would
kind of semantically a carrot to have a
different meaning depending if it's the
first or the second or the third item in

437
01:27:33,029 --> 01:27:44,189
a sequence like it's not like we're even
starting every sequence at the start of
a sentence we're just arbitrarily
chopped it into groups of three right so
you would expect these to all have the
same kind of conceptual mapping and

438
01:27:44,189 --> 01:27:48,659
ditto like when we're moving from
claritin or character one you know to
kind of say build up some state here why

439
01:27:51,149 --> 01:27:57,159
would that be any different kind of
operation to moving from character
wonder character to so that's the basic

440
01:27:57,259 --> 01:28:11,819
idea so let's create a three character
model and so we're going to create one
linear layer for our Green Arrow one
linear layer fat orange arrow and one
linear layer for our blue arrow and then

441
01:28:11,819 --> 01:28:29,519
also one embedding okay so the embedding
is going to bring in something with size
whatever it was 84
I think vocab size and spit out
something with an
factors in the embedding well then put
that through a linear layer and then

442
01:28:29,519 --> 01:28:51,920
we've got our hidden layers before the
output layer so when we call forward
they're going to be passing in one two
three characters so if each one will
stick it through an embedding we'll
stick it through a linear layer and
we'll stick it through a value just to
do it the character one character - and
character three okay

443
01:28:51,920 --> 01:29:30,239
then I'm going to create this circle of
activations here okay and that matrix
I'm going to call H right and so it's
going to be equal to my input
activations okay after going through the
value and the linear layer and the
embedding right and then I'm going to
apply this l hidden so the orange arrow
and that's going to get me to here okay
so that's what this layer here does and

444
01:29:32,489 --> 01:29:52,619
then to get to the next one I need to
reply the same thing and it apply the
orange arrow to that okay but I also
have to add in this second input right
so take my second input and add in okay
my previous layer your neck could you

445
01:29:57,619 --> 01:30:04,930
pass it back three rows I don't really
see how these dimensions are the same
from eight and in2 from literature which

446
01:30:05,030 --> 01:30:24,000
from yeah okay let's go through so let's
figure out the dimensions together so
self dot E is gonna be of length 42 okay
and then it's gonna go through L in I'm
just gonna make it of size n hidden okay

447
01:30:24,000 --> 01:30:42,685
and so then we're going to pass that
which is now size n hidden through this
which is also going to return something
of size n hidden
okay so it's a really important to
notice that this is square this is a
square weight matrix okay so we now know

448
01:30:42,785 --> 01:30:50,460
that this is of size n hidden into it's
going to be exactly the same size as in
one was which is n hidden so we can now

449
01:30:50,460 --> 01:30:59,035
sum together two sets of activations
both the size n hidden passing it into
here and again it returns something of

450
01:30:59,135 --> 01:31:09,330
size n hidden so basically the trick was
to make this a square matrix and to make
sure that it's square matrix was the
same size as the output of this hidden
well thanks for the great question can

451
01:31:14,675 --> 01:31:21,240
summing the only thing people can do in
these cases I'll come back to that in a

452
01:31:21,340 --> 01:31:25,500
moment that's great point okay um I

453
01:31:25,500 --> 01:31:49,110
don't like it when I have like three
bits of code that look identical and
then three bits of codes that look
nearly identical but aren't quiet
because it's harder to refactor so I'm
going to put a make H into a bunch of
zeros so that I can then put H here and
these are now identical okay so that the

454
01:31:49,110 --> 01:32:12,685
hugely complex trick that we're going to
do very shortly is to replace these
three things with a for loop okay and
it's going to loop through one two and
three that's that's going to be the for
loop or actually zero one two okay at
that point we'll be able to call it a
recurrent neural network okay so just to
skip ahead a little bit alright so we

455
01:32:16,470 --> 01:32:26,740
run all these so we can actually run
this thing okay so we can now just use
the same columnar model data class that

456
01:32:26,840 --> 01:32:40,910
we've used before and if we use from
arrays then it's basically it's going to
spit back the exact arrays we gave it
right so if we pass if we stack together
those three arrays then it's going to
feed us those three things back to our

457
01:32:41,010 --> 01:33:02,604
forward method so if you want to like
play around with training models using
like you know as roar
approach as possible but without writing
lots of boilerplate this is kind of how
to do it here's column Namit model data
from arrays and then if you pass in
whatever you pass in here right you're

458
01:33:02,704 --> 01:33:14,059
going to get back here okay so I've
passed in three things which means I'm
going to get sent three things okay so
that's how that works

459
01:33:14,189 --> 01:33:18,209
batch size 512 because this is you know
this data is tiny so I can use a bigger
batch size so I'm not using really much

460
01:33:21,809 --> 01:33:35,019
faster i stuff at all I'm using fast AI
stuff just to save me fiddling around
with data loaders and data sets and
stuff but I'm actually going to create a
standard ply torch model I'm not going
to create a loner okay so this is a

461
01:33:35,119 --> 01:33:45,729
standard paper model and because I'm
using ply towards that means I have to
remember to write CUDA okay let's tick
it on the GPU so here is how we can look

462
01:33:45,829 --> 01:33:56,429
inside at what's going on right so we
can say it er MD train data loader to
grab the iterator to iterate through the

463
01:33:56,429 --> 01:34:12,174
training set we can then call next on
that to grab a mini batch and that's
going to return all of our X's and why
tensor and so we can then take a look at
you know here's our X's for example all

464
01:34:12,274 --> 01:34:42,959
right and so you would expect have a
think about what you would expect for
this length three not surprisingly
because these are the three things okay
and so then XS 0 not surprisingly okay
is of length 512 and it's not actually
one hot encoded because we use an
embedding to pretend it is okay and so
then we can use a model as if it's a

465
01:34:42,959 --> 01:34:56,280
function okay by passing to it
the variable eyes version of our tensors
and so have a think about what you would
expect to be returned here okay so not

466
01:34:56,280 --> 01:35:09,815
surprisingly we had a mini batch of 512
so we still have 5 12 and then 85 is the
probability of each of the possible
vocab items and of course we've got the
log of them because that's kind of what
we do in pi torch okay you can see here

467
01:35:09,915 --> 01:35:19,025
the softmax alright so that's how you
can look inside alright so you can see
here how to do everything really very
much by hand so we can create an

468
01:35:19,125 --> 01:35:32,179
optimizer again using standard pipe
torch so with PI torch when you use a
plate or optimizer you have to pass in a
list of the things to optimize and so if
you call m dot parameters that will

469
01:35:32,279 --> 01:35:39,259
return that list for you and then we can
fit and there it goes

470
01:35:39,359 --> 01:35:55,790
okay and so we don't have learning rate
finders and sttr and all that stuff
because we're not using a learner so
we'll have to manually do learning rate
annealing so set the learning rate a
little bit lower and fit again okay and

471
01:35:55,890 --> 01:36:09,430
so now we can write a little function to
to test this thing out okay
so here's something called getnext where

472
01:36:09,630 --> 01:36:14,255
we can pass in three characters like why
full top space right and so I can then

473
01:36:14,355 --> 01:36:31,700
go through and turn that into a tensor
with capital T of an array of the
character index for each character in
that list
so basically turn those into the
integers turn those into variables pass

474
01:36:31,800 --> 01:36:45,730
that to our model right and then we can
do an Arg max on that to grab which
character number is it and in order to
do stuff in none pile and I use two NP
to turn that variable into a lumpy array
right and then I can return that

475
01:36:47,320 --> 01:36:56,200
character and so for example a capital T
because what it thinks would be
reasonable after seeing why . space that
seems like a very reasonable way to
start a sentence if it was ppl a that

476
01:36:58,719 --> 01:37:11,860
sounds reasonable space th a that's
bouncer e small a and D space that
sounds reasonable
so it seems to reflect created something
sensible alright so you know the

477
01:37:11,960 --> 01:37:42,510
important thing to note here is our
character model
is a totally standard fully connected
model right the only slightly
interesting thing we did was to kind of
do this addition of each of the inputs
one at a time okay but there's nothing
new conceptually here we're training it
in the usual way

478
01:37:42,960 --> 01:37:51,150
all right let's now create an errand in

479
01:37:51,250 --> 01:38:32,530
so an iron in is when we do exactly the
same thing that we did here all right
but I could draw this more simply by
saying you know what if we've got a
green arrow going to a circle let's not
draw a green arrow go into a circle
again and again and again so let's just
draw it like this green arrow going to a
circle right and rather than drawing an
orange arrow going to a circle let's
just draw it like this okay so this is
the same picture exactly the same
picture as this one right and so you

480
01:38:32,530 --> 01:38:39,790
just have to say how many times to go
around this circle right so in this case
if we were to predict character number n
from characters one through n minus one

481
01:38:39,790 --> 01:38:46,940
then we can take the character one input
get some activations feed that to some
new activations that go through remember

482
01:38:47,040 --> 01:38:57,470
orange is the hidden to hidden weight
matrix right and each time we'll also
bring in the next character of input
through its embeddings okay so that
picture and that picture I have two ways

483
01:38:57,570 --> 01:39:16,330
of writing the same thing but this one
is more flexible because rather than me
having to say hey let's do it for H I
don't have to draw eight circles right I
can just say I'll just repeat this so I

484
01:39:16,330 --> 01:39:30,699
could simplify this a little bit further
by saying you know what rather than
having this thing as a special case
let's actually start out with a bunch of
zeros right and then let's have all of
our characters
inside here yes yeah so I was wondering

485
01:39:36,350 --> 01:39:57,369
if you can explain it be better why are
you reusing those why you think oh
they're the same yeah where are you you
kind of seem to be reusing the same same
weight matrices weight matrices yeah
maybe this is kind of similar to what we
did in convolution your Nets like if

486
01:39:57,369 --> 01:40:02,679
somehow no I don't think so
at least not that I can see so the idea

487
01:40:02,679 --> 01:40:30,179
is just kind of semantically speaking
like this arrow here this this arrow
here is saying take a character of
import and represented as some says some
set of features right and this arrow is
saying the same thing take some
character and represent as a set of
features and so is this one okay so like

488
01:40:30,179 --> 01:40:35,875
why would the three be represented with
different weight matrices because it's
all doing the same thing right and this

489
01:40:35,975 --> 01:40:55,630
orange arrow is saying kind of
transition from character 0 state to
character 1 state 2 characters to state
again it's it's the same thing it's like
why would the transition from character
0 to 1 be different to character from
transition from one or two so the idea

490
01:40:55,630 --> 01:41:05,549
is like but is to like say hey if if
it's doing the same conceptual thing
let's use the exact same white matrix my

491
01:41:07,360 --> 01:41:12,159
comment on convolution neural networks
is that a filter or so this apply to

492
01:41:12,159 --> 01:41:24,935
multiple places I think something like a
convolution is almost like a kind of a
special dot product with shared weights
yeah no that's okay
that's very good point and in fact one

493
01:41:25,035 --> 01:41:34,175
of our students actually wrote a good
blog post about that last year we should
dig that up okay I totally see where
you're coming from and I totally agree
with you all right so let's let's

494
01:41:34,275 --> 01:41:45,079
implement this version so this time
we're going to do eight

495
01:41:45,179 --> 01:42:05,290
as eight sees okay and so let's create a
list of every eighth character from zero
through seven and then our outputs will
be the next character and so we can
stack them together and so now we've got
six hundred thousand by eight so here's

496
01:42:05,290 --> 01:42:23,826
an example so for example after this
series of eight characters right so this
is characters north through eight
this is characters one through nine this
is two through ten these are all
overlapping okay so after characters one
north through eight this is going to be

497
01:42:23,926 --> 01:42:42,675
the next one okay and then after these
characters this will be the next one all
right so you can see that this one here
has 43 is its Y value right because
after those the next one will be 43 okay

498
01:42:42,775 --> 01:42:57,270
so so this is the first eight characters
this is two through nine three through
ten and so forth right so these are
overlapping groups of eight characters
and then this is the the next one okay

499
01:43:03,250 --> 01:43:21,664
we use from arrays to create a model
data class and so you'll see here we
have exactly the same code as we had
before
there's our embedding Linea hidden
output these are literally identical

500
01:43:21,764 --> 01:43:38,730
okay and then we've replaced our value
of the linear input of the embedding
with something that's inside a loop okay
and then we've replaced the cell hidden
thing okay
also inside the loop

501
01:43:43,940 --> 01:43:59,719
I just realize didn't mentioned last
time the use of the hyperbolic tan
hyperbolic tan looks like this okay so
it's just a sigmoid that's offset right

502
01:43:59,719 --> 01:44:12,705
and it's very common to use a hyperbolic
tan inside this trend this state to
state transition because it kind of
stops it from flying off too high or too
low you know it's nicely controlled back

503
01:44:12,805 --> 01:44:25,649
in the old days we used to use
hyperbolic tanh or the equivalent
sigmoid a lot as most of our activation
functions nowadays we tend to use value

504
01:44:25,749 --> 01:44:47,084
but in these hidden state to here in the
hidden state transition weight matrices
we still tend to use hyperbolic tanh
quite a lot so you'll see I've done that
also yeah hyperbolic tanh okay so this
is exactly the same as before but I've
just replaced it with a Pollard and then

505
01:44:47,184 --> 01:44:55,779
here's my output yes you know so a does
he have to do anything with convergence
these networks yeah we'll talk about

506
01:44:55,879 --> 01:45:26,630
that a little bit over time let's let's
let's come back to that though for now
we're not really going to do anything
special at all you know recognizing this
is just a standard fully connected
Network you know interestingly it's
quite a deep one right like because this
is actually this that we've got eight of
these things now we've now got a deep
eight layer Network which is why units
starting suggest we should be concerned

507
01:45:26,630 --> 01:45:29,989
as you know as we get deeper and deeper
networks they can be harder and harder
to train but let's try training this

508
01:45:37,530 --> 01:45:48,115
all right so when it goes as before
we've got a batch size of 512 we're
using Adam and where it goes so we will

509
01:45:48,215 --> 01:46:02,050
sit there watching it so we can then set
the learning rate down back to 20 neg 3
we can fit it again and yeah it's
actually it seems to be training fun

510
01:46:02,150 --> 01:46:12,100
okay but we're gonna try something else
which is we're going to use this a trick
that your net rather hinted at before
which is maybe we shouldn't be adding

511
01:46:12,200 --> 01:46:28,645
these things together and so the reason
you might want to be feeling a little
uncomfortable about adding these things
together is that the input state and the
hidden state are kind of qualitatively
different kinds of things right the

512
01:46:28,745 --> 01:46:34,030
input state is the is the encoding of
this character for us H represents the
encoding of the series of characters so

513
01:46:34,130 --> 01:46:42,235
far and so adding them together is kind
of potentially going to lose information

514
01:46:42,335 --> 01:46:50,800
so I think what your net was going to
prefer that we might do is maybe to
concatenate these instead of adding them
so it sound good to you you know she's

515
01:46:50,900 --> 01:46:56,765
not it okay so let's now make a copy of
the previous cell all the same right

516
01:46:56,865 --> 01:47:19,590
rather than using plus let's use cat
okay now if we can cat then we need to
make sure now that our input layer is
not from n fac-2 hidden which is what we
had before but because we're
concatenated it needs to be in fact plus
and hidden to end hidden okay and so now

517
01:47:19,590 --> 01:47:33,270
that's going to make all the dimensions
work nicely so this now is of size n
fact plus and hidden this now makes it
back to size n hidden again okay and

518
01:47:33,270 --> 01:47:41,110
then this is putting it through the same
square matrix as before so it's still a
size n here okay so this is like a good

519
01:47:41,210 --> 01:47:51,945
design heuristic if you're designing an
architecture is if you've got different
types of information that you want to
combine you generally want

520
01:47:52,045 --> 01:48:00,850
concatenate it okay you know adding
things together even if they're the same
shape is losing information okay and so

521
01:48:00,850 --> 01:48:11,560
once you've concatenated things together
you can always convert it back down to a
fixed size by just tracking it through a
matrix product okay so that's what we've

522
01:48:11,560 --> 01:48:29,650
done here again it's the same thing but
now we're concatenating instead and so
we can fit that and so last time we got
one point seven two this time you go at
one point six six so it's not setting
the world on fire but it's an
improvement and the improvements of it
okay

523
01:48:29,650 --> 01:48:45,550
so we can now test that with get next
and so now we can pass in eight things
right so it's no before those let's go
to a part of that sounds good as well so
Queens and that sounds good too all
right so great so that's enough

524
01:48:45,650 --> 01:48:56,300
manual hackery let's see if pi torch
couldn't do some of this for us and so

525
01:48:56,400 --> 01:49:11,680
basically what pi torch will do for us
is it will write this loop automatically
okay and it will create these linear
input layers automatically okay and so
to ask it to do that we can use the n n

526
01:49:11,680 --> 01:49:34,370
dot R and n plus so here's the exact
same thing in less code by taking
advantage of height choice and again I'm
not using a conceptual analogy to say
player torches doing something like it
I'm saying play torch is doing it now
this is just the code you just saw
wrapped up a little bit
reflect it a little bit for your

527
01:49:34,470 --> 01:49:43,660
convenience right so where we say we now
want to create an era ten call our it n
then what this does is it does that for

528
01:49:43,660 --> 01:50:05,139
live now notice that our for loop needed
a starting point you remember why right
because otherwise our for loop didn't
quite work we couldn't quite refactor it
out and because this is exactly the same
this needs our starting point to and so
let's give it a starting point and so
you have to pass in your initial hidden
State

529
01:50:05,139 --> 01:50:27,980
for reasons that will become apparent
later on it turns out to be quite useful
to be able to get back that here in the
state at the end and just like we could
here we could actually keep track of the
hidden state we get back to things we
get back both the output and the hidden

530
01:50:28,080 --> 01:50:34,250
state right so we pass in the input in
the hidden State when we get back the
output and the hidden state yes could

531
01:50:34,350 --> 01:50:41,265
you remind us what the hint state
represents the hidden state is H so it's
the it's the orange circle ellipse of

532
01:50:41,365 --> 01:51:01,972
activations okay and so it is of size
256 okay all right so we can okay

533
01:51:06,010 --> 01:51:33,310
there's one other thing too to know
which is in our case we were replacing H
with a new hidden state the one minor
difference in pi torch is they append
the new hidden state to a list or to a
tensor which gets bigger and bigger so
they actually give you back all of the
hidden states so in other words rather
than just giving you back the final
ellipse they give you back all the
ellipses stacked on top of each other

534
01:51:33,310 --> 01:51:43,594
and so because we just want the final
one I was got indexed into it with minus
one here okay other than that this is
the same code as before put that through

535
01:51:43,694 --> 01:51:50,940
our output layer to get the correct
vocab size and then we can train that

536
01:51:57,630 --> 01:52:07,924
alright so you can see here I can do it
manually I can create some hidden state
I can pass it to that area and I can see
the stuff I get back you'll see that the

537
01:52:08,024 --> 01:52:26,380
dimensionality of H it's actually a rank
3 tensor where else in my version it was
a
let's see it was a rank two tensor okay
and the difference is here we've got
just a unit axis at the front we'll

538
01:52:26,480 --> 01:52:48,730
learn more about why that is later but
basically it turns out you can have a
second R and n that goes backwards
alright one that goes forwards one that
goes backwards from the idea is neck and
then it's going to be better at finding
relationships that kind of go backwards
that's quite a bi-directional eridan
also it turns out you can have an error
in feed to an iron in that's got a
multi-layer eridan so basically if you

539
01:52:51,580 --> 01:52:56,170
have those things you need an additional
access on your tensor to keep track of

540
01:52:56,170 --> 01:53:09,740
those additional layers of hidden state
but for now we'll always have a one yeah
and we'll always also get back a one at
the end okay so if we go ahead and fit

541
01:53:09,840 --> 01:53:27,885
this now let's actually trade it for a
bit longer
okay so last time we only kind of did a
couple of epochs this time we're due for
a pox
what have we sit at one in egg three and
then we'll do another to epochs at one

542
01:53:27,985 --> 01:53:37,455
in egg four and so we've now got our
lost down to one point five so getting
better and better so here's our get next
again okay and you know let's just it

543
01:53:37,555 --> 01:53:44,492
was the same thing so what we can now do

544
01:53:44,592 --> 01:54:18,190
is we can look through like forty times
calling get next each time and then each
time will replace our input by removing
the first character and adding the thing
that we just predicted and so that way
we can like feed in a new set of eight
characters that get them again and again
and so that way we'll call that get next
in so here are 40 characters that we've
generated so we started out with four th
OS so we got four those of the same -
the same - the same you can probably
guess what happens if you can't
predicting the same - the same all right

545
01:54:20,620 --> 01:54:39,200
so it's you know it's doing okay we we
now have something which you know
we've basically built from scratch and
then we've said here's how high torture
effected it for us so if you want to

546
01:54:39,300 --> 01:55:08,105
like have an interesting little homework
assignment this week try to write your
own version of an RNN plus all right
like try to like literally like create
your like you know Jeremy's aren't in
and then like type in here
Jeremy's aren't in or in your case maybe
your name's not Jeremy which is okay too
and then get it to run writing your
implementation that's fast from scratch
without looking at the piped water

547
01:55:08,205 --> 01:55:16,850
source code you know like basically it's
just a case of like going up and seeing
what we did back here right and like
make sure you get the same answers and

548
01:55:16,950 --> 01:55:29,910
confirm that you do so that's kind of a
good little test simply simple at all
assignment but I think you'll feel
really good when you seem like oh I've
just reimplemented an end alone in

549
01:55:30,390 --> 01:55:49,705
alright so I'm going to do one other
thing when I switched from this one when
I've moved the car one input inside the
dotted line right this dotted rectangle
represents the thing I'm repeating I
also watch the triangle the output I
moved that inside as well now that's a

550
01:55:49,805 --> 01:56:13,780
big difference
because now what I've actually done is
I'm actually saying spit out an output
after every one of these circles so spit
out an output here and here and here
alright so in other words if I have a
three character input I'm going to spit
out a three character output I'm saying

551
01:56:13,780 --> 01:56:19,087
half the character 1 this will be next
after character to this be next after
character 3 this will be next

552
01:56:19,187 --> 01:56:24,240
so again nothing different
and again this you know if you want to

553
01:56:26,470 --> 01:57:02,480
go a bit further with the assignment you
could write this by hand as well but
basically what we're saying is in the
for loop would be saying like you know
results equals some empty list right and
then would be going through and rather
than returning that
we're instead be saying you know results
dot append that right and then like
return whatever torch dot stat something
like that right that it made me right in

554
01:57:02,480 --> 01:57:21,560
my question so now you know we now have
like every step we've created an output
okay so which is basically this picture
and so the reason was lots of reasons
that's interesting but I think the main
reason right now that's interesting is

555
01:57:21,560 --> 01:57:38,660
that you probably noticed this this
approach to dealing with our data seems
terribly inefficient like we're grabbing
the first eight right but then this next
set all but one of them overlap the

556
01:57:41,960 --> 01:57:52,950
previous one right so we're kind of like
recalculating the exact set of
embeddings seven out of eight of them
are going to be exact same embeddings

557
01:57:53,050 --> 01:58:05,550
right exact same transitions it kind of
seems weird to like do all this
calculation to just predict one thing
and then go back and recalculate seven
out of eight of them and add one more to
the end to calculate the next thing all
right

558
01:58:05,650 --> 01:58:34,000
so the basic idea then is to say well
let's not do it that way instead let's
taking non overlapping sets of
characters all right so like so here is
our first eight characters here is the
next day characters here are the next
day characters so like if you read this
top left to bottom right that would be

559
01:58:34,100 --> 01:59:02,005
the whole nature right and so then if
these are the first eight characters
then offset this by one starting here
that's a list of outputs right so after
we see characters zero through seven
we should predict characters 1 through 8
the XS so after 40 should come 42
as it did after 42 should come 29 as it

560
01:59:02,105 --> 01:59:21,290
did okay and so now that can be our
inputs and labels for that model and so
it shouldn't be any more or less
accurate it should just be the same
right pretty much but it should allow us
to do it more efficiently so let's try

561
01:59:33,645 --> 01:59:48,239
so I mentioned last time that we had a
minus 1 index here because we just
wanted to grab the last triangle okay so

562
01:59:48,239 --> 02:00:07,890
in this case we're going to grab all the
triangles so this this is actually the
way it end on RNN creates things we we
only kept the last one but this time
we're going to keep all of them so we've
made one change which is to remove that

563
02:00:07,890 --> 02:00:34,915
minus one other than that this is the
exact same code as before okay so but
there's nothing much to show you here I
mean except of course at this time if we
look at the labels it's now 512 by eight
factors we're trying to predict eight

564
02:00:35,015 --> 02:00:56,605
things every time through so there is
one complexity here which is that we
want to use the negative log likelihood
loss function as before right but the
ligand if lost likelihood loss function
just like our MSE expects to receive to

565
02:00:56,705 --> 02:01:05,550
rank one tensors actually with the
mini-batch access to rank two tensors
all right so two to mini-batches of

566
02:01:05,550 --> 02:01:37,389
vectors problem is that we've got
eight-time steps you know it characters
in an RNN we call it a time step right
we have eight time steps and then for
each one we have 84 probabilities we
have the probability for every single
one of those eight times deaths and then
we have that for each of our 512 items
in the mini batch so we have a rank 3

567
02:01:37,389 --> 02:01:44,230
tensor not a rank two tensor um so that
means that the negative log likelihood
loss function is going to spit out an

568
02:01:46,179 --> 02:02:05,260
error now frankly I think this is kind
of dumb you know I think it would be
better if PI torch had written the loss
functions in such a way that they didn't
care at all about rank and they just
applied it to whatever rank you gave it
but for now at least it does care about

569
02:02:05,260 --> 02:02:09,310
rick but the nice thing is I get to show
you how to write a custom loss function

570
02:02:09,310 --> 02:02:14,530
okay so we're going to create a special
negative log likelihood loss function
for sequences okay and so it's going to

571
02:02:17,110 --> 02:02:21,550
take an input in the target and it's got
a call f dot negative log likelihood

572
02:02:21,550 --> 02:02:26,362
lost so the pipe launched one all right
but what we're going to do is we're

573
02:02:26,462 --> 02:02:37,290
going to flatten our input and we're
going to flatten our targets right and

574
02:02:37,290 --> 02:02:45,340
so and it turns out these are going to
be the first two axes that I have to be

575
02:02:45,340 --> 02:03:04,449
transposed so the way PI torch handles
are and end data by default is the first
axis is the sequence length in this case
eight right so the sequence length of an
R and n is how many times deaths so we
have eight characters so a sequence

576
02:03:04,449 --> 02:03:15,989
length of eight the second axis is the
batch size and then as would expect the
third axis is the actual hidden state
itself okay so this is going to be eight
by 512 by n hidden which I think was 256

577
02:03:25,740 --> 02:03:34,000
okay so we can grab the size and unpack
it into each of these sequence length

578
02:03:34,100 --> 02:04:00,360
batch size and I'm hidden now target
mighty dot size is 512 by 8 where else
this one here was 8 by 512 so to make
them match we're going to have to
transpose the first two axis okay
[Music]

579
02:04:01,540 --> 02:04:08,965
hi torch when you do something like
transpose doesn't generally actually
shuffle the memory order but instead it

580
02:04:09,065 --> 02:04:17,830
just kind of keeps some internal
metadata to say like hey you should
treat this as if it's transposed and

581
02:04:17,830 --> 02:04:44,239
some things in pi torch will give you an
error if you try and use it when it has
these like this internal state and I
basically say error this tensor is not
contiguous if you ever see that error at
the word contiguous after it and it goes
away so I don't know they can't do that
for you apparently so in this particular
case I got that error so I wrote the
code contiguous after it okay and so

582
02:04:44,239 --> 02:04:57,869
then finally we need to flatten it out
into a single vector and so we can just
go a dot view which is the same as non
PI dot reshape and minus one means as
long as it needs to be okay and then the

583
02:04:57,969 --> 02:05:12,835
input again we also reshape that right
but remember the input sorry the the the
predictions also have this axis of
length 84 all of the predicted

584
02:05:12,935 --> 02:05:23,775
probabilities okay so so here's a custom
these are custom lost function that's it
right so if you ever want to play around
with your own loss functions you can
just do that like so and then pass that

585
02:05:23,875 --> 02:05:42,830
to fit okay so it's important to
remember that Fitch is this like lowest
level fast AI abstraction
that's--it's that this is the thing that
implements the training look okay and so

586
02:05:42,930 --> 02:05:53,290
like you're the stuff you pass it in is
all standard pi torch stuff except for
this this is our model data object this

587
02:05:53,290 --> 02:06:02,480
is the thing that wraps up the test set
the training set and the validation set
to get that okay your neck could you

588
02:06:04,570 --> 02:06:23,240
triangle into the replicator structure
right so the the first n minus one
iterations of the sequence length we
don't see the whole sequence length yeah
so does that mean that the batch size
should be much bigger so that be careful

589
02:06:23,340 --> 02:06:46,480
you don't mean that size you main
sequence length right because the batch
size is like some firing yeah okay so
yes yes if you have a short sequence
length like eight yeah
the first character has nothing to go on
it starts with an empty hidden state of

590
02:06:46,480 --> 02:06:58,595
zeros okay so what we're going to start
with next week
is we're going to learn how to avoid
that problem right and so it's a really
insightful question or concern right and
but if you think about it the basic idea

591
02:06:58,695 --> 02:07:18,640
is why should we reset this to zero
every time you know like if we can kind
of line up these mini batches somehow so
that the next mini batch joins up
correctly it represents like the next

592
02:07:18,640 --> 02:07:45,620
letter in leaches works then we'd want
to move this up into the constructor
right and then like pass that here and
then store it here right and now we're
not resetting the hidden state each time
we're actually we're actually keeping
the hidden state from call to call and

593
02:07:45,720 --> 02:07:54,784
so the only time that it would be
failing to benefit from
learning state would be like literally
at the very start of the document so

594
02:07:54,884 --> 02:08:02,764
that's where but that's where we're
going to try and ahead next week

595
02:08:07,592 --> 02:08:16,157
got a punch line coming somebody asks me
a question where I have to like do the
punch line ahead of time okay so we can

596
02:08:17,294 --> 02:08:29,839
fit that and we can fit that and I want
to show you something interesting and
this is coming to the punch line that
another punch line that you net try to

597
02:08:29,939 --> 02:08:37,689
spoil which is when we're you know
remember this is just doing a loop right
applying the same matrix multiply again

598
02:08:37,689 --> 02:08:51,471
and again if that matrix multiply tends
to increase the activations each time
then effectively we're doing that to the
power of eight right so it's going to
like to shoot off really high or if it's

599
02:08:51,571 --> 02:08:55,769
decreasing it a little bit each time
that's going to shoot off really low so

600
02:08:55,960 --> 02:09:00,659
this is what we call a gradient
explosion right and so we really want to
make sure that the initial H naught H

601
02:09:00,759 --> 02:09:25,994
the initial but if we call it the
initial L hidden that we create is is
like oversize that's not going to cause
our activations on average to increase
or decrease right and there's actually a

602
02:09:26,094 --> 02:09:42,309
very nice matrix that does exactly that
called the identity matrix so the
identity matrix for those that don't
quite remember their linear algebra is
this this would be a size 3 identity

603
02:09:42,309 --> 02:09:59,289
matrix all right and so the trick about
an identity matrix is anything times an
identity matrix is itself right and so
therefore you could multiply it by this
again and again and again and again and
still end up with itself right so
there's no gradient explosion so what we

604
02:10:03,280 --> 02:10:15,830
could do is instead
of using whatever the default random in
it is for this matrix we could instead
after we create our errand in is we can
go into that
Erol in right and notice this right we

605
02:10:15,930 --> 02:10:46,390
can go m dot RN n right and if we now go
like so we can get the docs for m dot R
and M right and as well as the arguments
for constructing it it also tells you
the inputs and outputs for calling the
layer and it also tells you the
attributes and so it tells you there's
something called weight
H H and these are the learn about hidden
to hidden weights that's that square
matrix right so after we've constructed

606
02:10:46,390 --> 02:10:55,030
our M we can just go in and say all
right m dot R and n dot weight h HL dot
data that's the tensor dot copy

607
02:10:58,680 --> 02:11:09,602
underscore in place torch I that is I
for identity in case you are wondering
so this is an identity matrix of size n

608
02:11:09,702 --> 02:11:20,935
hidden so this both puts into this
weight matrix and returns the identity
matrix and so this was like actually a

609
02:11:21,035 --> 02:11:34,769
Geoffrey Hinton paper was like hey you
know after it was it's 2015
so after recurrent neural Nets have been

610
02:11:34,869 --> 02:11:47,105
around for decades here's like hey gang
maybe we should just use the identity
matrix to initialize this and like it
actually turns out to work really well

611
02:11:47,205 --> 02:11:56,270
and so that was a 2015 paper believe it
or not from the father of neural
networks and so here is the here is our
implementation of his paper and this is

612
02:11:56,370 --> 02:12:05,205
an important thing to know right when
very famous people like Geoffrey Hinton
write a paper sometimes in entire
implementation of that paper looks like

613
02:12:05,305 --> 02:12:22,360
one line of code okay so let's do it
before we got point six one two five
seven we'll fit it with exactly the same
parameters and now we get 0.5 1 and in
fact
can keep training 0.50 so like this
tweak really really really helped okay

614
02:12:22,360 --> 02:12:32,620
now one of the nice things about this
tweak was before I could only use a
learning rate of one in x3 before it
started going crazy but after identity

615
02:12:32,620 --> 02:12:41,890
matrix I found I could use one in egg
too because it's you know it's better
behaved weight initialization I found I
could use a higher learning rate okay

616
02:12:41,890 --> 02:12:56,345
and honestly these things you know
increasingly we're trying to incorporate
into the defaults in first day I you
know you don't necessarily personally
need to actually know them but you know

617
02:12:56,445 --> 02:13:03,934
at this point we're still at a point
where you know most things in most
libraries most of the time don't have
great defaults it's good to know all

618
02:13:04,034 --> 02:13:12,230
these little tricks it's also nice to
know if you want to improve something
what kind of tricks people have used
elsewhere because you can often borrow

619
02:13:12,330 --> 02:13:22,145
them yourself all right well that's the
end of the lesson today and so next week
we will look at this idea of a stateful
RNN that's going to keep this hidden
state around and then we're going to go

620
02:13:22,245 --> 02:13:35,350
back to looking at language models again
and then finally we're going to go all
the way back to computer vision and
learn about things like rez nets and
batch norm and all the tricks that were

621
02:13:35,350 --> 02:13:40,679
in figured out in cats versus dogs see
you then

