1
00:00:00,060 --> 00:00:14,991
Добро пожаловать на шестую лекцию, она предпоследняя в этом курсе.

2
00:00:15,091 --> 00:00:22,704
Пару недель назад я обещал показать запись четвёртой лекции исследователю Себастьяну Рудеру.

3
00:00:22,804 --> 00:00:39,250
Ему понравилось, и вчера вышел его пост Optimization for Deep Learning Highlights in 2017 по мотивам нашей лекции.

4
00:00:39,350 --> 00:00:44,455
В посте упоминается то, что сделали студенты нашего курса -

5
00:00:44,555 --> 00:00:58,020
например, отделение ограничения весов от вычисления инерции в AdamW,

6
00:00:58,020 --> 00:01:13,034
он пишет про возможности, которые это открывает, и приводит ссылку на реализацию AdamW от Anand Saha.

7
00:01:13,134 --> 00:01:16,710
Как видите, код библиотеки fastai уже приводят в пример.

8
00:01:18,060 --> 00:01:25,170
После этого Себастьян пишет об алгоритмах подбора скорости обучения.

9
00:01:25,170 --> 00:01:36,780
На графике показана зависимость скорости обучения от эпохи, выглядит непривычно из-за логарифмической шкалы.

10
00:01:36,780 --> 00:01:53,430
В качестве примера - ссылки на посты наших двух студентов, Vitaly Bushaev и Anand Saha.

11
00:01:53,430 --> 00:02:00,335
Очень радует то, что работы наших студентов уже замечают и используют.

12
00:02:00,435 --> 00:02:06,070
Пост Себастьяна попал на главную страницу Hacker News, это очень круто.

13
00:02:06,170 --> 00:02:14,885
Будем надеяться, что распространение информации продолжится.

14
00:02:14,985 --> 00:02:31,090
На прошлой неделе мы обсуждали коллаборативную фильтрацию, давайте вспомним, как выглядела модель.

15
00:02:34,300 --> 00:02:42,200
Последняя версия модели была почти идентична коду класса EmbeddingDotBias библиотеки fastai.

16
00:02:47,630 --> 00:02:56,370
Сначала создаются матрицы эмбеддинга для пользователей и товаров и заполняются случайными числами.

17
00:02:56,470 --> 00:03:00,870
Пользователи и товары - устоявшиеся термины, в нашей модели товары - это фильмы.

18
00:03:00,970 --> 00:03:11,540
Таким же образом созданы матрицы смещений - в функцию get_emb() передаётся не nf=n_factors, а nf=1.

19
00:03:14,210 --> 00:03:22,180
Матрицы эмбеддинга перемножаются, полученная матрица превращается в вектор, к ней прибавляются смещения.

20
00:03:22,730 --> 00:03:27,430
Полученный вектор пропускается через сигмоиду для калибровки до желаемого диапазона.

21
00:03:27,430 --> 00:03:39,865
Вы спрашивали, можно ли визуализизовать этот процесс, и я обещал это показать.

22
00:03:39,965 --> 00:04:04,050
Мы начнём с модели, использующей только функции и методы fastai, она обучалась 19 секунд, результаты неплохие.

23
00:04:04,150 --> 00:04:09,770
Давайте анализировать процесс.

24
00:04:09,770 --> 00:04:24,370
Мы считывали файл movies.csv, содержащий пары "ID фильма - название фильма", сейчас будем его использовать.

25
00:04:24,640 --> 00:04:36,660
Так как не все в аудитории - заядлые кинозрители, для наглядности я возьму 3000 самых популярных фильмов.

26
00:04:36,660 --> 00:04:51,785
После этого заменю индексы на новые, используя словарь нумерации cf.item2idx, с которым работает модель.

27
00:04:51,885 --> 00:05:06,410
Модель fastai лежит в переменной learn, модель PyTorch можно получить вызовом learn.model.

28
00:05:06,510 --> 00:05:18,140
В коде библиотеки fastai model реализовано как @property - вычисляемое свойство.

29
00:05:18,240 --> 00:05:32,800
Вычисляемое свойство в Python выглядит как метод, но вызывается без скобок.

30
00:05:32,800 --> 00:05:42,669
Это выглядит как обычное поле, но каждый раз при вызове выполняется этот код.

31
00:05:42,669 --> 00:05:48,789
Здесь при вызове вычисляемого свойства возвращается поле self.models.model.

32
00:05:48,789 --> 00:06:03,019
self.models - объект класса CollabFilterModel, это тонкая обёртка для моделей PyTorch.

33
00:06:03,119 --> 00:06:19,720
Код класса CollabFilterModel состоит из одной строки.

34
00:06:19,720 --> 00:06:33,380
Мы обсудим это во второй части курса, это сделано для возможности задания разных скоростей обучения для групп слоёв.

35
00:06:33,480 --> 00:06:42,820
Группировка слоёв не реализована в PyTorch, и этот класс позволяет добавить её в готовую модель PyTorch.

36
00:06:42,820 --> 00:06:54,950
Детали не важны, но, в общем, примерно так и выглядят обёртки для моделей PyTorch.

37
00:06:55,050 --> 00:07:07,820
Итак, модель PyTorch содержится в поле learn.models.model, это же можно получить присваиванием m=learn.model.

38
00:07:07,920 --> 00:07:21,815
Модель PyTorch в переменной m содержит массив всех слоёв модели,

39
00:07:21,915 --> 00:07:39,460
это реализовано с помощью встроенных средств объектно-ориентированного программирования Python.

40
00:07:39,460 --> 00:08:07,960
Имена слоёв эмбеддинга автоматически берутся из их определения в коде.

41
00:08:12,910 --> 00:08:28,660
Это модель PyTorch. Слои модели получаются соответствующими методами -  метом m.ib() (item bias)

42
00:08:28,660 --> 00:08:34,000
возвращает матрицу смещений товаров, в нашем случае - матрицу смещений фильмов.

43
00:08:34,000 --> 00:08:40,990
В этой матрице каждому из 9066 фильмов соответствует одно число - смещение.

44
00:08:40,990 --> 00:08:50,710
Модели и слои PyTorch удобно использовать, потому что их можно вызывать как функции.

45
00:08:50,710 --> 00:09:12,520
Метод ib() возвращает слой ib, а функция m() возвращает предсказания модели m.

46
00:09:12,520 --> 00:09:22,570
Здесь в метод ib() передаются индексы самых популярных фильмов.

47
00:09:22,570 --> 00:09:34,835
Напомню, что матрицы эмбеддинга - объекты класса Variable, а не тензоры, поэтому здесь стоит конструктор V().

48
00:09:34,935 --> 00:10:00,425
PyTorch избавляется от класса Variable с версии 0.4. Если вы смотрите курс в записи, его может уже не быть.

49
00:10:00,525 --> 00:10:07,025
Это будет удобно, а пока нужно не забывать использовать конструктор V() при передаче данных в модель.

50
00:10:07,125 --> 00:10:20,140
Помните, что всё, что можно делать с тензором, можно делать и с объектами класса Variable.

51
00:10:20,140 --> 00:10:38,820
Итак, m.ib(V(topMovieIdx)) содержит смещения самых популярных фильмов.

52
00:10:41,790 --> 00:11:06,550
Эти смещения содежатся в объекте класса Variable размера 3000x1, так как мы выбрали 3000 фильмов.

53
00:11:08,830 --> 00:11:14,675
Мы получили объект класса Variable, так как входные данные - такого же формата.

54
00:11:14,775 --> 00:11:22,385
Объект хранится в GPU, так как используется модуль torch.cuda.

55
00:11:22,485 --> 00:11:36,740
Фукнция to_np() превращает полученный объект в массив numpy, иногда это удобнее.

56
00:11:36,840 --> 00:11:58,930
Функция работает и с тензорами, и с объектами класса Variable, ей неважно, находится пеменная в CPU или в GPU.

57
00:11:58,930 --> 00:12:18,970
Функция очень удобная, я использую её всегда, если не надо явно использовать GPU или доставать градиенты.

58
00:12:19,330 --> 00:12:39,700
numpy гораздо старше PyTorch и поэтому удобнее, к тому же с ним хорошо работают библиотеки вроде OpenCV и pandas.

59
00:12:39,700 --> 00:12:45,460
Поэтому я использую массивы numpy и перевожу их в формат PyTorch, когда

60
00:12:45,460 --> 00:12:53,410
нужно произвести вычисления в GPU или использовать градиенты, а потом перехожу обратно к массивам numpy.

61
00:12:53,410 --> 00:13:03,430
Так реализована библиотека fastai, что отличает её от других библиотек компьютерного зрения на основе PyTorch,

62
00:13:05,830 --> 00:13:11,767
использующих PyTorch везде, где это возможно. Вопрос?

63
00:13:11,867 --> 00:13:31,660
Вопрос из зала: Если я построил модель в GPU с PyTorch, функцию to_np() придётся использовать в цикле для каждого слоя?

64
00:13:31,660 --> 00:13:38,090
Хороший вопрос. Предсказания лучше делать в CPU, а не в GPU - это расширяет круг применения модели,

65
00:13:38,190 --> 00:13:44,045
нет необходимости разбивать данные на минибатчи и так далее.

66
00:13:44,145 --> 00:14:03,620
Модель переводится в CPU методом m.cpu(), аналогично для переменных.

67
00:14:03,720 --> 00:14:27,560
Если у вашей машины нет GPU, этого делать не надо, CPU будет использоваться автоматически.

68
00:14:27,660 --> 00:14:41,390
Вопрос из зала: Если модель обучена в GPU и сохранена, её надо особым образом загружать?

69
00:14:41,490 --> 00:14:51,990
Нет, не нужно, хотя это зависит от того, какие средства fastai вы используете, сейчас покажу.

70
00:14:51,990 --> 00:15:08,350
Один из наших студентов придумал, как избежать необходимости загружать модель с использованием того же GPU,

71
00:15:09,050 --> 00:15:24,160
в котором модель обучалась до сохранения. Это делается этой магической строкой.

72
00:15:28,819 --> 00:15:36,619
Для переноса модели из CPU в GPU используется метод m.cuda().

73
00:15:36,619 --> 00:15:50,509
Важно понимать работу функции zip() в Python, она позволяет проходить по нескольким массивам сразу.

74
00:15:50,509 --> 00:16:00,259
Я хочу получить массив пар "название фильма - смещение", поэтому соединяю массивы индексов фильмов и смещений

75
00:16:00,259 --> 00:16:11,289
и использую генератор списков Python для сопоставления имён и индексов.

76
00:16:11,289 --> 00:16:15,714
Я сортирую полученный список.

77
00:16:15,814 --> 00:16:33,539
Худший фильм - Battlefield Earth Джона Траволты, причём отрыв от других фильмов достаточно большой.

78
00:16:33,639 --> 00:16:46,844
Это - худший фильм всех времён по версии пользователей IMDB. Такой способ оценивания фильмов хорош,

79
00:16:46,944 --> 00:16:57,319
потому что он учитывает то, что некоторые люди просто ставят плохие оценки всем фильмам.

80
00:17:02,149 --> 00:17:10,399
После поправки на то, что средняя оценка у каждого человека разная и все люди смотрят разные фильмы,

81
00:17:11,689 --> 00:17:20,169
фильм Battlefield Earth получается худшим фильмом в истории человечества.

82
00:17:21,638 --> 00:17:29,360
Это способ посмотреть внутрь модели и визуализировать работу векторов смещения.

83
00:17:29,360 --> 00:17:59,520
Для сортировки списка я использовал лямбда-функцию, а до этого - itemgetter, здесь они работают одинаково.

84
00:17:59,620 --> 00:18:06,290
Лямбда-функции очень полезные, убедитесь, что вы умеете их писать.

85
00:18:06,290 --> 00:18:18,435
Во время работы функции sorted() лямбда-функция вызывается каждый раз при сравнении двух элементов.

86
00:18:18,535 --> 00:18:31,340
Выведем список в другом порядке. Shawshank Redemption, Godfather, Usual Suspects - отличные фильмы, я согласен с рейтингом.

87
00:18:35,120 --> 00:18:43,100
Это была демонстрация смещений для фильмов.

88
00:18:43,100 --> 00:18:50,205
Теперь давайте рассмотрим матрицы эмбеддинга.

89
00:18:50,305 --> 00:19:01,315
Матрица эмбеддинга для фильмов называется i, мы достаём её функцией m.i() и кладём в переменную movie_emb.

90
00:19:01,415 --> 00:19:08,670
Для каждого из 3000 самых популярных фильмов есть вектор эмбеддинга длиной 50.

91
00:19:08,770 --> 00:19:20,355
Если вы не Джеффри Хинтон, очень сложно представить 50-мерное пространство, поэтому мы перейдём в трёхмерное.

92
00:19:20,455 --> 00:19:29,155
Есть много алгоритмов уменьшения размерности, один из самых популярных -

93
00:19:29,255 --> 00:19:36,073
метод главных компонент, или PCA (principal component analysis).

94
00:19:36,173 --> 00:19:41,575
Это линейный алгоритм, такие алгоритмы обычно хорошо работают на матрицах эмбеддинга.

95
00:19:41,675 --> 00:19:54,510
Принцип его работы рассказывает Рейчел в курсе fast.ai Computational Linear Algebra.

96
00:19:54,610 --> 00:20:04,360
Это очень важный алгоритм, он очень похож на сингулярное разложение матриц (SVD).

97
00:20:04,460 --> 00:20:18,740
SVD иногда всплывает в глубоком обучении, с ним стоит разобраться, если вы заинтересованы в теории.

98
00:20:18,740 --> 00:20:26,220
Термины SVD, PCA, собственные векторы и собственные значения означают примерно одно и то же.

99
00:20:26,320 --> 00:20:33,750
Метод главных компонент реализован в sklearn.decomposition,

100
00:20:33,850 --> 00:20:41,540
соответствующий объект создаётся конструктором PCA(), в который передаётся желаемое количество измерений.

101
00:20:41,540 --> 00:20:48,380
Алгоритм найдёт три максимально различающиеся линейные комбинации пятидесяти измерений,

102
00:20:48,380 --> 00:20:53,180
которые передают максимальное количество информации.

103
00:20:53,690 --> 00:21:02,270
Это называется наилучшее приближение матрицы матрицей меньшего ранга.

104
00:21:02,270 --> 00:21:09,005
После работы алгоритма мы получаем матрицу размера 3х3000.

105
00:21:09,105 --> 00:21:21,615
После этого мы обрабатываем одну из трёх строк таблицы так, как обрабатывали вектор смещений.

106
00:21:21,715 --> 00:21:42,420
Мы не знаем, как работает PCA, поэтому давайте отсортируем данные по этой компоненте и посмотрим, что получится.

107
00:21:42,520 --> 00:22:11,315
По названиям фильмов я предполагаю, что эта компонента говорит, насколько фильм серьёзный или лёгкий для просмотра.

108
00:22:11,415 --> 00:22:25,060
Другого способа интерпретации нет, нужно смотреть на данные и искать в них смысл.

109
00:22:27,910 --> 00:22:35,465
Следующая компонента обрабатывается аналогичным образом.

110
00:22:35,565 --> 00:22:48,320
Похоже на то, что положительные значения отвечают за наличие диалогов, а отрицательные - за наличие компьютерной графики.

111
00:22:48,420 --> 00:23:13,574
Эта компонента показывает, что кто-то любит классические фильмы Вуди Аллена, а кто-то - зрелищные голливудские фильмы.

112
00:23:13,674 --> 00:23:21,930
По первой компоненте видно, что кто-то любит серьёзные фильмы, а кто-то - одноразовые боевики.

113
00:23:22,810 --> 00:23:28,892
Я надеюсь, идея понятна.

114
00:23:28,992 --> 00:23:43,990
Модель очень простая - перемножение двух матриц и алгоритм оптимизации, а умеет довольно много, это круто.

115
00:23:48,600 --> 00:24:04,640
Можно распределить фильмы на графике по первым двум компонентам, я нарисовал несколько фильмов.

116
00:24:04,740 --> 00:24:23,700
Сейчас я хочу поговорить о том, что происходит при вызове метода learn.fit().

117
00:24:24,850 --> 00:24:30,790
Вопрос из зала: Как интерпретировать матрицы эмбеддинга для задачи предсказания продаж?

118
00:24:30,790 --> 00:24:43,220
Как в соревновании Rossman? Сейчас покажу.

119
00:24:57,840 --> 00:25:18,790
На экране - иллюстрация из статьи Entity Embeddings of Categorical Variables, статья довольно простая на нашем уровне.

120
00:25:18,790 --> 00:25:23,260
Если вы прошли курс по машинному обучению, её будет ещё проще понять.

121
00:25:23,260 --> 00:25:39,550
Статья описывает слои эмбеддинга (entity embedding layers) как сочетание прямого кодирования и перемножения матриц.

122
00:25:39,550 --> 00:25:45,122
На картинке три слоя эмбеддинга, то есть три прямых кодирования, умноженных на соответствующие матрицы.

123
00:25:45,222 --> 00:25:56,040
Результат умножения подаётся на линейный слой (dense layer).

124
00:25:56,040 --> 00:26:21,610
Это одна из первых статей про использование эмбеддинга для категориальных признаков, поэтому она очень подробная.

125
00:26:21,610 --> 00:26:34,930
После обучения эмбеддингов с помощью нейронной сети авторы задумались, что можно с ними сделать.

126
00:26:34,930 --> 00:26:45,070
Они заменили категориальные переменные на вычисленные эмбеддинги

127
00:26:48,070 --> 00:27:07,920
и обучили с их помощью алгоритм градиентного бустинга над решающими деревьями (gradient boosted trees).

128
00:27:08,020 --> 00:27:14,775
Таким образом они выполнили отбор признаков, и он оказался удачным, как видите в таблице.

129
00:27:14,875 --> 00:27:25,960
Функция потерь MAPE (mean absolute percentage error) уменьшилась с 0.152 при прямом кодировании до 0.115 с эмбеддингами.

130
00:27:25,960 --> 00:27:36,650
Аналогичные результаты получены для алгоритмов случайный лес и метод ближайших соседей.

131
00:27:36,750 --> 00:27:56,195
Это значит, что вы можете обучить эмбеддинги признаков любой мощности с помощью нейронной сети,

132
00:27:56,295 --> 00:28:03,580
а потом все смогут использовать их в алгоритмах градиентного бустинга или случайного леса.

133
00:28:07,210 --> 00:28:14,670
Даже метод ближайших соседей даст неплохие результаты.

134
00:28:14,770 --> 00:28:30,730
Сотрудникам вашей компании не придётся проходить наш курс, но они смогут использовать мощь нейронных сетей.

135
00:28:30,730 --> 00:28:59,355
Эмбеддинги - это соответствия индексов и чисел, поэтому их удобно хранить в базе данных.

136
00:28:59,455 --> 00:29:05,115
Градиентный бустинг и случайный лес обучаются гораздо быстрее нейронных сетей,

137
00:29:05,215 --> 00:29:09,915
поэтому в сочетании с предвычисленными эмбеддингами они могут быть даже эффективнее нейронных сетей.

138
00:29:10,015 --> 00:29:26,790
Авторы статьи распределили штаты Германии в зависимости от значений первых двух главных компонент.

139
00:29:26,890 --> 00:29:41,600
Удивительным образом расположение городов примерно совпало с их географическим расположением,

140
00:29:41,600 --> 00:29:54,672
хотя в изначальных данных нет ни расстояния между штатами, ни географической привязки.

141
00:29:54,772 --> 00:30:10,190
Этот график показывает зависимость расстояния между двумя магазинами в пространстве эмбеддинга

142
00:30:10,190 --> 00:30:15,980
от географического расстояния между ними для всех возможных пар.

143
00:30:15,980 --> 00:30:36,050
Получилась красивая корреляция. Видимо, у рядом стоящих магазинов схожие характеристики продаж.

144
00:30:36,050 --> 00:30:48,170
Я нарисовал такие же графики для дней недели и месяцев и соединил точки временной линией.

145
00:30:48,170 --> 00:30:52,850
На графике месяцев видно, что точки летних месяцев стоят отдельно, точки весенних - отдельно.

146
00:30:58,640 --> 00:31:08,345
Визуализировать эмбеддинги интересно. Имеет смысл сначала проверить, отражают ли они очевидные закономерности,

147
00:31:08,445 --> 00:31:18,869
а потом попробовать разглядеть неочевидные на различных графиках.

148
00:31:18,969 --> 00:31:30,605
Эта техника ещё не изучена, поэтому я пока не могу указать область применения.

149
00:31:30,705 --> 00:31:44,830
Вопрос из зала: Есть другие способы создания эмбеддингов, например, N-граммы с пропусками (skip-grams). Какой лучше?

150
00:31:44,830 --> 00:31:49,679
N-граммы с пропусками специфичны для обработки естественного языка.

151
00:31:49,860 --> 00:32:16,720
Задачи по обработке естественного языка - обычно обучение без учителя, нет целевой переменной.

152
00:32:16,720 --> 00:32:22,210
Для перехода к задаче обучения с учителем можно создать целевую переменную.

153
00:32:22,310 --> 00:32:34,645
В Word2vec это было устроено так: из данных берётся правильное предложение из, например, 11 слов,

154
00:32:34,945 --> 00:32:53,500
и среднее слово в нём заменяется на случайное. Например, предложение

155
00:32:53,500 --> 00:33:00,334
"Милая маленькая кошечка сидела на пушистом коврике" превращалось в "Милая маленькая справедливость сидела на пушистом коврике".

156
00:33:00,434 --> 00:33:14,365
В итоге есть два предложения - изначальное и с заменённым словом.

157
00:33:14,465 --> 00:33:25,930
Изначальное предложение помечается единицей, изменённое - нулём.

158
00:33:26,030 --> 00:33:40,800
На таких данных была построена модель машинного обучения по поиску неправильных предложений.

159
00:33:40,800 --> 00:33:45,113
Нет необходимости уметь искать неправильные предложения, важны вычисленные в процессе обучения эмбеддинги.

160
00:33:45,213 --> 00:33:50,360
Эти эмбеддинги используются в других задачах, так работает Word2vec.

161
00:33:50,720 --> 00:34:01,860
Если реализовать это без глубокого обучения как умножение матриц, обучение будет очень быстрым.

162
00:34:01,860 --> 00:34:15,989
При использовании неглубокого обучения теряется точность предсказаний. Из преимуществ -

163
00:34:15,989 --> 00:34:27,265
модель можно обучать на огромных датасетах, а полученные эмбеддинги будут близки к линейным,

164
00:34:27,365 --> 00:34:33,715
поэтому их можно будет складывать и вычитать.

165
00:34:33,815 --> 00:34:52,710
Если нам нужна линейность эмбеддингов категориальных переменных,

166
00:34:52,710 --> 00:34:57,810
например, для использования метода ближайших соседей,

167
00:34:57,810 --> 00:35:03,990
то для их вычисления стоит использовать неглубокое обучение.

168
00:35:03,990 --> 00:35:09,300
Если хочется большей точности предсказаний, нужно использовать нейронные сети.

169
00:35:09,300 --> 00:35:19,495
Я считаю, что в обработке естественного языка нужно отбросить линейные Word2vec и Glove,

170
00:35:19,595 --> 00:35:25,260
потому что получаемые с их помощью эмбеддинги сильно уступают моделям глубокого обучения.

171
00:35:25,260 --> 00:35:30,520
Модель обработки естественного языка с прошлой лекции для анализа тональностей

172
00:35:30,750 --> 00:35:35,790
основывалась на рекуррентной нейронной сети, а не на Word2vec, и в результате обучения

173
00:35:35,790 --> 00:35:44,620
мы получили не только эмбеддинги, но и предобученную модель для других задач.

174
00:35:44,720 --> 00:35:51,010
Вопрос из зала: То есть для создания эмбеддингов нужна фиктивная задача обучения?

175
00:35:51,110 --> 00:35:59,810
Не обязательно фиктивная - мы создали эмбеддинги для соревнования Rossman, пытаясь предсказать реальные продажи.

176
00:35:59,910 --> 00:36:06,335
Идея более общая - для создания любого пространства признаков, не обязательно эмбеддингов,

177
00:36:06,435 --> 00:36:16,200
нужны либо размеченные данные, либо фиктивная задача обучения.

178
00:36:16,200 --> 00:36:26,550
Вопрос из зала: Как выбрать хорошую фиктивную задачу?

179
00:36:26,550 --> 00:36:31,050
Отличный вопрос, к сожалению, этот вопрос мало изучен.

180
00:36:31,050 --> 00:36:45,030
Мало кто хотя бы понимает, что обучение без учителя - это обучение с учителем на фиктивных задачах.

181
00:36:45,030 --> 00:36:49,650
Я не видел статей на эту тему.

182
00:36:49,650 --> 00:37:05,665
Вам нужна задача, которая позволит лучше изучить те закономерности в данных, которые вам нужны.

183
00:37:05,765 --> 00:37:19,930
Например, в компьютерном зрении используют дополнение данных, искажающее реальность -

184
00:37:20,030 --> 00:37:37,200
например, заменяют цвета на неестественные. Нейронную сеть обучают находить неискажённые изображения.

185
00:37:37,200 --> 00:37:42,565
Вопрос очень интересный, хотелось бы в нём разобраться.

186
00:37:42,665 --> 00:37:47,310
Будет классно, если кто-то из вас скачает несколько датасетов для обучения без учителя или частичного обучения

187
00:37:47,310 --> 00:37:59,640
и посмотрит, как на качество модели влияют разные фиктивные задачи обучения.

188
00:37:59,640 --> 00:38:09,869
Не можете придумать гениальную фиктивную задачу - поставьте хоть какую-нибудь, многого не требуется.

189
00:38:09,869 --> 00:38:15,119
Показательно топорная фиктивная задача - автокодировщик,

190
00:38:15,119 --> 00:38:23,965
его использовали победители недавнего соревнования Kaggle по предсказанию страховых возмещений.

191
00:38:24,065 --> 00:38:28,675
В данных были страховые полисы с известными страховыми возмещениями

192
00:38:28,775 --> 00:38:37,090
и страховые полисы, выплаты по которым ещё не были произведены и поэтому неизвстны.

193
00:38:37,190 --> 00:38:44,880
На вход нейронной сети для обучения подавались данные о страховом полисе,

194
00:38:44,980 --> 00:38:52,733
и на выходе сеть должна была воссоздать эту информацию.

195
00:38:52,833 --> 00:39:01,585
При этом хотя бы в одном скрытом слоё должно быть меньше активаций, чем во входных данных.

196
00:39:01,685 --> 00:39:13,590
Например, если страховой полис определяется 100 признаками, в середине должен быть слой с 20 активациями.

197
00:39:13,590 --> 00:39:23,970
Такая постановка задачи не требует дополнительного кода, можно использовать модели PyTorch или fastai,

198
00:39:25,619 --> 00:39:31,710
просто передав в качестве целевой переменной начальные данные.

199
00:39:31,710 --> 00:39:39,490
Автокодировщик - самая простая фиктивная задача из возможных, и работает он на удивление хорошо.

200
00:39:39,590 --> 00:39:44,392
Настолько хорошо, что благодаря ему люди смогли выиграть соревнование на Kaggle.

201
00:39:44,492 --> 00:39:53,155
Победители использовали созданные автокодировщиком признаки для обучения другой нейронной сети.

202
00:39:53,255 --> 00:40:06,024
Если наберётся достаточно заинтересованных, мы обсудим обучение без учителя во второй части курса.

203
00:40:09,424 --> 00:40:24,760
Вопрос из зала: Полезна ли будет модель, обученная на данных ArXiv, для работы с датасетом IMDb?

204
00:40:24,860 --> 00:40:36,839
Отличный вопрос. Мы обсуждали это с Себастьяном и планировали заняться этим в январе.

205
00:40:36,839 --> 00:40:47,877
В компьютерном зрении предобученные на кошках и собаках модели отлично работают для рентген-диагностики,

206
00:40:48,577 --> 00:40:53,220
но этому пока нет аналога в обработке естественного языка.

207
00:40:53,320 --> 00:41:01,680
Людям кажется, что это не сработает, поэтому они даже не пробуют, хотя я считаю это перспективным.

208
00:41:04,390 --> 00:41:17,810
Мне было интересно качество нашей модели для соревнования Rossman,

209
00:41:17,810 --> 00:41:23,410
потому что в публичном рейтинге она выглядела неубедительно.

210
00:41:23,510 --> 00:41:30,690
Ещё мне было интересны результаты на настоящей тестовой выборке.

211
00:41:30,690 --> 00:41:46,650
Я добавил обработку тестовой выборки в Jupyter ноутбук lesson3-rossman.ipynb.

212
00:41:46,650 --> 00:42:00,380
Я мог обернуть этот код в метод и вызвать его дважды, а не городить копии кода, но решил оставить так,

213
00:42:00,380 --> 00:42:08,010
чтобы вы видели все этапы и спободнее экспериментировали.

214
00:42:08,010 --> 00:42:24,480
Обработка одинаковая для обучающей и тестовой выборке, кое-где я использовал циклы.

215
00:42:24,480 --> 00:42:34,140
Ячейки [34] и [56] выполняются не подряд. Сначала выполняется ячейка [34], ячейка [56] пропускается

216
00:42:34,140 --> 00:42:45,900
и выполняется код под ними, а потом нужно вернуться к ячейке [56] и проделать то же самое для неё.

217
00:42:45,900 --> 00:42:49,950
На этой неделе меня спрашивали, почему код не работает, и это важное напоминание о том,

218
00:42:52,260 --> 00:43:00,180
что не стоит бездумно выполнять все ячейки подряд.

219
00:43:03,960 --> 00:43:15,680
В Jupyter ноутбуках к первым лекциям можно было так делать, но сейчас пора думать о том, что вы делаете.

220
00:43:15,780 --> 00:43:21,275
Вопрос из зала: Чем неглубокое обучение отличается от глубокого обучения?

221
00:43:21,375 --> 00:43:32,975
В неглубоком обучении нет скрытых слоёв, только скалярное произведение векторов или произведение матриц.

222
00:43:35,975 --> 00:43:50,040
Итак, после обработки у нас есть обучающая и тестовая выборка, остальное без изменений.

223
00:43:50,040 --> 00:43:58,000
Многое из этого Jupyter ноутбука не относится к глубокому обучению, поэтому обсуждается в курсе "Машинное обучение".

224
00:43:58,100 --> 00:44:10,040
Вместо функции train_cats() используется apply_cats(), чтобы у выборок была общая нумерация категориальных признаков.

225
00:44:12,815 --> 00:44:27,440
Объект mapper, содержащий средние и стандартные отклонения, тоже должен быть общим.

226
00:44:27,440 --> 00:44:46,665
После этого тестовая выборка передаётся в модель привычным образом, обучение проходит как обычно.

227
00:44:46,765 --> 00:44:57,710
Для получения предсказаний используется метод m.predict, параметр True указывает на наличие тестовой выборки.

228
00:44:57,710 --> 00:45:03,650
Полученные предсказания отправляются на Kaggle.

229
00:45:03,650 --> 00:45:38,890
Результат интересный. В публичном рейтинге я занял бы примерно трёхсотое место, а в приватном - пятое.

230
00:45:38,890 --> 00:45:56,420
Если вы участвуете в соревновании и не работали над валидационной выборкой, с вами может случиться обратное.

231
00:45:56,420 --> 00:46:01,970
Например, в соревновании по распознаванию айсбергов валидационная выборка публичного рейтинга

232
00:46:04,940 --> 00:46:10,640
содержит много дополненных до бессмыслицы данных.

233
00:46:10,640 --> 00:46:21,859
Тут созданная вами валидационная выборка принесёт гораздо больше пользы, чем место в публичном рейтинге.

234
00:46:21,859 --> 00:46:35,820
Значение функции потерь на наших предсказаниях близко к третьему месту, я считаю, что мы поняли их подход.

235
00:46:40,700 --> 00:46:53,940
После соревнования Rossman люди начали писать о своих моделях, есть много полезных замечаний.

236
00:46:54,040 --> 00:47:02,480
Например, здесь авторы разделили продажи отдельных магазинов на продажи в воскресенье и другие дни,

237
00:47:02,480 --> 00:47:11,870
и выяснилось, что у некоторых магазинов продажи в воскресенье сильно отличаются.

238
00:47:11,870 --> 00:47:16,455
Такие описания помогают лучше понять, как и зачем анализировать ваши данные.

239
00:47:16,555 --> 00:47:29,990
Вот интересная закономерность, которую не учли обладатели третьего места.

240
00:47:29,990 --> 00:47:48,480
Видны всплески в продажах сразу перед закрытием и сразу после открытия.

241
00:47:48,580 --> 00:47:57,230
Обладатели третьего места удалили все данные с закрытыми магазинами, а зря.

242
00:47:57,230 --> 00:48:05,500
Не меняйте ваши данные, пока не убедитесь в том, что понимаете, что делаете.

243
00:48:05,600 --> 00:48:15,260
Я не проверял своё предположение, но думаю, что с учётом этой закономерности они бы победили.

244
00:48:15,260 --> 00:48:22,335
Насколько мне известно, в тестовой выборке не было закрытых магазинов, но их все равно стоило учесть.

245
00:48:22,435 --> 00:48:42,869
Если не обучать модель на выбросах, она не научится хорошо их предсказывать, и это её ухудшит.

246
00:48:42,869 --> 00:48:55,149
Давайте ещё раз посмотрим на код библиотеки, чтобы лучше понять модель Rossman.

247
00:49:02,559 --> 00:49:08,689
Я хочу убедиться, что вы ориентируетесь в коде и сможете разобраться сами.

248
00:49:08,789 --> 00:49:27,999
Давайте изучим класс ColumnarModelData. Раньше я показывал, как с помощью "??" смотреть исходный код.

249
00:49:31,259 --> 00:49:36,649
Иногда после чтения кода нужно уточнить код одной из составляющих, поэтому это не всегда удобно.

250
00:49:36,749 --> 00:49:50,674
Скорее всего, ваш текстовый редактор может открывать файлы по SSH и перемещаться по ним.

251
00:49:50,774 --> 00:50:08,285
В Vim я могу перейти к реализации класса ColumnarModelData командой :tag ColumnarModelData.

252
00:50:08,385 --> 00:50:19,569
Наведя курсор на класс DataLoader, я могу перейти к его реализации комбинацией клавиш Ctrl+|.

253
00:50:19,569 --> 00:50:26,170
Вернуться назад можно комбинацией клавиш Ctrl+t.

254
00:50:26,170 --> 00:50:41,140
Навигация по местам использования класса осуществляется звёздочкой *.

255
00:50:41,140 --> 00:50:59,420
Сейчас мы хотим изучить функцию get_learner(), которая возвращает модель.

256
00:50:59,420 --> 00:51:23,129
Модель PyTorch класса MixedInputModel оборачивается в конструктор fastai StructuredLearner(), соединяющий данные и модель.

257
00:51:23,229 --> 00:51:30,200
Реализацию класса MixedInputModel можно увидеть, нажав Ctrl+].

258
00:51:34,519 --> 00:51:42,890
Почти всю реализацию класса мы уже обсуждали.

259
00:51:42,890 --> 00:52:02,919
В функцию get_learner() мы передали размеры эмбеддингов emb_szs. Вопрос?

260
00:52:03,119 --> 00:52:15,710
Вопрос из зала: Конструктор класса MixedInputModel всегда ожидает и категориальные, и количественные признаки?

261
00:52:15,410 --> 00:52:30,079
Да, и если есть признаки только одного типа, модель заменит их столбцом нулей и все равно будет работать.

262
00:52:30,079 --> 00:52:49,720
Реализация некрасивая, мы надеемся её исправить, но да, можно передать пустой список признаков одного типа.

263
00:52:49,720 --> 00:52:59,234
Я не исправляю некоторые некрасивые вещи, потому что с выходом PyTorch 0.4 не только исчезнет класс Variable,

264
00:52:59,334 --> 00:53:20,509
но и добавится поддержка тензоров нулевого ранга как элементов тензоров первого ранга.

265
00:53:20,509 --> 00:53:29,019
Благодаря этому упростится код некоторых классов, поэтому мы не улучшаем его сейчас.

266
00:53:30,400 --> 00:53:42,549
Вопрос из зала: Как писать классы, немного отличающиеся от классов fastai?

267
00:53:43,229 --> 00:53:53,779
Мы затронем это на следующей неделе, но основная работа с этим будет во второй части курса.

268
00:53:53,779 --> 00:54:00,710
Во второй части будет много интересного, например, генеративные модели, выдающие целые предложения или изображения.

269
00:54:00,710 --> 00:54:12,539
Также мы обсудим, как изменять модели fastai или создавать новые модели на их основе -

270
00:54:12,639 --> 00:54:20,609
если будет время, начнём со следующей недели.

271
00:54:20,709 --> 00:54:26,984
Итак, в функцию get_learner() передаются размеры эмбеддингов emb_szs,

272
00:54:27,084 --> 00:54:32,569
это просто количества строк и столбцов в каждой матрице эмбеддинга.

273
00:54:32,569 --> 00:54:46,754
Количество строк - это мощность признака, количество столбцов - половина мощности, но не больше 50.

274
00:54:46,854 --> 00:55:05,869
Этот массив пар передаётся в конструктор класса, где конструктором списков по каждой паре размеров создаётся эмбеддинг.

275
00:55:05,869 --> 00:55:45,960
Конструктор nn.ModuleList() превращает полученный массив в список слоёв, воспринимаемый моделью.

276
00:55:46,060 --> 00:55:56,960
После создания списка эмбеддингов аналогично создаётся список линейных слоёв.

277
00:55:56,960 --> 00:56:06,320


278
00:56:06,320 --> 00:56:13,400


279
00:56:13,400 --> 00:56:19,610


280
00:56:19,610 --> 00:56:24,710


281
00:56:24,710 --> 00:56:28,220


282
00:56:30,830 --> 00:56:36,380


283
00:56:36,380 --> 00:56:44,750


284
00:56:44,750 --> 00:56:48,920


285
00:56:48,920 --> 00:56:52,970


286
00:56:55,910 --> 00:57:00,470


287
00:57:00,470 --> 00:57:08,180


288
00:57:08,180 --> 00:57:11,900


289
00:57:11,900 --> 00:57:16,550


290
00:57:16,550 --> 00:57:22,250


291
00:57:22,250 --> 00:57:29,240


292
00:57:29,240 --> 00:57:33,940


293
00:57:33,940 --> 00:57:40,160


294
00:57:40,160 --> 00:57:49,880


295
00:57:49,880 --> 00:57:55,010


296
00:57:55,010 --> 00:57:59,960


297
00:57:59,960 --> 00:58:03,260


298
00:58:03,260 --> 00:58:07,730


299
00:58:07,730 --> 00:58:11,120


300
00:58:11,120 --> 00:58:17,890


301
00:58:17,890 --> 00:58:25,730


302
00:58:25,730 --> 00:58:30,950


303
00:58:30,950 --> 00:58:37,580


304
00:58:37,580 --> 00:58:43,250


305
00:58:43,250 --> 00:58:47,960


306
00:58:47,960 --> 00:58:51,200


307
00:58:51,200 --> 00:58:56,630


308
00:58:56,630 --> 00:59:04,210


309
00:59:04,210 --> 00:59:09,320


310
00:59:09,320 --> 00:59:12,500


311
00:59:12,500 --> 00:59:19,400


312
00:59:19,400 --> 00:59:27,580


313
00:59:30,640 --> 00:59:35,319


314
00:59:35,319 --> 00:59:42,160


315
00:59:42,160 --> 00:59:47,650


316
00:59:47,650 --> 00:59:56,160


317
00:59:56,670 --> 01:00:02,079


318
01:00:02,079 --> 01:00:06,760


319
01:00:06,760 --> 01:00:13,390


320
01:00:17,549 --> 01:00:26,500


321
01:00:26,500 --> 01:00:32,160


322
01:00:32,160 --> 01:00:38,710


323
01:00:38,710 --> 01:00:46,750


324
01:00:46,750 --> 01:00:56,019


325
01:00:56,019 --> 01:01:00,609


326
01:01:00,609 --> 01:01:05,500


327
01:01:05,500 --> 01:01:10,990


328
01:01:10,990 --> 01:01:22,000


329
01:01:22,000 --> 01:01:25,990


330
01:01:25,990 --> 01:01:30,130


331
01:01:30,130 --> 01:01:33,849


332
01:01:33,849 --> 01:01:36,339


333
01:01:36,339 --> 01:01:39,970


334
01:01:39,970 --> 01:01:44,020


335
01:01:44,020 --> 01:01:48,250


336
01:01:48,250 --> 01:01:54,160


337
01:01:54,160 --> 01:01:59,110


338
01:01:59,110 --> 01:02:02,500


339
01:02:02,500 --> 01:02:07,420


340
01:02:07,420 --> 01:02:13,510


341
01:02:13,510 --> 01:02:21,520


342
01:02:24,520 --> 01:02:29,560


343
01:02:29,560 --> 01:02:35,350


344
01:02:35,350 --> 01:02:38,110


345
01:02:38,110 --> 01:02:41,260


346
01:02:41,260 --> 01:02:45,700


347
01:02:47,440 --> 01:02:51,970


348
01:02:53,980 --> 01:03:00,550


349
01:03:00,550 --> 01:03:03,940


350
01:03:03,940 --> 01:03:07,140


351
01:03:07,140 --> 01:03:14,530


352
01:03:14,530 --> 01:03:19,060


353
01:03:20,560 --> 01:03:31,090


354
01:03:31,090 --> 01:03:40,600


355
01:03:42,520 --> 01:03:45,570


356
01:03:45,570 --> 01:03:52,390


357
01:03:56,330 --> 01:04:01,040


358
01:04:01,040 --> 01:04:05,420


359
01:04:05,420 --> 01:04:10,160


360
01:04:10,160 --> 01:04:15,950


361
01:04:15,950 --> 01:04:21,050


362
01:04:21,050 --> 01:04:26,090


363
01:04:26,090 --> 01:04:32,180


364
01:04:34,130 --> 01:04:38,210


365
01:04:38,210 --> 01:04:43,790


366
01:04:43,790 --> 01:04:48,830


367
01:04:48,830 --> 01:04:57,620


368
01:04:57,620 --> 01:05:02,390


369
01:05:02,390 --> 01:05:07,880


370
01:05:07,880 --> 01:05:14,530


371
01:05:14,530 --> 01:05:21,380


372
01:05:21,380 --> 01:05:26,750


373
01:05:26,750 --> 01:05:31,340


374
01:05:31,340 --> 01:05:37,030


375
01:05:37,030 --> 01:05:43,220


376
01:05:45,980 --> 01:05:51,950


377
01:05:51,950 --> 01:05:56,560


378
01:05:56,560 --> 01:06:00,830


379
01:06:00,830 --> 01:06:05,960


380
01:06:05,960 --> 01:06:08,430


381
01:06:08,430 --> 01:06:12,510


382
01:06:12,510 --> 01:06:18,860


383
01:06:18,860 --> 01:06:25,710


384
01:06:25,710 --> 01:06:34,260


385
01:06:34,260 --> 01:06:38,850


386
01:06:41,850 --> 01:06:48,600


387
01:06:48,600 --> 01:06:53,430


388
01:06:53,430 --> 01:06:56,730


389
01:06:56,730 --> 01:07:04,320


390
01:07:04,320 --> 01:07:10,740


391
01:07:12,960 --> 01:07:18,060


392
01:07:18,060 --> 01:07:23,970


393
01:07:23,970 --> 01:07:28,230


394
01:07:28,230 --> 01:07:32,760


395
01:07:32,760 --> 01:07:37,920


396
01:07:37,920 --> 01:07:41,400


397
01:07:41,400 --> 01:07:46,260


398
01:07:46,260 --> 01:07:52,770


399
01:07:54,810 --> 01:08:02,670


400
01:08:02,670 --> 01:08:07,590


401
01:08:07,590 --> 01:08:14,820


402
01:08:14,820 --> 01:08:20,370


403
01:08:20,370 --> 01:08:24,029


404
01:08:24,029 --> 01:08:30,119


405
01:08:33,238 --> 01:08:40,249


406
01:08:40,250 --> 01:08:46,739


407
01:08:46,738 --> 01:08:52,198


408
01:08:52,198 --> 01:08:55,408


409
01:08:55,408 --> 01:08:59,218


410
01:08:59,219 --> 01:09:04,710


411
01:09:04,710 --> 01:09:09,779


412
01:09:09,779 --> 01:09:15,839


413
01:09:19,439 --> 01:09:29,629


414
01:09:29,630 --> 01:09:34,520


415
01:09:34,549 --> 01:09:41,960


416
01:09:41,960 --> 01:09:47,339


417
01:09:47,339 --> 01:09:49,799


418
01:09:49,799 --> 01:09:54,599


419
01:09:54,599 --> 01:09:59,909


420
01:10:02,159 --> 01:10:07,800


421
01:10:07,800 --> 01:10:16,110


422
01:10:18,119 --> 01:10:21,570


423
01:10:21,570 --> 01:10:26,760


424
01:10:26,760 --> 01:10:31,469


425
01:10:31,469 --> 01:10:35,700


426
01:10:35,700 --> 01:10:41,130


427
01:10:41,130 --> 01:10:44,370


428
01:10:44,370 --> 01:10:48,780


429
01:10:48,780 --> 01:10:55,640


430
01:10:55,640 --> 01:10:59,790


431
01:10:59,790 --> 01:11:04,620


432
01:11:04,620 --> 01:11:10,620


433
01:11:10,620 --> 01:11:15,630


434
01:11:15,630 --> 01:11:19,710


435
01:11:19,710 --> 01:11:23,550


436
01:11:23,550 --> 01:11:29,670


437
01:11:29,670 --> 01:11:34,890


438
01:11:34,890 --> 01:11:38,520


439
01:11:38,520 --> 01:11:42,030


440
01:11:43,530 --> 01:11:47,370


441
01:11:47,370 --> 01:11:53,310


442
01:11:53,310 --> 01:11:57,030


443
01:12:02,250 --> 01:12:06,840


444
01:12:06,840 --> 01:12:10,950


445
01:12:10,950 --> 01:12:15,780


446
01:12:15,780 --> 01:12:19,530


447
01:12:19,530 --> 01:12:23,190


448
01:12:23,190 --> 01:12:27,180


449
01:12:27,180 --> 01:12:30,600


450
01:12:30,600 --> 01:12:35,310


451
01:12:35,310 --> 01:12:39,630


452
01:12:39,630 --> 01:12:45,420


453
01:12:45,420 --> 01:12:49,550


454
01:12:49,550 --> 01:12:54,600


455
01:12:54,600 --> 01:12:58,890


456
01:12:58,890 --> 01:13:07,890


457
01:13:07,890 --> 01:13:15,330


458
01:13:15,330 --> 01:13:23,969


459
01:13:23,969 --> 01:13:35,489


460
01:13:38,670 --> 01:13:43,980


461
01:13:47,960 --> 01:13:56,340


462
01:13:56,340 --> 01:14:02,340


463
01:14:02,340 --> 01:14:06,660


464
01:14:06,660 --> 01:14:11,969


465
01:14:11,969 --> 01:14:17,040


466
01:14:17,040 --> 01:14:23,940


467
01:14:23,940 --> 01:14:29,670


468
01:14:29,670 --> 01:14:35,760


469
01:14:35,760 --> 01:14:41,250


470
01:14:41,250 --> 01:14:45,120


471
01:14:45,120 --> 01:14:50,190


472
01:14:50,190 --> 01:14:56,940


473
01:14:56,940 --> 01:15:02,520


474
01:15:02,520 --> 01:15:06,540


475
01:15:08,450 --> 01:15:12,540


476
01:15:12,540 --> 01:15:17,520


477
01:15:17,520 --> 01:15:21,300


478
01:15:21,300 --> 01:15:29,520


479
01:15:29,520 --> 01:15:35,910


480
01:15:39,720 --> 01:15:43,950


481
01:15:43,950 --> 01:15:47,910


482
01:15:47,910 --> 01:15:51,900


483
01:15:51,900 --> 01:15:56,070


484
01:15:56,070 --> 01:16:02,880


485
01:16:02,880 --> 01:16:08,460


486
01:16:08,460 --> 01:16:13,380


487
01:16:13,380 --> 01:16:18,720


488
01:16:18,720 --> 01:16:21,570


489
01:16:26,070 --> 01:16:30,780


490
01:16:30,780 --> 01:16:34,620


491
01:16:34,620 --> 01:16:38,670


492
01:16:38,670 --> 01:16:42,720


493
01:16:42,720 --> 01:16:45,960


494
01:16:45,960 --> 01:16:50,430


495
01:16:50,430 --> 01:16:56,100


496
01:16:56,100 --> 01:17:01,320


497
01:17:03,390 --> 01:17:08,070


498
01:17:09,270 --> 01:17:15,420


499
01:17:15,420 --> 01:17:17,580


500
01:17:17,580 --> 01:17:21,810


501
01:17:21,810 --> 01:17:28,020


502
01:17:28,020 --> 01:17:33,240


503
01:17:34,920 --> 01:17:40,770


504
01:17:40,770 --> 01:17:46,260


505
01:17:46,260 --> 01:17:51,800


506
01:17:51,800 --> 01:17:59,190


507
01:18:02,220 --> 01:18:06,360


508
01:18:07,500 --> 01:18:14,250


509
01:18:14,250 --> 01:18:18,930


510
01:18:18,930 --> 01:18:25,640


511
01:18:25,640 --> 01:18:33,810


512
01:18:33,810 --> 01:18:39,390


513
01:18:39,390 --> 01:18:43,680


514
01:18:43,680 --> 01:18:50,700


515
01:18:54,660 --> 01:18:59,490


516
01:18:59,490 --> 01:19:04,020


517
01:19:04,020 --> 01:19:08,400


518
01:19:09,720 --> 01:19:15,000


519
01:19:15,000 --> 01:19:23,880


520
01:19:23,880 --> 01:19:27,030


521
01:19:27,030 --> 01:19:31,670


522
01:19:31,670 --> 01:19:36,410


523
01:19:36,410 --> 01:19:39,410


524
01:19:39,410 --> 01:19:45,050


525
01:19:45,050 --> 01:19:50,230


526
01:19:53,600 --> 01:19:56,600


527
01:19:56,600 --> 01:20:02,960


528
01:20:02,960 --> 01:20:05,690


529
01:20:05,690 --> 01:20:10,520


530
01:20:10,520 --> 01:20:18,350


531
01:20:18,350 --> 01:20:23,090


532
01:20:23,090 --> 01:20:29,240


533
01:20:29,240 --> 01:20:34,760


534
01:20:34,760 --> 01:20:39,530


535
01:20:46,160 --> 01:20:51,050


536
01:20:51,050 --> 01:20:56,030


537
01:20:56,030 --> 01:21:02,390


538
01:21:02,390 --> 01:21:08,090


539
01:21:08,090 --> 01:21:15,110


540
01:21:17,210 --> 01:21:20,180


541
01:21:20,180 --> 01:21:29,030


542
01:21:33,350 --> 01:21:41,200


543
01:21:42,600 --> 01:21:48,390


544
01:21:48,390 --> 01:21:54,720


545
01:21:54,720 --> 01:22:08,010


546
01:22:08,010 --> 01:22:13,560


547
01:22:13,560 --> 01:22:20,060


548
01:22:20,060 --> 01:22:23,430


549
01:22:23,430 --> 01:22:30,330


550
01:22:30,330 --> 01:22:36,540


551
01:22:36,540 --> 01:22:42,180


552
01:22:42,180 --> 01:22:50,100


553
01:22:50,100 --> 01:22:54,390


554
01:22:54,390 --> 01:22:58,560


555
01:22:58,560 --> 01:23:02,250


556
01:23:02,250 --> 01:23:05,970


557
01:23:05,970 --> 01:23:09,390


558
01:23:09,390 --> 01:23:13,200


559
01:23:15,810 --> 01:23:22,830


560
01:23:24,020 --> 01:23:30,540


561
01:23:33,170 --> 01:23:38,430


562
01:23:38,430 --> 01:23:43,110


563
01:23:44,940 --> 01:23:52,650


564
01:23:52,650 --> 01:23:55,679


565
01:23:55,679 --> 01:24:00,659


566
01:24:00,659 --> 01:24:04,559


567
01:24:04,559 --> 01:24:08,459


568
01:24:08,459 --> 01:24:17,909


569
01:24:17,909 --> 01:24:21,929


570
01:24:21,929 --> 01:24:28,559


571
01:24:28,559 --> 01:24:36,840


572
01:24:36,840 --> 01:24:41,070


573
01:24:41,070 --> 01:24:46,110


574
01:24:46,110 --> 01:24:53,909


575
01:24:53,909 --> 01:25:01,639


576
01:25:01,639 --> 01:25:05,519


577
01:25:05,519 --> 01:25:23,249


578
01:25:23,249 --> 01:25:33,749


579
01:25:33,749 --> 01:25:40,949


580
01:25:40,949 --> 01:25:46,289


581
01:25:46,289 --> 01:25:52,739


582
01:25:52,739 --> 01:25:59,030


583
01:25:59,030 --> 01:26:08,550


584
01:26:08,550 --> 01:26:11,159


585
01:26:18,299 --> 01:26:21,899


586
01:26:21,899 --> 01:26:27,449


587
01:26:28,709 --> 01:26:32,729


588
01:26:32,729 --> 01:26:38,579


589
01:26:41,249 --> 01:26:45,869


590
01:26:45,869 --> 01:26:51,299


591
01:26:51,299 --> 01:26:58,319


592
01:26:58,319 --> 01:27:05,609


593
01:27:05,609 --> 01:27:09,689


594
01:27:09,689 --> 01:27:15,359


595
01:27:15,359 --> 01:27:22,499


596
01:27:24,089 --> 01:27:29,369


597
01:27:29,369 --> 01:27:33,029


598
01:27:33,029 --> 01:27:36,599


599
01:27:36,599 --> 01:27:39,959


600
01:27:39,959 --> 01:27:44,189


601
01:27:44,189 --> 01:27:48,659


602
01:27:51,149 --> 01:27:53,819


603
01:27:53,819 --> 01:28:00,599


604
01:28:00,599 --> 01:28:06,899


605
01:28:06,899 --> 01:28:11,819


606
01:28:11,819 --> 01:28:18,959


607
01:28:20,609 --> 01:28:23,010


608
01:28:23,010 --> 01:28:29,519


609
01:28:29,519 --> 01:28:34,650


610
01:28:37,110 --> 01:28:42,480


611
01:28:42,480 --> 01:28:46,769


612
01:28:46,769 --> 01:28:51,920


613
01:28:51,920 --> 01:29:07,769


614
01:29:07,769 --> 01:29:12,539


615
01:29:17,250 --> 01:29:21,510


616
01:29:21,510 --> 01:29:30,239


617
01:29:32,489 --> 01:29:36,960


618
01:29:36,960 --> 01:29:43,710


619
01:29:43,710 --> 01:29:52,619


620
01:29:52,619 --> 01:30:01,980


621
01:30:01,980 --> 01:30:07,980


622
01:30:07,980 --> 01:30:16,079


623
01:30:16,079 --> 01:30:24,000


624
01:30:24,000 --> 01:30:30,869


625
01:30:30,869 --> 01:30:35,970


626
01:30:35,970 --> 01:30:39,180


627
01:30:39,180 --> 01:30:46,290


628
01:30:46,290 --> 01:30:50,460


629
01:30:50,460 --> 01:30:56,400


630
01:30:56,400 --> 01:31:01,770


631
01:31:01,770 --> 01:31:05,250


632
01:31:05,250 --> 01:31:09,330


633
01:31:09,330 --> 01:31:19,920


634
01:31:19,920 --> 01:31:25,500


635
01:31:25,500 --> 01:31:29,790


636
01:31:29,790 --> 01:31:32,970


637
01:31:32,970 --> 01:31:40,200


638
01:31:40,200 --> 01:31:49,110


639
01:31:49,110 --> 01:31:54,360


640
01:31:54,360 --> 01:32:00,810


641
01:32:00,810 --> 01:32:05,130


642
01:32:05,130 --> 01:32:09,000


643
01:32:09,000 --> 01:32:16,470


644
01:32:16,470 --> 01:32:24,870


645
01:32:24,870 --> 01:32:28,710


646
01:32:28,710 --> 01:32:33,570


647
01:32:33,570 --> 01:32:38,490


648
01:32:38,490 --> 01:32:43,430


649
01:32:43,430 --> 01:32:49,889


650
01:32:49,889 --> 01:32:53,579


651
01:32:53,579 --> 01:32:58,349


652
01:32:58,349 --> 01:33:06,959


653
01:33:06,959 --> 01:33:11,369


654
01:33:11,369 --> 01:33:14,059


655
01:33:14,189 --> 01:33:18,209


656
01:33:23,309 --> 01:33:27,059


657
01:33:27,059 --> 01:33:30,239


658
01:33:33,209 --> 01:33:36,929


659
01:33:36,929 --> 01:33:40,619


660
01:33:40,619 --> 01:33:50,939


661
01:33:50,939 --> 01:33:56,429


662
01:33:56,429 --> 01:34:01,530


663
01:34:01,530 --> 01:34:08,280


664
01:34:08,280 --> 01:34:16,169


665
01:34:16,169 --> 01:34:22,050


666
01:34:22,050 --> 01:34:32,459


667
01:34:32,459 --> 01:34:38,579


668
01:34:38,579 --> 01:34:42,959


669
01:34:42,959 --> 01:34:49,079


670
01:34:49,079 --> 01:34:56,280


671
01:34:56,280 --> 01:35:01,439


672
01:35:01,439 --> 01:35:05,739


673
01:35:07,480 --> 01:35:12,250


674
01:35:12,250 --> 01:35:15,880


675
01:35:15,880 --> 01:35:22,270


676
01:35:22,270 --> 01:35:27,160


677
01:35:29,739 --> 01:35:34,719


678
01:35:34,719 --> 01:35:43,900


679
01:35:43,900 --> 01:35:47,469


680
01:35:47,469 --> 01:35:50,530


681
01:35:50,530 --> 01:36:01,150


682
01:36:01,150 --> 01:36:10,630


683
01:36:10,630 --> 01:36:17,980


684
01:36:19,750 --> 01:36:26,590


685
01:36:26,590 --> 01:36:28,420


686
01:36:28,420 --> 01:36:35,080


687
01:36:35,080 --> 01:36:40,719


688
01:36:40,719 --> 01:36:45,730


689
01:36:47,320 --> 01:36:51,310


690
01:36:51,310 --> 01:36:56,200


691
01:37:00,219 --> 01:37:05,110


692
01:37:05,110 --> 01:37:08,820


693
01:37:08,820 --> 01:37:15,000


694
01:37:15,000 --> 01:37:21,610


695
01:37:21,610 --> 01:37:27,190


696
01:37:27,190 --> 01:37:36,540


697
01:37:36,540 --> 01:37:42,510


698
01:37:42,960 --> 01:37:59,440


699
01:37:59,440 --> 01:38:05,800


700
01:38:05,800 --> 01:38:10,210


701
01:38:10,210 --> 01:38:14,710


702
01:38:14,710 --> 01:38:18,550


703
01:38:18,550 --> 01:38:24,280


704
01:38:24,280 --> 01:38:32,530


705
01:38:32,530 --> 01:38:35,950


706
01:38:35,950 --> 01:38:39,790


707
01:38:39,790 --> 01:38:44,710


708
01:38:44,710 --> 01:38:49,270


709
01:38:49,270 --> 01:38:53,560


710
01:38:53,560 --> 01:39:01,480


711
01:39:04,210 --> 01:39:07,930


712
01:39:07,930 --> 01:39:16,330


713
01:39:16,330 --> 01:39:21,670


714
01:39:21,670 --> 01:39:26,170


715
01:39:26,170 --> 01:39:30,699


716
01:39:39,550 --> 01:39:45,909


717
01:39:47,800 --> 01:39:52,929


718
01:39:52,929 --> 01:39:57,369


719
01:39:57,369 --> 01:40:02,679


720
01:40:02,679 --> 01:40:12,579


721
01:40:12,579 --> 01:40:21,999


722
01:40:21,999 --> 01:40:26,019


723
01:40:26,019 --> 01:40:30,179


724
01:40:30,179 --> 01:40:34,300


725
01:40:34,300 --> 01:40:40,900


726
01:40:44,019 --> 01:40:49,539


727
01:40:51,550 --> 01:40:55,630


728
01:40:55,630 --> 01:41:02,949


729
01:41:07,360 --> 01:41:12,159


730
01:41:12,159 --> 01:41:18,699


731
01:41:18,699 --> 01:41:22,630


732
01:41:22,630 --> 01:41:27,340


733
01:41:27,340 --> 01:41:31,300


734
01:41:31,300 --> 01:41:37,150


735
01:41:41,499 --> 01:41:48,760


736
01:41:48,760 --> 01:41:55,119


737
01:41:57,400 --> 01:42:05,290


738
01:42:05,290 --> 01:42:15,699


739
01:42:15,699 --> 01:42:20,050


740
01:42:20,050 --> 01:42:25,119


741
01:42:27,190 --> 01:42:33,130


742
01:42:33,130 --> 01:42:39,070


743
01:42:39,070 --> 01:42:46,380


744
01:42:46,380 --> 01:42:51,280


745
01:42:51,280 --> 01:42:57,270


746
01:42:57,270 --> 01:43:09,130


747
01:43:09,130 --> 01:43:14,110


748
01:43:14,110 --> 01:43:18,219


749
01:43:18,219 --> 01:43:25,210


750
01:43:25,210 --> 01:43:30,489


751
01:43:33,310 --> 01:43:38,730


752
01:43:43,940 --> 01:43:48,699


753
01:43:48,699 --> 01:43:59,719


754
01:43:59,719 --> 01:44:05,270


755
01:44:05,270 --> 01:44:09,800


756
01:44:09,800 --> 01:44:15,710


757
01:44:15,710 --> 01:44:22,969


758
01:44:22,969 --> 01:44:28,430


759
01:44:28,430 --> 01:44:33,949


760
01:44:36,650 --> 01:44:43,550


761
01:44:43,550 --> 01:44:50,719


762
01:44:50,719 --> 01:44:57,500


763
01:44:57,500 --> 01:45:03,140


764
01:45:03,140 --> 01:45:06,980


765
01:45:09,050 --> 01:45:16,010


766
01:45:16,010 --> 01:45:22,160


767
01:45:22,160 --> 01:45:26,630


768
01:45:26,630 --> 01:45:29,989


769
01:45:37,530 --> 01:45:45,030


770
01:45:45,030 --> 01:45:51,300


771
01:45:51,300 --> 01:45:58,710


772
01:45:58,710 --> 01:46:05,490


773
01:46:05,490 --> 01:46:09,510


774
01:46:09,510 --> 01:46:14,790


775
01:46:14,790 --> 01:46:18,030


776
01:46:18,030 --> 01:46:26,130


777
01:46:26,130 --> 01:46:31,260


778
01:46:31,260 --> 01:46:36,900


779
01:46:39,780 --> 01:46:44,790


780
01:46:44,790 --> 01:46:47,940


781
01:46:47,940 --> 01:46:53,760


782
01:46:53,760 --> 01:46:59,870


783
01:46:59,870 --> 01:47:06,650


784
01:47:06,650 --> 01:47:12,630


785
01:47:12,630 --> 01:47:19,590


786
01:47:19,590 --> 01:47:26,160


787
01:47:26,160 --> 01:47:33,270


788
01:47:33,270 --> 01:47:37,170


789
01:47:37,170 --> 01:47:45,150


790
01:47:45,150 --> 01:47:49,440


791
01:47:49,440 --> 01:47:54,550


792
01:47:54,550 --> 01:48:00,850


793
01:48:00,850 --> 01:48:05,460


794
01:48:05,460 --> 01:48:11,560


795
01:48:11,560 --> 01:48:17,469


796
01:48:17,469 --> 01:48:23,620


797
01:48:23,620 --> 01:48:26,440


798
01:48:26,440 --> 01:48:29,650


799
01:48:29,650 --> 01:48:34,719


800
01:48:34,719 --> 01:48:41,760


801
01:48:41,760 --> 01:48:49,440


802
01:48:53,860 --> 01:48:58,840


803
01:48:58,840 --> 01:49:05,290


804
01:49:05,290 --> 01:49:11,680


805
01:49:11,680 --> 01:49:19,180


806
01:49:19,180 --> 01:49:23,830


807
01:49:23,830 --> 01:49:28,030


808
01:49:28,030 --> 01:49:32,500


809
01:49:32,500 --> 01:49:36,340


810
01:49:36,340 --> 01:49:43,660


811
01:49:43,660 --> 01:49:51,550


812
01:49:51,550 --> 01:49:54,850


813
01:49:54,850 --> 01:49:59,739


814
01:50:01,480 --> 01:50:05,139


815
01:50:05,139 --> 01:50:14,080


816
01:50:14,080 --> 01:50:20,409


817
01:50:20,409 --> 01:50:25,540


818
01:50:25,540 --> 01:50:30,520


819
01:50:30,520 --> 01:50:36,340


820
01:50:36,340 --> 01:50:42,940


821
01:50:49,960 --> 01:51:06,010


822
01:51:06,010 --> 01:51:11,980


823
01:51:16,360 --> 01:51:22,330


824
01:51:22,330 --> 01:51:25,929


825
01:51:25,929 --> 01:51:29,400


826
01:51:29,400 --> 01:51:33,310


827
01:51:33,310 --> 01:51:38,110


828
01:51:40,989 --> 01:51:46,300


829
01:51:46,300 --> 01:51:50,940


830
01:51:57,630 --> 01:52:01,239


831
01:52:03,520 --> 01:52:12,429


832
01:52:12,429 --> 01:52:18,239


833
01:52:19,020 --> 01:52:25,180


834
01:52:25,180 --> 01:52:30,280


835
01:52:32,440 --> 01:52:36,940


836
01:52:36,940 --> 01:52:40,530


837
01:52:40,530 --> 01:52:44,920


838
01:52:44,920 --> 01:52:48,730


839
01:52:51,580 --> 01:52:56,170


840
01:52:56,170 --> 01:53:01,690


841
01:53:05,650 --> 01:53:13,930


842
01:53:13,930 --> 01:53:17,950


843
01:53:20,230 --> 01:53:24,780


844
01:53:24,780 --> 01:53:31,090


845
01:53:31,090 --> 01:53:39,610


846
01:53:43,060 --> 01:53:49,090


847
01:53:49,090 --> 01:53:54,460


848
01:53:54,460 --> 01:53:58,780


849
01:53:58,780 --> 01:54:02,610


850
01:54:02,610 --> 01:54:09,010


851
01:54:09,010 --> 01:54:14,890


852
01:54:14,890 --> 01:54:18,190


853
01:54:20,620 --> 01:54:31,420


854
01:54:31,420 --> 01:54:37,240


855
01:54:37,240 --> 01:54:41,260


856
01:54:41,260 --> 01:54:48,160


857
01:54:48,160 --> 01:54:53,560


858
01:54:53,560 --> 01:54:58,690


859
01:55:00,810 --> 01:55:06,100


860
01:55:06,100 --> 01:55:10,210


861
01:55:10,210 --> 01:55:14,680


862
01:55:14,680 --> 01:55:19,120


863
01:55:19,120 --> 01:55:24,130


864
01:55:24,130 --> 01:55:29,910


865
01:55:30,390 --> 01:55:37,750


866
01:55:37,750 --> 01:55:41,530


867
01:55:41,530 --> 01:55:47,580


868
01:55:47,580 --> 01:55:51,930


869
01:55:51,930 --> 01:55:59,140


870
01:55:59,140 --> 01:56:07,200


871
01:56:09,730 --> 01:56:13,780


872
01:56:13,780 --> 01:56:17,470


873
01:56:17,470 --> 01:56:24,240


874
01:56:26,470 --> 01:56:31,450


875
01:56:31,450 --> 01:56:37,590


876
01:56:37,590 --> 01:56:42,730


877
01:56:44,960 --> 01:56:55,400


878
01:56:55,400 --> 01:57:02,480


879
01:57:02,480 --> 01:57:09,610


880
01:57:09,610 --> 01:57:16,610


881
01:57:16,610 --> 01:57:21,560


882
01:57:21,560 --> 01:57:32,060


883
01:57:32,060 --> 01:57:38,660


884
01:57:41,960 --> 01:57:48,230


885
01:57:50,210 --> 01:57:55,790


886
01:57:55,790 --> 01:58:00,740


887
01:58:00,740 --> 01:58:04,220


888
01:58:04,220 --> 01:58:06,980


889
01:58:09,590 --> 01:58:18,080


890
01:58:18,080 --> 01:58:25,460


891
01:58:25,460 --> 01:58:29,860


892
01:58:29,860 --> 01:58:38,240


893
01:58:38,240 --> 01:58:43,940


894
01:58:43,940 --> 01:58:51,110


895
01:58:51,110 --> 01:58:58,440


896
01:58:58,440 --> 01:59:05,670


897
01:59:05,670 --> 01:59:14,489


898
01:59:17,250 --> 01:59:27,390


899
01:59:27,390 --> 01:59:39,800


900
01:59:40,489 --> 01:59:48,239


901
01:59:48,239 --> 01:59:52,410


902
01:59:52,410 --> 01:59:58,350


903
01:59:58,350 --> 02:00:07,890


904
02:00:07,890 --> 02:00:22,739


905
02:00:24,120 --> 02:00:31,260


906
02:00:31,260 --> 02:00:38,670


907
02:00:38,670 --> 02:00:46,670


908
02:00:46,670 --> 02:00:53,400


909
02:00:53,400 --> 02:00:59,910


910
02:00:59,910 --> 02:01:05,550


911
02:01:05,550 --> 02:01:11,960


912
02:01:12,330 --> 02:01:18,190


913
02:01:18,190 --> 02:01:24,370


914
02:01:24,370 --> 02:01:30,940


915
02:01:30,940 --> 02:01:37,389


916
02:01:37,389 --> 02:01:44,230


917
02:01:46,179 --> 02:01:50,230


918
02:01:54,429 --> 02:01:59,110


919
02:01:59,110 --> 02:02:05,260


920
02:02:05,260 --> 02:02:09,310


921
02:02:09,310 --> 02:02:14,530


922
02:02:17,110 --> 02:02:21,550


923
02:02:21,550 --> 02:02:28,000


924
02:02:28,000 --> 02:02:37,290


925
02:02:37,290 --> 02:02:45,340


926
02:02:45,340 --> 02:02:54,070


927
02:02:54,070 --> 02:02:59,199


928
02:02:59,199 --> 02:03:04,449


929
02:03:04,449 --> 02:03:10,090


930
02:03:10,090 --> 02:03:15,989


931
02:03:22,830 --> 02:03:28,550


932
02:03:28,550 --> 02:03:39,550


933
02:03:39,790 --> 02:03:53,270


934
02:03:53,270 --> 02:04:00,360


935
02:04:01,540 --> 02:04:06,500


936
02:04:06,500 --> 02:04:11,530


937
02:04:11,530 --> 02:04:17,830


938
02:04:17,830 --> 02:04:23,270


939
02:04:23,270 --> 02:04:30,949


940
02:04:33,260 --> 02:04:38,150


941
02:04:39,739 --> 02:04:44,239


942
02:04:44,239 --> 02:04:49,130


943
02:04:49,130 --> 02:04:53,719


944
02:04:53,719 --> 02:05:02,120


945
02:05:02,120 --> 02:05:09,920


946
02:05:09,920 --> 02:05:15,850


947
02:05:15,850 --> 02:05:20,420


948
02:05:20,420 --> 02:05:27,230


949
02:05:30,710 --> 02:05:37,480


950
02:05:39,790 --> 02:05:45,970


951
02:05:45,970 --> 02:05:53,290


952
02:05:53,290 --> 02:05:58,600


953
02:05:58,600 --> 02:06:06,460


954
02:06:06,460 --> 02:06:13,480


955
02:06:15,910 --> 02:06:20,500


956
02:06:20,500 --> 02:06:26,080


957
02:06:26,080 --> 02:06:31,930


958
02:06:31,930 --> 02:06:37,120


959
02:06:37,120 --> 02:06:46,480


960
02:06:46,480 --> 02:06:51,310


961
02:06:51,310 --> 02:06:55,390


962
02:06:55,390 --> 02:07:01,900


963
02:07:05,910 --> 02:07:13,750


964
02:07:13,750 --> 02:07:18,640


965
02:07:18,640 --> 02:07:26,880


966
02:07:26,880 --> 02:07:38,020


967
02:07:38,020 --> 02:07:43,000


968
02:07:43,000 --> 02:07:48,340


969
02:07:48,340 --> 02:07:53,109


970
02:07:53,109 --> 02:07:56,559


971
02:07:56,559 --> 02:08:09,069


972
02:08:09,069 --> 02:08:13,149


973
02:08:13,149 --> 02:08:21,339


974
02:08:21,339 --> 02:08:25,869


975
02:08:25,869 --> 02:08:33,909


976
02:08:33,909 --> 02:08:37,689


977
02:08:37,689 --> 02:08:46,419


978
02:08:46,419 --> 02:08:50,439


979
02:08:50,439 --> 02:08:54,869


980
02:08:57,760 --> 02:09:01,659


981
02:09:07,209 --> 02:09:16,359


982
02:09:16,359 --> 02:09:22,149


983
02:09:22,149 --> 02:09:29,939


984
02:09:29,939 --> 02:09:35,589


985
02:09:35,589 --> 02:09:42,309


986
02:09:42,309 --> 02:09:49,839


987
02:09:49,839 --> 02:09:54,519


988
02:09:54,519 --> 02:09:59,289


989
02:10:03,280 --> 02:10:08,080


990
02:10:08,080 --> 02:10:14,350


991
02:10:14,350 --> 02:10:17,410


992
02:10:22,770 --> 02:10:29,110


993
02:10:31,930 --> 02:10:35,770


994
02:10:35,770 --> 02:10:39,610


995
02:10:42,070 --> 02:10:46,390


996
02:10:46,390 --> 02:10:55,030


997
02:10:58,680 --> 02:11:07,920


998
02:11:07,920 --> 02:11:14,949


999
02:11:18,310 --> 02:11:29,110


1000
02:11:31,449 --> 02:11:38,190


1001
02:11:38,190 --> 02:11:44,440


1002
02:11:44,440 --> 02:11:49,870


1003
02:11:49,870 --> 02:11:54,550


1004
02:11:54,550 --> 02:11:58,090


1005
02:11:58,090 --> 02:12:02,620


1006
02:12:02,620 --> 02:12:07,890


1007
02:12:07,890 --> 02:12:12,969


1008
02:12:12,969 --> 02:12:16,900


1009
02:12:16,900 --> 02:12:22,360


1010
02:12:22,360 --> 02:12:26,739


1011
02:12:26,739 --> 02:12:32,620


1012
02:12:32,620 --> 02:12:36,429


1013
02:12:36,429 --> 02:12:41,890


1014
02:12:41,890 --> 02:12:49,179


1015
02:12:49,179 --> 02:12:53,650


1016
02:12:53,650 --> 02:12:59,140


1017
02:12:59,140 --> 02:13:02,290


1018
02:13:02,290 --> 02:13:05,679


1019
02:13:05,679 --> 02:13:09,310


1020
02:13:09,310 --> 02:13:15,250


1021
02:13:15,250 --> 02:13:20,560


1022
02:13:20,560 --> 02:13:23,830


1023
02:13:27,070 --> 02:13:30,070


1024
02:13:30,070 --> 02:13:35,350


1025
02:13:35,350 --> 02:13:40,679


1026
02:13:41,040 --> 02:13:43,960


