1
00:00:00,060 --> 00:00:14,991
Добро пожаловать на шестую лекцию, она предпоследняя в этом курсе.

2
00:00:15,091 --> 00:00:22,704
Пару недель назад я обещал показать запись четвёртой лекции исследователю Себастьяну Рудеру.

3
00:00:22,804 --> 00:00:39,250
Ему понравилось, и вчера вышел его пост Optimization for Deep Learning Highlights in 2017 по мотивам нашей лекции.

4
00:00:39,350 --> 00:00:44,455
В посте упоминается то, что сделали студенты нашего курса -

5
00:00:44,555 --> 00:00:58,020
например, отделение ограничения весов от вычисления инерции в AdamW,

6
00:00:58,020 --> 00:01:13,034
он пишет про возможности, которые это открывает, и приводит ссылку на реализацию AdamW от Anand Saha.

7
00:01:13,134 --> 00:01:16,710
Как видите, код библиотеки fastai уже приводят в пример.

8
00:01:18,060 --> 00:01:25,170
После этого Себастьян пишет об алгоритмах подбора скорости обучения.

9
00:01:25,170 --> 00:01:36,780
На графике показана зависимость скорости обучения от эпохи, выглядит непривычно из-за логарифмической шкалы.

10
00:01:36,780 --> 00:01:53,430
В качестве примера - ссылки на посты наших двух студентов, Vitaly Bushaev и Anand Saha.

11
00:01:53,430 --> 00:02:00,335
Очень радует то, что работы наших студентов уже замечают и используют.

12
00:02:00,435 --> 00:02:06,070
Пост Себастьяна попал на главную страницу Hacker News, это очень круто.

13
00:02:06,170 --> 00:02:14,885
Будем надеяться, что распространение информации продолжится.

14
00:02:14,985 --> 00:02:31,090
На прошлой неделе мы обсуждали коллаборативную фильтрацию, давайте вспомним, как выглядела модель.

15
00:02:34,300 --> 00:02:42,200
Последняя версия модели была почти идентична коду класса EmbeddingDotBias библиотеки fastai.

16
00:02:47,630 --> 00:02:56,370
Сначала создаются матрицы эмбеддинга для пользователей и товаров и заполняются случайными числами.

17
00:02:56,470 --> 00:03:00,870
Пользователи и товары - устоявшиеся термины, в нашей модели товары - это фильмы.

18
00:03:00,970 --> 00:03:11,540
Таким же образом созданы матрицы смещений - в функцию get_emb() передаётся не nf=n_factors, а nf=1.

19
00:03:14,210 --> 00:03:22,180
Матрицы эмбеддинга перемножаются, полученная матрица превращается в вектор, к ней прибавляются смещения.

20
00:03:22,730 --> 00:03:27,430
Полученный вектор пропускается через сигмоиду для калибровки до желаемого диапазона.

21
00:03:27,430 --> 00:03:39,865
Вы спрашивали, можно ли визуализизовать этот процесс, и я обещал это показать.

22
00:03:39,965 --> 00:04:04,050
Мы начнём с модели, использующей только функции и методы fastai, она обучалась 19 секунд, результаты неплохие.

23
00:04:04,150 --> 00:04:09,770
Давайте анализировать процесс.

24
00:04:09,770 --> 00:04:24,370
Мы считывали файл movies.csv, содержащий пары "ID фильма - название фильма", сейчас будем его использовать.

25
00:04:24,640 --> 00:04:36,660
Так как не все в аудитории - заядлые кинозрители, для наглядности я возьму 3000 самых популярных фильмов.

26
00:04:36,660 --> 00:04:51,785
После этого заменю индексы на новые, используя словарь нумерации cf.item2idx, с которым работает модель.

27
00:04:51,885 --> 00:05:06,410
Модель fastai лежит в переменной learn, модель PyTorch можно получить вызовом learn.model.

28
00:05:06,510 --> 00:05:18,140
В коде библиотеки fastai model реализовано как @property - вычисляемое свойство.

29
00:05:18,240 --> 00:05:32,800
Вычисляемое свойство в Python выглядит как метод, но вызывается без скобок.

30
00:05:32,800 --> 00:05:42,669
Это выглядит как обычное поле, но каждый раз при вызове выполняется этот код.

31
00:05:42,669 --> 00:05:48,789
Здесь при вызове вычисляемого свойства возвращается поле self.models.model.

32
00:05:48,789 --> 00:06:03,019
self.models - объект класса CollabFilterModel, это тонкая обёртка для моделей PyTorch.

33
00:06:03,119 --> 00:06:19,720
Код класса CollabFilterModel состоит из одной строки.

34
00:06:19,720 --> 00:06:33,380
Мы обсудим это во второй части курса, это сделано для возможности задания разных скоростей обучения для групп слоёв.

35
00:06:33,480 --> 00:06:42,820
Группировка слоёв не реализована в PyTorch, и этот класс позволяет добавить её в готовую модель PyTorch.

36
00:06:42,820 --> 00:06:54,950
Детали не важны, но, в общем, примерно так и выглядят обёртки для моделей PyTorch.

37
00:06:55,050 --> 00:07:07,820
Итак, модель PyTorch содержится в поле learn.models.model, это же можно получить присваиванием m=learn.model.

38
00:07:07,920 --> 00:07:21,815
Модель PyTorch в переменной m содержит массив всех слоёв модели,

39
00:07:21,915 --> 00:07:39,460
это реализовано с помощью встроенных средств объектно-ориентированного программирования Python.

40
00:07:39,460 --> 00:08:07,960
Имена слоёв эмбеддинга автоматически берутся из их определения в коде.

41
00:08:12,910 --> 00:08:28,660
Это модель PyTorch. Слои модели получаются соответствующими методами -  метом m.ib() (item bias)

42
00:08:28,660 --> 00:08:34,000
возвращает матрицу смещений товаров, в нашем случае - матрицу смещений фильмов.

43
00:08:34,000 --> 00:08:40,990
В этой матрице каждому из 9066 фильмов соответствует одно число - смещение.

44
00:08:40,990 --> 00:08:50,710
Модели и слои PyTorch удобно использовать, потому что их можно вызывать как функции.

45
00:08:50,710 --> 00:09:12,520
Метод ib() возвращает слой ib, а функция m() возвращает предсказания модели m.

46
00:09:12,520 --> 00:09:22,570
Здесь в метод ib() передаются индексы самых популярных фильмов.

47
00:09:22,570 --> 00:09:34,835
Напомню, что матрицы эмбеддинга - объекты класса Variable, а не тензоры, поэтому здесь стоит конструктор V().

48
00:09:34,935 --> 00:10:00,425
PyTorch избавляется от класса Variable с версии 0.4. Если вы смотрите курс в записи, его может уже не быть.

49
00:10:00,525 --> 00:10:07,025
Это будет удобно, а пока нужно не забывать использовать конструктор V() при передаче данных в модель.

50
00:10:07,125 --> 00:10:20,140
Помните, что всё, что можно делать с тензором, можно делать и с объектами класса Variable.

51
00:10:20,140 --> 00:10:38,820
Итак, m.ib(V(topMovieIdx)) содержит смещения самых популярных фильмов.

52
00:10:41,790 --> 00:11:06,550
Эти смещения содежатся в объекте класса Variable размера 3000x1, так как мы выбрали 3000 фильмов.

53
00:11:08,830 --> 00:11:14,675
Мы получили объект класса Variable, так как входные данные - такого же формата.

54
00:11:14,775 --> 00:11:22,385
Объект хранится в GPU, так как используется модуль torch.cuda.

55
00:11:22,485 --> 00:11:36,740
Фукнция to_np() превращает полученный объект в массив numpy, иногда это удобнее.

56
00:11:36,840 --> 00:11:58,930
Функция работает и с тензорами, и с объектами класса Variable, ей неважно, находится пеменная в CPU или в GPU.

57
00:11:58,930 --> 00:12:18,970
Функция очень удобная, я использую её всегда, если не надо явно использовать GPU или доставать градиенты.

58
00:12:19,330 --> 00:12:39,700
numpy гораздо старше PyTorch и поэтому удобнее, к тому же с ним хорошо работают библиотеки вроде OpenCV и pandas.

59
00:12:39,700 --> 00:12:45,460
Поэтому я использую массивы numpy и перевожу их в формат PyTorch, когда

60
00:12:45,460 --> 00:12:53,410
нужно произвести вычисления в GPU или использовать градиенты, а потом перехожу обратно к массивам numpy.

61
00:12:53,410 --> 00:13:03,430
Так реализована библиотека fastai, что отличает её от других библиотек компьютерного зрения на основе PyTorch,

62
00:13:05,830 --> 00:13:11,767
использующих PyTorch везде, где это возможно. Вопрос?

63
00:13:11,867 --> 00:13:31,660
Вопрос из зала: Если я построил модель в GPU с PyTorch, функцию to_np() придётся использовать в цикле для каждого слоя?

64
00:13:31,660 --> 00:13:38,090
Хороший вопрос. Предсказания лучше делать в CPU, а не в GPU - это расширяет круг применения модели,

65
00:13:38,190 --> 00:13:44,045
нет необходимости разбивать данные на минибатчи и так далее.

66
00:13:44,145 --> 00:14:03,620
Модель переводится в CPU методом m.cpu(), аналогично для переменных.

67
00:14:03,720 --> 00:14:27,560
Если у вашей машины нет GPU, этого делать не надо, CPU будет использоваться автоматически.

68
00:14:27,660 --> 00:14:41,390
Вопрос из зала: Если модель обучена в GPU и сохранена, её надо особым образом загружать?

69
00:14:41,490 --> 00:14:51,990
Нет, не нужно, хотя это зависит от того, какие средства fastai вы используете, сейчас покажу.

70
00:14:51,990 --> 00:15:08,350
Один из наших студентов придумал, как избежать необходимости загружать модель с использованием того же GPU,

71
00:15:09,050 --> 00:15:24,160
в котором модель обучалась до сохранения. Это делается этой магической строкой.

72
00:15:28,819 --> 00:15:36,619
Для переноса модели из CPU в GPU используется метод m.cuda().

73
00:15:36,619 --> 00:15:50,509
Важно понимать работу функции zip() в Python, она позволяет проходить по нескольким массивам сразу.

74
00:15:50,509 --> 00:16:00,259
Я хочу получить массив пар "название фильма - смещение", поэтому соединяю массивы индексов фильмов и смещений

75
00:16:00,259 --> 00:16:11,289
и использую генератор списков Python для сопоставления имён и индексов.

76
00:16:11,289 --> 00:16:15,714
Я сортирую полученный список.

77
00:16:15,814 --> 00:16:33,539
Худший фильм - Battlefield Earth Джона Траволты, причём отрыв от других фильмов достаточно большой.

78
00:16:33,639 --> 00:16:46,844
Это - худший фильм всех времён по версии пользователей IMDB. Такой способ оценивания фильмов хорош,

79
00:16:46,944 --> 00:16:57,319
потому что он учитывает то, что некоторые люди просто ставят плохие оценки всем фильмам.

80
00:17:02,149 --> 00:17:10,399
После поправки на то, что средняя оценка у каждого человека разная и все люди смотрят разные фильмы,

81
00:17:11,689 --> 00:17:20,169
фильм Battlefield Earth получается худшим фильмом в истории человечества.

82
00:17:21,638 --> 00:17:29,360
Это способ посмотреть внутрь модели и визуализировать работу векторов смещения.

83
00:17:29,360 --> 00:17:59,520
Для сортировки списка я использовал лямбда-функцию, а до этого - itemgetter, здесь они работают одинаково.

84
00:17:59,620 --> 00:18:06,290
Лямбда-функции очень полезные, убедитесь, что вы умеете их писать.

85
00:18:06,290 --> 00:18:18,435
Во время работы функции sorted() лямбда-функция вызывается каждый раз при сравнении двух элементов.

86
00:18:18,535 --> 00:18:31,340
Выведем список в другом порядке. Shawshank Redemption, Godfather, Usual Suspects - отличные фильмы, я согласен с рейтингом.

87
00:18:35,120 --> 00:18:43,100
Это была демонстрация смещений для фильмов.

88
00:18:43,100 --> 00:18:50,205
Теперь давайте рассмотрим матрицы эмбеддинга.

89
00:18:50,305 --> 00:19:01,315
Матрица эмбеддинга для фильмов называется i, мы достаём её функцией m.i() и кладём в переменную movie_emb.

90
00:19:01,415 --> 00:19:08,670
Для каждого из 3000 самых популярных фильмов есть вектор эмбеддинга длиной 50.

91
00:19:08,770 --> 00:19:20,355
Если вы не Джеффри Хинтон, очень сложно представить 50-мерное пространство, поэтому мы перейдём в трёхмерное.

92
00:19:20,455 --> 00:19:29,155
Есть много алгоритмов уменьшения размерности, один из самых популярных -

93
00:19:29,255 --> 00:19:36,073
метод главных компонент, или PCA (principal component analysis).

94
00:19:36,173 --> 00:19:41,575
Это линейный алгоритм, такие алгоритмы обычно хорошо работают на матрицах эмбеддинга.

95
00:19:41,675 --> 00:19:54,510
Принцип его работы рассказывает Рейчел в курсе fast.ai Computational Linear Algebra.

96
00:19:54,610 --> 00:20:04,360
Это очень важный алгоритм, он очень похож на сингулярное разложение матриц (SVD).

97
00:20:04,460 --> 00:20:18,740
SVD иногда всплывает в глубоком обучении, с ним стоит разобраться, если вы заинтересованы в теории.

98
00:20:18,740 --> 00:20:26,220
Термины SVD, PCA, собственные векторы и собственные значения означают примерно одно и то же.

99
00:20:26,320 --> 00:20:33,750
Метод главных компонент реализован в sklearn.decomposition,

100
00:20:33,850 --> 00:20:41,540
соответствующий объект создаётся конструктором PCA(), в который передаётся желаемое количество измерений.

101
00:20:41,540 --> 00:20:48,380
Алгоритм найдёт три максимально различающиеся линейные комбинации пятидесяти измерений,

102
00:20:48,380 --> 00:20:53,180
которые передают максимальное количество информации.

103
00:20:53,690 --> 00:21:02,270
Это называется наилучшее приближение матрицы матрицей меньшего ранга.

104
00:21:02,270 --> 00:21:09,005
После работы алгоритма мы получаем матрицу размера 3х3000.

105
00:21:09,105 --> 00:21:21,615
После этого мы обрабатываем одну из трёх строк таблицы так, как обрабатывали вектор смещений.

106
00:21:21,715 --> 00:21:42,420
Мы не знаем, как работает PCA, поэтому давайте отсортируем данные по этой компоненте и посмотрим, что получится.

107
00:21:42,520 --> 00:22:11,315
По названиям фильмов я предполагаю, что эта компонента говорит, насколько фильм серьёзный или лёгкий для просмотра.

108
00:22:11,415 --> 00:22:25,060
Другого способа интерпретации нет, нужно смотреть на данные и искать в них смысл.

109
00:22:27,910 --> 00:22:35,465
Следующая компонента обрабатывается аналогичным образом.

110
00:22:35,565 --> 00:22:48,320
Похоже на то, что положительные значения отвечают за наличие диалогов, а отрицательные - за наличие компьютерной графики.

111
00:22:48,420 --> 00:23:13,574
Эта компонента показывает, что кто-то любит классические фильмы Вуди Аллена, а кто-то - зрелищные голливудские фильмы.

112
00:23:13,674 --> 00:23:21,930
По первой компоненте видно, что кто-то любит серьёзные фильмы, а кто-то - одноразовые боевики.

113
00:23:22,810 --> 00:23:28,892
Я надеюсь, идея понятна.

114
00:23:28,992 --> 00:23:43,990
Модель очень простая - перемножение двух матриц и алгоритм оптимизации, а умеет довольно много, это круто.

115
00:23:48,600 --> 00:24:04,640
Можно распределить фильмы на графике по первым двум компонентам, я нарисовал несколько фильмов.

116
00:24:04,740 --> 00:24:23,700
Сейчас я хочу поговорить о том, что происходит при вызове метода learn.fit().

117
00:24:24,850 --> 00:24:30,790
Вопрос из зала: Как интерпретировать матрицы эмбеддинга для задачи предсказания продаж?

118
00:24:30,790 --> 00:24:43,220
Как в соревновании Rossman? Сейчас покажу.

119
00:24:57,840 --> 00:25:18,790
На экране - иллюстрация из статьи Entity Embeddings of Categorical Variables, статья довольно простая на нашем уровне.

120
00:25:18,790 --> 00:25:23,260
Если вы прошли курс по машинному обучению, её будет ещё проще понять.

121
00:25:23,260 --> 00:25:39,550
Статья описывает слои эмбеддинга (entity embedding layers) как сочетание прямого кодирования и перемножения матриц.

122
00:25:39,550 --> 00:25:45,122
На картинке три слоя эмбеддинга, то есть три прямых кодирования, умноженных на соответствующие матрицы.

123
00:25:45,222 --> 00:25:56,040
Результат умножения подаётся на линейный слой (dense layer).

124
00:25:56,040 --> 00:26:21,610
Это одна из первых статей про использование эмбеддинга для категориальных признаков, поэтому она очень подробная.

125
00:26:21,610 --> 00:26:34,930
После обучения эмбеддингов с помощью нейронной сети авторы задумались, что можно с ними сделать.

126
00:26:34,930 --> 00:26:45,070
Они заменили категориальные переменные на вычисленные эмбеддинги

127
00:26:48,070 --> 00:27:07,920
и обучили с их помощью алгоритм градиентного бустинга над решающими деревьями (gradient boosted trees).

128
00:27:08,020 --> 00:27:14,775
Таким образом они выполнили отбор признаков, и он оказался удачным, как видите в таблице.

129
00:27:14,875 --> 00:27:25,960
Функция потерь MAPE (mean absolute percentage error) уменьшилась с 0.152 при прямом кодировании до 0.115 с эмбеддингами.

130
00:27:25,960 --> 00:27:36,650
Аналогичные результаты получены для алгоритмов случайный лес и метод ближайших соседей.

131
00:27:36,750 --> 00:27:56,195
Это значит, что вы можете обучить эмбеддинги признаков любой мощности с помощью нейронной сети,

132
00:27:56,295 --> 00:28:03,580
а потом все смогут использовать их в алгоритмах градиентного бустинга или случайного леса.

133
00:28:07,210 --> 00:28:14,670
Даже метод ближайших соседей даст неплохие результаты.

134
00:28:14,770 --> 00:28:30,730
Сотрудникам вашей компании не придётся проходить наш курс, но они смогут использовать мощь нейронных сетей.

135
00:28:30,730 --> 00:28:59,355
Эмбеддинги - это соответствия индексов и чисел, поэтому их удобно хранить в базе данных.

136
00:28:59,455 --> 00:29:05,115
Градиентный бустинг и случайный лес обучаются гораздо быстрее нейронных сетей,

137
00:29:05,215 --> 00:29:09,915
поэтому в сочетании с предвычисленными эмбеддингами они могут быть даже эффективнее нейронных сетей.

138
00:29:10,015 --> 00:29:26,790
Авторы статьи распределили штаты Германии в зависимости от значений первых двух главных компонент.

139
00:29:26,890 --> 00:29:41,600
Удивительным образом расположение городов примерно совпало с их географическим расположением,

140
00:29:41,600 --> 00:29:54,672
хотя в изначальных данных нет ни расстояния между штатами, ни географической привязки.

141
00:29:54,772 --> 00:30:10,190
Этот график показывает зависимость расстояния между двумя магазинами в пространстве эмбеддинга

142
00:30:10,190 --> 00:30:15,980
от географического расстояния между ними для всех возможных пар.

143
00:30:15,980 --> 00:30:36,050
Получилась красивая корреляция. Видимо, у рядом стоящих магазинов схожие характеристики продаж.

144
00:30:36,050 --> 00:30:48,170
Я нарисовал такие же графики для дней недели и месяцев и соединил точки временной линией.

145
00:30:48,170 --> 00:30:52,850
На графике месяцев видно, что точки летних месяцев стоят отдельно, точки весенних - отдельно.

146
00:30:58,640 --> 00:31:08,345
Визуализировать эмбеддинги интересно. Имеет смысл сначала проверить, отражают ли они очевидные закономерности,

147
00:31:08,445 --> 00:31:18,869
а потом попробовать разглядеть неочевидные на различных графиках.

148
00:31:18,969 --> 00:31:30,605
Эта техника ещё не изучена, поэтому я пока не могу указать область применения.

149
00:31:30,705 --> 00:31:44,830
Вопрос из зала: Есть другие способы создания эмбеддингов, например, N-граммы с пропусками (skip-grams). Какой лучше?

150
00:31:44,830 --> 00:31:49,679
N-граммы с пропусками специфичны для обработки естественного языка.

151
00:31:49,860 --> 00:32:16,720
Задачи по обработке естественного языка - обычно обучение без учителя, нет целевой переменной.

152
00:32:16,720 --> 00:32:22,210
Для перехода к задаче обучения с учителем можно создать целевую переменную.

153
00:32:22,310 --> 00:32:34,645
В Word2vec это было устроено так: из данных берётся правильное предложение из, например, 11 слов,

154
00:32:34,945 --> 00:32:53,500
и среднее слово в нём заменяется на случайное. Например, предложение

155
00:32:53,500 --> 00:33:00,334
"Милая маленькая кошечка сидела на пушистом коврике" превращалось в "Милая маленькая справедливость сидела на пушистом коврике".

156
00:33:00,434 --> 00:33:14,365
В итоге есть два предложения - изначальное и с заменённым словом.

157
00:33:14,465 --> 00:33:25,930
Изначальное предложение помечается единицей, изменённое - нулём.

158
00:33:26,030 --> 00:33:40,800
На таких данных была построена модель машинного обучения по поиску неправильных предложений.

159
00:33:40,800 --> 00:33:45,113
Нет необходимости уметь искать неправильные предложения, важны вычисленные в процессе обучения эмбеддинги.

160
00:33:45,213 --> 00:33:50,360
Эти эмбеддинги используются в других задачах, так работает Word2vec.

161
00:33:50,720 --> 00:34:01,860
Если реализовать это без глубокого обучения как умножение матриц, обучение будет очень быстрым.

162
00:34:01,860 --> 00:34:15,989
При использовании неглубокого обучения теряется точность предсказаний. Из преимуществ -

163
00:34:15,989 --> 00:34:27,265
модель можно обучать на огромных датасетах, а полученные эмбеддинги будут близки к линейным,

164
00:34:27,365 --> 00:34:33,715
поэтому их можно будет складывать и вычитать.

165
00:34:33,815 --> 00:34:52,710
Если нам нужна линейность эмбеддингов категориальных переменных,

166
00:34:52,710 --> 00:34:57,810
например, для использования метода ближайших соседей,

167
00:34:57,810 --> 00:35:03,990
то для их вычисления стоит использовать неглубокое обучение.

168
00:35:03,990 --> 00:35:09,300
Если хочется большей точности предсказаний, нужно использовать нейронные сети.

169
00:35:09,300 --> 00:35:19,495
Я считаю, что в обработке естественного языка нужно отбросить линейные Word2vec и Glove,

170
00:35:19,595 --> 00:35:25,260
потому что получаемые с их помощью эмбеддинги сильно уступают моделям глубокого обучения.

171
00:35:25,260 --> 00:35:30,520
Модель обработки естественного языка с прошлой лекции для анализа тональностей

172
00:35:30,750 --> 00:35:35,790
основывалась на рекуррентной нейронной сети, а не на Word2vec, и в результате обучения

173
00:35:35,790 --> 00:35:44,620
мы получили не только эмбеддинги, но и предобученную модель для других задач.

174
00:35:44,720 --> 00:35:51,010
Вопрос из зала: То есть для создания эмбеддингов нужна фиктивная задача обучения?

175
00:35:51,110 --> 00:35:59,810
Не обязательно фиктивная - мы создали эмбеддинги для соревнования Rossman, пытаясь предсказать реальные продажи.

176
00:35:59,910 --> 00:36:06,335
Идея более общая - для создания любого пространства признаков, не обязательно эмбеддингов,

177
00:36:06,435 --> 00:36:16,200
нужны либо размеченные данные, либо фиктивная задача обучения.

178
00:36:16,200 --> 00:36:26,550
Вопрос из зала: Как выбрать хорошую фиктивную задачу?

179
00:36:26,550 --> 00:36:31,050
Отличный вопрос, к сожалению, этот вопрос мало изучен.

180
00:36:31,050 --> 00:36:45,030
Мало кто хотя бы понимает, что обучение без учителя - это обучение с учителем на фиктивных задачах.

181
00:36:45,030 --> 00:36:49,650
Я не видел статей на эту тему.

182
00:36:49,650 --> 00:37:05,665
Вам нужна задача, которая позволит лучше изучить те закономерности в данных, которые вам нужны.

183
00:37:05,765 --> 00:37:19,930
Например, в компьютерном зрении используют дополнение данных, искажающее реальность -

184
00:37:20,030 --> 00:37:37,200
например, заменяют цвета на неестественные. Нейронную сеть обучают находить неискажённые изображения.

185
00:37:37,200 --> 00:37:42,565
Вопрос очень интересный, хотелось бы в нём разобраться.

186
00:37:42,665 --> 00:37:47,310
Будет классно, если кто-то из вас скачает несколько датасетов для обучения без учителя или частичного обучения

187
00:37:47,310 --> 00:37:59,640
и посмотрит, как на качество модели влияют разные фиктивные задачи обучения.

188
00:37:59,640 --> 00:38:09,869
Не можете придумать гениальную фиктивную задачу - поставьте хоть какую-нибудь, многого не требуется.

189
00:38:09,869 --> 00:38:15,119
Показательно топорная фиктивная задача - автокодировщик,

190
00:38:15,119 --> 00:38:23,965
его использовали победители недавнего соревнования Kaggle по предсказанию страховых возмещений.

191
00:38:24,065 --> 00:38:28,675
В данных были страховые полисы с известными страховыми возмещениями

192
00:38:28,775 --> 00:38:37,090
и страховые полисы, выплаты по которым ещё не были произведены и поэтому неизвстны.

193
00:38:37,190 --> 00:38:44,880
На вход нейронной сети для обучения подавались данные о страховом полисе,

194
00:38:44,980 --> 00:38:52,733
и на выходе сеть должна была воссоздать эту информацию.

195
00:38:52,833 --> 00:39:01,585
При этом хотя бы в одном скрытом слоё должно быть меньше активаций, чем во входных данных.

196
00:39:01,685 --> 00:39:13,590
Например, если страховой полис определяется 100 признаками, в середине должен быть слой с 20 активациями.

197
00:39:13,590 --> 00:39:23,970
Такая постановка задачи не требует дополнительного кода, можно использовать модели PyTorch или fastai,

198
00:39:25,619 --> 00:39:31,710
просто передав в качестве целевой переменной начальные данные.

199
00:39:31,710 --> 00:39:39,490
Автокодировщик - самая простая фиктивная задача из возможных, и работает он на удивление хорошо.

200
00:39:39,590 --> 00:39:44,392
Настолько хорошо, что благодаря ему люди смогли выиграть соревнование на Kaggle.

201
00:39:44,492 --> 00:39:53,155
Победители использовали созданные автокодировщиком признаки для обучения другой нейронной сети.

202
00:39:53,255 --> 00:40:06,024
Если наберётся достаточно заинтересованных, мы обсудим обучение без учителя во второй части курса.

203
00:40:09,424 --> 00:40:24,760
Вопрос из зала: Полезна ли будет модель, обученная на данных ArXiv, для работы с датасетом IMDb?

204
00:40:24,860 --> 00:40:36,839
Отличный вопрос. Мы обсуждали это с Себастьяном и планировали заняться этим в январе.

205
00:40:36,839 --> 00:40:47,877
В компьютерном зрении предобученные на кошках и собаках модели отлично работают для рентген-диагностики,

206
00:40:48,577 --> 00:40:53,220
но этому пока нет аналога в обработке естественного языка.

207
00:40:53,320 --> 00:41:01,680
Людям кажется, что это не сработает, поэтому они даже не пробуют, хотя я считаю это перспективным.

208
00:41:04,390 --> 00:41:17,810
Мне было интересно качество нашей модели для соревнования Rossman,

209
00:41:17,810 --> 00:41:23,410
потому что в публичном рейтинге она выглядела неубедительно.

210
00:41:23,510 --> 00:41:30,690
Ещё мне было интересны результаты на настоящей тестовой выборке.

211
00:41:30,690 --> 00:41:46,650
Я добавил обработку тестовой выборки в Jupyter ноутбук lesson3-rossman.ipynb.

212
00:41:46,650 --> 00:42:00,380
Я мог обернуть этот код в метод и вызвать его дважды, а не городить копии кода, но решил оставить так,

213
00:42:00,380 --> 00:42:08,010
чтобы вы видели все этапы и спободнее экспериментировали.

214
00:42:08,010 --> 00:42:24,480
Обработка одинаковая для обучающей и тестовой выборке, кое-где я использовал циклы.

215
00:42:24,480 --> 00:42:34,140
Ячейки [34] и [56] выполняются не подряд. Сначала выполняется ячейка [34], ячейка [56] пропускается

216
00:42:34,140 --> 00:42:45,900
и выполняется код под ними, а потом нужно вернуться к ячейке [56] и проделать то же самое для неё.

217
00:42:45,900 --> 00:42:49,950
На этой неделе меня спрашивали, почему код не работает, и это важное напоминание о том,

218
00:42:52,260 --> 00:43:00,180
что не стоит бездумно выполнять все ячейки подряд.

219
00:43:03,960 --> 00:43:15,680
В Jupyter ноутбуках к первым лекциям можно было так делать, но сейчас пора думать о том, что вы делаете.

220
00:43:15,780 --> 00:43:21,275
Вопрос из зала: Чем неглубокое обучение отличается от глубокого обучения?

221
00:43:21,375 --> 00:43:32,975
В неглубоком обучении нет скрытых слоёв, только скалярное произведение векторов или произведение матриц.

222
00:43:35,975 --> 00:43:50,040
Итак, после обработки у нас есть обучающая и тестовая выборка, остальное без изменений.

223
00:43:50,040 --> 00:43:58,000
Многое из этого Jupyter ноутбука не относится к глубокому обучению, поэтому обсуждается в курсе "Машинное обучение".

224
00:43:58,100 --> 00:44:10,040
Вместо функции train_cats() используется apply_cats(), чтобы у выборок была общая нумерация категориальных признаков.

225
00:44:12,815 --> 00:44:27,440
Объект mapper, содержащий средние и стандартные отклонения, тоже должен быть общим.

226
00:44:27,440 --> 00:44:46,665
После этого тестовая выборка передаётся в модель привычным образом, обучение проходит как обычно.

227
00:44:46,765 --> 00:44:57,710
Для получения предсказаний используется метод m.predict, параметр True указывает на наличие тестовой выборки.

228
00:44:57,710 --> 00:45:03,650
Полученные предсказания отправляются на Kaggle.

229
00:45:03,650 --> 00:45:38,890
Результат интересный. В публичном рейтинге я занял бы примерно трёхсотое место, а в приватном - пятое.

230
00:45:38,890 --> 00:45:56,420
Если вы участвуете в соревновании и не работали над валидационной выборкой, с вами может случиться обратное.

231
00:45:56,420 --> 00:46:01,970
Например, в соревновании по распознаванию айсбергов валидационная выборка публичного рейтинга

232
00:46:04,940 --> 00:46:10,640
содержит много дополненных до бессмыслицы данных.

233
00:46:10,640 --> 00:46:21,859
Тут созданная вами валидационная выборка принесёт гораздо больше пользы, чем место в публичном рейтинге.

234
00:46:21,859 --> 00:46:35,820
Значение функции потерь на наших предсказаниях близко к третьему месту, я считаю, что мы поняли их подход.

235
00:46:40,700 --> 00:46:53,940
После соревнования Rossman люди начали писать о своих моделях, есть много полезных замечаний.

236
00:46:54,040 --> 00:47:02,480
Например, здесь авторы разделили продажи отдельных магазинов на продажи в воскресенье и другие дни,

237
00:47:02,480 --> 00:47:11,870
и выяснилось, что у некоторых магазинов продажи в воскресенье сильно отличаются.

238
00:47:11,870 --> 00:47:16,455
Такие описания помогают лучше понять, как и зачем анализировать ваши данные.

239
00:47:16,555 --> 00:47:29,990
Вот интересная закономерность, которую не учли обладатели третьего места.

240
00:47:29,990 --> 00:47:48,480
Видны всплески в продажах сразу перед закрытием и сразу после открытия.

241
00:47:48,580 --> 00:47:57,230
Обладатели третьего места удалили все данные с закрытыми магазинами, а зря.

242
00:47:57,230 --> 00:48:05,500
Не меняйте ваши данные, пока не убедитесь в том, что понимаете, что делаете.

243
00:48:05,600 --> 00:48:15,260
Я не проверял своё предположение, но думаю, что с учётом этой закономерности они бы победили.

244
00:48:15,260 --> 00:48:22,335
Насколько мне известно, в тестовой выборке не было закрытых магазинов, но их все равно стоило учесть.

245
00:48:22,435 --> 00:48:42,869
Если не обучать модель на выбросах, она не научится хорошо их предсказывать, и это её ухудшит.

246
00:48:42,869 --> 00:48:55,149
Давайте ещё раз посмотрим на код библиотеки, чтобы лучше понять модель Rossman.

247
00:49:02,559 --> 00:49:08,689
Я хочу убедиться, что вы ориентируетесь в коде и сможете разобраться сами.

248
00:49:08,789 --> 00:49:27,999
Давайте изучим класс ColumnarModelData. Раньше я показывал, как с помощью "??" смотреть исходный код.

249
00:49:31,259 --> 00:49:36,649
Иногда после чтения кода нужно уточнить код одной из составляющих, поэтому это не всегда удобно.

250
00:49:36,749 --> 00:49:50,674
Скорее всего, ваш текстовый редактор может открывать файлы по SSH и перемещаться по ним.

251
00:49:50,774 --> 00:50:08,285
В Vim я могу перейти к реализации класса ColumnarModelData командой :tag ColumnarModelData.

252
00:50:08,385 --> 00:50:19,569
Наведя курсор на класс DataLoader, я могу перейти к его реализации комбинацией клавиш Ctrl+|.

253
00:50:19,569 --> 00:50:26,170
Вернуться назад можно комбинацией клавиш Ctrl+t.

254
00:50:26,170 --> 00:50:30,729


255
00:50:34,710 --> 00:50:41,140


256
00:50:41,140 --> 00:50:49,739


257
00:50:49,739 --> 00:50:55,880


258
00:50:55,880 --> 00:50:59,420


259
00:50:59,420 --> 00:51:10,970


260
00:51:13,549 --> 00:51:20,990


261
00:51:20,990 --> 00:51:25,369


262
00:51:25,369 --> 00:51:30,200


263
00:51:34,519 --> 00:51:42,890


264
00:51:42,890 --> 00:52:04,819


265
00:52:04,819 --> 00:52:15,410


266
00:52:15,410 --> 00:52:21,859


267
00:52:21,859 --> 00:52:30,079


268
00:52:30,079 --> 00:52:36,950


269
00:52:39,289 --> 00:52:43,759


270
00:52:43,759 --> 00:52:49,720


271
00:52:49,720 --> 00:52:56,599


272
00:52:56,599 --> 00:53:01,970


273
00:53:04,789 --> 00:53:08,839


274
00:53:11,479 --> 00:53:16,819


275
00:53:16,819 --> 00:53:20,509


276
00:53:20,509 --> 00:53:23,210


277
00:53:23,210 --> 00:53:29,019


278
00:53:30,400 --> 00:53:35,269


279
00:53:38,960 --> 00:53:44,749


280
00:53:47,329 --> 00:53:53,779


281
00:53:53,779 --> 00:53:57,079


282
00:53:57,079 --> 00:54:00,710


283
00:54:00,710 --> 00:54:08,930


284
00:54:08,930 --> 00:54:16,249


285
00:54:16,249 --> 00:54:25,069


286
00:54:25,069 --> 00:54:28,999


287
00:54:28,999 --> 00:54:32,569


288
00:54:32,569 --> 00:54:38,809


289
00:54:38,809 --> 00:54:44,299


290
00:54:44,299 --> 00:54:49,309


291
00:54:49,309 --> 00:54:53,210


292
00:54:53,210 --> 00:55:00,200


293
00:55:00,200 --> 00:55:05,869


294
00:55:05,869 --> 00:55:11,269


295
00:55:13,759 --> 00:55:18,109


296
00:55:18,109 --> 00:55:21,200


297
00:55:21,200 --> 00:55:27,020


298
00:55:27,020 --> 00:55:33,470


299
00:55:33,470 --> 00:55:38,120


300
00:55:38,120 --> 00:55:42,650


301
00:55:42,650 --> 00:55:49,370


302
00:55:49,370 --> 00:55:56,960


303
00:55:56,960 --> 00:56:06,320


304
00:56:06,320 --> 00:56:13,400


305
00:56:13,400 --> 00:56:19,610


306
00:56:19,610 --> 00:56:24,710


307
00:56:24,710 --> 00:56:28,220


308
00:56:30,830 --> 00:56:36,380


309
00:56:36,380 --> 00:56:44,750


310
00:56:44,750 --> 00:56:48,920


311
00:56:48,920 --> 00:56:52,970


312
00:56:55,910 --> 00:57:00,470


313
00:57:00,470 --> 00:57:08,180


314
00:57:08,180 --> 00:57:11,900


315
00:57:11,900 --> 00:57:16,550


316
00:57:16,550 --> 00:57:22,250


317
00:57:22,250 --> 00:57:29,240


318
00:57:29,240 --> 00:57:33,940


319
00:57:33,940 --> 00:57:40,160


320
00:57:40,160 --> 00:57:49,880


321
00:57:49,880 --> 00:57:55,010


322
00:57:55,010 --> 00:57:59,960


323
00:57:59,960 --> 00:58:03,260


324
00:58:03,260 --> 00:58:07,730


325
00:58:07,730 --> 00:58:11,120


326
00:58:11,120 --> 00:58:17,890


327
00:58:17,890 --> 00:58:25,730


328
00:58:25,730 --> 00:58:30,950


329
00:58:30,950 --> 00:58:37,580


330
00:58:37,580 --> 00:58:43,250


331
00:58:43,250 --> 00:58:47,960


332
00:58:47,960 --> 00:58:51,200


333
00:58:51,200 --> 00:58:56,630


334
00:58:56,630 --> 00:59:04,210


335
00:59:04,210 --> 00:59:09,320


336
00:59:09,320 --> 00:59:12,500


337
00:59:12,500 --> 00:59:19,400


338
00:59:19,400 --> 00:59:27,580


339
00:59:30,640 --> 00:59:35,319


340
00:59:35,319 --> 00:59:42,160


341
00:59:42,160 --> 00:59:47,650


342
00:59:47,650 --> 00:59:56,160


343
00:59:56,670 --> 01:00:02,079


344
01:00:02,079 --> 01:00:06,760


345
01:00:06,760 --> 01:00:13,390


346
01:00:17,549 --> 01:00:26,500


347
01:00:26,500 --> 01:00:32,160


348
01:00:32,160 --> 01:00:38,710


349
01:00:38,710 --> 01:00:46,750


350
01:00:46,750 --> 01:00:56,019


351
01:00:56,019 --> 01:01:00,609


352
01:01:00,609 --> 01:01:05,500


353
01:01:05,500 --> 01:01:10,990


354
01:01:10,990 --> 01:01:22,000


355
01:01:22,000 --> 01:01:25,990


356
01:01:25,990 --> 01:01:30,130


357
01:01:30,130 --> 01:01:33,849


358
01:01:33,849 --> 01:01:36,339


359
01:01:36,339 --> 01:01:39,970


360
01:01:39,970 --> 01:01:44,020


361
01:01:44,020 --> 01:01:48,250


362
01:01:48,250 --> 01:01:54,160


363
01:01:54,160 --> 01:01:59,110


364
01:01:59,110 --> 01:02:02,500


365
01:02:02,500 --> 01:02:07,420


366
01:02:07,420 --> 01:02:13,510


367
01:02:13,510 --> 01:02:21,520


368
01:02:24,520 --> 01:02:29,560


369
01:02:29,560 --> 01:02:35,350


370
01:02:35,350 --> 01:02:38,110


371
01:02:38,110 --> 01:02:41,260


372
01:02:41,260 --> 01:02:45,700


373
01:02:47,440 --> 01:02:51,970


374
01:02:53,980 --> 01:03:00,550


375
01:03:00,550 --> 01:03:03,940


376
01:03:03,940 --> 01:03:07,140


377
01:03:07,140 --> 01:03:14,530


378
01:03:14,530 --> 01:03:19,060


379
01:03:20,560 --> 01:03:31,090


380
01:03:31,090 --> 01:03:40,600


381
01:03:42,520 --> 01:03:45,570


382
01:03:45,570 --> 01:03:52,390


383
01:03:56,330 --> 01:04:01,040


384
01:04:01,040 --> 01:04:05,420


385
01:04:05,420 --> 01:04:10,160


386
01:04:10,160 --> 01:04:15,950


387
01:04:15,950 --> 01:04:21,050


388
01:04:21,050 --> 01:04:26,090


389
01:04:26,090 --> 01:04:32,180


390
01:04:34,130 --> 01:04:38,210


391
01:04:38,210 --> 01:04:43,790


392
01:04:43,790 --> 01:04:48,830


393
01:04:48,830 --> 01:04:57,620


394
01:04:57,620 --> 01:05:02,390


395
01:05:02,390 --> 01:05:07,880


396
01:05:07,880 --> 01:05:14,530


397
01:05:14,530 --> 01:05:21,380


398
01:05:21,380 --> 01:05:26,750


399
01:05:26,750 --> 01:05:31,340


400
01:05:31,340 --> 01:05:37,030


401
01:05:37,030 --> 01:05:43,220


402
01:05:45,980 --> 01:05:51,950


403
01:05:51,950 --> 01:05:56,560


404
01:05:56,560 --> 01:06:00,830


405
01:06:00,830 --> 01:06:05,960


406
01:06:05,960 --> 01:06:08,430


407
01:06:08,430 --> 01:06:12,510


408
01:06:12,510 --> 01:06:18,860


409
01:06:18,860 --> 01:06:25,710


410
01:06:25,710 --> 01:06:34,260


411
01:06:34,260 --> 01:06:38,850


412
01:06:41,850 --> 01:06:48,600


413
01:06:48,600 --> 01:06:53,430


414
01:06:53,430 --> 01:06:56,730


415
01:06:56,730 --> 01:07:04,320


416
01:07:04,320 --> 01:07:10,740


417
01:07:12,960 --> 01:07:18,060


418
01:07:18,060 --> 01:07:23,970


419
01:07:23,970 --> 01:07:28,230


420
01:07:28,230 --> 01:07:32,760


421
01:07:32,760 --> 01:07:37,920


422
01:07:37,920 --> 01:07:41,400


423
01:07:41,400 --> 01:07:46,260


424
01:07:46,260 --> 01:07:52,770


425
01:07:54,810 --> 01:08:02,670


426
01:08:02,670 --> 01:08:07,590


427
01:08:07,590 --> 01:08:14,820


428
01:08:14,820 --> 01:08:20,370


429
01:08:20,370 --> 01:08:24,029


430
01:08:24,029 --> 01:08:30,119


431
01:08:33,238 --> 01:08:40,249


432
01:08:40,250 --> 01:08:46,739


433
01:08:46,738 --> 01:08:52,198


434
01:08:52,198 --> 01:08:55,408


435
01:08:55,408 --> 01:08:59,218


436
01:08:59,219 --> 01:09:04,710


437
01:09:04,710 --> 01:09:09,779


438
01:09:09,779 --> 01:09:15,839


439
01:09:19,439 --> 01:09:29,629


440
01:09:29,630 --> 01:09:34,520


441
01:09:34,549 --> 01:09:41,960


442
01:09:41,960 --> 01:09:47,339


443
01:09:47,339 --> 01:09:49,799


444
01:09:49,799 --> 01:09:54,599


445
01:09:54,599 --> 01:09:59,909


446
01:10:02,159 --> 01:10:07,800


447
01:10:07,800 --> 01:10:16,110


448
01:10:18,119 --> 01:10:21,570


449
01:10:21,570 --> 01:10:26,760


450
01:10:26,760 --> 01:10:31,469


451
01:10:31,469 --> 01:10:35,700


452
01:10:35,700 --> 01:10:41,130


453
01:10:41,130 --> 01:10:44,370


454
01:10:44,370 --> 01:10:48,780


455
01:10:48,780 --> 01:10:55,640


456
01:10:55,640 --> 01:10:59,790


457
01:10:59,790 --> 01:11:04,620


458
01:11:04,620 --> 01:11:10,620


459
01:11:10,620 --> 01:11:15,630


460
01:11:15,630 --> 01:11:19,710


461
01:11:19,710 --> 01:11:23,550


462
01:11:23,550 --> 01:11:29,670


463
01:11:29,670 --> 01:11:34,890


464
01:11:34,890 --> 01:11:38,520


465
01:11:38,520 --> 01:11:42,030


466
01:11:43,530 --> 01:11:47,370


467
01:11:47,370 --> 01:11:53,310


468
01:11:53,310 --> 01:11:57,030


469
01:12:02,250 --> 01:12:06,840


470
01:12:06,840 --> 01:12:10,950


471
01:12:10,950 --> 01:12:15,780


472
01:12:15,780 --> 01:12:19,530


473
01:12:19,530 --> 01:12:23,190


474
01:12:23,190 --> 01:12:27,180


475
01:12:27,180 --> 01:12:30,600


476
01:12:30,600 --> 01:12:35,310


477
01:12:35,310 --> 01:12:39,630


478
01:12:39,630 --> 01:12:45,420


479
01:12:45,420 --> 01:12:49,550


480
01:12:49,550 --> 01:12:54,600


481
01:12:54,600 --> 01:12:58,890


482
01:12:58,890 --> 01:13:07,890


483
01:13:07,890 --> 01:13:15,330


484
01:13:15,330 --> 01:13:23,969


485
01:13:23,969 --> 01:13:35,489


486
01:13:38,670 --> 01:13:43,980


487
01:13:47,960 --> 01:13:56,340


488
01:13:56,340 --> 01:14:02,340


489
01:14:02,340 --> 01:14:06,660


490
01:14:06,660 --> 01:14:11,969


491
01:14:11,969 --> 01:14:17,040


492
01:14:17,040 --> 01:14:23,940


493
01:14:23,940 --> 01:14:29,670


494
01:14:29,670 --> 01:14:35,760


495
01:14:35,760 --> 01:14:41,250


496
01:14:41,250 --> 01:14:45,120


497
01:14:45,120 --> 01:14:50,190


498
01:14:50,190 --> 01:14:56,940


499
01:14:56,940 --> 01:15:02,520


500
01:15:02,520 --> 01:15:06,540


501
01:15:08,450 --> 01:15:12,540


502
01:15:12,540 --> 01:15:17,520


503
01:15:17,520 --> 01:15:21,300


504
01:15:21,300 --> 01:15:29,520


505
01:15:29,520 --> 01:15:35,910


506
01:15:39,720 --> 01:15:43,950


507
01:15:43,950 --> 01:15:47,910


508
01:15:47,910 --> 01:15:51,900


509
01:15:51,900 --> 01:15:56,070


510
01:15:56,070 --> 01:16:02,880


511
01:16:02,880 --> 01:16:08,460


512
01:16:08,460 --> 01:16:13,380


513
01:16:13,380 --> 01:16:18,720


514
01:16:18,720 --> 01:16:21,570


515
01:16:26,070 --> 01:16:30,780


516
01:16:30,780 --> 01:16:34,620


517
01:16:34,620 --> 01:16:38,670


518
01:16:38,670 --> 01:16:42,720


519
01:16:42,720 --> 01:16:45,960


520
01:16:45,960 --> 01:16:50,430


521
01:16:50,430 --> 01:16:56,100


522
01:16:56,100 --> 01:17:01,320


523
01:17:03,390 --> 01:17:08,070


524
01:17:09,270 --> 01:17:15,420


525
01:17:15,420 --> 01:17:17,580


526
01:17:17,580 --> 01:17:21,810


527
01:17:21,810 --> 01:17:28,020


528
01:17:28,020 --> 01:17:33,240


529
01:17:34,920 --> 01:17:40,770


530
01:17:40,770 --> 01:17:46,260


531
01:17:46,260 --> 01:17:51,800


532
01:17:51,800 --> 01:17:59,190


533
01:18:02,220 --> 01:18:06,360


534
01:18:07,500 --> 01:18:14,250


535
01:18:14,250 --> 01:18:18,930


536
01:18:18,930 --> 01:18:25,640


537
01:18:25,640 --> 01:18:33,810


538
01:18:33,810 --> 01:18:39,390


539
01:18:39,390 --> 01:18:43,680


540
01:18:43,680 --> 01:18:50,700


541
01:18:54,660 --> 01:18:59,490


542
01:18:59,490 --> 01:19:04,020


543
01:19:04,020 --> 01:19:08,400


544
01:19:09,720 --> 01:19:15,000


545
01:19:15,000 --> 01:19:23,880


546
01:19:23,880 --> 01:19:27,030


547
01:19:27,030 --> 01:19:31,670


548
01:19:31,670 --> 01:19:36,410


549
01:19:36,410 --> 01:19:39,410


550
01:19:39,410 --> 01:19:45,050


551
01:19:45,050 --> 01:19:50,230


552
01:19:53,600 --> 01:19:56,600


553
01:19:56,600 --> 01:20:02,960


554
01:20:02,960 --> 01:20:05,690


555
01:20:05,690 --> 01:20:10,520


556
01:20:10,520 --> 01:20:18,350


557
01:20:18,350 --> 01:20:23,090


558
01:20:23,090 --> 01:20:29,240


559
01:20:29,240 --> 01:20:34,760


560
01:20:34,760 --> 01:20:39,530


561
01:20:46,160 --> 01:20:51,050


562
01:20:51,050 --> 01:20:56,030


563
01:20:56,030 --> 01:21:02,390


564
01:21:02,390 --> 01:21:08,090


565
01:21:08,090 --> 01:21:15,110


566
01:21:17,210 --> 01:21:20,180


567
01:21:20,180 --> 01:21:29,030


568
01:21:33,350 --> 01:21:41,200


569
01:21:42,600 --> 01:21:48,390


570
01:21:48,390 --> 01:21:54,720


571
01:21:54,720 --> 01:22:08,010


572
01:22:08,010 --> 01:22:13,560


573
01:22:13,560 --> 01:22:20,060


574
01:22:20,060 --> 01:22:23,430


575
01:22:23,430 --> 01:22:30,330


576
01:22:30,330 --> 01:22:36,540


577
01:22:36,540 --> 01:22:42,180


578
01:22:42,180 --> 01:22:50,100


579
01:22:50,100 --> 01:22:54,390


580
01:22:54,390 --> 01:22:58,560


581
01:22:58,560 --> 01:23:02,250


582
01:23:02,250 --> 01:23:05,970


583
01:23:05,970 --> 01:23:09,390


584
01:23:09,390 --> 01:23:13,200


585
01:23:15,810 --> 01:23:22,830


586
01:23:24,020 --> 01:23:30,540


587
01:23:33,170 --> 01:23:38,430


588
01:23:38,430 --> 01:23:43,110


589
01:23:44,940 --> 01:23:52,650


590
01:23:52,650 --> 01:23:55,679


591
01:23:55,679 --> 01:24:00,659


592
01:24:00,659 --> 01:24:04,559


593
01:24:04,559 --> 01:24:08,459


594
01:24:08,459 --> 01:24:17,909


595
01:24:17,909 --> 01:24:21,929


596
01:24:21,929 --> 01:24:28,559


597
01:24:28,559 --> 01:24:36,840


598
01:24:36,840 --> 01:24:41,070


599
01:24:41,070 --> 01:24:46,110


600
01:24:46,110 --> 01:24:53,909


601
01:24:53,909 --> 01:25:01,639


602
01:25:01,639 --> 01:25:05,519


603
01:25:05,519 --> 01:25:23,249


604
01:25:23,249 --> 01:25:33,749


605
01:25:33,749 --> 01:25:40,949


606
01:25:40,949 --> 01:25:46,289


607
01:25:46,289 --> 01:25:52,739


608
01:25:52,739 --> 01:25:59,030


609
01:25:59,030 --> 01:26:08,550


610
01:26:08,550 --> 01:26:11,159


611
01:26:18,299 --> 01:26:21,899


612
01:26:21,899 --> 01:26:27,449


613
01:26:28,709 --> 01:26:32,729


614
01:26:32,729 --> 01:26:38,579


615
01:26:41,249 --> 01:26:45,869


616
01:26:45,869 --> 01:26:51,299


617
01:26:51,299 --> 01:26:58,319


618
01:26:58,319 --> 01:27:05,609


619
01:27:05,609 --> 01:27:09,689


620
01:27:09,689 --> 01:27:15,359


621
01:27:15,359 --> 01:27:22,499


622
01:27:24,089 --> 01:27:29,369


623
01:27:29,369 --> 01:27:33,029


624
01:27:33,029 --> 01:27:36,599


625
01:27:36,599 --> 01:27:39,959


626
01:27:39,959 --> 01:27:44,189


627
01:27:44,189 --> 01:27:48,659


628
01:27:51,149 --> 01:27:53,819


629
01:27:53,819 --> 01:28:00,599


630
01:28:00,599 --> 01:28:06,899


631
01:28:06,899 --> 01:28:11,819


632
01:28:11,819 --> 01:28:18,959


633
01:28:20,609 --> 01:28:23,010


634
01:28:23,010 --> 01:28:29,519


635
01:28:29,519 --> 01:28:34,650


636
01:28:37,110 --> 01:28:42,480


637
01:28:42,480 --> 01:28:46,769


638
01:28:46,769 --> 01:28:51,920


639
01:28:51,920 --> 01:29:07,769


640
01:29:07,769 --> 01:29:12,539


641
01:29:17,250 --> 01:29:21,510


642
01:29:21,510 --> 01:29:30,239


643
01:29:32,489 --> 01:29:36,960


644
01:29:36,960 --> 01:29:43,710


645
01:29:43,710 --> 01:29:52,619


646
01:29:52,619 --> 01:30:01,980


647
01:30:01,980 --> 01:30:07,980


648
01:30:07,980 --> 01:30:16,079


649
01:30:16,079 --> 01:30:24,000


650
01:30:24,000 --> 01:30:30,869


651
01:30:30,869 --> 01:30:35,970


652
01:30:35,970 --> 01:30:39,180


653
01:30:39,180 --> 01:30:46,290


654
01:30:46,290 --> 01:30:50,460


655
01:30:50,460 --> 01:30:56,400


656
01:30:56,400 --> 01:31:01,770


657
01:31:01,770 --> 01:31:05,250


658
01:31:05,250 --> 01:31:09,330


659
01:31:09,330 --> 01:31:19,920


660
01:31:19,920 --> 01:31:25,500


661
01:31:25,500 --> 01:31:29,790


662
01:31:29,790 --> 01:31:32,970


663
01:31:32,970 --> 01:31:40,200


664
01:31:40,200 --> 01:31:49,110


665
01:31:49,110 --> 01:31:54,360


666
01:31:54,360 --> 01:32:00,810


667
01:32:00,810 --> 01:32:05,130


668
01:32:05,130 --> 01:32:09,000


669
01:32:09,000 --> 01:32:16,470


670
01:32:16,470 --> 01:32:24,870


671
01:32:24,870 --> 01:32:28,710


672
01:32:28,710 --> 01:32:33,570


673
01:32:33,570 --> 01:32:38,490


674
01:32:38,490 --> 01:32:43,430


675
01:32:43,430 --> 01:32:49,889


676
01:32:49,889 --> 01:32:53,579


677
01:32:53,579 --> 01:32:58,349


678
01:32:58,349 --> 01:33:06,959


679
01:33:06,959 --> 01:33:11,369


680
01:33:11,369 --> 01:33:14,059


681
01:33:14,189 --> 01:33:18,209


682
01:33:23,309 --> 01:33:27,059


683
01:33:27,059 --> 01:33:30,239


684
01:33:33,209 --> 01:33:36,929


685
01:33:36,929 --> 01:33:40,619


686
01:33:40,619 --> 01:33:50,939


687
01:33:50,939 --> 01:33:56,429


688
01:33:56,429 --> 01:34:01,530


689
01:34:01,530 --> 01:34:08,280


690
01:34:08,280 --> 01:34:16,169


691
01:34:16,169 --> 01:34:22,050


692
01:34:22,050 --> 01:34:32,459


693
01:34:32,459 --> 01:34:38,579


694
01:34:38,579 --> 01:34:42,959


695
01:34:42,959 --> 01:34:49,079


696
01:34:49,079 --> 01:34:56,280


697
01:34:56,280 --> 01:35:01,439


698
01:35:01,439 --> 01:35:05,739


699
01:35:07,480 --> 01:35:12,250


700
01:35:12,250 --> 01:35:15,880


701
01:35:15,880 --> 01:35:22,270


702
01:35:22,270 --> 01:35:27,160


703
01:35:29,739 --> 01:35:34,719


704
01:35:34,719 --> 01:35:43,900


705
01:35:43,900 --> 01:35:47,469


706
01:35:47,469 --> 01:35:50,530


707
01:35:50,530 --> 01:36:01,150


708
01:36:01,150 --> 01:36:10,630


709
01:36:10,630 --> 01:36:17,980


710
01:36:19,750 --> 01:36:26,590


711
01:36:26,590 --> 01:36:28,420


712
01:36:28,420 --> 01:36:35,080


713
01:36:35,080 --> 01:36:40,719


714
01:36:40,719 --> 01:36:45,730


715
01:36:47,320 --> 01:36:51,310


716
01:36:51,310 --> 01:36:56,200


717
01:37:00,219 --> 01:37:05,110


718
01:37:05,110 --> 01:37:08,820


719
01:37:08,820 --> 01:37:15,000


720
01:37:15,000 --> 01:37:21,610


721
01:37:21,610 --> 01:37:27,190


722
01:37:27,190 --> 01:37:36,540


723
01:37:36,540 --> 01:37:42,510


724
01:37:42,960 --> 01:37:59,440


725
01:37:59,440 --> 01:38:05,800


726
01:38:05,800 --> 01:38:10,210


727
01:38:10,210 --> 01:38:14,710


728
01:38:14,710 --> 01:38:18,550


729
01:38:18,550 --> 01:38:24,280


730
01:38:24,280 --> 01:38:32,530


731
01:38:32,530 --> 01:38:35,950


732
01:38:35,950 --> 01:38:39,790


733
01:38:39,790 --> 01:38:44,710


734
01:38:44,710 --> 01:38:49,270


735
01:38:49,270 --> 01:38:53,560


736
01:38:53,560 --> 01:39:01,480


737
01:39:04,210 --> 01:39:07,930


738
01:39:07,930 --> 01:39:16,330


739
01:39:16,330 --> 01:39:21,670


740
01:39:21,670 --> 01:39:26,170


741
01:39:26,170 --> 01:39:30,699


742
01:39:39,550 --> 01:39:45,909


743
01:39:47,800 --> 01:39:52,929


744
01:39:52,929 --> 01:39:57,369


745
01:39:57,369 --> 01:40:02,679


746
01:40:02,679 --> 01:40:12,579


747
01:40:12,579 --> 01:40:21,999


748
01:40:21,999 --> 01:40:26,019


749
01:40:26,019 --> 01:40:30,179


750
01:40:30,179 --> 01:40:34,300


751
01:40:34,300 --> 01:40:40,900


752
01:40:44,019 --> 01:40:49,539


753
01:40:51,550 --> 01:40:55,630


754
01:40:55,630 --> 01:41:02,949


755
01:41:07,360 --> 01:41:12,159


756
01:41:12,159 --> 01:41:18,699


757
01:41:18,699 --> 01:41:22,630


758
01:41:22,630 --> 01:41:27,340


759
01:41:27,340 --> 01:41:31,300


760
01:41:31,300 --> 01:41:37,150


761
01:41:41,499 --> 01:41:48,760


762
01:41:48,760 --> 01:41:55,119


763
01:41:57,400 --> 01:42:05,290


764
01:42:05,290 --> 01:42:15,699


765
01:42:15,699 --> 01:42:20,050


766
01:42:20,050 --> 01:42:25,119


767
01:42:27,190 --> 01:42:33,130


768
01:42:33,130 --> 01:42:39,070


769
01:42:39,070 --> 01:42:46,380


770
01:42:46,380 --> 01:42:51,280


771
01:42:51,280 --> 01:42:57,270


772
01:42:57,270 --> 01:43:09,130


773
01:43:09,130 --> 01:43:14,110


774
01:43:14,110 --> 01:43:18,219


775
01:43:18,219 --> 01:43:25,210


776
01:43:25,210 --> 01:43:30,489


777
01:43:33,310 --> 01:43:38,730


778
01:43:43,940 --> 01:43:48,699


779
01:43:48,699 --> 01:43:59,719


780
01:43:59,719 --> 01:44:05,270


781
01:44:05,270 --> 01:44:09,800


782
01:44:09,800 --> 01:44:15,710


783
01:44:15,710 --> 01:44:22,969


784
01:44:22,969 --> 01:44:28,430


785
01:44:28,430 --> 01:44:33,949


786
01:44:36,650 --> 01:44:43,550


787
01:44:43,550 --> 01:44:50,719


788
01:44:50,719 --> 01:44:57,500


789
01:44:57,500 --> 01:45:03,140


790
01:45:03,140 --> 01:45:06,980


791
01:45:09,050 --> 01:45:16,010


792
01:45:16,010 --> 01:45:22,160


793
01:45:22,160 --> 01:45:26,630


794
01:45:26,630 --> 01:45:29,989


795
01:45:37,530 --> 01:45:45,030


796
01:45:45,030 --> 01:45:51,300


797
01:45:51,300 --> 01:45:58,710


798
01:45:58,710 --> 01:46:05,490


799
01:46:05,490 --> 01:46:09,510


800
01:46:09,510 --> 01:46:14,790


801
01:46:14,790 --> 01:46:18,030


802
01:46:18,030 --> 01:46:26,130


803
01:46:26,130 --> 01:46:31,260


804
01:46:31,260 --> 01:46:36,900


805
01:46:39,780 --> 01:46:44,790


806
01:46:44,790 --> 01:46:47,940


807
01:46:47,940 --> 01:46:53,760


808
01:46:53,760 --> 01:46:59,870


809
01:46:59,870 --> 01:47:06,650


810
01:47:06,650 --> 01:47:12,630


811
01:47:12,630 --> 01:47:19,590


812
01:47:19,590 --> 01:47:26,160


813
01:47:26,160 --> 01:47:33,270


814
01:47:33,270 --> 01:47:37,170


815
01:47:37,170 --> 01:47:45,150


816
01:47:45,150 --> 01:47:49,440


817
01:47:49,440 --> 01:47:54,550


818
01:47:54,550 --> 01:48:00,850


819
01:48:00,850 --> 01:48:05,460


820
01:48:05,460 --> 01:48:11,560


821
01:48:11,560 --> 01:48:17,469


822
01:48:17,469 --> 01:48:23,620


823
01:48:23,620 --> 01:48:26,440


824
01:48:26,440 --> 01:48:29,650


825
01:48:29,650 --> 01:48:34,719


826
01:48:34,719 --> 01:48:41,760


827
01:48:41,760 --> 01:48:49,440


828
01:48:53,860 --> 01:48:58,840


829
01:48:58,840 --> 01:49:05,290


830
01:49:05,290 --> 01:49:11,680


831
01:49:11,680 --> 01:49:19,180


832
01:49:19,180 --> 01:49:23,830


833
01:49:23,830 --> 01:49:28,030


834
01:49:28,030 --> 01:49:32,500


835
01:49:32,500 --> 01:49:36,340


836
01:49:36,340 --> 01:49:43,660


837
01:49:43,660 --> 01:49:51,550


838
01:49:51,550 --> 01:49:54,850


839
01:49:54,850 --> 01:49:59,739


840
01:50:01,480 --> 01:50:05,139


841
01:50:05,139 --> 01:50:14,080


842
01:50:14,080 --> 01:50:20,409


843
01:50:20,409 --> 01:50:25,540


844
01:50:25,540 --> 01:50:30,520


845
01:50:30,520 --> 01:50:36,340


846
01:50:36,340 --> 01:50:42,940


847
01:50:49,960 --> 01:51:06,010


848
01:51:06,010 --> 01:51:11,980


849
01:51:16,360 --> 01:51:22,330


850
01:51:22,330 --> 01:51:25,929


851
01:51:25,929 --> 01:51:29,400


852
01:51:29,400 --> 01:51:33,310


853
01:51:33,310 --> 01:51:38,110


854
01:51:40,989 --> 01:51:46,300


855
01:51:46,300 --> 01:51:50,940


856
01:51:57,630 --> 01:52:01,239


857
01:52:03,520 --> 01:52:12,429


858
01:52:12,429 --> 01:52:18,239


859
01:52:19,020 --> 01:52:25,180


860
01:52:25,180 --> 01:52:30,280


861
01:52:32,440 --> 01:52:36,940


862
01:52:36,940 --> 01:52:40,530


863
01:52:40,530 --> 01:52:44,920


864
01:52:44,920 --> 01:52:48,730


865
01:52:51,580 --> 01:52:56,170


866
01:52:56,170 --> 01:53:01,690


867
01:53:05,650 --> 01:53:13,930


868
01:53:13,930 --> 01:53:17,950


869
01:53:20,230 --> 01:53:24,780


870
01:53:24,780 --> 01:53:31,090


871
01:53:31,090 --> 01:53:39,610


872
01:53:43,060 --> 01:53:49,090


873
01:53:49,090 --> 01:53:54,460


874
01:53:54,460 --> 01:53:58,780


875
01:53:58,780 --> 01:54:02,610


876
01:54:02,610 --> 01:54:09,010


877
01:54:09,010 --> 01:54:14,890


878
01:54:14,890 --> 01:54:18,190


879
01:54:20,620 --> 01:54:31,420


880
01:54:31,420 --> 01:54:37,240


881
01:54:37,240 --> 01:54:41,260


882
01:54:41,260 --> 01:54:48,160


883
01:54:48,160 --> 01:54:53,560


884
01:54:53,560 --> 01:54:58,690


885
01:55:00,810 --> 01:55:06,100


886
01:55:06,100 --> 01:55:10,210


887
01:55:10,210 --> 01:55:14,680


888
01:55:14,680 --> 01:55:19,120


889
01:55:19,120 --> 01:55:24,130


890
01:55:24,130 --> 01:55:29,910


891
01:55:30,390 --> 01:55:37,750


892
01:55:37,750 --> 01:55:41,530


893
01:55:41,530 --> 01:55:47,580


894
01:55:47,580 --> 01:55:51,930


895
01:55:51,930 --> 01:55:59,140


896
01:55:59,140 --> 01:56:07,200


897
01:56:09,730 --> 01:56:13,780


898
01:56:13,780 --> 01:56:17,470


899
01:56:17,470 --> 01:56:24,240


900
01:56:26,470 --> 01:56:31,450


901
01:56:31,450 --> 01:56:37,590


902
01:56:37,590 --> 01:56:42,730


903
01:56:44,960 --> 01:56:55,400


904
01:56:55,400 --> 01:57:02,480


905
01:57:02,480 --> 01:57:09,610


906
01:57:09,610 --> 01:57:16,610


907
01:57:16,610 --> 01:57:21,560


908
01:57:21,560 --> 01:57:32,060


909
01:57:32,060 --> 01:57:38,660


910
01:57:41,960 --> 01:57:48,230


911
01:57:50,210 --> 01:57:55,790


912
01:57:55,790 --> 01:58:00,740


913
01:58:00,740 --> 01:58:04,220


914
01:58:04,220 --> 01:58:06,980


915
01:58:09,590 --> 01:58:18,080


916
01:58:18,080 --> 01:58:25,460


917
01:58:25,460 --> 01:58:29,860


918
01:58:29,860 --> 01:58:38,240


919
01:58:38,240 --> 01:58:43,940


920
01:58:43,940 --> 01:58:51,110


921
01:58:51,110 --> 01:58:58,440


922
01:58:58,440 --> 01:59:05,670


923
01:59:05,670 --> 01:59:14,489


924
01:59:17,250 --> 01:59:27,390


925
01:59:27,390 --> 01:59:39,800


926
01:59:40,489 --> 01:59:48,239


927
01:59:48,239 --> 01:59:52,410


928
01:59:52,410 --> 01:59:58,350


929
01:59:58,350 --> 02:00:07,890


930
02:00:07,890 --> 02:00:22,739


931
02:00:24,120 --> 02:00:31,260


932
02:00:31,260 --> 02:00:38,670


933
02:00:38,670 --> 02:00:46,670


934
02:00:46,670 --> 02:00:53,400


935
02:00:53,400 --> 02:00:59,910


936
02:00:59,910 --> 02:01:05,550


937
02:01:05,550 --> 02:01:11,960


938
02:01:12,330 --> 02:01:18,190


939
02:01:18,190 --> 02:01:24,370


940
02:01:24,370 --> 02:01:30,940


941
02:01:30,940 --> 02:01:37,389


942
02:01:37,389 --> 02:01:44,230


943
02:01:46,179 --> 02:01:50,230


944
02:01:54,429 --> 02:01:59,110


945
02:01:59,110 --> 02:02:05,260


946
02:02:05,260 --> 02:02:09,310


947
02:02:09,310 --> 02:02:14,530


948
02:02:17,110 --> 02:02:21,550


949
02:02:21,550 --> 02:02:28,000


950
02:02:28,000 --> 02:02:37,290


951
02:02:37,290 --> 02:02:45,340


952
02:02:45,340 --> 02:02:54,070


953
02:02:54,070 --> 02:02:59,199


954
02:02:59,199 --> 02:03:04,449


955
02:03:04,449 --> 02:03:10,090


956
02:03:10,090 --> 02:03:15,989


957
02:03:22,830 --> 02:03:28,550


958
02:03:28,550 --> 02:03:39,550


959
02:03:39,790 --> 02:03:53,270


960
02:03:53,270 --> 02:04:00,360


961
02:04:01,540 --> 02:04:06,500


962
02:04:06,500 --> 02:04:11,530


963
02:04:11,530 --> 02:04:17,830


964
02:04:17,830 --> 02:04:23,270


965
02:04:23,270 --> 02:04:30,949


966
02:04:33,260 --> 02:04:38,150


967
02:04:39,739 --> 02:04:44,239


968
02:04:44,239 --> 02:04:49,130


969
02:04:49,130 --> 02:04:53,719


970
02:04:53,719 --> 02:05:02,120


971
02:05:02,120 --> 02:05:09,920


972
02:05:09,920 --> 02:05:15,850


973
02:05:15,850 --> 02:05:20,420


974
02:05:20,420 --> 02:05:27,230


975
02:05:30,710 --> 02:05:37,480


976
02:05:39,790 --> 02:05:45,970


977
02:05:45,970 --> 02:05:53,290


978
02:05:53,290 --> 02:05:58,600


979
02:05:58,600 --> 02:06:06,460


980
02:06:06,460 --> 02:06:13,480


981
02:06:15,910 --> 02:06:20,500


982
02:06:20,500 --> 02:06:26,080


983
02:06:26,080 --> 02:06:31,930


984
02:06:31,930 --> 02:06:37,120


985
02:06:37,120 --> 02:06:46,480


986
02:06:46,480 --> 02:06:51,310


987
02:06:51,310 --> 02:06:55,390


988
02:06:55,390 --> 02:07:01,900


989
02:07:05,910 --> 02:07:13,750


990
02:07:13,750 --> 02:07:18,640


991
02:07:18,640 --> 02:07:26,880


992
02:07:26,880 --> 02:07:38,020


993
02:07:38,020 --> 02:07:43,000


994
02:07:43,000 --> 02:07:48,340


995
02:07:48,340 --> 02:07:53,109


996
02:07:53,109 --> 02:07:56,559


997
02:07:56,559 --> 02:08:09,069


998
02:08:09,069 --> 02:08:13,149


999
02:08:13,149 --> 02:08:21,339


1000
02:08:21,339 --> 02:08:25,869


1001
02:08:25,869 --> 02:08:33,909


1002
02:08:33,909 --> 02:08:37,689


1003
02:08:37,689 --> 02:08:46,419


1004
02:08:46,419 --> 02:08:50,439


1005
02:08:50,439 --> 02:08:54,869


1006
02:08:57,760 --> 02:09:01,659


1007
02:09:07,209 --> 02:09:16,359


1008
02:09:16,359 --> 02:09:22,149


1009
02:09:22,149 --> 02:09:29,939


1010
02:09:29,939 --> 02:09:35,589


1011
02:09:35,589 --> 02:09:42,309


1012
02:09:42,309 --> 02:09:49,839


1013
02:09:49,839 --> 02:09:54,519


1014
02:09:54,519 --> 02:09:59,289


1015
02:10:03,280 --> 02:10:08,080


1016
02:10:08,080 --> 02:10:14,350


1017
02:10:14,350 --> 02:10:17,410


1018
02:10:22,770 --> 02:10:29,110


1019
02:10:31,930 --> 02:10:35,770


1020
02:10:35,770 --> 02:10:39,610


1021
02:10:42,070 --> 02:10:46,390


1022
02:10:46,390 --> 02:10:55,030


1023
02:10:58,680 --> 02:11:07,920


1024
02:11:07,920 --> 02:11:14,949


1025
02:11:18,310 --> 02:11:29,110


1026
02:11:31,449 --> 02:11:38,190


1027
02:11:38,190 --> 02:11:44,440


1028
02:11:44,440 --> 02:11:49,870


1029
02:11:49,870 --> 02:11:54,550


1030
02:11:54,550 --> 02:11:58,090


1031
02:11:58,090 --> 02:12:02,620


1032
02:12:02,620 --> 02:12:07,890


1033
02:12:07,890 --> 02:12:12,969


1034
02:12:12,969 --> 02:12:16,900


1035
02:12:16,900 --> 02:12:22,360


1036
02:12:22,360 --> 02:12:26,739


1037
02:12:26,739 --> 02:12:32,620


1038
02:12:32,620 --> 02:12:36,429


1039
02:12:36,429 --> 02:12:41,890


1040
02:12:41,890 --> 02:12:49,179


1041
02:12:49,179 --> 02:12:53,650


1042
02:12:53,650 --> 02:12:59,140


1043
02:12:59,140 --> 02:13:02,290


1044
02:13:02,290 --> 02:13:05,679


1045
02:13:05,679 --> 02:13:09,310


1046
02:13:09,310 --> 02:13:15,250


1047
02:13:15,250 --> 02:13:20,560


1048
02:13:20,560 --> 02:13:23,830


1049
02:13:27,070 --> 02:13:30,070


1050
02:13:30,070 --> 02:13:35,350


1051
02:13:35,350 --> 02:13:40,679


1052
02:13:41,040 --> 02:13:43,960


