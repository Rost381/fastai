1
00:00:00,060 --> 00:00:14,991
Добро пожаловать на шестую лекцию, она предпоследняя в этом курсе.

2
00:00:15,091 --> 00:00:22,704
Пару недель назад я обещал показать запись четвёртой лекции исследователю Себастьяну Рудеру.

3
00:00:22,804 --> 00:00:39,250
Ему понравилось, и вчера он опубликовал пост Optimization for Deep Learning Highlights in 2017 по её мотивам.

4
00:00:39,350 --> 00:00:44,455
В посте упоминаются работы студентов нашего курса —

5
00:00:44,555 --> 00:00:58,020
например, отделение ограничения весов от вычисления инерции в AdamW.

6
00:00:58,020 --> 00:01:13,034
Себастьян пишет про возможные применения AdamW и приводит ссылку на его реализацию от Anand Saha.

7
00:01:13,134 --> 00:01:16,710
Как видите, код библиотеки fastai уже приводят в пример.

8
00:01:18,060 --> 00:01:25,170
После этого Себастьян пишет об алгоритмах подбора скорости обучения.

9
00:01:25,170 --> 00:01:36,780
На графике показана зависимость скорости обучения от эпохи, выглядит непривычно из-за логарифмической шкалы.

10
00:01:36,780 --> 00:01:53,430
В качестве примера — ссылки на посты двух наших студентов, Vitaly Bushaev и Anand Saha.

11
00:01:53,430 --> 00:02:00,335
Очень радует то, что работы наших студентов уже замечают и используют.

12
00:02:00,435 --> 00:02:06,070
Пост Себастьяна попал на главную страницу Hacker News, это очень круто.

13
00:02:06,170 --> 00:02:14,885
Будем надеяться, что распространение информации продолжится.

14
00:02:14,985 --> 00:02:31,090
На прошлой неделе мы строили модель коллаборативной фильтрации, давайте вспомним, как она выглядела.

15
00:02:34,300 --> 00:02:42,200
Последняя версия модели была почти идентична коду класса EmbeddingDotBias библиотеки fastai.

16
00:02:47,630 --> 00:02:56,370
Матрицы эмбеддинга для пользователей и товаров создаются и заполняются случайными числами.

17
00:02:56,470 --> 00:03:00,870
Пользователи и товары — устоявшиеся термины, в нашей задаче товары — это фильмы.

18
00:03:00,970 --> 00:03:11,540
Таким же образом созданы матрицы смещений — в функцию get_emb() передаётся значение параметра nf=1, а не nf=n_factors.

19
00:03:14,210 --> 00:03:22,180
Матрицы эмбеддинга перемножаются, полученная матрица преобразуется в вектор, к ней прибавляются смещения.

20
00:03:22,730 --> 00:03:27,430
Полученный вектор пропускается через сигмоиду для калибровки до желаемого диапазона.

21
00:03:27,430 --> 00:03:39,865
Вы спрашивали, можно ли визуализировать этот процесс, я обещал показать.

22
00:03:39,965 --> 00:04:04,050
Мы начнём с модели, использующей только средства fastai, она обучалась 19 секунд, результаты неплохие.

23
00:04:04,150 --> 00:04:09,770
Давайте анализировать её работу.

24
00:04:09,770 --> 00:04:24,370
Мы считали заранее файл movies.csv, содержащий пары «ID фильма — название фильма», сейчас будем его использовать.

25
00:04:24,640 --> 00:04:36,660
Так как не все в аудитории — заядлые кинозрители, для наглядности я возьму 3000 самых популярных фильмов.

26
00:04:36,660 --> 00:04:51,785
После этого заменю индексы на новые, используя словарь нумерации cf.item2idx, с которым работает модель.

27
00:04:51,885 --> 00:05:06,410
Модель fastai лежит в переменной learn, модель PyTorch можно получить вызовом learn.model.

28
00:05:06,510 --> 00:05:18,140
В коде библиотеки fastai вызов learn.model реализован как @property — вычисляемое свойство.

29
00:05:18,240 --> 00:05:32,800
Вычисляемое свойство в Python выглядит как метод, но вызывается без скобок.

30
00:05:32,800 --> 00:05:42,669
Это выглядит как обычное поле, но каждый раз при вызове выполняется этот код.

31
00:05:42,669 --> 00:05:48,789
Здесь при вызове вычисляемого свойства возвращается поле self.models.model.

32
00:05:48,789 --> 00:06:03,019
self.models — объект класса CollabFilterModel, это тонкая обёртка для моделей PyTorch.

33
00:06:03,119 --> 00:06:19,720
Код класса CollabFilterModel состоит из одной строки.

34
00:06:19,720 --> 00:06:33,380
Мы обсудим это во второй части курса, это сделано для возможности задания разных скоростей обучения для групп слоёв.

35
00:06:33,480 --> 00:06:42,820
Группировка слоёв не реализована в PyTorch, и этот класс позволяет добавить её в готовую модель PyTorch.

36
00:06:42,820 --> 00:06:54,950
Детали не важны, но примерно так и выглядят обёртки для моделей PyTorch.

37
00:06:55,050 --> 00:07:07,820
Итак, модель PyTorch содержится в поле learn.models.model, её можно получить присваиванием m=learn.model.

38
00:07:07,920 --> 00:07:21,815
Модель PyTorch в переменной m содержит список всех слоёв модели,

39
00:07:21,915 --> 00:07:39,460
это реализовано с помощью средств объектно-ориентированного программирования в Python.

40
00:07:39,460 --> 00:08:07,960
Имена слоёв эмбеддинга автоматически берутся из их реализации.

41
00:08:12,910 --> 00:08:28,660
Это модель PyTorch. Слои модели получаются соответствующими методами —  метод m.ib() (item bias)

42
00:08:28,660 --> 00:08:34,000
возвращает матрицу смещений товаров, в нашем случае — матрицу смещений фильмов.

43
00:08:34,000 --> 00:08:40,990
В этой матрице каждому из 9066 фильмов соответствует одно число — смещение.

44
00:08:40,990 --> 00:08:50,710
Модели и слои PyTorch удобно использовать, потому что их можно вызывать как функции.

45
00:08:50,710 --> 00:09:12,520
Метод ib() возвращает слой ib, а функция m() возвращает предсказания модели m.

46
00:09:12,520 --> 00:09:22,570
Здесь в метод ib() передаются индексы самых популярных фильмов.

47
00:09:22,570 --> 00:09:34,835
Напомню, что матрицы эмбеддинга — объекты класса Variable, а не тензоры, поэтому здесь стоит конструктор V().

48
00:09:34,935 --> 00:10:00,425
PyTorch избавится от класса Variable в версии 0.4. Если вы смотрите курс в записи, это могло уже произойти.

49
00:10:00,525 --> 00:10:07,025
Пока нужно не забывать использовать конструктор V() при передаче данных в модель.

50
00:10:07,125 --> 00:10:20,140
Помните, что всё, что можно делать с тензором, можно делать и с объектами класса Variable.

51
00:10:20,140 --> 00:10:38,820
Итак, m.ib(V(topMovieIdx)) возвращает смещения самых популярных фильмов.

52
00:10:41,790 --> 00:11:06,550
Эти смещения содержатся в объекте класса Variable размера 3000x1, так как мы выбрали 3000 фильмов.

53
00:11:08,830 --> 00:11:14,675
Мы получили объект класса Variable, так как входные данные тоже принадлежат этому классу.

54
00:11:14,775 --> 00:11:22,385
Объект хранится в GPU, так как используется модуль torch.cuda.

55
00:11:22,485 --> 00:11:36,740
Функция to_np() превращает полученный объект в массив numpy, так это удобнее.

56
00:11:36,840 --> 00:11:58,930
Функция работает и с тензорами, и с объектами класса Variable. Не имеет значения, находится переменная в CPU или в GPU.

57
00:11:58,930 --> 00:12:18,970
Функция очень удобная, я использую её всегда, если не надо явно использовать GPU или доставать градиенты.

58
00:12:19,330 --> 00:12:39,700
numpy гораздо старше PyTorch, поэтому с ним легче работать, к тому же он совместим с OpenCV и pandas.

59
00:12:39,700 --> 00:12:45,460
Я использую массивы numpy везде и перехожу в формат PyTorch, когда

60
00:12:45,460 --> 00:12:53,410
нужно произвести вычисления в GPU или использовать градиенты, не забывая переходить обратно к numpy.

61
00:12:53,410 --> 00:13:03,430
Так реализована библиотека fastai, что отличает её от других библиотек компьютерного зрения на основе PyTorch,

62
00:13:05,830 --> 00:13:11,767
использующих PyTorch везде, где это возможно. Вопрос?

63
00:13:11,867 --> 00:13:31,660
Вопрос из зала: Если я построил модель в GPU с PyTorch, функцию to_np() придётся использовать в цикле для каждого слоя?

64
00:13:31,660 --> 00:13:38,090
Хороший вопрос. Предсказания лучше делать в CPU, а не в GPU — это расширяет круг применения модели,

65
00:13:38,190 --> 00:13:44,045
нет необходимости разбивать данные на минибатчи и так далее.

66
00:13:44,145 --> 00:14:03,620
Модель переводится в CPU методом m.cpu(), аналогично для объектов класса Variable.

67
00:14:03,720 --> 00:14:27,560
Если у вашей машины нет GPU, этого делать не надо, CPU будет использоваться автоматически.

68
00:14:27,660 --> 00:14:41,390
Вопрос из зала: Если модель обучена в GPU и сохранена, её надо особым образом загружать?

69
00:14:41,490 --> 00:14:51,990
Нет, не нужно, хотя это зависит от того, какие средства fastai вы используете, сейчас покажу.

70
00:14:51,990 --> 00:15:08,350
Один из наших студентов придумал, как избежать необходимости загружать модель с использованием того же GPU,

71
00:15:09,050 --> 00:15:24,160
в котором модель обучалась до сохранения. Это делается этой волшебной строкой.

72
00:15:28,819 --> 00:15:36,619
Для переноса модели из CPU в GPU используется метод m.cuda().

73
00:15:36,619 --> 00:15:50,509
Важно понимать работу функции zip() в Python, она позволяет проходить несколько массивов сразу.

74
00:15:50,509 --> 00:16:00,259
Я хочу получить массив пар «название фильма — смещение», поэтому соединяю массивы индексов фильмов и смещений

75
00:16:00,259 --> 00:16:11,289
и использую генератор списков Python для сопоставления имён и индексов.

76
00:16:11,289 --> 00:16:15,714
Сортирую полученный список.

77
00:16:15,814 --> 00:16:33,539
Худший фильм — Battlefield Earth Джона Траволты, причём отрыв от других фильмов достаточно большой.

78
00:16:33,639 --> 00:16:46,844
Это — худший фильм всех времён по версии пользователей IMDb. Такой способ оценки фильмов хорош,

79
00:16:46,944 --> 00:16:57,319
потому что он учитывает то, что некоторые люди просто ставят плохие оценки всем фильмам.

80
00:17:02,149 --> 00:17:10,399
После поправки на то, что средняя оценка у каждого человека разная и все люди смотрят разные фильмы,

81
00:17:11,689 --> 00:17:20,169
фильм Battlefield Earth получается худшим фильмом в истории человечества.

82
00:17:21,638 --> 00:17:29,360
Так можно смотреть внутрь модели и визуализировать работу векторов смещения.

83
00:17:29,360 --> 00:17:59,520
Для сортировки списка я использовал лямбда-функцию, перед этим — itemgetter, здесь они работают одинаково.

84
00:17:59,620 --> 00:18:06,290
Лямбда-функции очень полезные, убедитесь, что умеете их писать.

85
00:18:06,290 --> 00:18:18,435
Во время работы функции sorted() лямбда-функция вызывается каждый раз при сравнении двух элементов.

86
00:18:18,535 --> 00:18:31,340
Выведем список в другом порядке. Shawshank Redemption, Godfather, Usual Suspects — отличные фильмы, я согласен с рейтингом.

87
00:18:35,120 --> 00:18:43,100
Это была демонстрация смещений для фильмов.

88
00:18:43,100 --> 00:18:50,205
Теперь давайте посмотрим на матрицы эмбеддинга.

89
00:18:50,305 --> 00:19:01,315
Матрица эмбеддинга для фильмов называется i, мы достаём её функцией m.i() и кладём в переменную movie_emb.

90
00:19:01,415 --> 00:19:08,670
У каждого из 3000 самых популярных фильмов есть вектор эмбеддинга длиной 50.

91
00:19:08,770 --> 00:19:20,355
Если вы не Джеффри Хинтон, очень сложно представить 50-мерное пространство, поэтому мы перейдём в трёхмерное.

92
00:19:20,455 --> 00:19:29,155
Есть много алгоритмов уменьшения размерности, один из самых популярных —

93
00:19:29,255 --> 00:19:36,073
метод главных компонент, или PCA (principal component analysis).

94
00:19:36,173 --> 00:19:41,575
Это линейный алгоритм, они обычно хорошо работают на матрицах эмбеддинга.

95
00:19:41,675 --> 00:19:54,510
Принцип его работы рассказывает Рейчел в курсе fast.ai Computational Linear Algebra.

96
00:19:54,610 --> 00:20:04,360
Алгоритм очень важный, он очень похож на сингулярное разложение матриц (SVD).

97
00:20:04,460 --> 00:20:18,740
SVD иногда всплывает в глубоком обучении, с ним стоит разобраться, если вы заинтересованы в теории.

98
00:20:18,740 --> 00:20:26,220
Термины SVD, PCA, собственные векторы и собственные значения означают примерно одно и то же.

99
00:20:26,320 --> 00:20:33,750
Метод главных компонент реализован в sklearn.decomposition,

100
00:20:33,850 --> 00:20:41,540
соответствующий объект создаётся конструктором PCA(), в который передаётся желаемое количество измерений.

101
00:20:41,540 --> 00:20:48,380
Алгоритм найдёт три максимально различающиеся линейные комбинации пятидесяти измерений,

102
00:20:48,380 --> 00:20:53,180
которые передают максимальное количество информации.

103
00:20:53,690 --> 00:21:02,270
Это называется наилучшее приближение матрицы матрицей меньшего ранга.

104
00:21:02,270 --> 00:21:09,005
После работы алгоритма получаем матрицу размера 3х3000.

105
00:21:09,105 --> 00:21:21,615
После этого обрабатываем одну из трёх строк таблицы (компоненту) так, как обрабатывали вектор смещений.

106
00:21:21,715 --> 00:21:42,420
Мы не знаем, что сделал PCA, поэтому отсортируем данные по этой компоненте и посмотрим, что получится.

107
00:21:42,520 --> 00:22:11,315
По названиям фильмов я предполагаю, что эта компонента показывает, насколько фильм серьёзный или лёгкий для просмотра.

108
00:22:11,415 --> 00:22:25,060
Другого способа интерпретации нет, только смотреть на данные и искать в них смысл.

109
00:22:27,910 --> 00:22:35,465
Следующая компонента обрабатывается аналогичным образом.

110
00:22:35,565 --> 00:22:48,320
Похоже на то, что положительные значения отвечают за наличие диалогов, а отрицательные — за наличие компьютерной графики.

111
00:22:48,420 --> 00:23:13,574
Эта компонента показывает, что кто-то любит классические фильмы Вуди Аллена, а кто-то — зрелищные голливудские фильмы.

112
00:23:13,674 --> 00:23:21,930
По первой компоненте видно, что кто-то любит серьёзные фильмы, а кто-то — одноразовые боевики.

113
00:23:22,810 --> 00:23:28,892
Я надеюсь, идея ясна.

114
00:23:28,992 --> 00:23:43,990
Модель очень простая — умножение двух матриц и алгоритм оптимизации, а умеет довольно много, это круто.

115
00:23:48,600 --> 00:24:04,640
Можно распределить фильмы на графике по первым двум компонентам, я нанёс на него несколько фильмов.

116
00:24:04,740 --> 00:24:23,700
Сейчас я хочу поговорить о том, что происходит при вызове метода learn.fit().

117
00:24:24,850 --> 00:24:30,790
Вопрос из зала: Как интерпретировать матрицы эмбеддинга для задачи предсказания продаж?

118
00:24:30,790 --> 00:24:43,220
В соревновании Rossman? Сейчас покажу.

119
00:24:57,840 --> 00:25:18,790
На экране — иллюстрация из статьи Entity Embeddings of Categorical Variables, статья для нас довольно простая.

120
00:25:18,790 --> 00:25:23,260
Если вы прошли курс по машинному обучению, её будет ещё проще понять.

121
00:25:23,260 --> 00:25:39,550
Статья описывает слои эмбеддинга (entity embedding layers) как сочетание прямого кодирования и умножения матриц.

122
00:25:39,550 --> 00:25:45,122
На картинке три слоя эмбеддинга, то есть три прямых кодирования, умноженных на соответствующие матрицы.

123
00:25:45,222 --> 00:25:56,040
Результат умножения подаётся на плотный линейный слой (dense layer).

124
00:25:56,040 --> 00:26:21,610
Это одна из первых статей про использование эмбеддинга для категориальных признаков, поэтому она очень подробная.

125
00:26:21,610 --> 00:26:34,930
После обучения эмбеддингов с помощью нейронной сети авторы задумались, что можно с ними сделать.

126
00:26:34,930 --> 00:26:45,070
Они заменили категориальные переменные на вычисленные эмбеддинги

127
00:26:48,070 --> 00:27:07,920
и обучили на них алгоритм градиентного бустинга над решающими деревьями (gradient boosted trees).

128
00:27:08,020 --> 00:27:14,775
Таким образом они выполнили отбор признаков, и он оказался удачным, как видите в таблице.

129
00:27:14,875 --> 00:27:25,960
Функция потерь MAPE (mean absolute percentage error) уменьшилась с 0.152 при прямом кодировании до 0.115 с эмбеддингами.

130
00:27:25,960 --> 00:27:36,650
Аналогичные результаты получены для алгоритмов случайного леса и метода ближайших соседей.

131
00:27:36,750 --> 00:27:56,195
Это значит, что вы можете вычислить эмбеддинги признаков любой мощности с помощью нейронной сети,

132
00:27:56,295 --> 00:28:03,580
а потом все смогут использовать их в алгоритмах градиентного бустинга или случайного леса.

133
00:28:07,210 --> 00:28:14,670
Даже метод ближайших соседей даст неплохие результаты.

134
00:28:14,770 --> 00:28:30,730
Сотрудникам вашей компании не придётся проходить наш курс, чтобы использовать мощь нейронных сетей.

135
00:28:30,730 --> 00:28:59,355
Эмбеддинги — это соответствия индексов и чисел, поэтому их удобно хранить в базе данных.

136
00:28:59,455 --> 00:29:05,115
Градиентный бустинг и случайный лес обучаются гораздо быстрее нейронных сетей,

137
00:29:05,215 --> 00:29:09,915
поэтому в сочетании с предвычисленными эмбеддингами могут быть даже эффективнее нейронных сетей.

138
00:29:10,015 --> 00:29:26,790
Авторы статьи распределили штаты Германии по значениям двух главных компонент.

139
00:29:26,890 --> 00:29:41,600
Удивительным образом расположение городов примерно совпало с их географическим расположением,

140
00:29:41,600 --> 00:29:54,672
хотя в изначальных данных нет ни расстояния между штатами, ни географической привязки.

141
00:29:54,772 --> 00:30:10,190
Этот график показывает зависимость расстояния между двумя магазинами в пространстве эмбеддингов

142
00:30:10,190 --> 00:30:15,980
от географического расстояния между ними для всех возможных пар магазинов.

143
00:30:15,980 --> 00:30:36,050
Получилась красивая корреляция. Видимо, у рядом стоящих магазинов схожие характеристики продаж.

144
00:30:36,050 --> 00:30:48,170
Я нарисовал такие же графики для дней недели и месяцев и соединил точки временной линией.

145
00:30:48,170 --> 00:30:52,850
На графике месяцев видно, что точки летних месяцев стоят отдельно, точки весенних — отдельно.

146
00:30:58,640 --> 00:31:08,345
Визуализировать эмбеддинги интересно. Имеет смысл сначала проверить, отражают ли они очевидные закономерности,

147
00:31:08,445 --> 00:31:18,869
а потом попробовать разглядеть неочевидные на различных графиках.

148
00:31:18,969 --> 00:31:30,605
Техника ещё не изучена, поэтому я пока не могу указать область её применения.

149
00:31:30,705 --> 00:31:44,830
Вопрос из зала: Есть другие способы создания эмбеддингов, например, N-граммы с пропусками (skip-grams). Какой лучше?

150
00:31:44,830 --> 00:31:49,679
N-граммы с пропусками специфичны для обработки естественного языка.

151
00:31:49,860 --> 00:32:16,720
Задачи по обработке естественного языка — обычно обучение без учителя, в них нет целевой переменной.

152
00:32:16,720 --> 00:32:22,210
Для перехода к задаче обучения с учителем можно создать целевую переменную.

153
00:32:22,310 --> 00:32:34,645
В Word2vec это было устроено так: из данных достаётся правильное предложение из, например, 7 слов,

154
00:32:34,945 --> 00:32:53,500
и среднее слово в нём заменяется на случайное. Например, предложение

155
00:32:53,500 --> 00:33:00,334
«Милая маленькая кошечка сидела на пушистом коврике» превращалось в «Милая маленькая справедливость сидела на пушистом коврике».

156
00:33:00,434 --> 00:33:14,365
В итоге есть два предложения — изначальное и с заменённым словом.

157
00:33:14,465 --> 00:33:25,930
Изначальное предложение помечается единицей, изменённое — нулём.

158
00:33:26,030 --> 00:33:40,800
На таких данных была построена модель машинного обучения по поиску неправильных предложений.

159
00:33:40,800 --> 00:33:45,113
Нет необходимости уметь искать неправильные предложения, важны вычисленные в процессе обучения эмбеддинги.

160
00:33:45,213 --> 00:33:50,360
Эти эмбеддинги используются в других задачах, так работает Word2vec.

161
00:33:50,720 --> 00:34:01,860
Если реализовать это без глубокого обучения как умножение матриц, обучение будет очень быстрым.

162
00:34:01,860 --> 00:34:15,989
При использовании неглубокого обучения теряется точность предсказаний. Из преимуществ —

163
00:34:15,989 --> 00:34:27,265
модель можно обучать на огромных датасетах, а полученные эмбеддинги будут близки к линейным,

164
00:34:27,365 --> 00:34:33,715
поэтому их можно будет складывать и вычитать.

165
00:34:33,815 --> 00:34:52,710
Если нам нужна линейность эмбеддингов категориальных переменных,

166
00:34:52,710 --> 00:34:57,810
например, для использования метода ближайших соседей,

167
00:34:57,810 --> 00:35:03,990
то для их вычисления стоит использовать неглубокое обучение.

168
00:35:03,990 --> 00:35:09,300
Если хочется большей точности предсказаний, нужно использовать нейронные сети.

169
00:35:09,300 --> 00:35:19,495
Я считаю, что в обработке естественного языка нужно отбросить линейные Word2vec и Glove,

170
00:35:19,595 --> 00:35:25,260
потому что получаемые с их помощью эмбеддинги сильно уступают моделям глубокого обучения.

171
00:35:25,260 --> 00:35:30,520
Модель обработки естественного языка с прошлой лекции для анализа тональностей

172
00:35:30,750 --> 00:35:35,790
основывалась на рекуррентной нейронной сети, а не на Word2vec, и в результате обучения

173
00:35:35,790 --> 00:35:44,620
мы получили не только эмбеддинги, но и предобученную модель для других задач.

174
00:35:44,720 --> 00:35:51,010
Вопрос из зала: То есть для создания эмбеддингов нужна фиктивная задача обучения?

175
00:35:51,110 --> 00:35:59,810
Не обязательно фиктивная — мы создали эмбеддинги для соревнования Rossman, пытаясь предсказать реальные продажи.

176
00:35:59,910 --> 00:36:06,335
Идея более общая — для создания любого пространства признаков, не обязательно эмбеддингов,

177
00:36:06,435 --> 00:36:16,200
нужны либо размеченные данные, либо фиктивная задача обучения.

178
00:36:16,200 --> 00:36:26,550
Вопрос из зала: Как выбрать хорошую фиктивную задачу?

179
00:36:26,550 --> 00:36:31,050
Отличный вопрос, к сожалению, он мало изучен.

180
00:36:31,050 --> 00:36:45,030
Мало кто хотя бы понимает, что обучение без учителя — почти всегда обучение с учителем на фиктивных задачах.

181
00:36:45,030 --> 00:36:49,650
Я не видел статей на эту тему.

182
00:36:49,650 --> 00:37:05,665
Вам нужна задача, которая позволит лучше изучить те закономерности в данных, которые вам нужны.

183
00:37:05,765 --> 00:37:19,930
В компьютерном зрении используют дополнение данных, искажающее реальность —

184
00:37:20,030 --> 00:37:37,200
например, заменяют цвета на неестественные. Нейронную сеть обучают находить неискажённые изображения.

185
00:37:37,200 --> 00:37:42,565
Вопрос очень интересный, хотелось бы в нём разобраться.

186
00:37:42,665 --> 00:37:47,310
Будет классно, если кто-то из вас скачает несколько датасетов для обучения без учителя или частичного обучения

187
00:37:47,310 --> 00:37:59,640
и посмотрит, как на качество модели влияют разные фиктивные задачи обучения.

188
00:37:59,640 --> 00:38:09,869
Не можете придумать гениальную фиктивную задачу — поставьте хоть какую-нибудь, многого не требуется.

189
00:38:09,869 --> 00:38:15,119
Показательно топорная фиктивная задача — автокодировщик,

190
00:38:15,119 --> 00:38:23,965
его использовали победители недавнего соревнования Kaggle по предсказанию страховых возмещений.

191
00:38:24,065 --> 00:38:28,675
В данных были страховые полисы с известными страховыми возмещениями

192
00:38:28,775 --> 00:38:37,090
и страховые полисы, выплаты по которым ещё не были произведены.

193
00:38:37,190 --> 00:38:44,880
На вход нейронной сети для обучения подавались данные о страховом полисе,

194
00:38:44,980 --> 00:38:52,733
и на выходе сеть должна была воссоздать эту же информацию.

195
00:38:52,833 --> 00:39:01,585
При этом хотя бы в одном скрытом слоё должно быть меньше активаций, чем во входных данных.

196
00:39:01,685 --> 00:39:13,590
Например, если страховой полис определяется 100 признаками, в середине должен быть слой с 20 активациями.

197
00:39:13,590 --> 00:39:23,970
Такая постановка задачи не требует дополнительного кода, можно использовать модели PyTorch или fastai,

198
00:39:25,619 --> 00:39:31,710
просто передав в качестве целевой переменной начальные данные.

199
00:39:31,710 --> 00:39:39,490
Автокодировщик — самая простая фиктивная задача из возможных, и работает он на удивление хорошо.

200
00:39:39,590 --> 00:39:44,392
Настолько хорошо, что благодаря ему люди победили в соревновании Kaggle.

201
00:39:44,492 --> 00:39:53,155
Победители использовали созданные автокодировщиком признаки для обучения другой нейронной сети.

202
00:39:53,255 --> 00:40:06,024
Если наберётся достаточно заинтересованных, мы обсудим обучение без учителя во второй части курса.

203
00:40:09,424 --> 00:40:24,760
Вопрос из зала: Полезна ли будет модель, обученная на данных arXiv, для работы с датасетом IMDb?

204
00:40:24,860 --> 00:40:36,839
Отличный вопрос. Мы обсуждали это с Себастьяном и планировали заняться этим в январе.

205
00:40:36,839 --> 00:40:47,877
В компьютерном зрении предобученные на кошках и собаках модели отлично работают для рентген-диагностики,

206
00:40:48,577 --> 00:40:53,220
но этому пока нет аналогов в обработке естественного языка.

207
00:40:53,320 --> 00:41:01,680
Людям кажется, что это не сработает, поэтому они даже не пробуют, хотя я считаю это перспективным.

208
00:41:04,390 --> 00:41:17,810
Мне было интересно качество нашей модели для соревнования Rossman,

209
00:41:17,810 --> 00:41:23,410
потому что в публичном рейтинге она выглядела неудачной.

210
00:41:23,510 --> 00:41:30,690
Ещё мне было интересны результаты на настоящей тестовой выборке.

211
00:41:30,690 --> 00:41:46,650
Я добавил обработку тестовой выборки в Jupyter ноутбук lesson3-rossman.ipynb.

212
00:41:46,650 --> 00:42:00,380
Я мог обернуть этот код в метод и вызвать его дважды, а не городить копии кода, но решил оставить так,

213
00:42:00,380 --> 00:42:08,010
чтобы вы видели все этапы и свободнее экспериментировали.

214
00:42:08,010 --> 00:42:24,480
Обработка одинаковая для обучающей и тестовой выборке, кое-где я использовал циклы.

215
00:42:24,480 --> 00:42:34,140
Ячейки [34] и [56] выполняются не подряд. Сначала выполняется ячейка [34], ячейка [56] пропускается

216
00:42:34,140 --> 00:42:45,900
и выполняется код под ней, а потом нужно вернуться к ячейке [56] и проделать то же самое для неё.

217
00:42:45,900 --> 00:42:49,950
На этой неделе меня спрашивали, почему код не работает, и это важное напоминание о том,

218
00:42:52,260 --> 00:43:00,180
что не стоит бездумно выполнять все ячейки подряд.

219
00:43:03,960 --> 00:43:15,680
В Jupyter ноутбуках к первым лекциям можно было так делать, но сейчас пора думать о том, что вы запускаете.

220
00:43:15,780 --> 00:43:21,275
Вопрос из зала: Чем неглубокое обучение отличается от глубокого?

221
00:43:21,375 --> 00:43:32,975
В неглубоком обучении нет скрытых слоёв, только скалярное произведение векторов или произведение матриц.

222
00:43:35,975 --> 00:43:50,040
Итак, после обработки у нас есть обучающая и тестовая выборка, остальное без изменений.

223
00:43:50,040 --> 00:43:58,000
Многое из этого Jupyter ноутбука не относится к глубокому обучению, поэтому обсуждается в курсе «Машинное обучение».

224
00:43:58,100 --> 00:44:10,040
Вместо функции train_cats() используется apply_cats(), чтобы у выборок была общая нумерация категориальных признаков.

225
00:44:12,815 --> 00:44:27,440
Объект mapper, содержащий средние значения и стандартные отклонения, тоже общий.

226
00:44:27,440 --> 00:44:46,665
После этого тестовая выборка передаётся в модель привычным образом, обучение проходит как обычно.

227
00:44:46,765 --> 00:44:57,710
Для получения предсказаний используется метод m.predict(), параметр True указывает на наличие тестовой выборки.

228
00:44:57,710 --> 00:45:03,650
Полученные предсказания отправляются на Kaggle.

229
00:45:03,650 --> 00:45:38,890
Результат интересный. В публичном рейтинге я занял бы примерно трёхсотое место, а в приватном — пятое.

230
00:45:38,890 --> 00:45:56,420
Если вы участвуете в соревновании и не работали над валидационной выборкой, с вами может случиться обратное.

231
00:45:56,420 --> 00:46:01,970
Например, в соревновании по распознаванию айсбергов валидационная выборка публичного рейтинга

232
00:46:04,940 --> 00:46:10,640
содержит много дополненных до бессмыслицы данных.

233
00:46:10,640 --> 00:46:21,859
Тут лучше ориентироваться на собственноручно созданную валидационную выборку, а не на место в публичном рейтинге.

234
00:46:21,859 --> 00:46:35,820
Значение функции потерь на наших предсказаниях близко к третьему месту, я считаю, что мы смогли воспроизвести их подход.

235
00:46:40,700 --> 00:46:53,940
После соревнования Rossman люди начали писать о своих моделях, есть много полезных замечаний.

236
00:46:54,040 --> 00:47:02,480
Например, здесь авторы разделили продажи отдельных магазинов на продажи в воскресенье и в другие дни,

237
00:47:02,480 --> 00:47:11,870
и выяснилось, что у некоторых магазинов продажи в воскресенье сильно отличаются.

238
00:47:11,870 --> 00:47:16,455
Такие описания помогают лучше понять, как и зачем анализировать ваши данные.

239
00:47:16,555 --> 00:47:29,990
Вот интересная закономерность, которую не учли обладатели третьего места.

240
00:47:29,990 --> 00:47:48,480
Видны всплески в продажах сразу перед закрытием и сразу после открытия магазинов.

241
00:47:48,580 --> 00:47:57,230
Обладатели третьего места удалили все данные с закрытыми магазинами, а зря.

242
00:47:57,230 --> 00:48:05,500
Не меняйте ваши данные, пока не убедитесь в том, что понимаете, что делаете.

243
00:48:05,600 --> 00:48:15,260
Я не проверял своё предположение, но думаю, что они победили бы, если бы учли это.

244
00:48:15,260 --> 00:48:22,335
Насколько мне известно, в тестовой выборке не было закрытых магазинов, но их всё равно стоило учесть.

245
00:48:22,435 --> 00:48:42,869
Если не обучать модель на выбросах, она не научится хорошо их предсказывать.

246
00:48:42,869 --> 00:48:55,149
Давайте ещё раз посмотрим на код библиотеки fastai, чтобы лучше понять модель Rossman.

247
00:49:02,559 --> 00:49:08,689
Я хочу убедиться, что вы ориентируетесь в коде и сможете разобраться сами.

248
00:49:08,789 --> 00:49:27,999
Давайте изучим класс ColumnarModelData. Раньше я показывал, как с помощью «??» смотреть исходный код.

249
00:49:31,259 --> 00:49:36,649
Иногда после чтения кода нужно уточнить код одной из составляющих, поэтому это не всегда удобно.

250
00:49:36,749 --> 00:49:50,674
Скорее всего, ваш текстовый редактор может открывать файлы по SSH и перемещаться по ним.

251
00:49:50,774 --> 00:50:08,285
В Vim я могу перейти к реализации класса ColumnarModelData командой :tag ColumnarModelData.

252
00:50:08,385 --> 00:50:19,569
Наведя курсор на класс DataLoader, я могу перейти к его реализации комбинацией клавиш Ctrl+|.

253
00:50:19,569 --> 00:50:26,170
Вернуться назад можно комбинацией клавиш Ctrl+t.

254
00:50:26,170 --> 00:50:41,140
Навигация по местам использования класса осуществляется звёздочкой «*».

255
00:50:41,140 --> 00:50:59,420
Сейчас мы хотим изучить функцию get_learner(), которая возвращает модель.

256
00:50:59,420 --> 00:51:23,129
Объект класса PyTorch MixedInputModel оборачивается в конструктор fastai StructuredLearner(), соединяющий данные и модель.

257
00:51:23,229 --> 00:51:30,200
Реализацию класса MixedInputModel можно увидеть, нажав Ctrl+].

258
00:51:34,519 --> 00:51:42,890
Почти весь код мы уже обсуждали.

259
00:51:42,890 --> 00:52:02,919
В функцию get_learner() мы передали размеры эмбеддингов emb_szs. Вопрос?

260
00:52:03,119 --> 00:52:15,710
Вопрос из зала: Конструктор класса MixedInputModel всегда ожидает и категориальные, и количественные признаки?

261
00:52:15,410 --> 00:52:30,079
Да, и если есть признаки только одного типа, он заменит их столбцом нулей и всё равно создаст модель.

262
00:52:30,079 --> 00:52:49,720
Реализация некрасивая, мы надеемся её исправить, но да, сейчас можно передать пустой список признаков одного типа.

263
00:52:49,720 --> 00:52:59,234
Я не исправляю некоторые некрасивые вещи, потому что с выходом PyTorch 0.4 не только исчезнет класс Variable,

264
00:52:59,334 --> 00:53:20,509
но и добавится поддержка тензоров нулевого ранга как элементов тензоров первого ранга.

265
00:53:20,509 --> 00:53:29,019
Благодаря этому упростится код некоторых классов, тогда и перепишем.

266
00:53:30,400 --> 00:53:42,549
Вопрос из зала: Как писать классы на основе классов fastai?

267
00:53:43,229 --> 00:53:53,779
Мы затронем это на следующей неделе, но основная работа будет во второй части курса.

268
00:53:53,779 --> 00:54:00,710
Во второй части будет много интересного, например, генеративные модели, выдающие целые предложения или изображения.

269
00:54:00,710 --> 00:54:12,539
Мы обсудим, как изменять модели fastai или создавать новые модели на их основе —

270
00:54:12,639 --> 00:54:20,609
и если будет время, начнём со следующей недели.

271
00:54:20,709 --> 00:54:26,984
Итак, в функцию get_learner() передаются размеры эмбеддингов emb_szs,

272
00:54:27,084 --> 00:54:32,569
это просто количества строк и столбцов в каждой матрице эмбеддинга.

273
00:54:32,569 --> 00:54:46,754
Количество строк — это мощность признака, количество столбцов — половина мощности, но не больше 50.

274
00:54:46,854 --> 00:55:05,869
Этот массив передаётся в конструктор класса, где генератором списков по каждой паре размеров создаётся эмбеддинг.

275
00:55:05,869 --> 00:55:45,960
Конструктор nn.ModuleList() превращает полученный массив в список слоёв, воспринимаемый моделью.

276
00:55:46,060 --> 00:55:56,960
После создания списка эмбеддингов аналогично создаётся список линейных слоёв.

277
00:55:56,960 --> 00:56:16,455
[1000, 500] — количество активаций в слоях, из этого массива конструктором nn.Linear() создаются линейные слои.

278
00:56:16,555 --> 00:56:26,415
Как видите, создание моделей и работа с параметрами очень простые.

279
00:56:26,515 --> 00:56:40,515
Конструктор nn.BatchNorm1d() мы обсудим на следующей неделе, инициализацию Kaiming He мы уже обсуждали.

280
00:56:40,615 --> 00:56:50,895
Дропаут создаётся из массива [0.001, 0.01] в параметрах метода get_learner() аналогично линейным слоям.

281
00:56:50,995 --> 00:57:04,275
Итак, в этом конструкторе понятно всё, кроме нормализации батчей, она пока неважна.

282
00:57:04,375 --> 00:57:22,250
Метод forward этого класса тоже понятен. Сначала объединяем слои эмбеддинга,

283
00:57:22,250 --> 00:57:33,940
добавляем дропаут, затем в цикле добавляем каждый линейный слой + выпрямитель + дропаут,

284
00:57:33,940 --> 00:57:49,880
потом добавляем последний линейный слой размера out_sz=1.

285
00:57:49,880 --> 00:58:11,120
В конце при наличии параметра y_range предсказание пропускается через сигмоиду,

286
00:58:11,120 --> 00:58:21,760
чтобы полученный уровень продаж был больше нуля и скорее всего меньше максимального уровня продаж за всё время.

287
00:58:21,860 --> 00:58:45,555
Параметр y_range = (0, max_log_y*1.2) содержит границы желаемого диапазона.

288
00:58:45,655 --> 00:58:56,630
На всякий случай я умножил максимальное значение на 1.2.

289
00:58:56,630 --> 00:59:06,715
Я бы хотел, чтобы вы воспринимали модели не как чёрные ящики,

290
00:59:06,815 --> 00:59:27,580
а как готовые шаблоны, код которых можно копировать и менять под свои задачи.

291
00:59:30,640 --> 00:59:51,855
Давайте сделаем перерыв до 7:45, а потом перейдём к рекуррентным нейронным сетям.
========================

292
00:59:56,670 --> 01:00:10,025
Перед тем, как перейти к рекуррентным нейронным сетям, я хочу ещё раз рассказать про стохастический градиентный спуск.

293
01:00:10,125 --> 01:00:29,280
Jupyter ноутбук называется lesson6-sgd.ipynb, в нём SGD реализован для задачи линейной регрессиии.

294
01:00:29,380 --> 01:00:46,750
Мы используем самую простую из возможных моделей - линейную. y = a * x + b, и создаём данные.

295
01:00:46,750 --> 01:00:58,264
Мы хотим предсказывать y, зная x, истинные значения параметров a=3 и b=8.

296
01:00:58,364 --> 01:01:16,445
Алгоритм для решения задачи с двумя параметрами сработает и для ста миллионов параметров.

297
01:01:16,545 --> 01:01:30,130
Для нахождения параметров a и b нужна функция потерь. Это задача линейной регрессии,

298
01:01:30,130 --> 01:01:35,044
в качестве функции потерь в ней обычно используется квадрат среднеквадратичной ошибки (MSE).

299
01:01:35,144 --> 01:01:41,945
Всё, что мы сейчас будем делать, уже реализовано в numpy и PyTorch, но мы строим всё с нуля в образовательных целях.

300
01:01:42,045 --> 01:01:56,585
Предсказания передаются через параметр y_hat, это вспомогательная функция mse(), вот пример её вызова.

301
01:01:56,685 --> 01:02:13,510
Функция потерь mse_loss() использует функции mse() и lin() для вычисления ошибки.

302
01:02:13,510 --> 01:02:29,560
Объединение линейных и нелинейных слоёв и функции потерь - просто построение сложной функции.

303
01:02:29,560 --> 01:02:39,635
Для иллюстрации работы нейронных сетей рисуют сложные диаграммы, хотя на самом деле это просто функция от функции от функции.

304
01:02:39,735 --> 01:02:51,970
В нашем случае - функция mse_loss() использует функцию mse(), которая использует функцию lin().

305
01:02:53,980 --> 01:03:07,140
Мы создаём 10,000 точек и превращаем их в объекты класса Variable, чтобы производные считались внутри PyTorch.

306
01:03:07,140 --> 01:03:19,060
Параметры a и b задаются случайными числами, флаг requires_grad=True позволит считать градиенты.

307
01:03:20,560 --> 01:03:31,090
a = 0.029873, b = 0.1116.

308
01:03:31,090 --> 01:03:35,795
Мы выбираем скорость обучения и выполняем 10,000 эпох стохастического градиентного спуска.

309
01:03:35,895 --> 01:03:43,995
На самом деле это обычный градиентный спуск, а не стохастический, потому что

310
01:03:44,095 --> 01:03:54,890
в каждой эпохе мы учитываем все параметры, а стохастический градиентный спуск выбирает подмножество параметров.

311
01:03:56,330 --> 01:04:21,050
В каждой эпохе сначала вычисляется значение функции потерь, оно выводится каждую тысячу эпох.

312
01:04:21,050 --> 01:04:29,085
После вычисления функции потерь мы считаем градиенты. Переменная loss содержит одно число,

313
01:04:29,185 --> 01:04:38,210
но функция mse_loss() возвращает объект класса Variable, поэтому и loss принадлежит этому классу

314
01:04:38,210 --> 01:04:48,830
и имеет метод loss.backward(), который вычисляет градиенты всего, где стоит флаг requires_grad=True.

315
01:04:48,830 --> 01:04:59,955
Значение этого флага лежит в полях a.grad.data и b.grad.data.

316
01:05:00,055 --> 01:05:14,530
После вычисления градиентов значения a и b уменьшаются на произведение скорости обучения и градиента.

317
01:05:14,530 --> 01:05:24,015
Здесь используется a.data, потому что это тензор, содержащийся в объектe класса Variable a.

318
01:05:24,115 --> 01:05:28,995
Это исчезнет в PyTorch 0.4, но пока нужно делать так.

319
01:05:29,095 --> 01:05:37,030
Тензор уменьшается на произведение скорости обучения на градиент.

320
01:05:37,030 --> 01:05:43,220
Так работает градиентный спуск.

321
01:05:45,980 --> 01:06:00,830
В PyTorch можно завести несколько функций потерь или несколько слоёв, влияющих на градиент.

322
01:06:00,830 --> 01:06:10,420
Если у вас несколько функций потерь, нужно вызывать метод loss.backward() для каждой из них,

323
01:06:10,520 --> 01:06:22,235
а в конце итерации обнулить методом .zero_().

324
01:06:22,335 --> 01:06:36,505
Последние четыре строки кода реализованы в классе optim.SGD методом optim.SGD.step.

325
01:06:36,605 --> 01:06:50,965
Нижнее подчёркивание в методе .zero_(), как и в других методах PyTorch для работы с тензорами,

326
01:06:51,065 --> 01:07:00,475
значит, что метод меняет переменную, от которой вызывается, а не возвращает её.

327
01:07:02,475 --> 01:07:07,580
То же самое можно сделать и без PyTorch, но придётся реализовать вычисление градиентов.

328
01:07:07,580 --> 01:07:20,965
Для ускорения вычислений мы создадим всего 50 точек.

329
01:07:21,065 --> 01:07:26,050
Функция upd() выполняет одну эпоху и использует только numpy.

330
01:07:26,150 --> 01:07:30,445
Предсказания y_pred опять вычисляются по линейной модели.

331
01:07:30,545 --> 01:07:41,400
Производные dy/db и dy/da лежат в переменных dydb и dyda, вы можете проверить их формулы аналитически.

332
01:07:41,400 --> 01:07:52,770
После вычисления производных значения параметров уменьшаются на произведение скорости обучения и градиента, как раньше.

333
01:07:54,810 --> 01:08:07,590
Для анимации процесса мы используем конструктор FuncAnimation() библиотеки matplotlib.

334
01:08:07,590 --> 01:08:30,119
Объект этого класса ani выполняет функцию animate, которая запускает 30 эпох функции upd() и рисует результат.

335
01:08:33,238 --> 01:08:46,739
Если вы хотите разобраться с градиентным спуском, это самый простой пример в мире.

336
01:08:46,738 --> 01:08:55,408
Тот факт, что внутри PyTorch при оптимизации сотни миллионов параметров происходит то же самое,

337
01:08:55,408 --> 01:09:04,710
может звучать странно, но это так и есть, можете проверить.

338
01:09:04,710 --> 01:09:12,759
Можно добавить инерцию и Adam, которые мы уже обсуждали, это несложно.

339
01:09:16,739 --> 01:09:23,134
Теперь перейдём к рекуррентным нейронным сетям, Jupyter ноутбук называется lesson6-rnn.ipynb.

340
01:09:27,730 --> 01:09:38,204
Сегодня мы будем изучать Ницше. Ницше писал - "Предположив, что истина есть женщина,.." - мне нравится это высказывание.

341
01:09:38,304 --> 01:09:54,599
"все философы... плохо понимали женщин" - видимо, в то время не было женщин-философов, или были, но тоже не понимали.

342
01:09:54,599 --> 01:10:04,929
Мы будем работать с текстами Ницше. На самом деле он совсем не такой ужасный, каким его считают.

343
01:10:05,029 --> 01:10:16,110
Мы создадим модель, которая будет генерировать высказывания в стиле Ницше по одному символу за раз.

344
01:10:16,719 --> 01:10:24,115
Она будет похожа на языковую модель из четвёртой лекции, но там генерировалось одно слово за раз, а здесь будет символ.

345
01:10:24,215 --> 01:10:33,534
Я постараюсь убедить вас в том, что RNN ничем не отличаются от всего, что мы уже знаем.

346
01:10:33,634 --> 01:10:42,700
Для этого я начну строить модель из простых слоёв PyTorch, которые мы уже использовали,

347
01:10:42,800 --> 01:10:48,780
а потом усложню её, добавив цикл обучения.

348
01:10:48,780 --> 01:10:59,790
Главная идея RNN - отслеживание долгосрочных зависимостей (long-term dependencies).

349
01:10:59,790 --> 01:11:07,570
Например, если вы обучаете модель читать такой язык разметки, она должна отслеживать,

350
01:11:07,670 --> 01:11:21,580
находится ли текст внутри блока комментариев <% comment do %> ... <% comment end %>.

351
01:11:21,680 --> 01:11:40,225
Это похоже на человеческую память и сложно реализовать с CNN, но легко с RNN.

352
01:11:40,325 --> 01:11:45,400
Рекуррентные нейронные сети позволяют легко реализовать вещи, невозможные со свёрточными -

353
01:11:45,500 --> 01:11:55,120
представление текущего состояния, память, отслеживание долгосрочных зависимостей и последовательности переменной длины.

354
01:11:55,220 --> 01:12:15,780
Год назад SwiftKey выпустили пост про свою новую языковую модель на основе нейронных сетей.

355
01:12:15,780 --> 01:12:27,180
Языковая модель принимала несколько слов в определённом порядке и предсказывала следующие слова.

356
01:12:27,180 --> 01:12:32,905
Модель получилась хорошая - если вы использовали SwiftKey, то могли заметить, что они дополняют предложения лучше всего.

357
01:12:33,005 --> 01:12:42,475
Андрей Карпатый пару лет назад обучил RNN генерировать текст на языке разметки LaTeX.

358
01:12:42,575 --> 01:12:54,600
Он не показывал модели скомпилированные документы, только исходный код, и обучал её генерировать схожий текст,

359
01:12:54,600 --> 01:13:03,340
и результаты выглядят для меня такими же осмысленными, как большинство математических статей.

360
01:13:03,440 --> 01:13:23,969
Мы начнём с обычной нейронной сети, я покажу свою нотацию описания нейронных сетей.

361
01:13:23,969 --> 01:13:35,489
Все геометрические фигуры - группы активаций, все стрелки - слои нейронной сети.

362
01:13:38,670 --> 01:13:56,340
Прямоугольник - активации входного слоя, круг - скрытого, треугольник - выходного.

363
01:13:56,340 --> 01:14:09,264
Высота прямоугольника - размер минибатча batch_size, ширина - количество признаков #inputs.

364
01:14:09,364 --> 01:14:17,040
Первая стрелка - умножение матриц + выпрямитель, она создаёт активации в круге.

365
01:14:17,040 --> 01:14:32,665
Напоминаю, что активация - это число, которое получается после выпрямителя, умножения матриц или другой операции.

366
01:14:32,765 --> 01:14:45,120
Круг - группа чисел, получившихся после перемножения матриц входного слоя и прохода через выпрямитель.

367
01:14:45,120 --> 01:14:59,680
Высота получившейся матрицы активаций - размер минибатча batch_size, ширина зависит от размера матрицы первого слоя.

368
01:14:59,780 --> 01:15:06,540
Эти активации передаются на следующий слой с очередным умножением матриц и функцией Softmax.

369
01:15:08,450 --> 01:15:12,540
Треугольник - полученная матрица активаций выходного слоя,

370
01:15:12,540 --> 01:15:21,300
её высота - размер минибатча batch_size, ширина - количество классов #classes.

371
01:15:21,300 --> 01:15:35,910
Это нейронная сеть с одним скрытым слоем, если вы до сих пор не реализовали эту артитектуру с нуля - сделайте это.

372
01:15:39,720 --> 01:15:49,855
Мы писали такие сети в лекциях 9, 10 и 11 курса по машинному обучению, посмотрите.

373
01:15:49,955 --> 01:16:02,880
В нашем курсе "Машинное обучение" знания даются с самых основ, а здесь наоборот.

374
01:16:02,880 --> 01:16:08,460
Вот схема свёрточной нейронной сети с одним плотным слоем.

375
01:16:08,460 --> 01:16:16,000
Размер тензора входных активаций - (количество каналов) x (высота) x(ширина).

376
01:16:16,100 --> 01:16:21,570
На прошлом слайде у каждой матрицы было измерение batch_size, здесь я его уже не указываю для удобства чтения.

377
01:16:26,070 --> 01:16:36,595
Аналогично я не указываю выпрямитель в скрытых слоях, кроме последнего, и Softmax на последнем.

378
01:16:36,695 --> 01:16:45,960
С каждой схемой я буду всё больше упрощать, поэтому пока опускаю размер минибатча и функции активации.

379
01:16:45,960 --> 01:16:58,660
Вместо матричного произведения в первом слое выполняется свёртка c шагом stride=2 или с подвыборкой максимумом.

380
01:16:58,760 --> 01:17:08,370
Размер матрицы активаций первого слоя Conv1 - (количество свёрточных фильтров) x (высота / 2) x (ширина / 2).

381
01:17:09,270 --> 01:17:17,580
Полученный тензор выпрямляется (flattening), мы обсудим это на следующей неделе,

382
01:17:17,580 --> 01:17:21,810
сейчас это делается с помощью адаптивной подвыборки максимумом (adaptive max pooling),

383
01:17:21,810 --> 01:17:30,580
в результате тензор преобразуется в вектор.

384
01:17:30,680 --> 01:17:35,995
Полученный вектор участвует в одном или нескольких матричных произведениях.

385
01:17:36,195 --> 01:17:40,770
FC1 - полносвязный слой нейронной сети с количеством активаций #activation.

386
01:17:40,770 --> 01:17:46,260
Последнее умножение матриц даёт нам выходной слой Output с количеством классов #classes.

387
01:17:46,260 --> 01:17:59,842
Итак, прямоугольники - тензоры активаций входного слоя, круги - скрытого, треугольники - выходного. Стрелки - операции слоя.

388
01:17:59,942 --> 01:18:25,640
Сейчас мы попробуем построить модель, предсказывающую третий символ по двум предыдущим.

389
01:18:25,640 --> 01:18:43,680
Напомню, что здесь не показано измерение размера batch_size, и я убрал обозначения со стрелок.

390
01:18:43,680 --> 01:18:50,700
char 1 input - это первый символ каждой строки в минибатче.

391
01:18:54,660 --> 01:19:01,705
Если символы передаются прямым кодированием, ширина входных данных равна количеству уникальных символов.

392
01:19:01,805 --> 01:19:12,310
Мы не будем использовать прямое кодирование, а представим символ числом и создадим эмбеддинги, суть не изменится.

393
01:19:12,410 --> 01:19:27,030
Полученные активации передаются в полносвязный слой FC1.

394
01:19:27,030 --> 01:19:33,990
После работы ещё одного полносвязного слоя добавляется второй символ.

395
01:19:34,090 --> 01:19:39,410
Размер тензора входных данных второго символа совпадает с первым.

396
01:19:39,410 --> 01:19:50,230
Так как стрелки означают произведение матриц, результаты работы двух стрелок в FC2 - одной размерности,

397
01:19:53,600 --> 01:19:59,730
и их можно сложить вместе для получения одной матрицы.

398
01:19:59,830 --> 01:20:04,275
После этого мы выполняем ещё одно матричное умножение.

399
01:20:04,375 --> 01:20:14,385
Напомню, что в конце каждого слоя, кроме последнего, стоит выпрямитель, а после последнего - Softmax.

400
01:20:14,485 --> 01:20:29,240
Это обычная нейронная сеть с двумя полносвязными слоями, единственное отличие -

401
01:20:29,240 --> 01:20:36,295
входной слой второго символа, но он просто прибавляется к результату, это не меняет идеи.

402
01:20:36,395 --> 01:20:40,630
Давайте реализуем эту модель на датасете текстов Ницше.

403
01:20:46,160 --> 01:20:53,490
Я не буду использовать torchtext и почти не буду использовать fastai.

404
01:20:53,590 --> 01:20:59,160
Вот первые 400 символов датасета.

405
01:20:59,260 --> 01:21:08,090
Мы создадим список уникальных символов текста, обернув его в множество set().

406
01:21:08,090 --> 01:21:20,180
Получилось 85 уникальных символов, мы добавим к ним символ '\0' для отступов.

407
01:21:20,180 --> 01:21:29,030
Так выглядит массив уникальных символов chars.

408
01:21:30,750 --> 01:21:41,200
Нам понадобятся словари для получения ID символа по символу и наоборот.

409
01:21:42,600 --> 01:22:01,315
После этого мы копируем датасет в переменную idx, заменив все символы на их индексы.

410
01:22:01,415 --> 01:22:20,060
Для проверки можно выполнить обратную операцию и убедиться, что преобразование прошло правильно.

411
01:22:20,060 --> 01:22:30,330
Теперь наш датасет - переменная idx.

412
01:22:30,330 --> 01:22:36,540
Вопрос из зала: Почему мы работаем с отдельными символами, а не целыми словами?

413
01:22:36,540 --> 01:22:42,180
Я подумал, что так будет проще.

414
01:22:42,180 --> 01:22:54,390
Модели, работающие с символами, в некоторых ситуациях очень полезны, мы обсудим это во второй части курса.

415
01:22:54,390 --> 01:23:00,355
Обычно нужно комбинировать модели, работающие со словами и с символами.

416
01:23:00,455 --> 01:23:05,970
Например, при переводах незнакомые слова полезнее обрабатывать как набор символов, а не как слово 'unknown',

417
01:23:05,970 --> 01:23:09,390
а известные слова обрабатывать как один элемент.

418
01:23:09,390 --> 01:23:19,270
Существует подход Byte Pair Encoding (BPE), в котором рассматриваютсяя N-граммы символов.

419
01:23:19,370 --> 01:23:38,430
Мы обсудим это во второй части курса, но это уже было в предыдущем запуске, можете посмотреть там.

420
01:23:38,430 --> 01:23:48,745
Мы перешли от Keras к PyTorch, когда обсуждали обработку естественного языка, стало гораздо удобнее.

421
01:23:48,845 --> 01:24:04,559
Мы реализуем слегка отличающуюся модель - предсказывать будем четвёртый символ по предыдущим трём.

422
01:24:04,559 --> 01:24:28,559
Структура будет та же, просто слоёв больше. Для этого понадобится четыре входных массива.

423
01:24:28,559 --> 01:24:46,110
В каждом массиве - каждый четвёртый символ текста со своим смещением.

424
01:24:46,110 --> 01:25:14,334
Входные данные - первые три массива, целевая переменная - четвёртый, метод np.stack() преобразует их в нужный формат.

425
01:25:14,434 --> 01:25:23,249
Вот примеры того, что находится внутри переменных x1, x2, x3 и y.

426
01:25:23,249 --> 01:25:40,949
Первая группа из трёх символов - [40, 42, 29], целевая переменная для этой группы - 30.

427
01:25:40,949 --> 01:25:46,289
Первый символ целевой переменной - это также второй символ первого массива.

428
01:25:46,289 --> 01:25:59,030
Для группы [30, 25, 27] нужно предсказать число 29 и так далее.

429
01:25:59,030 --> 01:26:08,550
Данные содержат 200,295 троек чисел.

430
01:26:08,550 --> 01:26:24,624
На скрытом слое модели будет n_hidden=256 активаций и n_fac=42 факторов, примерно половина длины словаря.

431
01:26:24,724 --> 01:26:32,729
Вы можете подобрать числа лучше, это просто эксперимент.

432
01:26:32,729 --> 01:26:43,509
Вот схема модели, которую мы строим.

433
01:26:43,609 --> 01:26:58,319
Структура не изменилась, но я покрасил стрелки. Все стрелки одного цвета используют одну матрицу весов.

434
01:26:58,319 --> 01:27:12,474
Есть своя матрица для всех входных данных, своя - для внутренних слоёв, и своя для финального слоя.

435
01:27:12,574 --> 01:27:17,069
Получается три матрицы весов - для зелёных, оранжевых и синей стрелок.

436
01:27:17,169 --> 01:27:33,029
Мы используем одну матрицу для входных данных, потому что смысл символа не меняется от его местоположения.

437
01:27:33,029 --> 01:27:44,189
Тройки символов в тексте идут подряд и не учитывают начало или конец предложения.

438
01:27:44,189 --> 01:27:48,659
Оранжевые стрелки используют одну и ту же матрицу по той же причине -

439
01:27:51,149 --> 01:27:57,159
это движение от первого символа ко второму и третьему, всё одинаковое.

440
01:27:57,259 --> 01:28:11,819
Реализуем модель в классе Char3Model. Для каждого цвета стрелок создадим свой линейный слой.

441
01:28:11,819 --> 01:28:29,519
Также мы создаём эмбеддинг, преобразующий вектор прямого кодирования длиной vocab_size в вектор длиной n_fac.

442
01:28:29,519 --> 01:28:51,920
В метод forward() передаются три символа c1, c2 и c3. Каждый проходит слой эмбеддинга, линейный слой и выпрямитель.

443
01:28:51,920 --> 01:29:30,239
Добавляется полносвязный слой - это активации первого символа in1 пропускают через скрытый слой (оранжевая стрелка).

444
01:29:32,489 --> 01:29:52,619
Для следующей оранжевой стрелки нужно сделать то же самое, предварительно добавив входные данные второго символа.

445
01:29:57,619 --> 01:30:04,930
Вопрос из зала: Я не понимаю, почему размеры складываемых матриц совпадают.

446
01:30:05,030 --> 01:30:24,000
Давайте посмотрим. self.e() возвращает вектор длиной n_fac, self.l_in() меняет его до длины n_hidden.

447
01:30:24,000 --> 01:30:42,685
Длина вектора in1 равна n_hidden, self.l_hidden() вернёт вектор той же длины, там квадратная матрица весов.

448
01:30:42,785 --> 01:30:50,460
Итак, после обработки первого символа длина вектора h равна n_hidden. Длина in2 равна длине in1, то есть n_hidden.

449
01:30:50,460 --> 01:30:59,035
Значит, можно сложить h и in2, self.l_hidden от их суммы снова вернёт вектор длины n_hidden.

450
01:30:59,135 --> 01:31:09,330
Для этого мы сделали матрицу весов скрытых слоёв квадратной и необходимой длины.

451
01:31:14,675 --> 01:31:21,240
Янет: Эти матрицы можно только складывать?

452
01:31:21,340 --> 01:31:25,500
Не только, я скоро об этом скажу.

453
01:31:25,500 --> 01:31:49,110
Три почти одинаковые строки сложнее рефакторить, чем три одинаковые, поэтому я сначала заполню h нулями.

454
01:31:49,110 --> 01:32:12,685
Когда мы превратим эти три строки в цикл, можно будет называть нашу нейронную сеть рекуррентной.

455
01:32:16,470 --> 01:32:26,740
Для загрузки данных в модель мы используем привычный класс ColumnarModelData.

456
01:32:26,840 --> 01:32:40,910
Метод ColumnarModelData.from_arrays() загрузит данные в необходимом для метода forward() формате.

457
01:32:41,010 --> 01:33:02,604
Если вы хотите удобно передавать данные без сложных модулей и библиотек, используйте его.

458
01:33:02,704 --> 01:33:14,059
Я передаю в метод массив [x1, x2, x3], эти данные распределятся в соответствии с сигнатурой метода forward().

459
01:33:14,189 --> 01:33:18,209
Размер минибатча bs=512 довольно большой, потому что один токен содержит всего один символ.

460
01:33:21,809 --> 01:33:35,019
Я использую fastai только для того, чтобы удобно загрузить данные, модель буду строить в PyTorch.

461
01:33:35,119 --> 01:33:45,729
Я дописываю .cuda() при создании модели, чтобы она хранилась в GPU.

462
01:33:45,829 --> 01:33:56,429
Итератор для прохождения по обучающей выборке можно получить функцией iter(md.trn_dl).

463
01:33:56,429 --> 01:34:12,174
Минибатчи достаются функцией next(it), можно посмотреть на пример данных.

464
01:34:12,274 --> 01:34:42,959
В матрице xs 3 строки и 512 столбцов, вместо прямого кодирования мы используем эмбеддинги.

465
01:34:42,959 --> 01:34:56,280
Модель можно вызывать как функцию, передавая в неё обернутые в класс Variable входные данные xs.

466
01:34:56,280 --> 01:35:09,815
Модель вернула матрицу логарифмов вероятностей 512x85, где 512 - размер минибатча, 85 - количество признаков.

467
01:35:09,915 --> 01:35:19,025
Так можно смотреть внутрь модели, как видите, всё прозрачно.

468
01:35:19,125 --> 01:35:32,179
После этого мы создаём алгоритм оптимизации PyTorch, передавая в него список оптимизируемых параметров.

469
01:35:32,279 --> 01:35:39,259
После этого можно начать обучать модель функцией PyTorch fit().

470
01:35:39,359 --> 01:35:55,790
Мы не использовали объект learner, поэтому вручную задаём алгоритм имитации отжига.

471
01:35:55,890 --> 01:36:09,430
Для тестирования модели я написал фукнцию get_next().

472
01:36:09,630 --> 01:36:14,255
Она принимает на вход три символа, например, 'y. '.

473
01:36:14,355 --> 01:36:31,700
Три символа заменяются на индексы и оборачиваются сначала в тензор, потом в объект класса Variable,

474
01:36:31,800 --> 01:36:45,730
передаются в модель и возвращается самое вероятное предсказание в формате numpy с помощую функции to_np().

475
01:36:47,320 --> 01:36:56,200
Модель хочет продолжить строку 'y. ' символом 'T', выглядит как начало предложения.

476
01:36:58,719 --> 01:37:11,860
Другие тесты тоже выглядят разумно, толковая модель.

477
01:37:11,960 --> 01:37:42,510
Напомню, что в этой модели нет ничего нового, кроме необычного введения входных данных.

478
01:37:42,960 --> 01:37:51,150
Мы готовы к созданию рекуррентной нейронной сети.

479
01:37:51,250 --> 01:38:32,530
Она выглядит так же, просто добавление новых данных занесено в цикл.

480
01:38:32,530 --> 01:38:39,790
Нужно указать, сколько раз повторять цикл. Допустим, мы предсказываем n-ый символ по n-1 предыдущим.

481
01:38:39,790 --> 01:38:46,940
Мы берём первый символ, получаем новые активации, пройдя через слой, а потом в цикле проходим скрытые слои,

482
01:38:47,040 --> 01:38:57,470
каждый раз добавляя новые данные через эмбеддинги. Оранжевая стрелка использует матрицу весов для скрытых слоёв.

483
01:38:57,570 --> 01:39:16,330
Две схемы демонстрируют одну и ту же модель, но последняя более масштабируема.

484
01:39:16,330 --> 01:39:30,699
Можно упростить ещё больше, подавая на вход нули и добавляя первый символ уже в цикле.

485
01:39:36,350 --> 01:39:57,369
Вопрос из зала: Есть ли какая-то аналогия между свёрточными нейронными сетями и переиспользованием матриц весов?

486
01:39:57,369 --> 01:40:02,679
Нет, не вижу аналогии.

487
01:40:02,679 --> 01:40:30,179
Семантический смысл зелёных стрелок - "возьми эти данные и представь их как список признаков".

488
01:40:30,179 --> 01:40:35,875
В том, чтбоы использовать различные представления для этих стрелок, нет смысла, потому что они одинаковые.

489
01:40:35,975 --> 01:40:55,630
Аналогично с оранжевыми стрелками - они показывают переход от одного символа к другому, все переходы одинаковы.

490
01:40:55,630 --> 01:41:05,549
Идея в том, чтобы не плодить лишние матрицы для одинаковых процессов.

491
01:41:07,360 --> 01:41:12,159
Вопрос из зала: CNN похожи на это тем, что один фильтр применяется много раз.

492
01:41:12,159 --> 01:41:24,935
Да, что-то в этом есть.

493
01:41:25,035 --> 01:41:34,175
В прошлом году один из студентов написал об этом хороший пост, надо найти ссылку, согласен с аналогией.

494
01:41:34,275 --> 01:41:45,079
Давайте реализуем эту версию нейронной сети, предсказывая каждый девятый символ.

495
01:41:45,179 --> 01:42:05,290
Для этого создадим входные данные, склеим их вместе, и создадим целевую переменную.

496
01:42:05,290 --> 01:42:23,826
Во входных данных теперь 600,884 восьмёрок чисел. Это символы с 0 до 8, это - с 1 до 9 и так далее.

497
01:42:23,926 --> 01:42:42,675
Для нулевой строки целевой переменной будет это число, аналогично для остальных.

498
01:42:42,775 --> 01:42:57,270
Итак, данные состоят из накладывающихся друг на друга восьмёрок чисел.

499
01:43:03,250 --> 01:43:21,664
Модель реализована в классе CharLoopModel, конструктор такой же, как и в классе Char3Model.

500
01:43:21,764 --> 01:43:38,730
В методе forward() связка выпрямитель + линейный слой + эмбеддинг и добавление новых данных внесены в цикл.

501
01:43:43,940 --> 01:43:59,719
Функция F.tanh() - это гиперболический тангенс, он выглядит как сигмоида в диапазоне от -1 до 1.

502
01:43:59,719 --> 01:44:12,705
В таких переходах между слоями его часто используют для того, чтобы активации не были слишком большие по модулю.

503
01:44:12,805 --> 01:44:25,649
Раньше гиперболический тангенс и сигмоида использовались как функции активации, сейчас вместо них выпрямитель.

504
01:44:25,749 --> 01:44:47,084
Но при переходе между матрицами весов мы продолжим использовать гиперболический тангенс, как, например, здесь.

505
01:44:47,184 --> 01:44:55,779
Янет: Количество циклов влияет на сходимость?

506
01:44:55,879 --> 01:45:26,630
Да, в какой-то степени. У нас получилась довольно глубокая нейронная сеть - восемь слоёв,

507
01:45:26,630 --> 01:45:29,989
такие сети сложно обучать, но давайте попробуем.

508
01:45:37,530 --> 01:45:48,115
Как и раньше, размер минибатча равен 512, в качестве алгоритма оптимизации используется Adam.

509
01:45:48,215 --> 01:46:02,050
После вызова функции fit() мы меняем скорость обучения и вызываем её ещё раз, обучается нормально.

510
01:46:02,150 --> 01:46:12,100
Янет спрашивала, можно ли не прибавлять входные данные к матрице, а делать что-нибудь другое.

511
01:46:12,200 --> 01:46:28,645
Входные данные изначально не стоило складывать с активациями скрытого слоя, потому что эти данные - разного происхождения.

512
01:46:28,745 --> 01:46:34,030
Входные данные - переданные прямым кодированием признаки символов,

513
01:46:34,130 --> 01:46:42,235
а активации скрытого слоя - информация о связях в группе символов.

514
01:46:42,335 --> 01:46:50,800
Возможно, Янет предложила бы склеить эти матрицы вместе вместо сложения. Да, она кивает.

515
01:46:50,900 --> 01:46:56,765
Это реализовано в классе CharLoopConcatModel, он идентичен CharLoopModel,

516
01:46:56,865 --> 01:47:19,590
но вместо метода char.plus() - char.cat(), и входной линейный слой теперь принимает длину n_fac+n_hidden, а не n_fac.

517
01:47:19,590 --> 01:47:33,270
Переменная inp теперь имеет размер n_fac+n_hidden, потом преобразуется к размеру n_hidden,

518
01:47:33,270 --> 01:47:41,110
и в третьей строке размер её не меняется, так как матрица весов квадратная.

519
01:47:41,210 --> 01:47:51,945
Хороший подход к дизайну архитектур - конкатенируйте данные разных типов, если они используются вместе.

520
01:47:52,045 --> 01:48:00,850
Сложение разных типов данных - потеря информации.

521
01:48:00,850 --> 01:48:11,560
Склеенные матрицы приводятся к необходимому размеру матричным умножением.

522
01:48:11,560 --> 01:48:29,650
Повторяем те же шаги и видим, что ошибка упала с 1.72 до 1.68, это приятно.

523
01:48:29,650 --> 01:48:45,550
Тестирование опять показывает хорошие результаты.

524
01:48:45,650 --> 01:48:56,300
Давайте перепишем некоторые этапы с применением PyTorch.

525
01:48:56,400 --> 01:49:11,680
PyTorch выполнит за нас цикл и создаст входные линейные слои, это реализовано в классе nn.RNN.

526
01:49:11,680 --> 01:49:34,370
Новая модель - в классе CharRnn. Строки заменены не на схожие аналоги в PyTorch, а на почти идентичный код.

527
01:49:34,470 --> 01:49:43,660
Слои нейронной сети создаются конструктором nn.RNN(), он же используется вместо цикла.

528
01:49:43,660 --> 01:50:05,139
Мы начинали с матрицы нулей, здесь она тоже нужна.

529
01:50:05,139 --> 01:50:27,980
Мы обновляем каждый раз переменную h с активациями скрытых слоёв, это потом пригодится.

530
01:50:28,080 --> 01:50:34,250
Метод self.rnn() принимает матрицу нулей и входные данные, возвращает выходные данные и состояние скрытого слоя (hidden state).

531
01:50:34,350 --> 01:50:41,265
Вопрос из зала: Что такое состояние скрытого слоя?

532
01:50:41,365 --> 01:51:01,972
Это матрица h размера 256, оранжевый круг на схеме.

533
01:51:06,010 --> 01:51:33,310
PyTorch не перезаписывает переменную h при работе нейронной сети, а дописывает новый слой в тензор скрытых слоёв.

534
01:51:33,310 --> 01:51:43,594
Нам нужен только последний скрытый слой, поэтому здесь индексировани out[-1].

535
01:51:43,694 --> 01:51:50,940
В остальном код такой же - активации последнего скрытого слоя передаются на выходной слой.

536
01:51:57,630 --> 01:52:07,924
Здесь я добавил ещё один скрытый слой.

537
01:52:08,024 --> 01:52:26,380
Матрица h - тензор третьего ранга, а не второго, как раньше, потому что мы добавили вспомогательную ось.

538
01:52:26,480 --> 01:52:48,730
Это нужно для двунаправленных (bidirectional RNN) и многослойных (multilayer RNN) рекуррентных нейронных сетей.

539
01:52:51,580 --> 01:52:56,170
Для учётв дополнительных слоёв этих сетей нужна дополнительная ось, мы это ещё обсудим,

540
01:52:56,170 --> 01:53:09,740
пока эта ось не используется и имеет длину 1.

541
01:53:09,840 --> 01:53:27,885
В прошлый раз мы обучали модель всего пару эпох, сейчас выполним 4 на скорости обучения 1e-3, а потом ещё 2 на скорости 1e-4.

542
01:53:27,985 --> 01:53:37,455
Функция потерь упала до 1.5, модель становится всё лучше и лучше.

543
01:53:37,555 --> 01:53:44,492
Вот очередной тест.

544
01:53:44,592 --> 01:54:18,190
В нём для каждой восьмёрки по одному символу за раз предсказывается ещё 40 символов, вот как это работает.

545
01:54:20,620 --> 01:54:39,200
Модель неплохая, мы создали её с нуля, а потом заменили код в двух местах на идентичный в PyTorch.

546
01:54:39,300 --> 01:55:08,105
В качестве домашнего задания на этой неделе вы можете написать свою версию этого класса, не глядя в PyTorch.

547
01:55:08,205 --> 01:55:16,850
В этом вам поможет предпоследняя версия модели. Убедитесь, что ваша модель нормально работает.

548
01:55:16,950 --> 01:55:29,910
Задание простое, но вам понравятся результаты вашей работы.

549
01:55:30,390 --> 01:55:49,705
Новое изменение - теперь внутри цикла (пунктирный прямоугольник) находятся не только входные данные, но и выходные.

550
01:55:49,805 --> 01:56:13,780
Теперь предсказания получаются после каждого добавления новых данных, по трём символам модель предскажет три новых -

551
01:56:13,780 --> 01:56:19,087
следующий за первым символом, следующий за вторым и следющий за третьим.

552
01:56:19,187 --> 01:56:24,240
Ничего нового, ничего сложного.

553
01:56:26,470 --> 01:57:02,480
В домашней работе вы можете это добавить, это выглядит вот так.

554
01:57:02,480 --> 01:57:21,560
Итак, мы получаем предсказания после каждого скрытого слоя. Этому есть причина.

555
01:57:21,560 --> 01:57:38,660
Входные данные состоят из накладывающихся восьмёрок чисел, это очень неэффективно,

556
01:57:41,960 --> 01:57:52,950
потому что семь из восьми эмбеддингов этих чисел уже посчитаны в предыдущей строке.

557
01:57:53,050 --> 01:58:05,550
Непонятно, зачем пересчитывать их каждый раз.

558
01:58:05,650 --> 01:58:34,000
Вместо этого используются ненакладывающиеся восьмёрки чисел, данные бьются на равные отрезки.

559
01:58:34,100 --> 01:59:02,005
Целевые переменные - сдвинутые на 1 входные данные, для каждого числа из восьмёрки предсказывается следующее.

560
01:59:02,105 --> 01:59:21,290
Так работает эта модель. Смысл не меняется, но вычисления будут эффективнее.

561
01:59:27,390 --> 01:59:39,800


562
01:59:40,489 --> 01:59:48,239


563
01:59:48,239 --> 01:59:52,410


564
01:59:52,410 --> 01:59:58,350


565
01:59:58,350 --> 02:00:07,890


566
02:00:07,890 --> 02:00:22,739


567
02:00:24,120 --> 02:00:31,260


568
02:00:31,260 --> 02:00:38,670


569
02:00:38,670 --> 02:00:46,670


570
02:00:46,670 --> 02:00:53,400


571
02:00:53,400 --> 02:00:59,910


572
02:00:59,910 --> 02:01:05,550


573
02:01:05,550 --> 02:01:11,960


574
02:01:12,330 --> 02:01:18,190


575
02:01:18,190 --> 02:01:24,370


576
02:01:24,370 --> 02:01:30,940


577
02:01:30,940 --> 02:01:37,389


578
02:01:37,389 --> 02:01:44,230


579
02:01:46,179 --> 02:01:50,230


580
02:01:54,429 --> 02:01:59,110


581
02:01:59,110 --> 02:02:05,260


582
02:02:05,260 --> 02:02:09,310


583
02:02:09,310 --> 02:02:14,530


584
02:02:17,110 --> 02:02:21,550


585
02:02:21,550 --> 02:02:28,000


586
02:02:28,000 --> 02:02:37,290


587
02:02:37,290 --> 02:02:45,340


588
02:02:45,340 --> 02:02:54,070


589
02:02:54,070 --> 02:02:59,199


590
02:02:59,199 --> 02:03:04,449


591
02:03:04,449 --> 02:03:10,090


592
02:03:10,090 --> 02:03:15,989


593
02:03:22,830 --> 02:03:28,550


594
02:03:28,550 --> 02:03:39,550


595
02:03:39,790 --> 02:03:53,270


596
02:03:53,270 --> 02:04:00,360


597
02:04:01,540 --> 02:04:06,500


598
02:04:06,500 --> 02:04:11,530


599
02:04:11,530 --> 02:04:17,830


600
02:04:17,830 --> 02:04:23,270


601
02:04:23,270 --> 02:04:30,949


602
02:04:33,260 --> 02:04:38,150


603
02:04:39,739 --> 02:04:44,239


604
02:04:44,239 --> 02:04:49,130


605
02:04:49,130 --> 02:04:53,719


606
02:04:53,719 --> 02:05:02,120


607
02:05:02,120 --> 02:05:09,920


608
02:05:09,920 --> 02:05:15,850


609
02:05:15,850 --> 02:05:20,420


610
02:05:20,420 --> 02:05:27,230


611
02:05:30,710 --> 02:05:37,480


612
02:05:39,790 --> 02:05:45,970


613
02:05:45,970 --> 02:05:53,290


614
02:05:53,290 --> 02:05:58,600


615
02:05:58,600 --> 02:06:06,460


616
02:06:06,460 --> 02:06:13,480


617
02:06:15,910 --> 02:06:20,500


618
02:06:20,500 --> 02:06:26,080


619
02:06:26,080 --> 02:06:31,930


620
02:06:31,930 --> 02:06:37,120


621
02:06:37,120 --> 02:06:46,480


622
02:06:46,480 --> 02:06:51,310


623
02:06:51,310 --> 02:06:55,390


624
02:06:55,390 --> 02:07:01,900


625
02:07:05,910 --> 02:07:13,750


626
02:07:13,750 --> 02:07:18,640


627
02:07:18,640 --> 02:07:26,880


628
02:07:26,880 --> 02:07:38,020


629
02:07:38,020 --> 02:07:43,000


630
02:07:43,000 --> 02:07:48,340


631
02:07:48,340 --> 02:07:53,109


632
02:07:53,109 --> 02:07:56,559


633
02:07:56,559 --> 02:08:09,069


634
02:08:09,069 --> 02:08:13,149


635
02:08:13,149 --> 02:08:21,339


636
02:08:21,339 --> 02:08:25,869


637
02:08:25,869 --> 02:08:33,909


638
02:08:33,909 --> 02:08:37,689


639
02:08:37,689 --> 02:08:46,419


640
02:08:46,419 --> 02:08:50,439


641
02:08:50,439 --> 02:08:54,869


642
02:08:57,760 --> 02:09:01,659


643
02:09:07,209 --> 02:09:16,359


644
02:09:16,359 --> 02:09:22,149


645
02:09:22,149 --> 02:09:29,939


646
02:09:29,939 --> 02:09:35,589


647
02:09:35,589 --> 02:09:42,309


648
02:09:42,309 --> 02:09:49,839


649
02:09:49,839 --> 02:09:54,519


650
02:09:54,519 --> 02:09:59,289


651
02:10:03,280 --> 02:10:08,080


652
02:10:08,080 --> 02:10:14,350


653
02:10:14,350 --> 02:10:17,410


654
02:10:22,770 --> 02:10:29,110


655
02:10:31,930 --> 02:10:35,770


656
02:10:35,770 --> 02:10:39,610


657
02:10:42,070 --> 02:10:46,390


658
02:10:46,390 --> 02:10:55,030


659
02:10:58,680 --> 02:11:07,920


660
02:11:07,920 --> 02:11:14,949


661
02:11:18,310 --> 02:11:29,110


662
02:11:31,449 --> 02:11:38,190


663
02:11:38,190 --> 02:11:44,440


664
02:11:44,440 --> 02:11:49,870


665
02:11:49,870 --> 02:11:54,550


666
02:11:54,550 --> 02:11:58,090


667
02:11:58,090 --> 02:12:02,620


668
02:12:02,620 --> 02:12:07,890


669
02:12:07,890 --> 02:12:12,969


670
02:12:12,969 --> 02:12:16,900


671
02:12:16,900 --> 02:12:22,360


672
02:12:22,360 --> 02:12:26,739


673
02:12:26,739 --> 02:12:32,620


674
02:12:32,620 --> 02:12:36,429


675
02:12:36,429 --> 02:12:41,890


676
02:12:41,890 --> 02:12:49,179


677
02:12:49,179 --> 02:12:53,650


678
02:12:53,650 --> 02:12:59,140


679
02:12:59,140 --> 02:13:02,290


680
02:13:02,290 --> 02:13:05,679


681
02:13:05,679 --> 02:13:09,310


682
02:13:09,310 --> 02:13:15,250


683
02:13:15,250 --> 02:13:20,560


684
02:13:20,560 --> 02:13:23,830


685
02:13:27,070 --> 02:13:30,070


686
02:13:30,070 --> 02:13:35,350


687
02:13:35,350 --> 02:13:40,679


688
02:13:41,040 --> 02:13:43,960


