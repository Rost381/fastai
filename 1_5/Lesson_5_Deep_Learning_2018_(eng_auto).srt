1
00:00:00,030 --> 00:00:05,784
welcome back so we had a busy lesson

2
00:00:05,884 --> 00:00:27,834
last week and I was really thrilled to
see actually one of our masters students
here at USF actually actually took what
we learned took what we learned with
structured deep learning and turned it
into a blog post which as I suspected

3
00:00:27,934 --> 00:00:37,824
has been incredibly popular because it's
just something people didn't know about
and so it actually ended up getting
picked up by the towards data science

4
00:00:37,924 --> 00:00:46,840
publication which I quite liked actually
if you're interested in keeping up with
what's going on a data science it's
quite good medium publication and so

5
00:00:46,940 --> 00:00:58,195
Karen talked about structured deep
learning and basically introduced you
know the the the basic ideas that we
learned about last week and it got

6
00:00:58,295 --> 00:01:05,452
picked up quite quite widely one of the
one of the things I was pleased to say
actually sebastian ruder who actually
mentioned in last week's class as being

7
00:01:05,552 --> 00:01:16,590
one of my favorite researchers tweeted
it and then somebody from stitch fix
said oh yeah we've actually been doing
that for ages which is kind of cute I I

8
00:01:16,590 --> 00:01:23,125
kind of know that this is happening in
industry a lot and I've been telling
people this is happening in industry a
lot but nobody's been talking about it

9
00:01:23,225 --> 00:01:31,509
and now the Karen's kind of published a
blog saying hey check out this cool
thing and they all stitch fixes like
yeah we're doing that already so so

10
00:01:31,609 --> 00:01:46,310
that's been great great to see and I
think there's still a lot more that can
be dug into with this structured people
learning stuff you know to build on top
of Karen's post would be that maybe like
experiment with some different datasets

11
00:01:46,410 --> 00:01:57,429
maybe find some old careful competitions
and see like there's some competitions
that you could now win with this or some
which doesn't work for would be equally
interesting and also like just

12
00:01:57,529 --> 00:02:13,970
experimenting a bit with different
amounts of dropout different layer sizes
you know because nobody much is written
about this I don't think there's been
any blog posts about this before that
I've seen anywhere
there's a lot of unexplored territory so

13
00:02:13,970 --> 00:02:21,950
I think there's a lot we could we could
build on top of here and there's
definitely a lot of interest as well one
person on Twitter saying this is what
I've been looking for for ages another

14
00:02:25,310 --> 00:02:36,940
thing which I was pleased to see is
Nikki or who we saw his cricket versus
baseball predictor as well as his a
currency predictor after less than one

15
00:02:36,940 --> 00:02:54,545
went on to download something a bit
bigger which was to download a couple of
hundred of images of actors and he
manually went through and checked which
well I think first of all he like used
Google to try and find ones with glasses
and ones without then he manually went
through and checked that that they put

16
00:02:54,645 --> 00:03:06,724
in the right spot and this was a good
example of one where vanilla ResNet
didn't do so well with just the last
layer and so what Nikhil did was he went
through and tried on freezing the layers

17
00:03:06,824 --> 00:03:11,440
and using differential learning rates
and got up to a hundred percent accuracy

18
00:03:11,709 --> 00:03:22,590
and the thing I like about these things
that Nikhil was doing is the way he's
he's not downloading a kegel data set
he's like deciding on a problem that
he's going to try and solve he's going

19
00:03:22,690 --> 00:03:30,930
from scratch from google and he's
actually got a link here even to the
suggested way to help you download
images from Google so I think this is

20
00:03:31,030 --> 00:03:41,379
great and actually gave a talk just this
afternoon at singularity University to
an executive team of one of the world's
largest telecommunications companies and

21
00:03:41,479 --> 00:03:56,220
actually show them this post because the
folks there were telling me that that
all the vendors that come to them and
tell them they need like millions of
images and huge data centers will have
hardware and you know they have to buy a
special software that only these vendors

22
00:03:56,320 --> 00:04:03,960
can provide and I said like actually
this person has been doing it of course
for three weeks now and look at what
he's just done with a computer that cost

23
00:04:04,060 --> 00:04:13,140
him 60 cents an hour and they were like
they were so happy to hear that like
okay they're you know this actually is
in the reach of normal people I'm

24
00:04:13,240 --> 00:04:20,030
assuming Nikhil is a normal person I
haven't actually
and if your proudly abnormal Nicole I

25
00:04:20,029 --> 00:04:31,650
apologize I actually went and actually
had a look at his cricket classifier and
I was really pleased to see that his
code actually is the exact same code
that were used in Lesson one I was

26
00:04:31,750 --> 00:04:42,852
hoping that would be the case you know
the only thing he changed was the number
of epochs I guess so this idea that we
can take those four lines of code and
reuse it to do other things that's
definitely turned out to be true and so

27
00:04:42,952 --> 00:04:50,894
these are good things to show like it
yeah your organization if you're
anything like the executives of this big
company I spoke to today there'll be a

28
00:04:50,994 --> 00:05:04,820
certain amount of like not to surprise
but almost like pushback like if this
was true somebody does it all that
message she said if this was true
somebody would have told us so like why
isn't everybody doing this already so

29
00:05:04,820 --> 00:05:15,885
we'd like it I think you might have to
actually show them you know maybe you
can build your own there's some internal
data you've got at work or something
like here it is you know didn't cost me

30
00:05:15,985 --> 00:05:28,700
anything it's all finished fiddly or
badly I don't know how to pronounce his
name correctly has done another very
nice post on just an introductory post
on how we train neural networks and I've

31
00:05:28,700 --> 00:05:40,340
wanted to point this one out as being
like I think this is one of the
participants in this course who has got
a particular knack for technical
communication and I think we can all
learn from you know from his post about
about good technical writing what I

32
00:05:43,280 --> 00:05:52,220
really like particularly is that he he
assumes almost nothing like he has a
kind of a very chatty tone and describes
everything but he also assumes that the

33
00:05:52,220 --> 00:06:03,610
reader is intelligent but you know so
like he's not afraid to kind of say
here's a paper or here's an equation or
or whatever but then he's going to go
through and tell you exactly what that
equation means so it's kind of like this

34
00:06:03,710 --> 00:06:11,450
nice mix of like writing for
respectfully for an intelligent audience
but also not assuming any particular

35
00:06:11,450 --> 00:06:21,480
background knowledge so then I made the
mistake earlier this week of posting a
picture of my first placing on the

36
00:06:21,580 --> 00:06:31,025
Carroll seedlings competition at which
point five other fast AI students posted
their pictures of them pass
over the next few days so this is the

37
00:06:31,125 --> 00:06:41,870
current leaderboard for the cattle plant
seedlings competition I believe the
product top six are all fast AI students
or in the worst of those teachers and so

38
00:06:41,970 --> 00:07:06,212
I think this is like a really Oh James
is just passed he was first this is a
really good example of like what you can
do but this is trying to think it was
like a small number of thousands of
images and most of the images were only
were less than a hundred pixels by a
hundred pixels and yet week you know I

39
00:07:06,312 --> 00:07:16,115
bet my approach was basically to say
let's just run through the notebook we
have pretty much default took the I

40
00:07:16,215 --> 00:07:23,170
don't know an hour and I'm I think the
other students doing a little bit more
than that but not a lot more and

41
00:07:23,170 --> 00:07:48,520
basically what this is saying is yeah
these these techniques work pretty
reliably to a point where people that
aren't using the fast I know libraries
you know literally really struggling
let's just pick off these are first aid
a students might have to go down quite a
way so I thought that was very
interesting and really really cool so

42
00:07:48,520 --> 00:07:55,510
today we are going to start what I would
kind of call like the second half of

43
00:07:55,510 --> 00:08:09,067
this course so the first half of this
course is being like getting through
like these are the applications that we
can use this for here's kind of the code
you have to write here's a fairly high

44
00:08:09,167 --> 00:08:13,590
level ish description of what it's doing
and we're kind of we're kind of done for

45
00:08:17,830 --> 00:08:27,245
that bit and what we're now going to do
is go in reverse we're going to go back
over all of those exact same things
again but this time we're going to dig
into the detail of every one and we're

46
00:08:27,345 --> 00:08:35,590
going to look inside the source code of
the first idea library to see what it's
doing and try to replicate that so in a

47
00:08:35,590 --> 00:08:45,370
students like there's not going to be a
lot more
best practices to show you like I've
kind of shown you the best best

48
00:08:45,370 --> 00:08:52,085
practices I know but I feel like for us
to now build on top of those to debug
those models to come back to part two

49
00:08:52,185 --> 00:08:58,780
where we're going to kind of try out
some new things you know it really helps
to understand what's going on behind the

50
00:08:58,780 --> 00:09:12,625
scenes okay so the goal here today is
we're going to try and create a pretty
effective collaborative filtering model
almost entirely from scratch so we'll

51
00:09:12,725 --> 00:09:19,330
use the kind of we'll use PI torch as a
automatic differentiation tool and
there's a GPU programming tool and not

52
00:09:19,330 --> 00:09:24,977
very much else we'll try not to use its
neural net features we'll try not to use
fast AI library anymore than necessary

53
00:09:25,077 --> 00:09:35,800
so that's the goal so let's go back and
you know we only very quickly know
collaborative filtering last time so
let's let's go back and have a look at

54
00:09:35,800 --> 00:09:45,155
collaborative filtering and so we're
going to look at this movie lens data
set so the movie lens data set basically

55
00:09:45,255 --> 00:09:58,900
is a list of ratings it's got a bunch of
different users that are represented by
some ID and a bunch of movies that are
represented by some ID and rating it
also has a timestamp

56
00:09:58,900 --> 00:10:12,650
I haven't actually ever tried to use
this I guess this is just like what what
time did that person read that movie so
that's all we're going to use for
modelling is three columns user ID movie

57
00:10:12,750 --> 00:10:28,410
ID and rating and so thinking of that in
kind of structured data terms user ID
and movie ID would be categorical
variables we have two of them and rating
would be a with the independent variable

58
00:10:28,410 --> 00:10:35,330
we're not going to use this for modeling
but we can use it for looking at stuff
later we can grab a list of the names of

59
00:10:35,430 --> 00:10:46,089
the movies as well and reproduce this
genre information I haven't tried to be
interested if during the week anybody
tries it and finds it helpful my guess
is you might not find it helpful

60
00:10:46,089 --> 00:10:55,180
we'll see so in order to kind of look at
this better I just grabbed
the users that have watched the most

61
00:10:57,880 --> 00:11:03,820
movies and the movies that have been the
most watched and made a crosstab of it

62
00:11:03,820 --> 00:11:12,680
right so this is exactly the same data
but it's a subset and now rather than
being user movie rating we've got user

63
00:11:12,780 --> 00:11:18,557
movie rating and so some users haven't
watched some of these movies that's why

64
00:11:18,657 --> 00:11:37,955
some of these okay then I copied that
into Excel and you'll see there's a
thing called collab your XLS if you
don't see it there now I'll make sure I
put it there back tomorrow and here is
where I've copied that table okay so as

65
00:11:38,055 --> 00:12:09,790
I go through this like setup of the
problem and kind of how its described
and stuff if you're ever feeling lost
feel free to ask either directly or
through the forum if you ask through the
forum and somebody answers there I want
you to answer it here but if somebody
else asks a question you would like
answered of course just like it and your
network keep an eye out for that because

66
00:12:09,790 --> 00:12:19,803
kind of that's we're digging in to the
details of what's going on behind the
scenes it's kind of important that at
each stage you feel like okay I can see

67
00:12:19,903 --> 00:12:47,420
what's going on okay so we can actually
not going to build a neural net to start
with instead we're going to do something
called a matrix factorization the reason
we're not going to build a neural net to
start with is that it so happens there's
a really really simple kind of way of
solving these kinds of problems which
I'm going to show you and so if I scroll

68
00:12:47,520 --> 00:12:57,800
down I've basically what I've got here
is the same the same thing but this time
these are my predictions rather than my
actuals and I'm going to show you how I

69
00:12:57,900 --> 00:13:18,645
created these predictions okay so here
my actuals right here my predictions and
then down here we have
our score which is the sum of the
different squared average square root
okay so this is I are MSE down here okay

70
00:13:18,745 --> 00:13:25,005
so on average we're randomly initialized
model is out by 2.8 so let me show you

71
00:13:25,105 --> 00:13:41,560
what this model is and I'm going to show
you by saying how do we guess how much
user ID number 14 likes movie ID number
27 and the prediction here this is just
at this stage this is still random is

72
00:13:41,560 --> 00:14:05,990
0.9 1 so how we calculate 0.9 1 and the
answer is we're taking it as this vector
here dot product with this vector here
so dot product means 0.71 times 0.1 9
plus 0.8 1 times point 6 3 plus point 7
volt plus point 3 1 and so forth and in

73
00:14:05,990 --> 00:14:22,509
you know linear algebra speak because
one of them is a column and one of them
is a row this is the same as a matrix
product so you can see here I've used
the Excel fashion matrix multiplier and
that's my prediction having said that if

74
00:14:22,609 --> 00:14:33,050
the original rating doesn't exist at all
then I'm just going to set this to 0
right because like there's no error in
predicting something that hasn't

75
00:14:34,220 --> 00:14:46,149
happened okay so what I'm going to do is
I'm basically going to say alright
everyone of my right rate my predictions
is not going to be a neural net it's
going to be a single matrix

76
00:14:46,249 --> 00:15:04,430
multiplication all right now the matrix
multiplication that it's doing is
basically in practice is between like
this matrix and this matrix right so
each one of these is a single part of

77
00:15:04,430 --> 00:15:28,165
that so I randomly initialize these
these are just random numbers that I've
just pasted in here so I've basically
started off with two random matrices and
I've said let's
assume for the time being that every
rating can be represented as the matrix

78
00:15:28,265 --> 00:15:43,135
product of those two so then in Excel
you can actually do a gradient descent
you have to go to your options to the
add-ins section and check the box to say
turn it on and once you do you'll see
there's something there called solver

79
00:15:43,235 --> 00:16:07,525
and if I go solver it says okay what's
your objective function and you just
choose the cell so in this case we chose
the cell that contains that repeats
grade error and then it says okay what
do you want to change and you can see
here we've selected this matrix and this
matrix and so it's going to do a

80
00:16:07,625 --> 00:16:20,110
gradient descent for us by changing
these matrices to try and in this case
minimize this min minimize this Excel so
right GRG nonlinear is a gradient just

81
00:16:20,520 --> 00:16:33,985
yet so I'll say solve and you'll see it
starts at 2.8 and then down here you'll
see that numbers drain down it's not
actually showing us what it's doing but
we can see that the numbers going down

82
00:16:34,085 --> 00:16:43,840
so this has kind of got a near or nettie
feel to it in that we're doing like a
matrix product and we're doing a
gradient descent but we don't have a

83
00:16:43,940 --> 00:16:51,630
nonlinear layer and we don't have a
second linear layer on top of that so we
don't get to call this deep learning so

84
00:16:51,630 --> 00:17:00,150
things where people do like deep
learning each things where they have
kind of matrix products and gradient
descents but it's not deep people tend
to just call that shallow learning okay

85
00:17:01,830 --> 00:17:17,189
so we're doing this chattering yeah all
right so I'm just going to go ahead and
press escape to stop it because I'm sick
of waiting and so you can see we've now
got down to the 0.39 all right so for

86
00:17:17,189 --> 00:17:36,650
example it guessed that movie 72 for
sorry movie 27 for user seventy two
would get 4.4 for rating 2772 and
actually got a four ready so you can see
like it's it's it's doing something
quite useful

87
00:17:36,650 --> 00:18:11,190
so why is it doing something quite
useful I mean something to note here is
the number of things we're trying to
predict here is there's 225 of them
right and the number of things we're
using to predict is that times two so
hundred and fifty of them so it's not
like we can just exactly fit we actually
have to do some kind of machine learning
here so basically what this is saying is
that there does seem to be some way of
making predictions in this way and so

88
00:18:11,190 --> 00:18:23,725
for those of you that have done some
linear algebra and this is actually a
matrix decomposition normally in linear
algebra you would do this using a
analytical technique or using some
techniques that are specifically

89
00:18:23,825 --> 00:18:29,430
designed for this purpose but the nice
thing is that we can use gradient
descent to solve pretty much everything

90
00:18:29,430 --> 00:18:36,960
including this I don't like to so much
think of it from a linear algebra point
of view though I like to think of it
from an insured point of view which is

91
00:18:36,960 --> 00:18:56,225
this let's say movie sorry let's say
movie id 27 is Lord of the Rings part 1
and let's say move and so let's say
we're trying to make that prediction for
user 2072 are they going to like Lord of
the Rings part 1 and so conceptually

92
00:18:56,325 --> 00:19:11,250
that particular movie maybe there's like
this 4
so there's 5 numbers here and we could
say like well what if the first one was
like how much is it sci-fi and fantasy

93
00:19:11,250 --> 00:19:21,390
and the second one is like how recent a
movie and how much special effects is
there you know and the one at the top
might be like how dialogue-driven is it

94
00:19:21,390 --> 00:19:27,820
right like let's say those kind of five
these five numbers represented
particular things about the movie and so

95
00:19:27,920 --> 00:19:45,265
if that was the case then we could have
the same five numbers for the user
saying like ok how much does the use of
like sci-fi fantasy how much does the
user like modern modern CGI driven
movies how much does this give us a like

96
00:19:45,365 --> 00:19:54,310
dialogue different movies and so if you
then took that
cross-product you would expect to have a
good model right would expect to have a

97
00:19:54,410 --> 00:20:13,060
reasonable reading now the problem is we
don't have this information for each
user we don't have the information for
each movie so we're just going to like
assume that this is a reasonable kind of
way of thinking about this system and
let's unless stochastic gradient descent
try and find these models right so so in

98
00:20:13,160 --> 00:20:36,030
other words these these factors we call
these things factors these factors and
we call them factors because you can
multiply them together to create this
not they're factors and how many
addresses these factors we call them
latent factors because they're not
actually this is not actually a vector
that we've like named and understood and

99
00:20:36,030 --> 00:20:46,740
like entered in manually we've kind of
assumed that we can think of movie
ratings this way we've assumed that we
can think of them as a dot product of

100
00:20:46,740 --> 00:20:53,605
some particular features about a movie
and some particular features of to look
what users like those kinds of movies

101
00:20:53,705 --> 00:21:06,175
right and then we've used gradient
descent to just say okay try and find
some numbers that work so that's that's
basically the technique right and it's

102
00:21:06,275 --> 00:21:14,889
kind of the end and the entirety is in
this printing right so that is
collaborative filtering using what we
call probabilistic matrix factorization

103
00:21:14,989 --> 00:21:26,490
and as you can see the whole thing is
easy to do in an excel spreadsheet and
the entirety of it really is this single
thing which is a single matrix
multiplication plus randomly

104
00:21:26,490 --> 00:21:38,250
initializing if it would be better to
cap this to 0 and 5 maybe yes yeah and

105
00:21:38,250 --> 00:21:46,380
we're gonna do that later right there's
a whole lot of stuff we can do to
improve this this is like our simple as
possible starting point all right so so

106
00:21:46,380 --> 00:21:50,850
what we're going to do now is we're
going to try and implement this in
Python and run it on the whole data set

107
00:21:54,750 --> 00:22:03,050
another question is how do you figure
out how many you know how it's clear how
long are the matrix

108
00:22:03,150 --> 00:22:43,220
five yeah yeah so something to think
about given that this is like movie 49
right and we're looking at a rating for
movie 49 think about this this is
actually at embedding matrix and so this
length is actually the size of the
embedding matrix I'm not saying this is
an analogy I'm saying it literally this
is literally an embedding mattress we
could have a one hot encoding where 72
where a one is in the 72nd position and
so we'd like to look it up and it would
return this list of five numbers so the

109
00:22:43,220 --> 00:22:53,780
question is actually how do we decide on
the dimensionality of our embedding
vectors and the answer to that question
is we have no idea we have to try a few

110
00:22:53,780 --> 00:23:10,410
things and see what was the underlying
concept is you need to pick an embedding
dimensionality which is enough to
reflect the kind of true complexity of
this causal system but not so big that

111
00:23:10,510 --> 00:23:17,460
you have too many parameters that it
could take forever to Tehran or even
with regularization in my overfit so

112
00:23:17,560 --> 00:23:26,865
what does it mean when the factor is
negative that the factor being negative

113
00:23:26,965 --> 00:23:34,610
in the movie case would mean like this
is not dialogue-driven in fact it's like
the opposite dialogue here is terrible a

114
00:23:34,610 --> 00:23:41,540
negative for the user would be like I
actually dislike modern CGI movies so

115
00:23:41,540 --> 00:23:49,645
it's not from zero to whatever it's the
range of score it'd be negative this is
a range of score even like no net Maxim

116
00:23:49,745 --> 00:23:54,447
no there's no constraints at all here
these are just standard embedding

117
00:23:54,547 --> 00:24:19,340
matrices questions so first question is
why do what why can we trust this
embeddings because like if you take a
number six it can be expressed as 1 into
6 or like 6 into 1 or 22 3 & 3 into 2
all so you're saying like we could like
reorder these higher
hardly the value itself might be
different as long as the product is

118
00:24:21,410 --> 00:24:33,380
something well but you see we're using
gradient descent to find the best
numbers so like once we've found a good
minimum the idea is like yeah there are

119
00:24:33,380 --> 00:24:42,720
other numbers but they don't give you as
good an objective value and of course we
should be checking that on a validation
set really which we'll be doing in the

120
00:24:42,820 --> 00:24:50,240
Python version okay and the second
question is when we have a new movie or
a new user to be a 30 trainer model that

121
00:24:50,240 --> 00:24:56,365
is a really good question and there
isn't a straightforward answer to that
time permitting will come back but

122
00:24:56,465 --> 00:25:11,000
basically you would need to have like a
kind of a new user model or a new movie
model that you would use initially and
then over time yes you would then have
to retrain the model so like I don't

123
00:25:11,000 --> 00:25:28,110
know if they still do it but Netflix
used to have this thing that when you
were first on boarded on Netflix it
would say like what movies do you like
and you'd have to go through and let's
say a bunch of movies you like and it
would then my train is moral just find

124
00:25:28,210 --> 00:25:38,895
the nearest movie yeah you could use
nearest neighbors for sure but the thing

125
00:25:38,995 --> 00:25:50,685
is initially at least in this case we
have no columns to describe a movie so
if you had something about like the
movies genre release date who was in it
or something you could have some kind of

126
00:25:50,785 --> 00:26:05,235
non collaborative filtering model and
that's kind of what I meant a new movie
model you have to have some some kind of
predictors okay so a lot of this is

127
00:26:07,635 --> 00:26:17,010
going to look familiar and and the way
I'm going to do this is again it's kind
of this top-down approach we're going to
start using a few features of Pi torch

128
00:26:17,110 --> 00:26:26,610
and fast AI and gradually we're going to
redo it
a few times in a few different ways kind
of doing a little bit deeper each time

129
00:26:26,710 --> 00:26:39,860
um regardless we do need a validation
set
so we can use our standard
cross-validation indexes approach to
grab a random set of ID's this is

130
00:26:39,860 --> 00:26:47,050
something called weight decay which
we'll talk about later in the course for
those of you that have done some machine
learning it's l2 regularization

131
00:26:47,050 --> 00:26:53,742
basically and this is where we choose
how big a embedding matrix do we want
okay

132
00:26:53,842 --> 00:27:06,375
so again you know here's where we get
our model data object from CSV passing
in that ratings file which remember

133
00:27:06,475 --> 00:27:28,425
looks like that okay so you'll see like
stuff tends to look pretty familiar
after a while and then you just have to
pass in the what are your rows
effectively what are your columns
effectively and what are your values
effectively alright so any any

134
00:27:28,525 --> 00:27:33,390
collaborative filtering recommendation
system approach there's basically a

135
00:27:33,490 --> 00:27:40,660
concept of like you know a user and an
item now they might not be users and
items like if you're doing the

136
00:27:40,660 --> 00:27:53,530
Ecuadorian groceries competition there
are stores and items and you're trying
to predict how many things are you going
to sell at this store of this type but

137
00:27:53,530 --> 00:28:09,045
generally speaking just this idea of
like you've got a couple of kind of high
cardinality categorical variables and
something that you're measuring and
you're kind of conceptualizing and
saying okay we could predict the rating
we can predict the value by doing this

138
00:28:09,145 --> 00:28:26,800
this dot for that interestingly this is
kind of relevant to that that last
question or suggestion an identical way
to think about this what I've expressed
this is to say when we're deciding
whether user 72 will like movie

139
00:28:26,900 --> 00:28:43,635
twenty-seven it's basically saying which
other users liked movies that 72 liked
and which other movies were liked by
people like you

140
00:28:43,735 --> 00:28:56,475
user 72 it turns out that these are
basically two ways of saying the exact
same thing so basically what
collaborative filtering is doing you
know kind of conceptually is to say okay

141
00:28:56,575 --> 00:29:11,670
this movie and this user which other
movies are similar to it in terms of
like similar people enjoyed them and
which people are similar to this person
based on people that like the same kind
of movies so that's kind of the

142
00:29:11,770 --> 00:29:21,240
underlying structure at any time there's
an underlying structure like this that
kind of collaborative filtering approach
is likely to be useful okay so so you

143
00:29:21,340 --> 00:29:28,460
yeah so there's basically two parts the
two bits of your thing that you're
factoring and then the the value of the

144
00:29:28,460 --> 00:29:34,310
dependent variable so as per usual we
can take our model data and ask for a

145
00:29:34,310 --> 00:29:48,285
learner from it and we need to tell it
what size of any matrix to use how many
sorry what validation set index is to
use what batch size to use and what
optimizer to use and we're going to be

146
00:29:48,385 --> 00:29:52,842
talking more about optimizes surely we
want to Adam today Adam next week or the

147
00:29:52,942 --> 00:30:03,210
week after and then we can go ahead and
say fit alright and it all looks pretty
similar interest is usually

148
00:30:03,310 --> 00:30:10,905
interestingly I only had to do three
pops like this kind of model seem to
Train
super quickly you can use the learning

149
00:30:11,005 --> 00:30:15,480
rate finder as per usual all the stuff
you're familiar with will work fine and

150
00:30:15,580 --> 00:30:24,590
that was it so this talk you know about
two seconds the Train there's no free
trained anything's here this is from
random from scratch okay so this is our

151
00:30:24,690 --> 00:30:34,335
validation set and we can compare it we
have this is a mean squared error not a
root mean squared error so we can take a

152
00:30:34,435 --> 00:30:58,880
square root so with that last time I ran
it was point seven seven six and that's
0.88 and there's some benchmarks
available for this data set and when I
scrolled through and found the bench the
best benchmark I could find here from
this recommendation system specific
library they had point nine one so we've
got a better loss in two seconds

153
00:30:58,880 --> 00:31:11,800
already so that's good so that's
basically how you can do collaborative
filtering with the faster I library
without thinking too much but so now

154
00:31:11,900 --> 00:31:23,470
we're going to dig in and try and
rebuild that we'll try and get to the
point that we're getting something
around 0.7 seven point seven eight from
scratch but if you want to do this

155
00:31:23,570 --> 00:31:32,935
yourself at home
you know without worry about the detail
that's you know those three lines of
code here's what you need okay so we can

156
00:31:33,035 --> 00:31:38,095
get the predictions in the usual way and
you know we could for example plot SNS

157
00:31:38,195 --> 00:31:48,960
is Seabourn see one's a really great
flooding library it sits on top of
matplotlib it actually leverages
matplotlib so anything you learn about
matplotlib will help you with SIBO and

158
00:31:48,960 --> 00:31:54,230
it's got a few like nice little plots
like this joint plot here is I'm doing

159
00:31:54,230 --> 00:32:00,120
predictions against against actuals so
these are my actual season my
predictions and you can kind of see the

160
00:32:02,340 --> 00:32:07,192
the shape here is that as we predict
higher numbers they actually are higher
numbers and you can also see the

161
00:32:07,292 --> 00:32:18,420
histogram of the predictions and a
histogram of the ashes so that's kind of
floating that is to show you another
interesting visualization would you

162
00:32:18,420 --> 00:32:23,615
please explain the n factors why it's
set to 50 it's set to 50 because I tried

163
00:32:23,915 --> 00:32:29,352
a few things in the world it's the
dimensionality of the embedding images

164
00:32:29,452 --> 00:32:39,860
or to think for it another way it's like
how you know rather than five it's fit

165
00:32:41,750 --> 00:32:54,715
Jeremy I have a question about suppose
that your recommendation system is more
implicit so you have zeros or ones
instead of just actual numbers right so

166
00:32:54,815 --> 00:33:02,575
basically we would then need to use a
classifier instead of regresa

167
00:33:02,675 --> 00:33:11,650
I have to sample the negative or
something like that so if you don't have
it which is up once let's say like just
kind of implicit feedback

168
00:33:11,750 --> 00:33:15,385
oh I'm not sure we'll get to that one in
this class but what I will say is like

169
00:33:15,485 --> 00:33:21,600
in the case that you just doing
classification rather than regression we
haven't actually built that in the

170
00:33:21,600 --> 00:33:26,315
library yet maybe somebody this week
that wants to try adding it it would
only be a small number of lines of code

171
00:33:26,415 --> 00:33:44,850
you basically have to change the
activation function to be a sigmoid and
you would have to change the criterion
or the loss function to be cross-entropy
rather than rmse and that will give you
a classifier rather than a regresar how

172
00:33:44,850 --> 00:33:50,940
those are the only things you'll have to
change so hopefully somebody this week
won't take up that challenge and by the
time we come back next week we've all

173
00:33:50,940 --> 00:34:04,800
have that working ok so I said that
we're basically doing a dot product
right or you know a dot product is kind
of the vector version I guess of this

174
00:34:04,800 --> 00:34:13,209
matrix product so we're basically doing
each of these things times each of these
things and then add it together so let's

175
00:34:13,309 --> 00:34:22,110
just have a look at how we do that in
Python so we can create a tensor in pi
torch just using this little capital T

176
00:34:22,110 --> 00:34:34,434
thing you can just say that's the first
day I version the full version is torch
dot from I'm pie or something but I've
got to set up so you can possibly pass
in even a list of lists so this is going

177
00:34:34,534 --> 00:34:46,090
to create a torch tensor with one two
three four and then here's a torch
tensor with two to ten ten ok so here
are two more chances

178
00:34:46,190 --> 00:34:52,150
I didn't say doc CUDA so they're not on
the GPU they're sitting on the CPU just

179
00:34:52,250 --> 00:35:19,590
FYI we can multiply them together right
and so anytime you have a mathematical
operator between tensors in numpy or
pipe torch it will do element wise
assuming that they're the same
dimensionality which they are they're
both to about two okay and so here we've
got 2 by 2 is 4 3 by 10 is 30 and so
forth ok so there's a a times B so if

180
00:35:19,590 --> 00:36:10,285
you think about basically what we want
to do here is we want to take
ok so I've got 1 times 2 is 2 2 times 2
is 4 2 plus 4 is 6 and so that is
actually the dot product between 1 2 & 2
4 and then here we've got 3 by 10 is 34
by 40 sorry 4 by 10 is 40 30 and 40 and
70 so in other words a times B dot some
along the first dimension
so that's summing up the columns in
other words across a row okay this thing
here is doing the dot product of each of
these rows with each of these rows so it

181
00:36:10,385 --> 00:36:25,220
makes sense and obviously we could do
that with you know some kind of matrix
multiplication approach but I'm trying
to really do things with this little
special case stuff as possible ok so
that's what we're going to use for our
dot products from now on so basically

182
00:36:28,080 --> 00:36:44,455
all we need to do now is remember we
have the data we have is not in that
crosstab format so in excel we've got it
in this crosstab format but we've got it
here in this listed format here's our
movie rating user movie revenue so

183
00:36:44,555 --> 00:36:50,962
conceptually we want to be like looking
up this user into our embedding matrix
to find their 50 factors looking up that

184
00:36:51,062 --> 00:37:02,680
movie to find their 50 factors and then
take the dot product of those two 50
long vectors so let's do that to do it

185
00:37:02,780 --> 00:37:21,120
we're going to build a layer our own
custom neural net layer that's not right
so the the the more generic vocabulary
we call this is we're going to build a
high torch module okay so a PI torch

186
00:37:21,120 --> 00:37:32,710
module is a very specific thing it's
something that you can use as a layer
and a neural net once you've created
your own height watch module you can
throw it into a mirror on it and a

187
00:37:32,810 --> 00:37:41,470
module works by assuming we've already
got once a cordon
model you can pass in some things in
parentheses and it will calculate it

188
00:37:41,570 --> 00:37:55,405
right so assuming that we already have a
modular product we can instantiate it
like so to create our product object and
we can basically now treat that like a

189
00:37:55,505 --> 00:38:08,369
function right but the thing is it's not
just a function because we'll be able to
do things like take derivatives of it
stack them up together into a big stack
of neural network layers blah blah blah

190
00:38:08,369 --> 00:38:14,730
so it's basically a function that we can
kind of compose very conveniently so

191
00:38:14,730 --> 00:38:23,260
here how do we define a module which as
you can see here returns a dot product
well we have to create a Python class

192
00:38:23,360 --> 00:38:31,410
and so if you haven't done - oo before
you're going to have to learn because
all my torch modules are written in
Python oo and that's one of the things I

193
00:38:33,690 --> 00:38:46,000
really like about PI torch is that it
doesn't reinvent totally new ways of
doing things by tensorflow does all the
time in pi torch that you know really
tend to use pythonic ways to do things

194
00:38:46,100 --> 00:38:55,770
so in this case how do you create you
know some kind of new behavior you
create a Python plus it's so Jeremy

195
00:38:55,770 --> 00:39:07,020
suppose that you have a lot of data not
just a little bit of data you can have
in memory will you be able to use fossae
I to solve glory filtering yes

196
00:39:07,020 --> 00:39:33,560
absolutely
it's it uses mini-batch stochastic
gradient descent which does have a batch
at a time the this particular version is
going to create a panda's data frame and
panda's data frame has to live in memory

197
00:39:33,560 --> 00:39:40,920
having said that you can get easily 512
gig you know instances on Amazon so like

198
00:39:40,920 --> 00:39:46,200
if you had a CSV that was bigger than
512 gig you know that would be
impressive if that did happen I guess

199
00:39:48,510 --> 00:39:52,769
you would have to instead
save that as a be calls array and create
a slightly different version that reads

200
00:39:54,210 --> 00:40:00,359
from a because array just streaming in
or maybe from a desk data frame which

201
00:40:00,359 --> 00:40:03,519
also so it would be easy to do I don't

202
00:40:03,619 --> 00:40:15,549
think I've seen real-world situations
where you have 512 gigabyte
collaborative filtering matrices but
yeah we can do it okay now this is PI

203
00:40:15,649 --> 00:40:32,640
torch specific this next bit is that
when you define like the actual work to
be done which is here return user times
movie dot some you have to put it in a
special method called forward okay and

204
00:40:32,640 --> 00:40:37,109
this is this idea that like it's very
likely you're on that right in a neural

205
00:40:37,109 --> 00:40:43,019
net the thing where you calculate the
next set of activations is called the

206
00:40:43,019 --> 00:40:48,180
the forward pass and so that's doing a
forward calculation the gradients is

207
00:40:48,180 --> 00:40:51,989
called the backward calculation we don't
have to do that because PI torch

208
00:40:51,989 --> 00:40:54,369
calculates that automatically so we just

209
00:40:54,469 --> 00:41:01,529
have to define forward so we create a
new class we define forward and here we
write in our definition of dot product

210
00:41:01,529 --> 00:41:13,979
ok so that's it so now that we've
created this class definition we can
instantiate our model right and we can
call our model and get back the numbers
be expected okay so that's it that's how

211
00:41:16,529 --> 00:41:29,811
we create a custom PI torch layer and if
you compare that to like any other
library around pretty much this is way
easier basically I guess because we're
leveraging what's already in person so

212
00:41:29,911 --> 00:41:34,499
let's go ahead and now create a more
complex module and we're going to

213
00:41:38,039 --> 00:41:49,670
basically do the same thing we've got to
have a forward again we're going to have
our users x movies dot sum but we're
going to do one more thing before hand
which is we're going to create two

214
00:41:49,670 --> 00:42:00,254
embedding matrices and then we're going
to look up our users and our movies in
those inventing matrices so let's go
through and and do that so the first

215
00:42:00,354 --> 00:42:07,054
thing to realize is
that the uses the user IDs and the movie
IDs may not be contiguous you know like

216
00:42:07,154 --> 00:42:16,864
they're maybe they start at a million
and go to a million in 1000 so right so

217
00:42:16,964 --> 00:42:25,952
if we just used those IDs directly to
look up into an embedding matrix we
would have to create an embedding matrix
of size 1 million 1000 right which we

218
00:42:26,052 --> 00:42:38,210
don't want to do so the first thing I do
is to get a list of the unique user IDs
and then I create a mapping from every
user ID to a contiguous integer this

219
00:42:38,310 --> 00:43:03,640
thing I've done here where I've created
a dictionary which maps from every
unique thing to a unique index is well
worth studying
during the week because like it's is
super super handy it's something you
very very often have to do in all kinds
of machine learning all right and so I
won't go through it here it's easy
enough to figure out if you can't figure
it out just ask on the forum anyway so

220
00:43:06,729 --> 00:43:18,439
once we've got the mapping from user to
a contiguous index we then can say let's
now replace the user ID column with that

221
00:43:18,539 --> 00:43:31,639
contiguous index right so pandas dot
apply applies an arbitrary function and
python lambda is how you create an
anonymous function on the fly and this
anonymous function simply returns the NS

222
00:43:31,739 --> 00:43:36,039
through the same thing for movies and so

223
00:43:36,039 --> 00:43:41,319
after that we now have the same ratings
table we had before but our IDs have
been mapped to contiguous integers

224
00:43:43,690 --> 00:43:49,239
therefore they're things that we can
look up into an embedding matrix so

225
00:43:49,239 --> 00:43:54,819
let's get the count of our users in our
movies and let's now go ahead and try

226
00:43:54,819 --> 00:44:00,609
and create our Python version of this
okay

227
00:44:00,609 --> 00:44:21,760
so earlier on when we created our
simplest possible PI torch module
there was no like state we didn't need a
constructor because we weren't like
saying how many users are there or how
many movies are there or how many
factors do we want or whatever right

228
00:44:21,760 --> 00:44:37,850
anytime we want to do something like
this where we're passing in and saying
we want to construct our module with
this number of users and this number of
movies then we need a constructor for

229
00:44:37,850 --> 00:44:50,550
our class and you create a constructor
in Python by defining a dunder init
underscore underscore init underscore
underscore yet special name
so this just creates a constructor and

230
00:44:50,650 --> 00:45:01,930
if you haven't done over before you
wanted to do some study during the week
but it's pretty simple idea this is just
the thing that when we create this
object this is what gets wrong okay

231
00:45:02,030 --> 00:45:16,920
again special python thing when you
create your own constructor you have to
call the parent class constructor and if
you want to have all of the cool
behavior of a PI porch module you get
that by inheriting from an end module

232
00:45:17,020 --> 00:45:27,685
neural net module okay so basically by
inheriting here and calling the
superclass constructor we now have a
fully functioning PI torch layer okay so

233
00:45:27,785 --> 00:45:41,870
now we have to give it some behavior and
so we give us some behavior by storing
some things in it all right so here
we're going to create something called
self dot you users and that is going to

234
00:45:41,870 --> 00:45:55,280
be an embedding layer a number of rows
is an user's number of columns is in
factors so that is exactly this right
the number of rows is M users number of

235
00:45:55,280 --> 00:46:01,880
columns is inventors and then we'll have
to do the same thing for movies okay so

236
00:46:01,880 --> 00:46:09,910
that's going to go ahead and create
these two randomly initialized arrays
however when you randomly initialize

237
00:46:12,920 --> 00:46:33,665
over an array it's important to randomly
initialize it to a reasonable set of
numbers like a reasonable scale right if
we randomly initialize them from like
naught to a million then we would start
out and you know these things would
start out being like you know billions
and billions of
writing and that's going to be very hard
to do gradient descent on so I just kind

238
00:46:33,765 --> 00:46:51,020
of manually figured here like okay about
what size numbers are going to give me
about the right readiness and so we
don't we know we did ratings between
about normal five so if we start out
with stuff between about naught and 0.05

239
00:46:51,020 --> 00:47:13,960
then we're going to get ratings of about
the right level you can easily enough
like that calculate that in in neural
nets there are standard algorithms for
basically doing doing that calculation
and the basic the key algorithm is
something called initialization from

240
00:47:14,060 --> 00:47:34,765
climbing her and the basic idea is that
you take the yeah you basically set the
weights equal to a normal distribution
with a standard deviation which is

241
00:47:34,865 --> 00:47:48,950
basically inversely proportional to the
number of things in the previous layer
and so in our previous layer so in this

242
00:47:48,950 --> 00:48:04,940
case we basically if you basically take
that nor to 0.05 and multiply it by the
fact that you've got 40 things with a 40
or 50 things coming out of it
50 50 things coming out of it and then
you're going to get something about the

243
00:48:04,940 --> 00:48:28,430
right size pi torch has already has like
her initialization class they're like we
don't in normally in real life have to
think about this we can just call the
existing initialization functions but
we're trying to do this all like from
scratch here okay without any special
stuff going on so there's quite a bit of

244
00:48:28,430 --> 00:48:53,774
pi torch notation here so self dot u
we've already set to an instance of the
embedding class it has a dot weight
attribute which contains the actual the
actual embed images
so that contains this the actual
embedding matrix is not a tensor it's a

245
00:48:53,874 --> 00:49:12,284
variable a variable is exactly the same
as a tensor in other words it supports
the exact same operations as a tensor
but it also does automatic
differentiation that's all a variable is
basically to pull the tensor out of a
variable you get its data attribute okay

246
00:49:12,384 --> 00:49:22,484
so this is so this is now the tensor of
the weight matrix of the self dot you're

247
00:49:22,584 --> 00:49:33,419
inventing and then something that's
really handy to know is that all of the
tensor functions in pi torch you can
stick an underscore at the end and that

248
00:49:33,519 --> 00:49:48,344
means do it in place all right so this
is say create a random uniform random
number of an appropriate size for this
tensor and don't return it but actually
fill in that matrix unless okay so

249
00:49:48,444 --> 00:50:12,869
that's a super handy thing to know about
I mean it wouldn't be rocket science
otherwise we would have to have gone
[Music]
okay here's the non in-place version
that's what saves us some typing saves
us some screen noise that's all okay so

250
00:50:12,969 --> 00:50:21,449
now we've got our randomly initialized
embedding weight matrices and so now the

251
00:50:21,549 --> 00:50:31,069
forward I'm actually going to use the
same columnar model data that we used
for Russman and so it's actually going

252
00:50:31,069 --> 00:50:38,579
to be passed both categorical variables
and continuous variables and in this
case there are no continuous variables

253
00:50:38,679 --> 00:50:48,329
so I'm just going to grab the 0th column
out of the categorical variables and
call it users and the first column and
call it movies okay so I'm just kind of

254
00:50:48,429 --> 00:50:58,264
too lazy to create my own I've lots to
do about too lazy out that we do have a
special class with this but I'm trying
to avoid creating a special class so
just going to leverage this columnar

255
00:50:58,364 --> 00:51:21,220
model data plus okay so we can basically
grab our user and movies mini-batches
right and remember this is not a single
user in a single movie this is going to
be a whole mini batch of them we can now
look up that mini batch of users in our
embedding matrix U and the movies in are
embedding matrix okay so this is like

256
00:51:21,320 --> 00:51:31,359
exactly the same is just doing an array
lookup to grab the user ID numbered
value but we're doing that a whole mini
batch at a time right and so it's

257
00:51:31,459 --> 00:51:43,449
because PI torch can do a whole mini
batch at a time with pretty much
everything that we can get really easy
speed up we don't have to write any
loops on the whole to do everything
through our mini batch and in fact if

258
00:51:43,549 --> 00:51:52,509
you do ever loop through your mini batch
manually you don't get GPU acceleration
that's really important to know right so
you never want to loop have a for loop

259
00:51:52,609 --> 00:51:59,339
going through your mini batch you always
want to do things in this kind of like
whole mini batch at a time but pretty

260
00:51:59,339 --> 00:52:05,604
much everything imply torch does things
are holding events at a time so you
shouldn't have to worry about it

261
00:52:05,704 --> 00:52:13,239
and then here's our product just like
before right so having to find that I'm

262
00:52:13,339 --> 00:52:27,684
now going to go ahead and say alright my
X values is everything except the rating
and the timestamp in my writings table
my Y is my rating and then I can just

263
00:52:27,784 --> 00:52:39,546
say okay let's grab a model data from a
data frame using that X and that Y and
here is our list of categorical
variables okay and then so let's now

264
00:52:39,646 --> 00:52:47,519
instantiate that PI torch object alright
so we've now created that from scratch

265
00:52:49,549 --> 00:52:55,469
and then the next thing we need to do is
to create an optimizer so this is part

266
00:52:55,469 --> 00:53:02,640
of pi torch the only fast AI thing here
is this line right because that's like I

267
00:53:02,640 --> 00:53:15,630
don't think showing you how to build
data sets and data load is interesting
enough really we might do that in part
two of the course and
it's actually so straightforward like a
lot of you are already doing it on the
forums so I'm not going to show you that

268
00:53:17,220 --> 00:53:33,610
in this part but if you're interested
feel free to talk on the forums about it
but I'm just going to basically take the
thing that feeds us data is a given
particularly cuz these things are so
flexible right you know if you've got
stuff enough data frame you can just use
this you don't have to rewrite it so

269
00:53:33,710 --> 00:53:39,130
that's the only fast AI thing we're
using so this is a PI torch thing and so

270
00:53:39,230 --> 00:53:51,579
optiom is the thing and pi torch that
gives us an optimizer will be learning
about that very shortly so it's actually
the thing that's going to update our

271
00:53:51,679 --> 00:53:57,939
weights pi torch calls them the
parameters of the model so earlier on we

272
00:53:58,039 --> 00:54:07,859
set model equals embedding dot blah blah
right and because embedding dot derives
from NN module we get all of the pi
torch module behavior and one of the

273
00:54:10,230 --> 00:54:29,319
things we got for free is the ability to
say got parameters so that's pretty
that's pretty any right that's the thing
that basically is going to automatically
give us a list of all of the weights in
our model that have to be updated and so
that's what gets passed to the optimizer

274
00:54:29,419 --> 00:54:38,721
we also passed the optimized at the
learning rate the weight decay which
we'll talk about later and momentum that
we'll talk about later okay one other

275
00:54:38,821 --> 00:54:43,950
thing that I'm not going to do right now
but we will do later is to write a

276
00:54:45,809 --> 00:54:58,230
training loop so the training loop is a
thing that lives for each mini batch and
updates the weight to subtract the
gradient times the moment there's a

277
00:54:58,230 --> 00:55:02,935
function in fast AI which is the
training loop and it's it's pretty

278
00:55:03,035 --> 00:55:25,099
simple here it is right for a POC in
epochs this is just the thing that shows
a progress bar so ignore this for X
comma Y in my training data loader
calculate the loss

279
00:55:25,490 --> 00:55:39,960
print out the lots you know in a
progress bar call any callbacks you have
and at the end call the call the metrics
on the validation alright so there's

280
00:55:39,960 --> 00:55:55,120
there's just eh
Apoc go through each mini batch and do
one step of optimizer step is basically
going to take advantage of this
optimizer but we'll be writing that from

281
00:55:55,220 --> 00:56:06,745
scratch shortly so this is notice we're
not using a learner okay we're just
using a hi book module so this this fit
thing although it's passed to a part of

282
00:56:06,845 --> 00:56:14,700
fast AI it's like lower down the layers
of abstraction now this is the thing
that takes a regular high torch model so

283
00:56:14,700 --> 00:56:33,355
if you ever want to like skip as much
faster eye stuff as possible like you've
got some high torch model you've got
some code on the internet you basically
want to run it that you don't want to
write your own training loop then this
is this is what you want to do you want
to call fast a high speed version and so

284
00:56:33,455 --> 00:56:41,250
what you'll find is like the library is
designed so that you can kind of dig in
at any layer abstraction you like right

285
00:56:41,250 --> 00:56:51,640
and so at this layer of abstraction
you're not going to get things like
stochastic gradient descent with
restarts you're not going to get like
differential learning rates like all

286
00:56:51,740 --> 00:57:02,100
that stuff that's in the learner like
you could do it but you'd have to write
it all about by hand yourself alright
and that's the downside of kind of going
down to this level of abstraction the

287
00:57:02,100 --> 00:57:13,375
upside is that as you saw the code for
this is very simple it's just a simple
training loop it takes a standard 5
torch model so this is like this is a
good thing for us to use here we can we
just call it and it looks exactly like

288
00:57:13,475 --> 00:57:24,230
what we used to see all right we got our
validation and training loss for the 3 e

289
00:57:24,330 --> 00:57:33,190
bus now you'll notice that we wanted
something around 0.76 so we're not there

290
00:57:33,290 --> 00:57:41,369
so in other words the the the default
fast AI collaborative dory
rhythm is doing something smarter than

291
00:57:41,369 --> 00:57:57,960
this so we're going to try and do that
one thing that we can do since we're
calling our you know this lower level
fifth function there's no learning rate
and kneeling we could do our own
learning rate annealing so you can hear
it see here there's a first day I
function called set learning rates you

292
00:57:57,960 --> 00:58:05,275
can pass in a standard height watch
optimizer and pass in your new learning
rate and then call fit again and so this

293
00:58:05,375 --> 00:58:17,185
is how we can let manually do a learning
rate schedule and so you can see we've
got a little bit better 1.13 where you
still got a long way to go okay so I

294
00:58:17,285 --> 00:58:33,669
think what we might do is we might have
a seven minute break and then we're
going to come back and try and improve
this core of it for those who are

295
00:58:33,769 --> 00:58:39,809
interested somebody was asking me the

296
00:58:39,809 --> 00:58:45,779
break for a kind of a quick walkthrough
so this is totally optional but if you

297
00:58:45,779 --> 00:58:54,960
go into the first day I library there's
a model py file and that's where fit is

298
00:58:54,960 --> 00:59:01,049
which we're just looking at which goes
through each epoch in epochs and then

299
00:59:01,049 --> 00:59:07,470
goes through each x and y in the mini
batch and then it calls this step

300
00:59:07,470 --> 00:59:17,039
function so the step function is here
and you can see the key thing is it

301
00:59:17,039 --> 00:59:20,940
calculates the output from the model the
models for M right and so if you

302
00:59:20,940 --> 00:59:28,500
remember our dot product we didn't
actually call model dot forward we just

303
00:59:28,500 --> 00:59:35,059
called model parentheses and that's
because the N n dot module automatically

304
00:59:35,059 --> 00:59:39,569
you know when you call it as if it's a
function it passes it along to forward
okay so that's that's what that's doing

305
00:59:42,059 --> 00:59:45,960
there right and then the rest of this
world will learn about shortly which is

306
00:59:45,960 --> 00:59:50,550
basically doing the the loss function
and

307
00:59:50,550 --> 00:59:55,740
the backward pass okay so for those who
are interested that's that's kind of
gets you a bit of a sense of how the

308
00:59:57,480 --> 01:00:02,880
cone it's structured if you want to look
at it and as I say like the the faster I

309
01:00:02,880 --> 01:00:09,780
code is designed to both be world-class
performance but also pretty easy to read

310
01:00:09,780 --> 01:00:15,210
so like feel free like take a look at it
and if you want to know what's going on

311
01:00:15,210 --> 01:00:19,230
just ask on the forums and if you you
know if you think is anything that could

312
01:00:19,230 --> 01:00:26,550
be clearer let us know because yeah the
code is definitely now we're going to be

313
01:00:26,550 --> 01:00:33,540
digging into the code or in law okay so
let's try and improve this a little bit

314
01:00:33,540 --> 01:00:40,140
and let's start off by improving it in
Excel so you might have noticed here

315
01:00:40,140 --> 01:00:44,790
that we've kind of got the idea that use
a 72

316
01:00:44,790 --> 01:00:50,970
you know like sci-fi modern movies with
special effects you know whatever and

317
01:00:50,970 --> 01:00:57,150
movie number 27 is sci-fi and that
special effects so much dialogue but

318
01:00:57,150 --> 01:01:05,400
we're missing an important case which is
like user 72 is pretty enthusiastic on

319
01:01:05,400 --> 01:01:10,710
the hall and on average rates things
higher and Highland you know and movie

320
01:01:10,710 --> 01:01:17,040
27 you know it's just a popular movie
you know it's just on average its higher

321
01:01:17,040 --> 01:01:24,300
so what would really like is to add a
constant for the user and a constant for

322
01:01:24,300 --> 01:01:29,640
the movie and remember in neural network
terms we call that a bias that's what we
want to add a bias so we could easily do

323
01:01:32,280 --> 01:01:37,380
that and if we go into the bias tab here
we've got the same data as before and

324
01:01:37,380 --> 01:01:44,420
we've got the same latent factors as
before and I've just got one extra row

325
01:01:44,420 --> 01:01:51,780
here and one extra column here and you
won't be surprised here that we now take

326
01:01:51,780 --> 01:01:58,400
these same matrix multiplication as
before and we add in that and we add in

327
01:01:58,400 --> 01:02:05,559
that okay so that's
bias so other than that we've got

328
01:02:05,559 --> 01:02:11,200
exactly the same loss function over here
and so just like before we can now go
ahead and solve that and now our

329
01:02:14,650 --> 01:02:20,980
changing variables include the bias and
we can say solve and if we leave that

330
01:02:20,980 --> 01:02:27,630
for a little while it will come to a
better result than we had before

331
01:02:27,630 --> 01:02:32,579
okay so that's the first thing we're
going to do to improve our model and

332
01:02:32,579 --> 01:02:41,559
there's really very little show just to
make the code a bit shorter I have to

333
01:02:41,559 --> 01:02:46,900
find a function called get embedding
which takes a number of inputs and a

334
01:02:46,900 --> 01:02:50,470
number of factors so the number of rows
and the embedding matrix Nomos they're
both medications creates the embedding

335
01:02:52,450 --> 01:02:59,020
and then randomly initializes it I don't
know why I'm doing negative to positive

336
01:02:59,020 --> 01:03:02,230
here and it zeroed last time honestly it
doesn't matter much as long as it's in

337
01:03:02,230 --> 01:03:09,400
the right ballpark and then we return
that initialized emitting so now we need

338
01:03:09,400 --> 01:03:14,710
not just our users by factors which are
Chuck into you our movies by factors

339
01:03:14,710 --> 01:03:20,440
which I've shocked into M but we also
need users by one which will put into UV

340
01:03:20,440 --> 01:03:25,900
user bias and movies by one which will
put into the movie bias okay so this is

341
01:03:25,900 --> 01:03:29,890
just doing a list comprehension going
through each of the tuples create an

342
01:03:29,890 --> 01:03:34,630
embedding for each of them and putting
them into these things okay so now our

343
01:03:34,630 --> 01:03:44,260
forward is exactly the same as before u
times M sub I mean this is actually a

344
01:03:44,260 --> 01:03:50,289
little confusing because we're doing it
into two steps maybe they make it a bit

345
01:03:50,289 --> 01:03:58,380
easier let's pull this out
put it up here put this in parentheses

346
01:03:58,440 --> 01:04:02,770
okay so maybe that looks a little bit
more familiar all right you times n dot

347
01:04:02,770 --> 01:04:07,150
some that's the same dot product and
then here it is going to add in our user

348
01:04:07,150 --> 01:04:14,260
pious and
our movie bus dot squeeze is the PI

349
01:04:14,260 --> 01:04:21,370
torch thing that adds an additional unit
axis that's not going to make any sense
if you haven't done broadcasting before

350
01:04:23,340 --> 01:04:27,640
I'm not going to do a broadcasting in
this course because we've already done
it

351
01:04:28,030 --> 01:04:31,750
and we're doing it in the machine
learning course but basically in in

352
01:04:31,750 --> 01:04:36,760
short broadcasting is what happens when
you do something like this where um is a

353
01:04:36,760 --> 01:04:44,740
matrix you be self-taught you the users
is a is a vector how do you add a vector
to a matrix and basically what it does

354
01:04:46,900 --> 01:04:53,800
is it duplicates the vector so that it
makes it the same size as the matrix and

355
01:04:53,800 --> 01:04:58,000
the particular way whether it duplicates
it across columns or down rows or how it

356
01:04:58,000 --> 01:05:03,300
does it is called broadcasting the
broadcasting rules are the same as numpy

357
01:05:03,300 --> 01:05:07,360
Pytor didn't actually used to support
broadcasting so I was actually the guy

358
01:05:07,360 --> 01:05:11,920
who first added broadcasting to PI torch
using an ugly hack and then the pipe or

359
01:05:11,920 --> 01:05:17,110
authors did an awesome job of supporting
it actually inside the language so now

360
01:05:17,110 --> 01:05:22,180
you can use the same broadcasting
operations in five torches non-player if
you haven't dealt with this before it's

361
01:05:24,640 --> 01:05:29,470
really important to learn it because
like it's it's kind of the most

362
01:05:29,470 --> 01:05:34,180
important fundamental way to do
computations quickly in the high-end

363
01:05:34,180 --> 01:05:37,390
paid warship it's the thing that lets
you not have to do loops

364
01:05:37,390 --> 01:05:40,510
how could you imagine here if I had to
look through every row of this matrix

365
01:05:40,510 --> 01:05:46,870
and add each did you know this vector to
every row it would be slow the you know
a lot more code and the idea of

366
01:05:50,590 --> 01:05:54,610
broadcasting it actually goes all the
way back to APL which was a language

367
01:05:54,610 --> 01:05:59,770
designed in the 50s by an extraordinary
guy called Ken Iverson yeah APL was

368
01:05:59,770 --> 01:06:05,110
originally designed or written out as a
new type of mathematical notation he has

369
01:06:05,110 --> 01:06:10,330
this great essay called notation as a
tool for thought and the idea was that

370
01:06:10,330 --> 01:06:15,220
like really good notation could actually
make you think of better things and part

371
01:06:15,220 --> 01:06:20,110
of that notation is this idea of
broadcasting I'm incredibly enthusiastic

372
01:06:20,110 --> 01:06:25,630
about it
and we're gonna use it plenty so either

373
01:06:25,630 --> 01:06:33,880
watch the machine learning lesson or you
know google numpy broadcasting for

374
01:06:33,880 --> 01:06:37,990
information anyway
so basically it works reasonably
intuitively we can add on we can add the

375
01:06:40,860 --> 01:06:49,630
vectors to the matrix all right
having done that we're now going to do

376
01:06:49,630 --> 01:06:56,220
one more trick which is I think it was
your net asked earlier about could we
squish the ratings to be between one and

377
01:06:59,890 --> 01:07:09,100
five and the answer is we could right
and specifically what we could do is we

378
01:07:09,100 --> 01:07:14,110
could put it through a sigmoid function
all right

379
01:07:14,110 --> 01:07:24,900
so remind you a sigmoid function looks
like that right and this is that's one

380
01:07:24,900 --> 01:07:29,500
okay we could put it through a secret
function so we could take like four

381
01:07:29,500 --> 01:07:33,520
point nine six and put it through a
sigmoid function and like that you know

382
01:07:33,520 --> 01:07:37,450
that's kind of high so it kind of be
over here somewhere right

383
01:07:37,450 --> 01:07:44,800
and then we could multiply that sigmoid
like the result of that by five for

384
01:07:44,800 --> 01:07:47,020
example all right
and in this case we want it to be

385
01:07:47,020 --> 01:07:51,610
between one and five right so maybe we
would multiply it by four and add one

386
01:07:51,610 --> 01:08:00,100
instance that's the basic idea and so
here is that trick we take the result so

387
01:08:00,100 --> 01:08:03,940
the result is basically the the thing
that comes straight out of the dot

388
01:08:03,940 --> 01:08:08,710
product plus the addition of the biases
and put it through a sigmoid function

389
01:08:08,710 --> 01:08:16,450
now in pi torch basically all of the
functions you can do to tensors are

390
01:08:16,450 --> 01:08:21,430
available inside this thing called
capital F and this is like totally

391
01:08:21,430 --> 01:08:26,470
standard in pi torch it's actually
called torch and or functional but

392
01:08:26,470 --> 01:08:30,580
everybody including all of the pipe
torch Doc's import torch start and end

393
01:08:30,580 --> 01:08:36,810
are functional as capital F all right so
capital F dot sigmoid means a function

394
01:08:36,810 --> 01:08:42,989
called sigmoid that is coming from
tortures functional module right and so
that's going to apply a sigmoid function

395
01:08:44,729 --> 01:08:48,388
for the result so I squish them all
between zero and one using that nice

396
01:08:48,389 --> 01:08:54,989
little shape and then I can multiply
that by five minus one plus four right
and then add on one and that's gonna

397
01:08:57,149 --> 01:09:03,119
give me plumbing between one and five
okay so like there's no need to do this

398
01:09:03,120 --> 01:09:08,429
I could comment it out and it'll still
work right but now it has to come up

399
01:09:08,429 --> 01:09:13,290
with a set of calculations that are
always between one and five right where

400
01:09:13,290 --> 01:09:16,710
else if I leave this in then it's like
makes it really easy it's basically like

401
01:09:16,710 --> 01:09:20,100
oh if you think this is a really good
movie just calculate a really high

402
01:09:20,100 --> 01:09:24,569
number it's a really crappy movies low
number and I'll make sure it's in the

403
01:09:24,569 --> 01:09:28,440
right regions so even though this is a
neural network it's still a good example
of this kind of like if you're doing any

404
01:09:30,689 --> 01:09:35,399
kind of parameter fitting try and make
it so that the thing that you want your

405
01:09:35,399 --> 01:09:40,619
function to return it's like it's easy
for it to return that okay so that's why

406
01:09:40,620 --> 01:09:48,089
we do that that function squishing so we
call this embedding dot bias so we can

407
01:09:48,089 --> 01:09:53,190
create that in the same way as before
you'll see here I'm calling dr. to put

408
01:09:53,189 --> 01:09:56,790
it on the GPU because we're not using
any learner stuff normally it'll all

409
01:09:56,790 --> 01:10:01,350
happen for you but we have to manually
say put it on the GPU this is the same

410
01:10:01,350 --> 01:10:05,580
as before create our optimizer fit
exactly the same as before and these

411
01:10:05,580 --> 01:10:11,520
numbers are looking good all right and
again we'll do a little change to our

412
01:10:11,520 --> 01:10:16,050
learning rate learning rate schedule and
we're down to 0.8 so we're actually

413
01:10:16,050 --> 01:10:31,590
pretty close pretty close so that's the
key steps and this is how this is how

414
01:10:31,590 --> 01:10:38,280
most collaborative filtering is done and
unit reminded me of an important point

415
01:10:38,280 --> 01:10:46,139
which is that this is not strictly
speaking a matrix factorization because

416
01:10:46,139 --> 01:10:49,290
strictly
a matrix factorization would take that

417
01:10:49,290 --> 01:11:03,150
matrix by that matrix to create this
matrix and remembering anywhere that

418
01:11:03,150 --> 01:11:11,730
this is empty like here or here we're
putting in a zero right we're saying if

419
01:11:11,730 --> 01:11:18,090
the original was empty put in a zero
right now normally you can't do that

420
01:11:18,090 --> 01:11:22,290
with normal matrix factorization normal
matrix factorization it creates the

421
01:11:22,290 --> 01:11:27,450
whole matrix and so it was a real
problem actually when people used to try

422
01:11:27,450 --> 01:11:31,950
and use traditional linear algebra for
this because when you have these sparse

423
01:11:31,950 --> 01:11:36,780
matrices like in practice
this matrix is not doesn't have many

424
01:11:36,780 --> 01:11:40,500
gaps because we picked the users that
watch the most movies and the movies

425
01:11:40,500 --> 01:11:43,650
that are the most watched but if you
look at the whole matrix it's it's

426
01:11:43,650 --> 01:11:49,980
mainly empty and so traditional
techniques treated empty is zero and so

427
01:11:49,980 --> 01:11:54,120
like you basically have to predict a
zero as if the fact that I haven't

428
01:11:54,120 --> 01:11:58,950
watched a movie means I don't like the
movie that's gives terrible answers so

429
01:11:58,950 --> 01:12:05,580
this probabilistic matrix factorization
approach takes advantage of the fact

430
01:12:05,580 --> 01:12:11,220
that our data structure actually looks
like this rather than that cross tab

431
01:12:11,220 --> 01:12:16,110
right and so it's only calculating the
loss for the user ID movie ID
combinations that actually appear that's

432
01:12:18,240 --> 01:12:22,860
its if you like use red a1 movie I think
102 9 should be 3 it's actually three

433
01:12:22,860 --> 01:12:27,300
and a half sauce is 0.5 like there's
nothing here that's ever going to

434
01:12:27,300 --> 01:12:32,850
calculate a prediction or a loss for a
user movie combination that doesn't

435
01:12:32,850 --> 01:12:38,130
appear in this table by definition the
only stuff that we can appear in a mini

436
01:12:38,130 --> 01:12:46,620
batch is what's in this table okay and
like a lot of this happened

437
01:12:46,620 --> 01:12:50,760
interestingly enough actually in the
Netflix price so before the Netflix

438
01:12:50,760 --> 01:12:56,340
prize came along there's probabilistic
matrix factorization it had actually

439
01:12:56,340 --> 01:13:00,640
already been invented but nobody noticed
all right and then in the

440
01:13:00,640 --> 01:13:04,540
first year of the Netflix price someone
wrote this like really really famous

441
01:13:04,540 --> 01:13:08,710
blog post where they basically said like
hey check this out

442
01:13:08,710 --> 01:13:12,640
incredibly simple technique works
incredibly well when suddenly all the

443
01:13:12,640 --> 01:13:19,180
net fix leaderboard entries and so
that's quite a few years ago now and

444
01:13:19,180 --> 01:13:23,920
this is like now every collaborative
filtering approach does this not every

445
01:13:23,920 --> 01:13:27,640
collaborative filtering approach adds
this sigmoid thing by the way it's not

446
01:13:27,640 --> 01:13:32,800
like rocket science this is this is not
like the NLP thing we saw last week

447
01:13:32,800 --> 01:13:35,770
which is like hey this is a new
state-of-the-art like this is you know

448
01:13:35,770 --> 01:13:39,640
not particularly uncommon but there are
still people that don't do this it

449
01:13:39,640 --> 01:13:45,280
definitely helps a lot I have to have
this and so actually you know what we
could do is maybe now's a good time to

450
01:13:47,800 --> 01:13:56,590
have a look at the definition of this
right so the column data module contains

451
01:13:56,590 --> 01:14:03,640
all these definitions and we can now
compare this to the thing we originally

452
01:14:03,640 --> 01:14:09,550
used which was whatever came out of
collaborative data set all right so

453
01:14:09,550 --> 01:14:20,050
let's go to collab filter data set here
it is and we called get learner all

454
01:14:20,050 --> 01:14:27,330
right so we can go down to get Elena and
that created a collab filter learner

455
01:14:27,330 --> 01:14:32,740
passing in the model from get model is
get model so created an embedding bias

456
01:14:32,740 --> 01:14:40,300
and so here is embedding drop bias and
you can see here here it is like it's

457
01:14:40,300 --> 01:14:44,920
the same thing there's the embedding for
each of the things here's our forward

458
01:14:44,920 --> 01:14:52,750
that does the you times I dot some plus
plus sigmoid so in fact we have just

459
01:14:52,750 --> 01:15:00,340
actually rebuilt what's in the past our
library literally okay it's a little

460
01:15:00,340 --> 01:15:04,960
shorter and easier because we're taking
advantage of the fact that there's a

461
01:15:04,960 --> 01:15:11,410
special collaborative filtering data set
so we can actually we're getting past in

462
01:15:11,410 --> 01:15:13,889
the users and the items and we don't
have to pull them out of cat

463
01:15:13,889 --> 01:15:18,420
Kant's but other than that this is
exactly the same so hopefully you can

464
01:15:18,420 --> 01:15:22,710
see like the faster you have ivory is
not some inscrutable code containing

465
01:15:22,710 --> 01:15:26,880
concepts you can never understand we've
actually just built up this entire thing

466
01:15:26,880 --> 01:15:38,429
from scratch ourselves and so why did we
get 0.76 rather than 0.8 you know I I

467
01:15:38,429 --> 01:15:41,940
think it's simply because we used
stochastic gradient descent with
restarts or the cycle multiplier and an

468
01:15:44,460 --> 01:15:51,690
atom optimizer you know like a few
little training chase some I'm looking

469
01:15:51,690 --> 01:15:58,159
at this and thinking that is we could
totally improve this small but maybe

470
01:15:58,159 --> 01:16:02,820
looking at the date and doing some
tricks with the date because this this

471
01:16:02,820 --> 01:16:08,219
is kind of a just a regular kind of
smaller no way yeah you can add more

472
01:16:08,219 --> 01:16:13,260
features yeah exactly exactly so like
now that you've seen this you could now

473
01:16:13,260 --> 01:16:18,179
you know even if you didn't have
embedding dot bias in a notebook that

474
01:16:18,179 --> 01:16:21,600
you've written yourself through some
other model that's in fast AI you could

475
01:16:21,600 --> 01:16:25,889
look at it in faster and be like oh that
does most of the things that I'd want to

476
01:16:25,889 --> 01:16:29,850
do but it doesn't deal with time and so
you could just go oh okay let's grab it

477
01:16:29,850 --> 01:16:36,449
copy it you know pop it into my notebook
and let's create you know the better
version all right and then you can start

478
01:16:38,969 --> 01:16:46,830
playing that and you can now create your
own model class from the open source

479
01:16:46,830 --> 01:16:50,850
code here and so yeah your that's
mentioning a couple things we could do

480
01:16:50,850 --> 01:16:55,560
we could try and incorporate in time
stamp so we could assume that maybe well

481
01:16:55,560 --> 01:17:00,389
maybe there's just like some for a
particular user over time users tend to

482
01:17:00,389 --> 01:17:06,690
get more or less positive about movies
also remember there was the list of
genres for each movie maybe we could

483
01:17:08,940 --> 01:17:13,800
incorporate that so one problem is it's
a little bit difficult to incorporate

484
01:17:13,800 --> 01:17:19,650
that stuff into this embedding bias
model because it's kind of it's pretty
custom right so what we're going to do

485
01:17:21,900 --> 01:17:27,110
next is we're going to try to create a
neural net version

486
01:17:27,110 --> 01:17:36,770
of this hey so the basic idea here is
we're going to take exactly the same

487
01:17:36,770 --> 01:17:43,450
thing as we had before here's our list
of users right and here is Erin Bates
alright and here's our list of movies

488
01:17:46,130 --> 01:17:52,840
and here is our embedded right and so as
you can see I've just kind of transposed

489
01:17:52,840 --> 01:17:58,880
the movie ones so that so that they're
all in the same orientation and here is

490
01:17:58,880 --> 01:18:05,300
our user movie rating but D cross tab
okay so in the original format so each

491
01:18:05,300 --> 01:18:13,540
row is a user movie rating okay so the
first thing I do is I need to replace

492
01:18:13,540 --> 01:18:22,180
user 14 with that users contiguous in
this right and so I can do that in Excel

493
01:18:22,180 --> 01:18:27,710
using this match that basically says
what you know how far down this list do

494
01:18:27,710 --> 01:18:34,040
you have to go and it said user 14 was
the first thing in that list okay

495
01:18:34,040 --> 01:18:39,980
user 29 was the second thing in that
list so forth okay so this is the same

496
01:18:39,980 --> 01:18:45,170
as that thing that we did in our Python
code where we basically created a

497
01:18:45,170 --> 01:18:52,180
dictionary to master so now we can for
this particular user movie rating

498
01:18:52,180 --> 01:18:58,640
combination we can look up the
appropriate embedding right and so you

499
01:18:58,640 --> 01:19:01,700
can see here what it's doing is it's
saying
all right let's basically offset from

500
01:19:05,120 --> 01:19:10,130
the start of this list and the number of
rows we're going to go down is equal to

501
01:19:10,130 --> 01:19:14,810
the user index and the number of columns
we're going to go across is one two

502
01:19:14,810 --> 01:19:18,590
three four or five okay and so you can
see what it does is it creates point one

503
01:19:18,590 --> 01:19:22,580
nine point six three point three one
here it is point one nine point okay so

504
01:19:22,580 --> 01:19:29,240
so this is literally modern embedding
this but remember this is exactly the

505
01:19:29,240 --> 01:19:37,820
same as doing a one hot encoding right
because if instead this was a vector

506
01:19:37,820 --> 01:19:44,150
containing one zero zero zero zero
right and we multiplied that by this

507
01:19:44,150 --> 01:19:51,800
matrix then the only row it's going to
return would be the first one okay so so

508
01:19:51,800 --> 01:19:56,210
it's really useful to remember that
embedding actually just is a matrix

509
01:19:56,210 --> 01:20:02,270
product the only reason it exists the
only reason it exists is because this is

510
01:20:02,270 --> 01:20:07,280
an optimization you know this let's pipe
or to know like okay this is just a
matrix multiply but I guarantee you that

511
01:20:10,460 --> 01:20:15,320
you know this thing is one hard encoded
therefore you don't have to actually do

512
01:20:15,320 --> 01:20:19,280
the matrix multiply you can just do a
directory of that okay so that's

513
01:20:19,280 --> 01:20:26,420
literally all an embedding is is it is a
computational performance thing for a

514
01:20:26,420 --> 01:20:31,340
particular kind of matrix multiplier all
right so that looks up that uses user

515
01:20:31,340 --> 01:20:38,420
and then we can look up that users movie
all right so here is movie ID movie ID

516
01:20:38,420 --> 01:20:43,550
four one seven which apparently is
indexed number fourteen here it is here

517
01:20:43,550 --> 01:20:47,900
so it should have been point seven five
point four seven yes it is point seven
five point plus it okay

518
01:20:49,460 --> 01:20:55,040
so we've now got the user embedding and
the movie embedding and rather than

519
01:20:55,040 --> 01:21:06,370
doing a dot product of those two okay
which is what we do normally instead

520
01:21:06,370 --> 01:21:15,310
what if we concatenate the two together
into a single vector of length 10 and

521
01:21:15,310 --> 01:21:24,620
then feed that into a neural net okay
and so anytime we've got you know a
tensor of import activations or in this

522
01:21:27,920 --> 01:21:31,820
case a tensor of actually this is a
tensor of output activations this is

523
01:21:31,820 --> 01:21:35,960
coming out of an embedding layer we can
chuck it in a neural net because neural

524
01:21:35,960 --> 01:21:43,190
Nets we now know can calculate anything
okay including hopefully collaborative

525
01:21:43,190 --> 01:21:51,599
filtering so let's try that so here is
our embedding net so

526
01:21:51,599 --> 01:22:01,320
this time I have not bothered to create
a separate bias because instead the

527
01:22:01,320 --> 01:22:10,050
linear layer in pi torch already has a
bias in it all right so when we go NN

528
01:22:10,050 --> 01:22:17,929
Linea right that's kind of draw this out

529
01:22:18,050 --> 01:22:27,650
so we've got our U matrix right and this
is the number of users and this is the
number of factors right and we've got

530
01:22:31,290 --> 01:22:39,210
our M matrix that so here's our number
of movies and here's our again number of

531
01:22:39,210 --> 01:22:50,099
factors okay and so remember we look up
a single user we look up a single movie

532
01:22:50,099 --> 01:22:57,239
and let's grab them and concatenate them
together okay so here's like the user
part here's the movie part and then

533
01:23:00,480 --> 01:23:08,010
let's put that through a matrix product
right so that number of rows here is

534
01:23:08,010 --> 01:23:12,090
going to have to be the number of users
plus the number of movies right because

535
01:23:12,090 --> 01:23:18,000
that's how long that is and then the
number of columns can be anything we

536
01:23:18,000 --> 01:23:25,020
want because we're going to take that so
in this case we're going to pick 10

537
01:23:25,020 --> 01:23:30,300
apparently so it's pick 10 and then
we're going to stick that through a rail

538
01:23:30,300 --> 01:23:37,349
you and then stick that through another
matrix which obviously needs to be of

539
01:23:37,349 --> 01:23:43,889
size 10 here and then the number of
columns is a size 1 because we want to

540
01:23:43,889 --> 01:23:54,869
predict a single rating okay and so
that's our kind of flow chart of what's

541
01:23:54,869 --> 01:24:00,989
going on right it is a standard I'm
called a one hidden layer neural net it

542
01:24:00,989 --> 01:24:05,190
depends how you think of it like there's
kind of an embedding layer but because

543
01:24:05,190 --> 01:24:09,360
is linear and this is linear the two
together is really one linear layer

544
01:24:09,360 --> 01:24:14,100
right this just a computational
convenience so it's really got one

545
01:24:14,100 --> 01:24:21,270
hidden layer because it's got one layer
before this nonlinear activation so in

546
01:24:21,270 --> 01:24:27,690
order to create a linear layer with some
number of rows and some number of
columns you just go in and on in the

547
01:24:31,290 --> 01:24:37,230
machine learning class this week we
learnt how to create a linear layer from

548
01:24:37,230 --> 01:24:41,760
scratch by creating our own weight
matrix and our own biases so if you want

549
01:24:41,760 --> 01:24:46,020
to check that out you couldn't do so
there right but it's the same basic

550
01:24:46,020 --> 01:24:52,680
technique we've already seen so we
create our embeddings we create our two

551
01:24:52,680 --> 01:24:57,750
linear layers that's all the stuff that
we need to start with you know really if

552
01:24:57,750 --> 01:25:01,770
I wanted to make this more general I
would have had another parameter here

553
01:25:01,770 --> 01:25:11,160
called like num hidden you know equals
equals 10 and then this would be a
parameter and then you could like more

554
01:25:14,250 --> 01:25:18,570
easily play around with different
numbers of activations so when we say

555
01:25:18,570 --> 01:25:21,810
like okay in this layer I'm going to
create a layer with this many

556
01:25:21,810 --> 01:25:28,469
activations all I mean assuming it's a
fully connected layer is my linear layer

557
01:25:28,469 --> 01:25:32,820
has how many columns in its weight
matrix that's how many activations it

558
01:25:32,820 --> 01:25:38,550
creates all right so we grab our users
and movies we put them through our

559
01:25:38,550 --> 01:25:41,670
embedding matrix and then we concatenate
them together

560
01:25:41,670 --> 01:25:47,100
okay so torch cat concatenate them
together on the first dimension so in

561
01:25:47,100 --> 01:25:52,320
other words we concatenate the columns
together to create longer rows okay so
that's concatenating on dimension one

562
01:25:56,300 --> 01:26:01,580
drop out we'll come back to her in a
moment we've got that briefly

563
01:26:01,580 --> 01:26:07,980
so then having done that we'll put it
through that linear layer we had we'll

564
01:26:07,980 --> 01:26:14,160
do our value and you'll notice that
value is again inside our capital F and

565
01:26:14,160 --> 01:26:16,199
end up optional right it's just a
function

566
01:26:16,199 --> 01:26:20,550
so remember activation function
are basically things that take one

567
01:26:20,550 --> 01:26:26,070
activation in and spit one activation
out in this case taking something that
can have negatives or positives and

568
01:26:28,220 --> 01:26:34,950
truncate the negatives to zero that's
what well you does and then here's a

569
01:26:34,950 --> 01:26:42,630
sigmoid so that's that that is now a
genuine neural network I don't know if I

570
01:26:42,630 --> 01:26:45,690
get to call it deep it's only got one
hidden layer but it's definitely a
neural network all right and so we can

571
01:26:48,030 --> 01:26:52,890
now construct it we can put it on the
GPU you can create an optimizer for it

572
01:26:52,890 --> 01:26:57,660
and we can fit it now you'll notice
there's one other thing I've been
passing to fit which is what loss

573
01:27:00,810 --> 01:27:05,280
function are we trying to minimize okay
this is the mean squared error loss and
again it's inside F okay pretty much all

574
01:27:08,130 --> 01:27:14,610
the functions are inside it okay so one
of the things that you have to pass fit

575
01:27:14,610 --> 01:27:20,340
is something saying like how do you
score it's what counts as good or bad so

576
01:27:20,340 --> 01:27:25,860
it should I mean now that we have a real
neural net do we have to use the same
number of embeddings for users and

577
01:27:27,840 --> 01:27:31,800
that's a great question you don't know
absolutely right

578
01:27:31,800 --> 01:27:38,100
you don't and so like we've got a lot of
benefits here right because if we you

579
01:27:38,100 --> 01:27:48,600
know think about you know we're grabbing
a user embedding or concatenating it

580
01:27:48,600 --> 01:27:54,810
with a movie embedding which maybe is
like some different size but then also

581
01:27:54,810 --> 01:27:59,820
perhaps we looked up the genre of the
movie and like you know there's actually
a embedding matrix of like number of

582
01:28:02,490 --> 01:28:07,830
genres by I don't know three or
something and so like we could then

583
01:28:07,830 --> 01:28:12,270
concatenate like a genre embedding and
then maybe the timestamp is in here as a

584
01:28:12,270 --> 01:28:18,570
continuous number right and so then that
whole thing we can then feed into you
know and you're on it all right and then

585
01:28:24,420 --> 01:28:30,750
at the end remember a final
non-linearity was a sigmoid right so we

586
01:28:30,750 --> 01:28:34,619
can now recognize that
thing we did where we did sigmoid x max

587
01:28:34,619 --> 01:28:40,789
reading vote - min reading + blah blah
blah is actually just another nonlinear

588
01:28:40,789 --> 01:28:46,590
activation function alright remember in
our last layer we use generally

589
01:28:46,590 --> 01:28:51,269
different kinds of activation functions
so as we said we don't need any

590
01:28:51,269 --> 01:28:59,309
activation function at all right we
could just do that right but by not

591
01:28:59,309 --> 01:29:03,030
having any nonlinear activation function
we're just making it harder so that's

592
01:29:03,030 --> 01:29:10,230
why we put the sigmoid in there as well
okay so we can then fit it in the usual

593
01:29:10,230 --> 01:29:15,599
way and there we go
you know interestingly we actually got a

594
01:29:15,599 --> 01:29:23,519
better score than we did with our this
model so I'll be interesting to try

595
01:29:23,519 --> 01:29:26,429
training this with stochastic gradient
descent with restarts and see if it's

596
01:29:26,429 --> 01:29:30,960
actually better you know maybe you can
play around with the number of hidden

597
01:29:30,960 --> 01:29:35,699
layers and the drop out and whatever
else and see if you can come up with you

598
01:29:35,699 --> 01:29:50,789
know get a better answer than point
seven six ish okay so so general so this

599
01:29:50,789 --> 01:29:55,110
is like if you were going deep into
collaborative filtering at your

600
01:29:55,110 --> 01:29:57,389
workplace
whatever this wouldn't be a bad way to

601
01:29:57,389 --> 01:30:01,499
go I could like I'd start out with like
oh okay here's like a flat footed

602
01:30:01,499 --> 01:30:06,269
dataset 30 in first day I get learner
there's you know not much I can send it

603
01:30:06,269 --> 01:30:10,920
basically number of factors is about the
only thing that I pass in I can learn

604
01:30:10,920 --> 01:30:15,420
for a while maybe try a few different
approaches and then you're like okay

605
01:30:15,420 --> 01:30:22,019
there's like that's how I go if I use
the defaults okay how do I make it

606
01:30:22,019 --> 01:30:24,960
better and then I'd be like dig into the
code and seeing like okay well what if

607
01:30:24,960 --> 01:30:30,869
Jeremy actually do here this is actually
what I want you know and so one of the

608
01:30:30,869 --> 01:30:36,150
nice things about the neural net
approach is that you know as unit

609
01:30:36,150 --> 01:30:43,349
mentioned we can have different numbers
of embeddings we can choose how many

610
01:30:43,349 --> 01:30:51,000
hidden and we can also choose
drop now right so so what we're actually

611
01:30:51,000 --> 01:30:57,620
doing is we haven't just got real you
that we're also going like okay let's

612
01:31:00,770 --> 01:31:08,489
let's delete a few things at random
alright let's drop out so in this case
we were deleting after the first linear

613
01:31:12,570 --> 01:31:18,510
layer 75% of them all right and then
after the second one in like 75% of them

614
01:31:18,510 --> 01:31:22,830
so we can add a whole lot of
regularization yes so you know this it

615
01:31:22,830 --> 01:31:28,320
kind of feels like the this this
embedding net you know you could you

616
01:31:28,320 --> 01:31:31,710
could change this again we could like
have it so that we can pass into the

617
01:31:31,710 --> 01:31:39,210
constructor well if you're gonna make it
look as much as possible like what we

618
01:31:39,210 --> 01:31:48,630
had before we could surpass him peace
peace equals 0.75 oh I'm not sure this

619
01:31:48,630 --> 01:31:53,520
is the best API but it's not terrible
probably what since we've only got

620
01:31:53,520 --> 01:32:08,790
exactly two layers we could say p1
equals 0.75 v p2 v and so then this will
be P 1 this will be Peter you know where

621
01:32:18,630 --> 01:32:24,719
we go and like if you wanted to go
further you could make it look more like

622
01:32:24,719 --> 01:32:31,469
our structured data learner you could
actually have a thing this number of

623
01:32:31,469 --> 01:32:37,590
hidden you know maybe you could make a
list and so then rather than creating

624
01:32:37,590 --> 01:32:42,780
exactly one hidden layer and one output
layer this could be a little loop that

625
01:32:42,780 --> 01:32:47,010
creates and hidden miners each one of
the size you want so like this is all

626
01:32:47,010 --> 01:32:52,230
stuff you can play with during the
hearing the week if you want to and I
feel like if you've got like a much

627
01:32:53,790 --> 01:32:57,989
smaller collaborative children data set
you know maybe you need like more

628
01:32:57,989 --> 01:33:02,500
regularization or whatever
it's a much bigger one maybe more layers

629
01:33:02,500 --> 01:33:08,710
would help I don't know you know III
haven't seen much discussion of this

630
01:33:08,710 --> 01:33:12,040
kind of neural network approach to
collaborative filtering but I'm not a

631
01:33:12,040 --> 01:33:15,880
collaborative filtering expert so maybe
it's maybe it's around but that'd be

632
01:33:15,880 --> 01:33:27,000
interesting thing to try so the next
thing I wanted to do was to talk about

633
01:33:27,000 --> 01:33:34,870
the training loop so what's actually
happening inside the training loop so at

634
01:33:34,870 --> 01:33:42,220
the moment we're basically passing off
the actual updating of the weights to PI

635
01:33:42,220 --> 01:33:49,330
torches optimizer but what I'm going to
do is like understand what that

636
01:33:49,330 --> 01:33:54,160
optimizer is is actually good and we're
also I also want to understand what this

637
01:33:54,160 --> 01:34:03,220
Momentum's him he's doing so you'll find
we have a spreadsheet called grab disk

638
01:34:03,220 --> 01:34:08,110
gradient descent and it's kind of
designed to be read left to right sorry
right to left worksheet was so the

639
01:34:11,200 --> 01:34:16,030
rightmost worksheet is some data right
and we're going to implement gradient

640
01:34:16,030 --> 01:34:20,560
descent in Excel because obviously
everybody wants to do deep learning in

641
01:34:20,560 --> 01:34:23,770
it Selman we've done collaborative
filtering in Excel we've done

642
01:34:23,770 --> 01:34:30,040
convolutions in Excel so now we need SJD
in Excel so we can replace - once and
for all

643
01:34:30,430 --> 01:34:37,270
okay so let's start by creating some
data right and so here's you know here's

644
01:34:37,270 --> 01:34:45,330
some independent you know I've got one
column of X's you know and one column of
wise and these are actually directly

645
01:34:48,400 --> 01:34:54,460
linearly related so this is this is
random right and this one here is equal

646
01:34:54,460 --> 01:35:09,160
to x times 2 plus 30 ok so let's try and
use Excel to take that data and try and

647
01:35:09,160 --> 01:35:18,290
learn those parameters
okay that's going to be able so let's

648
01:35:18,290 --> 01:35:23,630
start with the most basic version of SGD
and so the first thing I'm going to do

649
01:35:23,630 --> 01:35:27,410
is I'm going to run a macro so you can
see what this looks like so I'll hit run

650
01:35:27,410 --> 01:35:32,180
and it does five eight bucks under
another five eight bucks

651
01:35:32,180 --> 01:35:38,300
- another five eight bucks okay so the
first one was pretty terrible it's hard

652
01:35:38,300 --> 01:35:45,650
to see so I'll just delete that first
one get better scaling alright so you
can see it actually it's pretty

653
01:35:47,450 --> 01:35:53,060
constantly improving the loss all right
this is the loss per pot all right so

654
01:35:53,060 --> 01:36:03,740
how do we do that so let's reset it so
here is my X's and my y's and what I do
is I start out by assuming some

655
01:36:06,500 --> 01:36:13,820
intercept and some slope right so this
is my randomly initialized weights so I

656
01:36:13,820 --> 01:36:18,440
have randomly initialized them both to
one you could pick a different random

657
01:36:18,440 --> 01:36:24,670
number if you like but I promise that I
randomly picked the number one twice

658
01:36:24,670 --> 01:36:27,850
there you go

659
01:36:27,880 --> 01:36:34,430
it was a random number between one and
one so here is my intercept and slope

660
01:36:34,430 --> 01:36:37,940
I'm just going to copy them over here
right so you can literally see this is
just equal see one here is equals c2

661
01:36:41,390 --> 01:36:47,240
okay so I'm gonna start with my very
first row of data x equals 14 y equals
58 and my goal is to come up after I

662
01:36:51,710 --> 01:36:54,860
look at this piece of data I want to
come up with a slightly better intercept

663
01:36:54,860 --> 01:37:02,950
and a slightly better slope okay so to
do that I need to first of all basically

664
01:37:02,950 --> 01:37:10,460
figure out which direction is is down in
other words if I make my intercept a

665
01:37:10,460 --> 01:37:15,050
little bit higher or a little bit lower
would it make my error a little bit

666
01:37:15,050 --> 01:37:20,330
better or a little bit worse so let's
start out by calculating the error so to

667
01:37:20,330 --> 01:37:24,410
calculate the error the first thing we
need is a prediction so the prediction

668
01:37:24,410 --> 01:37:33,019
is equal to the interest
at plus x times so that is our zero

669
01:37:33,019 --> 01:37:40,129
hidden layer neural network okay
and so here is our era it's equal to our

670
01:37:40,129 --> 01:37:45,769
prediction - our actual squared so we
could like play around with this I don't

671
01:37:45,769 --> 01:37:49,760
want my error to be 18-49 I'd like it to
be lower so what if we set the

672
01:37:49,760 --> 01:37:57,049
intercepts to one point one 18-49 goes
to 1840 okay so a higher intercept would

673
01:37:57,049 --> 01:38:05,599
be better okay what about the slope to
increase that it goes from 1849 to 1730

674
01:38:05,599 --> 01:38:09,969
okay a higher slope would be better as
well not surprising because we know

675
01:38:09,969 --> 01:38:18,129
actually that there should be 30 into so
one way to figure that out

676
01:38:18,129 --> 01:38:23,059
you know encode and this protein is to
do literally what I just did is to add a

677
01:38:23,059 --> 01:38:26,419
little bit to the intercept and the
slope and see what happens and that's

678
01:38:26,419 --> 01:38:30,769
called finding the derivative through
finite differencing right and so let's

679
01:38:30,769 --> 01:38:40,969
go ahead and do that so here is the
value of my error if I add 0.01 to my

680
01:38:40,969 --> 01:38:46,219
intercept all right so it's c4 plus 0.01
and then I just put that into my Lydian

681
01:38:46,219 --> 01:38:51,229
function and then I subtract my actual
all squared all right and so that causes

682
01:38:51,229 --> 01:38:59,089
my arrow to go down a bit
that's our increasing my is that

683
01:38:59,089 --> 01:39:03,349
increasing will see for increasing the
intercept a little bit has caused my

684
01:39:03,349 --> 01:39:07,399
arrow to go down so what's the
derivative well the derivative is equal

685
01:39:07,399 --> 01:39:11,749
to how much the dependent variable
changed by divided by how much the
independent variable changed by all

686
01:39:13,999 --> 01:39:18,469
right and so there it is right our
dependent variable changed by that -

687
01:39:18,469 --> 01:39:24,049
that right and our independent variable
we changed by 0.01 so there is the

688
01:39:24,049 --> 01:39:30,530
estimated value of the error dB so
remember when people talking about

689
01:39:30,530 --> 01:39:34,609
derivatives right this is this is all
they're doing is they're saying what's

690
01:39:34,609 --> 01:39:39,570
this value but as we make this number
smaller and smaller and smaller
and smaller as it as limits to zero

691
01:39:43,530 --> 01:39:47,760
I'm not mad enough to think in terms of
like derivatives and integrals and stuff

692
01:39:47,760 --> 01:39:51,540
like that so whatever I think about this
I always think about you know an actual

693
01:39:51,540 --> 01:39:56,430
like plus 0.01 and divided by 0.01
because like I just find that easier

694
01:39:56,430 --> 01:40:00,540
just like I'd ever think about
probability density functions I always

695
01:40:00,540 --> 01:40:03,240
think about actual probabilities of that
toss a coin

696
01:40:03,240 --> 01:40:08,910
something happens three times so I
always think like remember it's it's

697
01:40:08,910 --> 01:40:14,220
totally fair to do this because a
computer is discrete it's not continuous

698
01:40:14,220 --> 01:40:18,390
like a computer can't do anything
infinitely small anyway right so it's

699
01:40:18,390 --> 01:40:23,460
actually got to be calculating things at
some level of precision right and our

700
01:40:23,460 --> 01:40:29,490
brains kind of need that as well so this
is like my version of Jeffery Clinton's

701
01:40:29,490 --> 01:40:33,660
like to visualize things in more than
two dimensions you just like say 12

702
01:40:33,660 --> 01:40:36,780
dimensions really quickly well
visualizing in two dimensions this is my

703
01:40:36,780 --> 01:40:41,970
equivalent you know to to think about
derivatives just think about division

704
01:40:41,970 --> 01:40:47,640
and like although all the mathematicians
say no you can't do that you actually

705
01:40:47,640 --> 01:40:51,810
can like if you think of DX dy is being
literally you know change in X over

706
01:40:51,810 --> 01:40:58,080
changing Y like the division actually
like the calculations do work like all
the time so okay so let's do the same

707
01:41:01,710 --> 01:41:07,680
thing now with changing my slope by a
little bit and so here's the same thing

708
01:41:07,680 --> 01:41:13,620
right and so you can see both of these
are negative okay so that's saying if I

709
01:41:13,620 --> 01:41:20,010
increase my intercept my loss goes down
if I increase my slope my loss goes down

710
01:41:20,010 --> 01:41:29,010
right and so my derivative of my error
with respect to my slope is is actually

711
01:41:29,010 --> 01:41:36,240
pretty high and that's not surprising
because it's actually you know the

712
01:41:36,240 --> 01:41:42,020
constant term is just being added where
else as slope is being multiplied by 40

713
01:41:42,890 --> 01:41:49,950
okay now find that differencing is all
very well and good but there's a big
problem with finite difference seeing in

714
01:41:52,040 --> 01:41:56,050
Hyden
no spaces and the problem is this right

715
01:41:56,050 --> 01:42:02,770
and this is like you don't need to learn
how to calculate derivatives or

716
01:42:02,770 --> 01:42:06,969
integrals but you need to learn how to
think about them spatially right and so

717
01:42:06,969 --> 01:42:13,630
remember we have some vector very high
dimensional vector it's got like a

718
01:42:13,630 --> 01:42:22,090
million items in it right and it's going
through some weight matrix right of size

719
01:42:22,090 --> 01:42:26,890
like 1 million by size a hundred
thousand or whatever and it's spitting

720
01:42:26,890 --> 01:42:33,580
out something of size I hundred thousand
and so you need to realize like there

721
01:42:33,580 --> 01:42:38,380
isn't like a gradient yeah but it's like
for every one of these things in this

722
01:42:38,380 --> 01:42:45,850
vector right there's a gradient in every
direction you know in every part of the

723
01:42:45,850 --> 01:42:52,719
output right so it actually has not a
single gradient number not even a

724
01:42:52,719 --> 01:43:03,100
gradient vector but a gradient matrix
right and so this this is a lot to

725
01:43:03,100 --> 01:43:07,989
calculate right I would literally have
to like add a little bit to this and see
what happens to all of these add a

726
01:43:09,790 --> 01:43:14,739
little bit to this see what happens to
all of these right to fill in one column

727
01:43:14,739 --> 01:43:21,400
of this at a time so that's going to be
horrendously slow like that so that's

728
01:43:21,400 --> 01:43:24,250
why like if you're ever thinking like
how we can just do this with finite

729
01:43:24,250 --> 01:43:28,480
differencing just remember like okay we
we're dealing in the with these very

730
01:43:28,480 --> 01:43:37,870
high dimensional vectors where you know
this this kind of matrix calculus like

731
01:43:37,870 --> 01:43:42,880
all the concepts are identical but when
you actually draw it out like this you

732
01:43:42,880 --> 01:43:47,199
suddenly realize like okay for each
number I could change there's a whole
bunch of numbers that impacts and I have

733
01:43:49,060 --> 01:43:54,310
this whole matrix of things to compute
right and so your gradient calculations

734
01:43:54,310 --> 01:43:59,260
can take up a lot of memory and they can
take up a lot of time so we want to find

735
01:43:59,260 --> 01:44:07,000
some way to do this more quickly okay
and it's definitely well worth like

736
01:44:07,000 --> 01:44:14,200
spending time kind of studying these
ideas of like you know the idea of like

737
01:44:14,200 --> 01:44:24,640
the gradients like look up things like
Jacobian and Hessian they're the things

738
01:44:24,640 --> 01:44:29,020
that you want to search for just that
unfortunately people normally write

739
01:44:29,020 --> 01:44:35,860
about them with you know lots of great
letters and bla bla bla right but there
are some there are some nice you know

740
01:44:39,760 --> 01:44:43,060
intuitive explanations out there and
hopefully you can share them on the

741
01:44:43,060 --> 01:44:47,830
forum if you find them because this is
stuff you really need to really need to
understand in here you know because

742
01:44:51,960 --> 01:44:56,380
you're trying to train something and
it's not working properly and like later

743
01:44:56,380 --> 01:45:00,640
on we'll learn how to like look inside
hi torch to like actually get the values

744
01:45:00,640 --> 01:45:04,240
of the gradients and you need to know
like okay well how would I like what the
gradients you know what would I consider

745
01:45:06,430 --> 01:45:10,840
unusual like you know these are the
things that turn you into a really

746
01:45:10,840 --> 01:45:15,580
awesome deep learning practitioner is
when you can like debug your problems by

747
01:45:15,580 --> 01:45:19,720
like grabbing the gradients and doing
histograms of them and like knowing you

748
01:45:19,720 --> 01:45:23,530
know that you could like plot that all
each layer my average gradients getting
worse or you know bigger okay so the

749
01:45:28,270 --> 01:45:34,270
trick to doing this more quickly is to
do it analytically rather than through

750
01:45:34,270 --> 01:45:40,150
finite differencing and so analytically
is basically there is a list you
probably all learned it at high school

751
01:45:41,500 --> 01:45:46,600
there is a literally a list of rules
that for every mathematical function

752
01:45:46,600 --> 01:45:53,050
there's a like this is the derivative of
that function so you probably remember a

753
01:45:53,050 --> 01:46:02,920
few of them for example x squared - it's
alright and so we actually have here an

754
01:46:02,920 --> 01:46:10,720
x squared so here is our two x right now
the one that I actually want you to know

755
01:46:10,720 --> 01:46:17,550
is not any of the individual rules but I
want you to know the chain rule right

756
01:46:17,550 --> 01:46:22,450
which
you've got some function of some

757
01:46:22,450 --> 01:46:26,380
function of something why is this
important

758
01:46:26,380 --> 01:46:33,670
I don't know that's a linear layer
that's a rally right and then we can

759
01:46:33,670 --> 01:46:42,160
kind of keep going backwards map etc
right a neural net is just a function of

760
01:46:42,160 --> 01:46:45,100
a function of a function of a function
where the innermost is you know it's

761
01:46:45,100 --> 01:47:00,940
basically linear rally your linear rally
your dot linear sigmoid or soft mass all
right and so it's a function of a

762
01:47:02,170 --> 01:47:06,940
function of a function and so therefore
to calculate the derivative of the
weights in your model the loss of your

763
01:47:10,960 --> 01:47:13,330
model with respect to the weights of
your model you're going to need to use

764
01:47:13,330 --> 01:47:17,860
the chain rule and specifically whatever
layer it is that you're up to like I

765
01:47:17,860 --> 01:47:23,020
want to calculate the derivative here
and got a need to use all of these all

766
01:47:23,020 --> 01:47:25,690
of these ones because that's all that's
that's the function that's being applied
right and that's why they call this back

767
01:47:28,150 --> 01:47:36,310
propagation because the value of the
derivative of that is equal to that

768
01:47:36,310 --> 01:47:43,810
derivative now basically you can do it
like this you can say let's call you is

769
01:47:43,810 --> 01:47:47,020
this right
let's call that you all right then it's

770
01:47:47,020 --> 01:47:56,440
simply equal to the derivative of that
times derivative of that right
you just multiply them together and so

771
01:47:59,170 --> 01:48:03,910
that's what back propagation is like
it's not that back propagation is a new

772
01:48:03,910 --> 01:48:10,240
thing for you to learn it's not a new
algorithm it is literally take the

773
01:48:10,240 --> 01:48:16,270
derivative of every one of your layers
and multiply them all together so like

774
01:48:16,270 --> 01:48:23,050
it doesn't deserve a new name right
apply the chain rule to my layers does

775
01:48:23,050 --> 01:48:27,790
not deserve a new name but it gets one
because us neural networks folk really

776
01:48:27,790 --> 01:48:31,750
need to seem as clever as possible it's
really important that everybody else

777
01:48:31,750 --> 01:48:34,929
thinks
we are way outside of their capabilities

778
01:48:34,929 --> 01:48:39,429
so the fact that you're here means that
we've failed because you guys somehow

779
01:48:39,429 --> 01:48:43,540
think that you're capable right so
remember it's really important when you

780
01:48:43,540 --> 01:48:48,010
talk to other people that you say
backpropagation and rectified linear

781
01:48:48,010 --> 01:48:54,400
unit rather than like multiply the
layers gradients or replace negatives
with zeros okay so so here we go

782
01:48:57,520 --> 01:49:03,159
so here is so I've just gone ahead and
grabbed the derivative unfortunately
there is no automatic differentiation in

783
01:49:05,079 --> 01:49:09,610
Excel yet so I did the alternative which
is to paste the formula into Wolfram

784
01:49:09,610 --> 01:49:14,020
Alpha and got back the derivative so
there's the first derivative and there's

785
01:49:14,020 --> 01:49:19,510
the second derivative analytically we
only have one layer in this infinite

786
01:49:19,510 --> 01:49:23,860
finally small neural network so we don't
have to worry about the chain rule and
we should see that this analytical

787
01:49:25,480 --> 01:49:28,599
derivative is pretty close to our
estimated derivative from the find out

788
01:49:28,599 --> 01:49:33,880
differencing and indeed it is right and
we should see that these ones are pretty

789
01:49:33,880 --> 01:49:38,790
similar as well and indeed they are
right and if you're you know back when I

790
01:49:38,790 --> 01:49:45,130
implemented my own neural Nets 20 years
ago I you know had to actually calculate

791
01:49:45,130 --> 01:49:48,550
the derivatives and so I always would
write like had something that would

792
01:49:48,550 --> 01:49:52,090
check the derivatives using finite
difference in and so for those poor

793
01:49:52,090 --> 01:49:55,420
people that they'd have to write these
things by hand you'll still see that

794
01:49:55,420 --> 01:49:59,710
they have like a finite differencing
checkout so if you ever do have to

795
01:49:59,710 --> 01:50:05,469
implement a derivative by hand please
make sure that you have a finite

796
01:50:05,469 --> 01:50:10,719
differencing checker so that you can
test it alright so there's no

797
01:50:10,719 --> 01:50:17,139
derivatives so we know that if we
increase B then we're going to get a

798
01:50:17,139 --> 01:50:22,239
slightly better loss so let's increase B
by a bit how much should we increase it
by well we'll increase it by some more

799
01:50:24,670 --> 01:50:28,060
for this so the motor-pod we're going to
choose is called a learning rate and so

800
01:50:28,060 --> 01:50:37,300
here's our learning rate so here's one
enoch 4 ok so our new value is equal to

801
01:50:37,300 --> 01:50:45,219
whatever it was before
- our derivative times our learning rate

802
01:50:45,219 --> 01:50:52,239
okay so we've gone from one to one point
or one and then a we've done the same
thing so it's gone from one to one point

803
01:50:56,349 --> 01:51:02,530
one two so this is a special kind of
mini batch it's a mini batch of size one

804
01:51:02,530 --> 01:51:09,099
okay so we call this online grading does
it just means mini batch of size one so

805
01:51:09,099 --> 01:51:11,800
then we can go into the next one next is
86

806
01:51:11,800 --> 01:51:17,590
why is 202 right this is my intercept
and slope copied across from the last

807
01:51:17,590 --> 01:51:26,170
row okay so here's my new wire
prediction here's my new era here are my

808
01:51:26,170 --> 01:51:32,559
derivatives here are my new a and B okay
so we keep doing that for every mini

809
01:51:32,559 --> 01:51:41,139
batch of one until eventually we run out
at the end of the new pocket okay and so

810
01:51:41,139 --> 01:51:48,849
then at the end of an epoch we would
grab our intercept and slope and paste

811
01:51:48,849 --> 01:51:56,440
them back over here as our new values
there we are and we can now continue
again all right so we're now starting

812
01:51:59,050 --> 01:52:06,070
with pops today's either in the wrong
spot it should be pasted special
transpose values all right okay

813
01:52:10,420 --> 01:52:13,809
so there's a new intercept there's any
slope possibly I got that the wrong way

814
01:52:13,809 --> 01:52:19,570
around but anyway you get the idea and
then we continue okay so I recorded the
world's tiniest macro which literally

815
01:52:22,989 --> 01:52:32,070
just copies the final slope and puts it
into the new slope copies the final

816
01:52:32,070 --> 01:52:40,989
intercept put the new intercept and does
that five times and after each time it

817
01:52:40,989 --> 01:52:45,550
grabs the root mean squared error and
pastes it into the next spare area and

818
01:52:45,550 --> 01:52:49,690
that is attached to this Run button and
so that's going to go ahead and do that
five times okay so that's stochastic

819
01:52:53,769 --> 01:52:58,990
gradient descent and if so so to turn
this into a CNN

820
01:52:58,990 --> 01:53:05,170
all right you would just replace this
error function right and therefore this

821
01:53:05,170 --> 01:53:12,220
prediction with the output of that
convolutional example spreadsheet okay

822
01:53:12,220 --> 01:53:19,750
and that then would be in CNN being
trained with with SGD okay

823
01:53:19,750 --> 01:53:30,610
now the problem is that you'll see when
I run this it's kind of going very

824
01:53:30,610 --> 01:53:35,410
slowly right we know that we need to get
to a slope of two and an intercept of

825
01:53:35,410 --> 01:53:40,180
thirty and you can kind of see it this
rate it's going to take a very long time

826
01:53:40,180 --> 01:53:51,850
right and specifically it's like it
keeps going the same direction so it's

827
01:53:51,850 --> 01:53:56,830
like come on take a hint that's a good
direction so they come on take a hint

828
01:53:56,830 --> 01:54:01,260
that's a good direction please keep
doing that but more is called momentum

829
01:54:01,260 --> 01:54:11,350
right so on our next spreadsheet we're
going to implement momentum okay so what

830
01:54:11,350 --> 01:54:16,510
momentum does is the same thing and what
to simplify this spreadsheet I've

831
01:54:16,510 --> 01:54:21,430
removed the finite difference cause okay
other than that this is just the same

832
01:54:21,430 --> 01:54:28,810
right so it's true what our X is our
wise A's and B's predictions our error

833
01:54:28,810 --> 01:54:39,340
is now over here okay and here's our
derivatives okay our new calculation for

834
01:54:39,340 --> 01:54:49,660
this particular row our new calculation
here for our new a term just like before

835
01:54:49,660 --> 01:54:59,110
is is equal to whatever a was before -
okay now this time I'm not taking the

836
01:54:59,110 --> 01:55:03,850
derivative but I'm - income other number
times the loan rate so what's this other

837
01:55:03,850 --> 01:55:13,020
number okay so this other number is
equal to the derivative

838
01:55:13,020 --> 01:55:26,800
times what's this k 1.02 plus 0.98 times
the thing just above it okay so this is

839
01:55:26,800 --> 01:55:32,650
a linear interpolation between this rows
derivative for this mini-batches
derivative and whatever direction we

840
01:55:35,830 --> 01:55:41,740
went last time right so in other words
keep going the same direction as you

841
01:55:41,740 --> 01:55:48,940
were before right then update it a
little bit right and so in our rich in

842
01:55:48,940 --> 01:55:55,600
our Python just before we had a momentum
of 0.9 okay so you can see what tends to

843
01:55:55,600 --> 01:56:03,280
happen is that our negative kind of gets
more and more negative right all the way
up to like 2,000 where else with our

844
01:56:08,080 --> 01:56:14,200
standard SGD approach a derivatives are
kind of all over the place right

845
01:56:14,200 --> 01:56:18,790
sometimes there's 700 something negative
7 positive 100 you know so this is

846
01:56:18,790 --> 01:56:24,100
basically saying like yeah if you've
been going down for quite a while keep

847
01:56:24,100 --> 01:56:28,300
doing that until finally here it's like
okay that's that seems to be far enough

848
01:56:28,300 --> 01:56:32,140
so that's being less and less and less
negative all right mister we start being

849
01:56:32,140 --> 01:56:35,680
positive again so you can kind of see
why it's called momentum it's like once

850
01:56:35,680 --> 01:56:40,240
you start traveling in a particular
direction for a particular weight you're
kind of the wheel start spinning and

851
01:56:41,950 --> 01:56:46,810
then once the gradient turns around the
other way it's like Oh slow down we've

852
01:56:46,810 --> 01:56:51,700
got this kind of event um and then
finally turn back around right so when
we do it this way all right we can do

853
01:56:57,340 --> 01:57:04,300
exactly the same thing right and after
five iterations we're at 89 where else

854
01:57:04,300 --> 01:57:11,950
before after five iterations we're at
104 right and after a few more let's go

855
01:57:11,950 --> 01:57:22,620
maybe 15 okay so get this 102 for us
here

856
01:57:26,160 --> 01:57:31,660
it's going right so it's it's it's a bit
better it's not hips better you can

857
01:57:31,660 --> 01:57:38,710
still see like these numbers they're not
zipping along right but it's definitely

858
01:57:38,710 --> 01:57:41,680
an improvement and it also gives us
something else to tune which is nice

859
01:57:41,680 --> 01:57:45,910
like so if this is kind of a
well-behaved error surface right in

860
01:57:45,910 --> 01:57:50,800
other words like although it might be
bumpy along the way there's kind of some

861
01:57:50,800 --> 01:57:55,300
overall direction like imagine you're
going down a hill right and there's like

862
01:57:55,300 --> 01:57:59,170
bumps oh alright so the mobile more
momentum you got going to skipping over

863
01:57:59,170 --> 01:58:03,330
the tops right so we could say like okay
let's increase our beater up to 0.98

864
01:58:03,330 --> 01:58:08,140
right and see if that like allows us to
train a little faster and whoa look at

865
01:58:08,140 --> 01:58:12,160
that suddenly what's going to okay so
one nice thing about things like
momentum is it's like another parameter

866
01:58:13,750 --> 01:58:21,670
that you can choose to try and make your
model train better in practice basically

867
01:58:21,670 --> 01:58:26,230
everybody does this every like you look
at any like image net winner or whatever
they all use momentum okay and so back

868
01:58:37,930 --> 01:58:44,740
over here when we said here's SGD that
basically means use the basic tab of our
Excel spreadsheet but then momentum

869
01:58:46,960 --> 01:58:58,000
equals 0.9 means add in put a point nine
over here okay and so that that's kind

870
01:58:58,000 --> 01:59:03,240
of your like default starting point so

871
01:59:03,720 --> 01:59:18,220
let's keep going and talk about Adam so
Adam is something which I actually was

872
01:59:18,220 --> 01:59:22,720
not right earlier on in this course I
said we've been using Adam by default we

873
01:59:22,720 --> 01:59:25,690
actually haven't we've actually been I
noticed our we've actually been using
SGD with momentum by default and the

874
01:59:29,020 --> 01:59:36,690
reason is that Adam has had
much faster as you'll see it's much much
faster to learn with but there's been

875
01:59:38,489 --> 01:59:41,700
some problems which is people who
haven't been getting quite as good like

876
01:59:41,700 --> 01:59:46,440
final answers with Adam as they have
with std with momentum and that's why

877
01:59:46,440 --> 01:59:50,670
you'll see like all the you know image
net winning solutions and so forth and

878
01:59:50,670 --> 01:59:56,880
all the academic papers always use SGD
with momentum and I'll Adam seems to be

879
01:59:56,880 --> 02:00:00,330
a particular problem in NLP people
really haven't got Adam working at all

880
02:00:00,330 --> 02:00:08,640
well the good news is this was I built
it looks like this was solved two weeks

881
02:00:08,640 --> 02:00:14,730
ago it basically it turned out that the
way people were dealing with a
combination of weight decay in Adam had

882
02:00:17,310 --> 02:00:22,200
a nasty kind of bargainer basically and
that's that's kind of carried through to
every single library and one of our

883
02:00:24,750 --> 02:00:31,170
students and then Sahara has actually
just completed a prototype of adding is

884
02:00:31,170 --> 02:00:36,390
this new version of Adam has called Adam
W into fast AI and he's confirmed that

885
02:00:36,390 --> 02:00:43,710
he's getting much faster both the faster
performance and also the better accuracy

886
02:00:43,710 --> 02:00:49,560
so hopefully we'll have this Adam W in
faster ideally before next week we'll

887
02:00:49,560 --> 02:00:53,660
see how we go very very soon
so so it is worth telling you about
about Adam so let's talk about it it's

888
02:00:58,830 --> 02:01:04,560
actually incredibly simple but again you
know make sure you make it sound really

889
02:01:04,560 --> 02:01:09,690
complicated when you tell people so that
you can so here's the same spreadsheet
again right and here's our randomly

890
02:01:13,920 --> 02:01:19,020
selected a and B again somehow it's
still one here's a prediction here's our

891
02:01:19,020 --> 02:01:25,230
derivatives okay so now how we count
letting on you hey you could immediately
see it's looking pretty hopeful because

892
02:01:28,290 --> 02:01:34,110
even by like row ten we're like we're
seeing the numbers move a lot more right

893
02:01:34,110 --> 02:01:42,960
so this is looking pretty encouraging so
how are we calculating this it's equal

894
02:01:42,960 --> 02:01:49,989
to our previous value with B minus j h
we're gonna have to find out what that
is times our learning rate divided by

895
02:01:56,739 --> 02:02:00,400
the square root of LH okay so I'm gonna
have to dig it and see what's going on

896
02:02:00,400 --> 02:02:05,650
one thing to notice here is that my
learning rate is way higher than it used

897
02:02:05,650 --> 02:02:12,730
to be but then we're dividing it by this
big number okay so let's start out by

898
02:02:12,730 --> 02:02:19,300
looking and seeing what this day-out
thing is okay

899
02:02:19,300 --> 02:02:25,690
j8 is identical to what we had before j8
is equal to the linear interpolation of
the derivative and the previous

900
02:02:28,750 --> 02:02:36,550
direction okay so that was easy so one
part of atom is to use momentum in the

901
02:02:36,550 --> 02:02:41,680
way we just defined it
okay the second piece was to divide by

902
02:02:41,680 --> 02:02:48,610
square root L 8 what is that square root
L 8 okay is another linear interpolation
of something and something else and

903
02:02:52,920 --> 02:03:00,610
specifically it's a linear interpolation
of F 8 squared okay it's a linear

904
02:03:00,610 --> 02:03:07,210
interpolation of the derivative squared
along with the derivative squared last

905
02:03:07,210 --> 02:03:13,210
time okay so in other words we've got
two pieces of momentum going on here

906
02:03:13,210 --> 02:03:22,420
one is calculating the momentum version
of the gradient the other is calculating

907
02:03:22,420 --> 02:03:28,780
the momentum version of the gradient
squared and we often refer to this idea

908
02:03:28,780 --> 02:03:34,090
as a exponentially weighted moving
average in other words it's basically

909
02:03:34,090 --> 02:03:37,630
equal to the average of this one and the
last one in the last one in the last one

910
02:03:37,630 --> 02:03:42,130
that we're like multiplicatively
decreasing the previous ones right

911
02:03:42,130 --> 02:03:47,830
because we're multiplying it by 0.9
times what 999 and so you actually see

912
02:03:47,830 --> 02:03:53,370
that for instance in the faster I code

913
02:04:00,180 --> 02:04:10,380
if you look at fish we don't just
calculate the average loss right because

914
02:04:10,380 --> 02:04:14,980
what I actually want we certainly don't
just report the loss for every mini
match because that just bounces around

915
02:04:16,210 --> 02:04:22,360
so much so instead I say average loss is
equal to whatever the average loss was

916
02:04:22,360 --> 02:04:31,900
last time times 0.98 plus the loss this
time times 0.02 right so in other words

917
02:04:31,900 --> 02:04:36,489
the faster you library the thing that
it's actually when you do like the

918
02:04:36,489 --> 02:04:40,930
learning rate finder or plot loss it's
actually showing you the exponentially

919
02:04:40,930 --> 02:04:47,290
weighted moving average of the loss okay
so it's like a really handy concept it

920
02:04:47,290 --> 02:04:52,390
appears quite a lot right the other in
handy concept know about is this idea of

921
02:04:52,390 --> 02:04:58,120
like you've got two numbers one of them
is multiplied by some value the other is

922
02:04:58,120 --> 02:05:02,770
multiplied by one minus that value so
this is a linear interpolation of two

923
02:05:02,770 --> 02:05:09,190
values you'll see it all the time and
for some reason deep learning people
nearly always use the value alpha when

924
02:05:11,949 --> 02:05:15,280
they do this so like keep an eye out if
you're reading a paper or something and

925
02:05:15,280 --> 02:05:22,840
you see like alpha times bla bla bla bla
bla plus one minus alpha times some

926
02:05:22,840 --> 02:05:28,180
other bla bla bla bla right immediately
like when people read papers none of us

927
02:05:28,180 --> 02:05:33,730
like read every thing in the equation we
look at it we go oh linear interpolation
right and I said something I was just

928
02:05:35,890 --> 02:05:39,670
talking to Rachel about yesterday is
like whether we could start trying to

929
02:05:39,670 --> 02:05:44,080
find like a a new way of writing papers
where we literally refactor them right

930
02:05:44,080 --> 02:05:50,620
like it'd be so much better to have
written like linear interpolate bla bla

931
02:05:50,620 --> 02:05:55,090
bla bla bla right because then you don't
have to have that pattern recognition

932
02:05:55,090 --> 02:05:59,440
right but until we convince the world to
change how they write papers this is
what you have to do is you have to look

933
02:06:00,820 --> 02:06:06,850
you know know what to look for right and
once you do suddenly the huge page with

934
02:06:06,850 --> 02:06:11,020
formulas
that at all like you often notice like

935
02:06:11,020 --> 02:06:15,580
for example the two things in here like
they might be totally identical but this
might be a time T and this might be at

936
02:06:17,260 --> 02:06:21,730
like time t minus y or something right
like it's very often these big ugly
formulas turn out to be really really

937
02:06:24,400 --> 02:06:30,340
simple if only they had ripped out them
okay so what are we doing with this

938
02:06:30,340 --> 02:06:37,540
gradient squared so what we were doing
with the gradient squared is we were

939
02:06:37,540 --> 02:06:42,370
taking the square root and then we were
adjusting the learning rate by dividing

940
02:06:42,370 --> 02:06:49,320
the learning rate by that okay so
gradient squared is always positive

941
02:06:49,320 --> 02:06:55,300
right and we're taking the exponentially
waiting move moving average of a bunch

942
02:06:55,300 --> 02:06:58,750
of things that are always positive and
then we're taking the square root of

943
02:06:58,750 --> 02:07:04,060
that right so when is this number going
to be high it's going to be particularly

944
02:07:04,060 --> 02:07:09,550
high if there's like one big you know if
the gradients got a lot of variation

945
02:07:09,550 --> 02:07:14,980
that's oh there's a high variance of
gradient then this G squared thing is
going to be a really high number for us

946
02:07:16,840 --> 02:07:22,630
if it's like a constant amount right
it's going to be smaller that cuz when

947
02:07:22,630 --> 02:07:26,980
you add things that are squared the
squared slight jump out much bigger for
us if there wasn't if there wasn't much

948
02:07:28,510 --> 02:07:34,870
change it's not going to be as big so
basically this number at the bottom here

949
02:07:34,870 --> 02:07:41,320
is going to be high if our Brady --nt is
changing a lot now what do you want to

950
02:07:41,320 --> 02:07:46,210
do if you've got something which is like
first negative and then positive and

951
02:07:46,210 --> 02:07:53,260
then small and then high right well you
probably want to be more careful right

952
02:07:53,260 --> 02:07:56,650
you probably don't want to take a big
step because you can't really trust it
right so when the when the variance of

953
02:07:59,290 --> 02:08:03,480
the gradient is high we're going to
divide our learning rate by a big number

954
02:08:03,480 --> 02:08:09,760
we also found learning rate is very
similar kind of size all the time then

955
02:08:09,760 --> 02:08:13,120
we probably feel pretty good about the
step so we're dividing it by a small

956
02:08:13,120 --> 02:08:18,100
amount yeah and so this is called an
adaptive learning rate yeah and like a

957
02:08:18,100 --> 02:08:22,240
lot of people have this confusion about
atom I've seen it on the forum actually
like there's some kind of adaptive

958
02:08:24,700 --> 02:08:29,650
learning rate where somehow you like
setting different learning rates for

959
02:08:29,650 --> 02:08:33,010
different layers or something it's like
no not really

960
02:08:33,010 --> 02:08:37,720
right all we're doing is we're just
saying like this keep track of the

961
02:08:37,720 --> 02:08:43,060
average of the squares of the gradients
and use that to adjust the learning rate

962
02:08:43,060 --> 02:08:48,550
so there's still one learning rate okay
in this case it's one okay but

963
02:08:48,550 --> 02:08:55,030
effectively every parameter at every
epoch is being kind of like getting a

964
02:08:55,030 --> 02:08:58,840
bigger jump if the learning rate if the
gradients been pretty constant for that

965
02:08:58,840 --> 02:09:04,660
wait and a smaller jump otherwise okay
and that's Adam that's the entirety of

966
02:09:04,660 --> 02:09:09,940
Adam in in Excel right so there's now no
reason at all why you can't train

967
02:09:09,940 --> 02:09:13,540
imagenet in Excel because you've got
you've got access to all of the pieces

968
02:09:13,540 --> 02:09:18,660
you need and so let's try this out run

969
02:09:19,830 --> 02:09:25,000
okay that's not bad right five and we
straight up to twenty nine and two right

970
02:09:25,000 --> 02:09:31,480
so the difference between like you know
standard SGD and this is is huge and

971
02:09:31,480 --> 02:09:35,140
basically that you know the key
difference was that it figured out that

972
02:09:35,140 --> 02:09:42,100
we need to be you know moving this
number much faster okay and so and so it

973
02:09:42,100 --> 02:09:48,370
do and so you can see we've now got like
two different parameters one is kind of
momentum for the gradient piece the

974
02:09:50,500 --> 02:09:56,500
other is the momentum for the gradient
squared piece and there I think they're

975
02:09:56,500 --> 02:10:00,670
called like I think there's just a
couple of the beta I think when you when

976
02:10:00,670 --> 02:10:03,790
you want to change it in PI tortes is I
think what beta which is just a couple

977
02:10:03,790 --> 02:10:15,430
of two numbers you can change Jeremy so
so you set the yeah I think I understand

978
02:10:15,430 --> 02:10:21,310
this concept of you know one day when a
gradient is it goes up and down then

979
02:10:21,310 --> 02:10:26,770
you're not really sure which direction
should should go so you should kind of

980
02:10:26,770 --> 02:10:29,590
slow things down
therefore you subtract that gradient

981
02:10:29,590 --> 02:10:35,500
from the learning rate so but how do you
implement how far do you go I guess

982
02:10:35,500 --> 02:10:38,610
maybe I miss something
early on you do you set a number

983
02:10:38,610 --> 02:10:45,930
somewhere we divide yeah we divide the
learning rate divided by the square root

984
02:10:45,930 --> 02:10:50,880
of the moving average gradient squared
so that's where we use it

985
02:10:50,880 --> 02:10:57,720
oh I'm sorry can you be a little more
sure so d2 is the learning rate which is

986
02:10:57,720 --> 02:11:02,850
one yeah m27
is our moving average of the squared

987
02:11:02,850 --> 02:11:12,750
gradients so we just go D 2 divided by
square root and preserve that's it okay

988
02:11:12,750 --> 02:11:19,860
thanks I have one question yeah
so the new method that you just

989
02:11:19,860 --> 02:11:27,480
mentioned which is in the process of
getting implemented in yes how different

990
02:11:27,480 --> 02:11:34,739
is it from here okay let's do that so to
understand Adam W we have to understand

991
02:11:34,739 --> 02:11:39,510
wait okay and maybe we'll learn more
about that later let's see how we go now

992
02:11:39,510 --> 02:11:46,440
with great okay
so the idea is that when you have lots
and lots of parameters like we do with

993
02:11:48,570 --> 02:11:54,330
you know most of the neural Nets we
train you very often have like more

994
02:11:54,330 --> 02:11:57,989
parameters and data points or you know
like regularization becomes important

995
02:11:57,989 --> 02:12:03,360
and we've learnt how to avoid
overfitting by using dropout right which

996
02:12:03,360 --> 02:12:08,910
randomly deletes some activations in the
hope that's going to learn some kind of

997
02:12:08,910 --> 02:12:14,100
more resilient set of weights there's
another kind of Ritter ization we can

998
02:12:14,100 --> 02:12:19,140
use called weight decay or l2
regularization and it's actually comes

999
02:12:19,140 --> 02:12:23,400
kind of as a kind of classic statistical
technique and the idea is that we take
our loss function right so we take out

1000
02:12:26,130 --> 02:12:32,880
like arrow squared loss function and we
add an additional piece to it let's add

1001
02:12:32,880 --> 02:12:39,270
weight decay right now the additional
piece we add is to basically add the

1002
02:12:39,270 --> 02:12:49,619
square of the weights so we'd say plus B
squared plus a squared
okay that is now wait 2 K or L tree

1003
02:12:55,960 --> 02:13:04,840
regularization and so the idea is that
now the the loss function wants to keep

1004
02:13:04,840 --> 02:13:10,750
the weight small because increasing the
weights makes the loss worse and so it's

1005
02:13:10,750 --> 02:13:15,909
only going to increase the weights if
the loss improves by more than the

1006
02:13:15,909 --> 02:13:19,630
amount of that penalty and in fact to
make this weight to get to proper weight

1007
02:13:19,630 --> 02:13:24,219
decay
we then need some multiplier yeah right

1008
02:13:24,219 --> 02:13:31,719
so if you remember back in our here we
said weight decay equals W d5e neg 4

1009
02:13:31,719 --> 02:13:39,159
okay so to actually use the same way to
K I would have to multiply by 0.005 all

1010
02:13:39,159 --> 02:13:47,679
right so that's actually now the same
weight okay so if you have a really high

1011
02:13:47,679 --> 02:13:52,150
weight decay that it's going to set all
the parameters to zero so it'll never

1012
02:13:52,150 --> 02:13:57,550
over fit right because it can't set any
parameter to anything and so as you

1013
02:13:57,550 --> 02:14:02,770
gradually decrease the weight decay a
few more weights can actually be used

1014
02:14:02,770 --> 02:14:07,210
right but the ones that don't help much
it's still going to leave at zero or

1015
02:14:07,210 --> 02:14:14,469
close to zero right so that's what
that's what weight decay is is is

1016
02:14:14,469 --> 02:14:21,780
literally to change the loss function to
a D in this sum of squares of weights

1017
02:14:21,780 --> 02:14:28,809
times some parameter some hyper
parameter I should say the problem is

1018
02:14:28,809 --> 02:14:35,829
that if you put that into the loss
function as I have here then it ends up
in the moving average of gradients and

1019
02:14:38,139 --> 02:14:42,940
the moving average of Squared's of
gradients for atom right and so
basically we end up when there's a lot

1020
02:14:46,929 --> 02:14:53,559
of variation we end up decreasing the
amount of weight decay and if there's

1021
02:14:53,559 --> 02:14:56,920
very little variation we end up
increasing the amount of weight decay so

1022
02:14:56,920 --> 02:15:02,739
we end up basically saying penalize
parameters you know weights that are
really

1023
02:15:03,070 --> 02:15:09,580
hi unless their gradient varies a lot
which is never what we intended right

1024
02:15:09,580 --> 02:15:15,460
that's just not not the plan at all so
the trick with Adam W is we basically

1025
02:15:15,460 --> 02:15:22,000
remove weight decay from here so it's
not in the last function it's not in the

1026
02:15:22,000 --> 02:15:28,480
G not in the G squared and we move it so
that instead it's it's it's added

1027
02:15:28,480 --> 02:15:32,110
directly to the when we update with the
learning rate

1028
02:15:32,110 --> 02:15:36,400
it's out of there instead so in other
words it would be we would put the

1029
02:15:36,400 --> 02:15:40,300
weight decay or I should a gradient of
the weight decay in here when we

1030
02:15:40,300 --> 02:15:47,410
calculate the new a mu V so it never
ends up in our G M G squared so that was

1031
02:15:47,410 --> 02:15:51,970
like a super fast description which will
probably only make sense if you listen

1032
02:15:51,970 --> 02:15:57,340
to a three or four times on the video
and then talk about it on the forum yeah

1033
02:15:57,340 --> 02:16:01,960
but if you're interested let me know and
we can also look at Ann Ann's code

1034
02:16:01,960 --> 02:16:11,440
that's implemented yes and you know the
the idea of using weight decay is it's a

1035
02:16:11,440 --> 02:16:17,410
really helpful regularizer because it's
basically this way that we can kind of

1036
02:16:17,410 --> 02:16:26,950
stay like you know please don't increase
any of the weight values unless the you

1037
02:16:26,950 --> 02:16:34,510
know improvement in the loss is worth it
and so generally speaking pretty much

1038
02:16:34,510 --> 02:16:40,150
all state of the art models have both
dropout and weight decay and I don't

1039
02:16:40,150 --> 02:16:46,540
claim to know like how to set each one
and how much of H to use to say like you

1040
02:16:46,540 --> 02:16:53,200
it's worth trying both to go back to the
idea of embeddings is there any way to

1041
02:16:53,200 --> 02:16:57,460
interpret the final to reduce it
embeddings like absolutely we're gonna
look at that next week I've it's super

1042
02:16:59,170 --> 02:17:03,130
fun it turns out that you know we'll
learn what some of the worst movies of

1043
02:17:03,129 --> 02:17:05,550
all time

1044
02:17:06,620 --> 02:17:11,460
it's Letham it's that John Travolta
Scientology once my battleship earth or
something I think that was like the

1045
02:17:12,660 --> 02:17:21,420
worst movie of all time according to our
beds to many recommendations for scaling

1046
02:17:21,420 --> 02:17:27,030
the l2 penalty or is that kind of based
on how how wide the notes are how many

1047
02:17:27,030 --> 02:17:32,640
notes about III have no suggestion at
all like I I kind of look for like
papers or cackle competitions or

1048
02:17:35,280 --> 02:17:40,350
whatever similar and try to set up
frankly the same it seems like in a

1049
02:17:40,350 --> 02:17:46,080
particular area like computer vision
object recognition it's like somewhere

1050
02:17:46,080 --> 02:17:50,340
between one in neck four or one in egg
five seems to work you know

1051
02:17:50,340 --> 02:17:57,120
actually in the Adam W paper the authors
point out that with this new approach it

1052
02:17:57,120 --> 02:18:00,480
actually becomes like it seems to be
much more stable as to what the right
way to K amounts are so hopefully now

1053
02:18:02,459 --> 02:18:06,149
when we start playing with it
we'll be able to have some definitive

1054
02:18:06,150 --> 02:18:10,680
recommendations by the time we get to
part two all right well that's nine

1055
02:18:10,680 --> 02:18:17,280
o'clock so this week you know practice
the thing that you're least familiar

1056
02:18:17,280 --> 02:18:21,060
with so if it's like jacobians and
Hessians read about those if it's
broadcasting read about those if it's

1057
02:18:23,309 --> 02:18:26,969
understanding python ooo read about that
you know try to implement your own

1058
02:18:26,969 --> 02:18:32,219
custom layers read the faster higher
layers you know and and talk on the

1059
02:18:32,219 --> 02:18:39,439
forum about anything that you find weird
or confusing alright see you next week

