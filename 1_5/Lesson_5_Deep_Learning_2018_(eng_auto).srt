1
00:00:00,030 --> 00:00:05,784
welcome back so we had a busy lesson

2
00:00:05,884 --> 00:00:27,834
last week and I was really thrilled to
see actually one of our masters students
here at USF actually actually took what
we learned took what we learned with
structured deep learning and turned it
into a blog post which as I suspected

3
00:00:27,934 --> 00:00:37,824
has been incredibly popular because it's
just something people didn't know about
and so it actually ended up getting
picked up by the towards data science

4
00:00:37,924 --> 00:00:46,840
publication which I quite liked actually
if you're interested in keeping up with
what's going on a data science it's
quite good medium publication and so

5
00:00:46,940 --> 00:00:58,195
Karen talked about structured deep
learning and basically introduced you
know the the the basic ideas that we
learned about last week and it got

6
00:00:58,295 --> 00:01:05,452
picked up quite quite widely one of the
one of the things I was pleased to say
actually sebastian ruder who actually
mentioned in last week's class as being

7
00:01:05,552 --> 00:01:16,590
one of my favorite researchers tweeted
it and then somebody from stitch fix
said oh yeah we've actually been doing
that for ages which is kind of cute I I

8
00:01:16,590 --> 00:01:23,125
kind of know that this is happening in
industry a lot and I've been telling
people this is happening in industry a
lot but nobody's been talking about it

9
00:01:23,225 --> 00:01:31,509
and now the Karen's kind of published a
blog saying hey check out this cool
thing and they all stitch fixes like
yeah we're doing that already so so

10
00:01:31,609 --> 00:01:46,310
that's been great great to see and I
think there's still a lot more that can
be dug into with this structured people
learning stuff you know to build on top
of Karen's post would be that maybe like
experiment with some different datasets

11
00:01:46,410 --> 00:01:57,429
maybe find some old careful competitions
and see like there's some competitions
that you could now win with this or some
which doesn't work for would be equally
interesting and also like just

12
00:01:57,529 --> 00:02:13,970
experimenting a bit with different
amounts of dropout different layer sizes
you know because nobody much is written
about this I don't think there's been
any blog posts about this before that
I've seen anywhere
there's a lot of unexplored territory so

13
00:02:13,970 --> 00:02:21,950
I think there's a lot we could we could
build on top of here and there's
definitely a lot of interest as well one
person on Twitter saying this is what
I've been looking for for ages another

14
00:02:25,310 --> 00:02:36,940
thing which I was pleased to see is
Nikki or who we saw his cricket versus
baseball predictor as well as his a
currency predictor after less than one

15
00:02:36,940 --> 00:02:54,545
went on to download something a bit
bigger which was to download a couple of
hundred of images of actors and he
manually went through and checked which
well I think first of all he like used
Google to try and find ones with glasses
and ones without then he manually went
through and checked that that they put

16
00:02:54,645 --> 00:03:06,724
in the right spot and this was a good
example of one where vanilla ResNet
didn't do so well with just the last
layer and so what Nikhil did was he went
through and tried on freezing the layers

17
00:03:06,824 --> 00:03:11,440
and using differential learning rates
and got up to a hundred percent accuracy

18
00:03:11,709 --> 00:03:22,590
and the thing I like about these things
that Nikhil was doing is the way he's
he's not downloading a kegel data set
he's like deciding on a problem that
he's going to try and solve he's going

19
00:03:22,690 --> 00:03:30,930
from scratch from google and he's
actually got a link here even to the
suggested way to help you download
images from Google so I think this is

20
00:03:31,030 --> 00:03:41,379
great and actually gave a talk just this
afternoon at singularity University to
an executive team of one of the world's
largest telecommunications companies and

21
00:03:41,479 --> 00:03:56,220
actually show them this post because the
folks there were telling me that that
all the vendors that come to them and
tell them they need like millions of
images and huge data centers will have
hardware and you know they have to buy a
special software that only these vendors

22
00:03:56,320 --> 00:04:03,960
can provide and I said like actually
this person has been doing it of course
for three weeks now and look at what
he's just done with a computer that cost

23
00:04:04,060 --> 00:04:13,140
him 60 cents an hour and they were like
they were so happy to hear that like
okay they're you know this actually is
in the reach of normal people I'm

24
00:04:13,240 --> 00:04:20,030
assuming Nikhil is a normal person I
haven't actually
and if your proudly abnormal Nicole I

25
00:04:20,029 --> 00:04:31,650
apologize I actually went and actually
had a look at his cricket classifier and
I was really pleased to see that his
code actually is the exact same code
that were used in Lesson one I was

26
00:04:31,750 --> 00:04:42,852
hoping that would be the case you know
the only thing he changed was the number
of epochs I guess so this idea that we
can take those four lines of code and
reuse it to do other things that's
definitely turned out to be true and so

27
00:04:42,952 --> 00:04:50,894
these are good things to show like it
yeah your organization if you're
anything like the executives of this big
company I spoke to today there'll be a

28
00:04:50,994 --> 00:05:04,820
certain amount of like not to surprise
but almost like pushback like if this
was true somebody does it all that
message she said if this was true
somebody would have told us so like why
isn't everybody doing this already so

29
00:05:04,820 --> 00:05:15,885
we'd like it I think you might have to
actually show them you know maybe you
can build your own there's some internal
data you've got at work or something
like here it is you know didn't cost me

30
00:05:15,985 --> 00:05:28,700
anything it's all finished fiddly or
badly I don't know how to pronounce his
name correctly has done another very
nice post on just an introductory post
on how we train neural networks and I've

31
00:05:28,700 --> 00:05:40,340
wanted to point this one out as being
like I think this is one of the
participants in this course who has got
a particular knack for technical
communication and I think we can all
learn from you know from his post about
about good technical writing what I

32
00:05:43,280 --> 00:05:52,220
really like particularly is that he he
assumes almost nothing like he has a
kind of a very chatty tone and describes
everything but he also assumes that the

33
00:05:52,220 --> 00:06:03,610
reader is intelligent but you know so
like he's not afraid to kind of say
here's a paper or here's an equation or
or whatever but then he's going to go
through and tell you exactly what that
equation means so it's kind of like this

34
00:06:03,710 --> 00:06:11,450
nice mix of like writing for
respectfully for an intelligent audience
but also not assuming any particular

35
00:06:11,450 --> 00:06:21,480
background knowledge so then I made the
mistake earlier this week of posting a
picture of my first placing on the

36
00:06:21,580 --> 00:06:31,025
Carroll seedlings competition at which
point five other fast AI students posted
their pictures of them pass
over the next few days so this is the

37
00:06:31,125 --> 00:06:41,870
current leaderboard for the cattle plant
seedlings competition I believe the
product top six are all fast AI students
or in the worst of those teachers and so

38
00:06:41,970 --> 00:07:06,212
I think this is like a really Oh James
is just passed he was first this is a
really good example of like what you can
do but this is trying to think it was
like a small number of thousands of
images and most of the images were only
were less than a hundred pixels by a
hundred pixels and yet week you know I

39
00:07:06,312 --> 00:07:16,115
bet my approach was basically to say
let's just run through the notebook we
have pretty much default took the I

40
00:07:16,215 --> 00:07:23,170
don't know an hour and I'm I think the
other students doing a little bit more
than that but not a lot more and

41
00:07:23,170 --> 00:07:48,520
basically what this is saying is yeah
these these techniques work pretty
reliably to a point where people that
aren't using the fast I know libraries
you know literally really struggling
let's just pick off these are first aid
a students might have to go down quite a
way so I thought that was very
interesting and really really cool so

42
00:07:48,520 --> 00:07:55,510
today we are going to start what I would
kind of call like the second half of

43
00:07:55,510 --> 00:08:09,067
this course so the first half of this
course is being like getting through
like these are the applications that we
can use this for here's kind of the code
you have to write here's a fairly high

44
00:08:09,167 --> 00:08:13,590
level ish description of what it's doing
and we're kind of we're kind of done for

45
00:08:17,830 --> 00:08:27,245
that bit and what we're now going to do
is go in reverse we're going to go back
over all of those exact same things
again but this time we're going to dig
into the detail of every one and we're

46
00:08:27,345 --> 00:08:35,590
going to look inside the source code of
the first idea library to see what it's
doing and try to replicate that so in a

47
00:08:35,590 --> 00:08:45,370
students like there's not going to be a
lot more
best practices to show you like I've
kind of shown you the best best

48
00:08:45,370 --> 00:08:52,085
practices I know but I feel like for us
to now build on top of those to debug
those models to come back to part two

49
00:08:52,185 --> 00:08:58,780
where we're going to kind of try out
some new things you know it really helps
to understand what's going on behind the

50
00:08:58,780 --> 00:09:12,625
scenes okay so the goal here today is
we're going to try and create a pretty
effective collaborative filtering model
almost entirely from scratch so we'll

51
00:09:12,725 --> 00:09:19,330
use the kind of we'll use PI torch as a
automatic differentiation tool and
there's a GPU programming tool and not

52
00:09:19,330 --> 00:09:24,977
very much else we'll try not to use its
neural net features we'll try not to use
fast AI library anymore than necessary

53
00:09:25,077 --> 00:09:35,800
so that's the goal so let's go back and
you know we only very quickly know
collaborative filtering last time so
let's let's go back and have a look at

54
00:09:35,800 --> 00:09:45,155
collaborative filtering and so we're
going to look at this movie lens data
set so the movie lens data set basically

55
00:09:45,255 --> 00:09:58,900
is a list of ratings it's got a bunch of
different users that are represented by
some ID and a bunch of movies that are
represented by some ID and rating it
also has a timestamp

56
00:09:58,900 --> 00:10:12,650
I haven't actually ever tried to use
this I guess this is just like what what
time did that person read that movie so
that's all we're going to use for
modelling is three columns user ID movie

57
00:10:12,750 --> 00:10:28,410
ID and rating and so thinking of that in
kind of structured data terms user ID
and movie ID would be categorical
variables we have two of them and rating
would be a with the independent variable

58
00:10:28,410 --> 00:10:35,330
we're not going to use this for modeling
but we can use it for looking at stuff
later we can grab a list of the names of

59
00:10:35,430 --> 00:10:46,089
the movies as well and reproduce this
genre information I haven't tried to be
interested if during the week anybody
tries it and finds it helpful my guess
is you might not find it helpful

60
00:10:46,089 --> 00:10:55,180
we'll see so in order to kind of look at
this better I just grabbed
the users that have watched the most

61
00:10:57,880 --> 00:11:03,820
movies and the movies that have been the
most watched and made a crosstab of it

62
00:11:03,820 --> 00:11:12,680
right so this is exactly the same data
but it's a subset and now rather than
being user movie rating we've got user

63
00:11:12,780 --> 00:11:18,557
movie rating and so some users haven't
watched some of these movies that's why

64
00:11:18,657 --> 00:11:37,955
some of these okay then I copied that
into Excel and you'll see there's a
thing called collab your XLS if you
don't see it there now I'll make sure I
put it there back tomorrow and here is
where I've copied that table okay so as

65
00:11:38,055 --> 00:12:09,790
I go through this like setup of the
problem and kind of how its described
and stuff if you're ever feeling lost
feel free to ask either directly or
through the forum if you ask through the
forum and somebody answers there I want
you to answer it here but if somebody
else asks a question you would like
answered of course just like it and your
network keep an eye out for that because

66
00:12:09,790 --> 00:12:19,803
kind of that's we're digging in to the
details of what's going on behind the
scenes it's kind of important that at
each stage you feel like okay I can see

67
00:12:19,903 --> 00:12:47,420
what's going on okay so we can actually
not going to build a neural net to start
with instead we're going to do something
called a matrix factorization the reason
we're not going to build a neural net to
start with is that it so happens there's
a really really simple kind of way of
solving these kinds of problems which
I'm going to show you and so if I scroll

68
00:12:47,520 --> 00:12:57,800
down I've basically what I've got here
is the same the same thing but this time
these are my predictions rather than my
actuals and I'm going to show you how I

69
00:12:57,900 --> 00:13:18,645
created these predictions okay so here
my actuals right here my predictions and
then down here we have
our score which is the sum of the
different squared average square root
okay so this is I are MSE down here okay

70
00:13:18,745 --> 00:13:25,005
so on average we're randomly initialized
model is out by 2.8 so let me show you

71
00:13:25,105 --> 00:13:41,560
what this model is and I'm going to show
you by saying how do we guess how much
user ID number 14 likes movie ID number
27 and the prediction here this is just
at this stage this is still random is

72
00:13:41,560 --> 00:14:05,990
0.9 1 so how we calculate 0.9 1 and the
answer is we're taking it as this vector
here dot product with this vector here
so dot product means 0.71 times 0.1 9
plus 0.8 1 times point 6 3 plus point 7
volt plus point 3 1 and so forth and in

73
00:14:05,990 --> 00:14:22,509
you know linear algebra speak because
one of them is a column and one of them
is a row this is the same as a matrix
product so you can see here I've used
the Excel fashion matrix multiplier and
that's my prediction having said that if

74
00:14:22,609 --> 00:14:33,050
the original rating doesn't exist at all
then I'm just going to set this to 0
right because like there's no error in
predicting something that hasn't

75
00:14:34,220 --> 00:14:46,149
happened okay so what I'm going to do is
I'm basically going to say alright
everyone of my right rate my predictions
is not going to be a neural net it's
going to be a single matrix

76
00:14:46,249 --> 00:15:04,430
multiplication all right now the matrix
multiplication that it's doing is
basically in practice is between like
this matrix and this matrix right so
each one of these is a single part of

77
00:15:04,430 --> 00:15:28,165
that so I randomly initialize these
these are just random numbers that I've
just pasted in here so I've basically
started off with two random matrices and
I've said let's
assume for the time being that every
rating can be represented as the matrix

78
00:15:28,265 --> 00:15:43,135
product of those two so then in Excel
you can actually do a gradient descent
you have to go to your options to the
add-ins section and check the box to say
turn it on and once you do you'll see
there's something there called solver

79
00:15:43,235 --> 00:16:07,525
and if I go solver it says okay what's
your objective function and you just
choose the cell so in this case we chose
the cell that contains that repeats
grade error and then it says okay what
do you want to change and you can see
here we've selected this matrix and this
matrix and so it's going to do a

80
00:16:07,625 --> 00:16:20,110
gradient descent for us by changing
these matrices to try and in this case
minimize this min minimize this Excel so
right GRG nonlinear is a gradient just

81
00:16:20,520 --> 00:16:33,985
yet so I'll say solve and you'll see it
starts at 2.8 and then down here you'll
see that numbers drain down it's not
actually showing us what it's doing but
we can see that the numbers going down

82
00:16:34,085 --> 00:16:43,840
so this has kind of got a near or nettie
feel to it in that we're doing like a
matrix product and we're doing a
gradient descent but we don't have a

83
00:16:43,940 --> 00:16:51,630
nonlinear layer and we don't have a
second linear layer on top of that so we
don't get to call this deep learning so

84
00:16:51,630 --> 00:17:00,150
things where people do like deep
learning each things where they have
kind of matrix products and gradient
descents but it's not deep people tend
to just call that shallow learning okay

85
00:17:01,830 --> 00:17:17,189
so we're doing this chattering yeah all
right so I'm just going to go ahead and
press escape to stop it because I'm sick
of waiting and so you can see we've now
got down to the 0.39 all right so for

86
00:17:17,189 --> 00:17:36,650
example it guessed that movie 72 for
sorry movie 27 for user seventy two
would get 4.4 for rating 2772 and
actually got a four ready so you can see
like it's it's it's doing something
quite useful

87
00:17:36,650 --> 00:18:11,190
so why is it doing something quite
useful I mean something to note here is
the number of things we're trying to
predict here is there's 225 of them
right and the number of things we're
using to predict is that times two so
hundred and fifty of them so it's not
like we can just exactly fit we actually
have to do some kind of machine learning
here so basically what this is saying is
that there does seem to be some way of
making predictions in this way and so

88
00:18:11,190 --> 00:18:23,725
for those of you that have done some
linear algebra and this is actually a
matrix decomposition normally in linear
algebra you would do this using a
analytical technique or using some
techniques that are specifically

89
00:18:23,825 --> 00:18:29,430
designed for this purpose but the nice
thing is that we can use gradient
descent to solve pretty much everything

90
00:18:29,430 --> 00:18:36,960
including this I don't like to so much
think of it from a linear algebra point
of view though I like to think of it
from an insured point of view which is

91
00:18:36,960 --> 00:18:56,225
this let's say movie sorry let's say
movie id 27 is Lord of the Rings part 1
and let's say move and so let's say
we're trying to make that prediction for
user 2072 are they going to like Lord of
the Rings part 1 and so conceptually

92
00:18:56,325 --> 00:19:11,250
that particular movie maybe there's like
this 4
so there's 5 numbers here and we could
say like well what if the first one was
like how much is it sci-fi and fantasy

93
00:19:11,250 --> 00:19:21,390
and the second one is like how recent a
movie and how much special effects is
there you know and the one at the top
might be like how dialogue-driven is it

94
00:19:21,390 --> 00:19:27,820
right like let's say those kind of five
these five numbers represented
particular things about the movie and so

95
00:19:27,920 --> 00:19:45,265
if that was the case then we could have
the same five numbers for the user
saying like ok how much does the use of
like sci-fi fantasy how much does the
user like modern modern CGI driven
movies how much does this give us a like

96
00:19:45,365 --> 00:19:54,310
dialogue different movies and so if you
then took that
cross-product you would expect to have a
good model right would expect to have a

97
00:19:54,410 --> 00:20:13,060
reasonable reading now the problem is we
don't have this information for each
user we don't have the information for
each movie so we're just going to like
assume that this is a reasonable kind of
way of thinking about this system and
let's unless stochastic gradient descent
try and find these models right so so in

98
00:20:13,160 --> 00:20:36,030
other words these these factors we call
these things factors these factors and
we call them factors because you can
multiply them together to create this
not they're factors and how many
addresses these factors we call them
latent factors because they're not
actually this is not actually a vector
that we've like named and understood and

99
00:20:36,030 --> 00:20:46,740
like entered in manually we've kind of
assumed that we can think of movie
ratings this way we've assumed that we
can think of them as a dot product of

100
00:20:46,740 --> 00:20:53,605
some particular features about a movie
and some particular features of to look
what users like those kinds of movies

101
00:20:53,705 --> 00:21:06,175
right and then we've used gradient
descent to just say okay try and find
some numbers that work so that's that's
basically the technique right and it's

102
00:21:06,275 --> 00:21:14,889
kind of the end and the entirety is in
this printing right so that is
collaborative filtering using what we
call probabilistic matrix factorization

103
00:21:14,989 --> 00:21:26,490
and as you can see the whole thing is
easy to do in an excel spreadsheet and
the entirety of it really is this single
thing which is a single matrix
multiplication plus randomly

104
00:21:26,490 --> 00:21:38,250
initializing if it would be better to
cap this to 0 and 5 maybe yes yeah and

105
00:21:38,250 --> 00:21:46,380
we're gonna do that later right there's
a whole lot of stuff we can do to
improve this this is like our simple as
possible starting point all right so so

106
00:21:46,380 --> 00:21:50,850
what we're going to do now is we're
going to try and implement this in
Python and run it on the whole data set

107
00:21:54,750 --> 00:22:03,050
another question is how do you figure
out how many you know how it's clear how
long are the matrix

108
00:22:03,150 --> 00:22:43,220
five yeah yeah so something to think
about given that this is like movie 49
right and we're looking at a rating for
movie 49 think about this this is
actually at embedding matrix and so this
length is actually the size of the
embedding matrix I'm not saying this is
an analogy I'm saying it literally this
is literally an embedding mattress we
could have a one hot encoding where 72
where a one is in the 72nd position and
so we'd like to look it up and it would
return this list of five numbers so the

109
00:22:43,220 --> 00:22:53,780
question is actually how do we decide on
the dimensionality of our embedding
vectors and the answer to that question
is we have no idea we have to try a few

110
00:22:53,780 --> 00:23:10,410
things and see what was the underlying
concept is you need to pick an embedding
dimensionality which is enough to
reflect the kind of true complexity of
this causal system but not so big that

111
00:23:10,510 --> 00:23:17,460
you have too many parameters that it
could take forever to Tehran or even
with regularization in my overfit so

112
00:23:17,560 --> 00:23:26,865
what does it mean when the factor is
negative that the factor being negative

113
00:23:26,965 --> 00:23:34,610
in the movie case would mean like this
is not dialogue-driven in fact it's like
the opposite dialogue here is terrible a

114
00:23:34,610 --> 00:23:41,540
negative for the user would be like I
actually dislike modern CGI movies so

115
00:23:41,540 --> 00:23:49,645
it's not from zero to whatever it's the
range of score it'd be negative this is
a range of score even like no net Maxim

116
00:23:49,745 --> 00:23:54,447
no there's no constraints at all here
these are just standard embedding

117
00:23:54,547 --> 00:24:19,340
matrices questions so first question is
why do what why can we trust this
embeddings because like if you take a
number six it can be expressed as 1 into
6 or like 6 into 1 or 22 3 & 3 into 2
all so you're saying like we could like
reorder these higher
hardly the value itself might be
different as long as the product is

118
00:24:21,410 --> 00:24:33,380
something well but you see we're using
gradient descent to find the best
numbers so like once we've found a good
minimum the idea is like yeah there are

119
00:24:33,380 --> 00:24:42,720
other numbers but they don't give you as
good an objective value and of course we
should be checking that on a validation
set really which we'll be doing in the

120
00:24:42,820 --> 00:24:50,240
Python version okay and the second
question is when we have a new movie or
a new user to be a 30 trainer model that

121
00:24:50,240 --> 00:24:56,365
is a really good question and there
isn't a straightforward answer to that
time permitting will come back but

122
00:24:56,465 --> 00:25:11,000
basically you would need to have like a
kind of a new user model or a new movie
model that you would use initially and
then over time yes you would then have
to retrain the model so like I don't

123
00:25:11,000 --> 00:25:28,110
know if they still do it but Netflix
used to have this thing that when you
were first on boarded on Netflix it
would say like what movies do you like
and you'd have to go through and let's
say a bunch of movies you like and it
would then my train is moral just find

124
00:25:28,210 --> 00:25:38,895
the nearest movie yeah you could use
nearest neighbors for sure but the thing

125
00:25:38,995 --> 00:25:50,685
is initially at least in this case we
have no columns to describe a movie so
if you had something about like the
movies genre release date who was in it
or something you could have some kind of

126
00:25:50,785 --> 00:26:05,235
non collaborative filtering model and
that's kind of what I meant a new movie
model you have to have some some kind of
predictors okay so a lot of this is

127
00:26:07,635 --> 00:26:17,010
going to look familiar and and the way
I'm going to do this is again it's kind
of this top-down approach we're going to
start using a few features of Pi torch

128
00:26:17,110 --> 00:26:26,610
and fast AI and gradually we're going to
redo it
a few times in a few different ways kind
of doing a little bit deeper each time

129
00:26:26,710 --> 00:26:39,860
um regardless we do need a validation
set
so we can use our standard
cross-validation indexes approach to
grab a random set of ID's this is

130
00:26:39,860 --> 00:26:47,050
something called weight decay which
we'll talk about later in the course for
those of you that have done some machine
learning it's l2 regularization

131
00:26:47,050 --> 00:26:53,742
basically and this is where we choose
how big a embedding matrix do we want
okay

132
00:26:53,842 --> 00:27:06,375
so again you know here's where we get
our model data object from CSV passing
in that ratings file which remember

133
00:27:06,475 --> 00:27:28,425
looks like that okay so you'll see like
stuff tends to look pretty familiar
after a while and then you just have to
pass in the what are your rows
effectively what are your columns
effectively and what are your values
effectively alright so any any

134
00:27:28,525 --> 00:27:33,390
collaborative filtering recommendation
system approach there's basically a

135
00:27:33,490 --> 00:27:40,660
concept of like you know a user and an
item now they might not be users and
items like if you're doing the

136
00:27:40,660 --> 00:27:53,530
Ecuadorian groceries competition there
are stores and items and you're trying
to predict how many things are you going
to sell at this store of this type but

137
00:27:53,530 --> 00:28:09,045
generally speaking just this idea of
like you've got a couple of kind of high
cardinality categorical variables and
something that you're measuring and
you're kind of conceptualizing and
saying okay we could predict the rating
we can predict the value by doing this

138
00:28:09,145 --> 00:28:26,800
this dot for that interestingly this is
kind of relevant to that that last
question or suggestion an identical way
to think about this what I've expressed
this is to say when we're deciding
whether user 72 will like movie

139
00:28:26,900 --> 00:28:43,635
twenty-seven it's basically saying which
other users liked movies that 72 liked
and which other movies were liked by
people like you

140
00:28:43,735 --> 00:28:56,475
user 72 it turns out that these are
basically two ways of saying the exact
same thing so basically what
collaborative filtering is doing you
know kind of conceptually is to say okay

141
00:28:56,575 --> 00:29:11,670
this movie and this user which other
movies are similar to it in terms of
like similar people enjoyed them and
which people are similar to this person
based on people that like the same kind
of movies so that's kind of the

142
00:29:11,770 --> 00:29:21,240
underlying structure at any time there's
an underlying structure like this that
kind of collaborative filtering approach
is likely to be useful okay so so you

143
00:29:21,340 --> 00:29:28,460
yeah so there's basically two parts the
two bits of your thing that you're
factoring and then the the value of the

144
00:29:28,460 --> 00:29:34,310
dependent variable so as per usual we
can take our model data and ask for a

145
00:29:34,310 --> 00:29:48,285
learner from it and we need to tell it
what size of any matrix to use how many
sorry what validation set index is to
use what batch size to use and what
optimizer to use and we're going to be

146
00:29:48,385 --> 00:29:52,842
talking more about optimizes surely we
want to Adam today Adam next week or the

147
00:29:52,942 --> 00:30:03,210
week after and then we can go ahead and
say fit alright and it all looks pretty
similar interest is usually

148
00:30:03,310 --> 00:30:10,905
interestingly I only had to do three
pops like this kind of model seem to
Train
super quickly you can use the learning

149
00:30:11,005 --> 00:30:15,480
rate finder as per usual all the stuff
you're familiar with will work fine and

150
00:30:15,580 --> 00:30:24,590
that was it so this talk you know about
two seconds the Train there's no free
trained anything's here this is from
random from scratch okay so this is our

151
00:30:24,690 --> 00:30:34,335
validation set and we can compare it we
have this is a mean squared error not a
root mean squared error so we can take a

152
00:30:34,435 --> 00:30:58,880
square root so with that last time I ran
it was point seven seven six and that's
0.88 and there's some benchmarks
available for this data set and when I
scrolled through and found the bench the
best benchmark I could find here from
this recommendation system specific
library they had point nine one so we've
got a better loss in two seconds

153
00:30:58,880 --> 00:31:11,800
already so that's good so that's
basically how you can do collaborative
filtering with the faster I library
without thinking too much but so now

154
00:31:11,900 --> 00:31:23,470
we're going to dig in and try and
rebuild that we'll try and get to the
point that we're getting something
around 0.7 seven point seven eight from
scratch but if you want to do this

155
00:31:23,570 --> 00:31:32,935
yourself at home
you know without worry about the detail
that's you know those three lines of
code here's what you need okay so we can

156
00:31:33,035 --> 00:31:38,095
get the predictions in the usual way and
you know we could for example plot SNS

157
00:31:38,195 --> 00:31:48,960
is Seabourn see one's a really great
flooding library it sits on top of
matplotlib it actually leverages
matplotlib so anything you learn about
matplotlib will help you with SIBO and

158
00:31:48,960 --> 00:31:54,230
it's got a few like nice little plots
like this joint plot here is I'm doing

159
00:31:54,230 --> 00:32:00,120
predictions against against actuals so
these are my actual season my
predictions and you can kind of see the

160
00:32:02,340 --> 00:32:07,192
the shape here is that as we predict
higher numbers they actually are higher
numbers and you can also see the

161
00:32:07,292 --> 00:32:18,420
histogram of the predictions and a
histogram of the ashes so that's kind of
floating that is to show you another
interesting visualization would you

162
00:32:18,420 --> 00:32:23,615
please explain the n factors why it's
set to 50 it's set to 50 because I tried

163
00:32:23,915 --> 00:32:29,352
a few things in the world it's the
dimensionality of the embedding images

164
00:32:29,452 --> 00:32:39,860
or to think for it another way it's like
how you know rather than five it's fit

165
00:32:41,750 --> 00:32:54,715
Jeremy I have a question about suppose
that your recommendation system is more
implicit so you have zeros or ones
instead of just actual numbers right so

166
00:32:54,815 --> 00:33:02,575
basically we would then need to use a
classifier instead of regresa

167
00:33:02,675 --> 00:33:11,650
I have to sample the negative or
something like that so if you don't have
it which is up once let's say like just
kind of implicit feedback

168
00:33:11,750 --> 00:33:15,385
oh I'm not sure we'll get to that one in
this class but what I will say is like

169
00:33:15,485 --> 00:33:21,600
in the case that you just doing
classification rather than regression we
haven't actually built that in the

170
00:33:21,600 --> 00:33:26,315
library yet maybe somebody this week
that wants to try adding it it would
only be a small number of lines of code

171
00:33:26,415 --> 00:33:44,850
you basically have to change the
activation function to be a sigmoid and
you would have to change the criterion
or the loss function to be cross-entropy
rather than rmse and that will give you
a classifier rather than a regresar how

172
00:33:44,850 --> 00:33:50,940
those are the only things you'll have to
change so hopefully somebody this week
won't take up that challenge and by the
time we come back next week we've all

173
00:33:50,940 --> 00:34:04,800
have that working ok so I said that
we're basically doing a dot product
right or you know a dot product is kind
of the vector version I guess of this

174
00:34:04,800 --> 00:34:13,209
matrix product so we're basically doing
each of these things times each of these
things and then add it together so let's

175
00:34:13,309 --> 00:34:22,110
just have a look at how we do that in
Python so we can create a tensor in pi
torch just using this little capital T

176
00:34:22,110 --> 00:34:34,434
thing you can just say that's the first
day I version the full version is torch
dot from I'm pie or something but I've
got to set up so you can possibly pass
in even a list of lists so this is going

177
00:34:34,534 --> 00:34:46,090
to create a torch tensor with one two
three four and then here's a torch
tensor with two to ten ten ok so here
are two more chances

178
00:34:46,190 --> 00:34:52,150
I didn't say doc CUDA so they're not on
the GPU they're sitting on the CPU just

179
00:34:52,250 --> 00:35:19,590
FYI we can multiply them together right
and so anytime you have a mathematical
operator between tensors in numpy or
pipe torch it will do element wise
assuming that they're the same
dimensionality which they are they're
both to about two okay and so here we've
got 2 by 2 is 4 3 by 10 is 30 and so
forth ok so there's a a times B so if

180
00:35:19,590 --> 00:36:10,285
you think about basically what we want
to do here is we want to take
ok so I've got 1 times 2 is 2 2 times 2
is 4 2 plus 4 is 6 and so that is
actually the dot product between 1 2 & 2
4 and then here we've got 3 by 10 is 34
by 40 sorry 4 by 10 is 40 30 and 40 and
70 so in other words a times B dot some
along the first dimension
so that's summing up the columns in
other words across a row okay this thing
here is doing the dot product of each of
these rows with each of these rows so it

181
00:36:10,385 --> 00:36:25,220
makes sense and obviously we could do
that with you know some kind of matrix
multiplication approach but I'm trying
to really do things with this little
special case stuff as possible ok so
that's what we're going to use for our
dot products from now on so basically

182
00:36:28,080 --> 00:36:44,455
all we need to do now is remember we
have the data we have is not in that
crosstab format so in excel we've got it
in this crosstab format but we've got it
here in this listed format here's our
movie rating user movie revenue so

183
00:36:44,555 --> 00:36:50,962
conceptually we want to be like looking
up this user into our embedding matrix
to find their 50 factors looking up that

184
00:36:51,062 --> 00:37:02,680
movie to find their 50 factors and then
take the dot product of those two 50
long vectors so let's do that to do it

185
00:37:02,780 --> 00:37:21,120
we're going to build a layer our own
custom neural net layer that's not right
so the the the more generic vocabulary
we call this is we're going to build a
high torch module okay so a PI torch

186
00:37:21,120 --> 00:37:32,710
module is a very specific thing it's
something that you can use as a layer
and a neural net once you've created
your own height watch module you can
throw it into a mirror on it and a

187
00:37:32,810 --> 00:37:41,470
module works by assuming we've already
got once a cordon
model you can pass in some things in
parentheses and it will calculate it

188
00:37:41,570 --> 00:37:55,405
right so assuming that we already have a
modular product we can instantiate it
like so to create our product object and
we can basically now treat that like a

189
00:37:55,505 --> 00:38:08,369
function right but the thing is it's not
just a function because we'll be able to
do things like take derivatives of it
stack them up together into a big stack
of neural network layers blah blah blah

190
00:38:08,369 --> 00:38:14,730
so it's basically a function that we can
kind of compose very conveniently so

191
00:38:14,730 --> 00:38:23,260
here how do we define a module which as
you can see here returns a dot product
well we have to create a Python class

192
00:38:23,360 --> 00:38:31,410
and so if you haven't done - oo before
you're going to have to learn because
all my torch modules are written in
Python oo and that's one of the things I

193
00:38:33,690 --> 00:38:46,000
really like about PI torch is that it
doesn't reinvent totally new ways of
doing things by tensorflow does all the
time in pi torch that you know really
tend to use pythonic ways to do things

194
00:38:46,100 --> 00:38:55,770
so in this case how do you create you
know some kind of new behavior you
create a Python plus it's so Jeremy

195
00:38:55,770 --> 00:39:07,020
suppose that you have a lot of data not
just a little bit of data you can have
in memory will you be able to use fossae
I to solve glory filtering yes

196
00:39:07,020 --> 00:39:33,560
absolutely
it's it uses mini-batch stochastic
gradient descent which does have a batch
at a time the this particular version is
going to create a panda's data frame and
panda's data frame has to live in memory

197
00:39:33,560 --> 00:39:40,920
having said that you can get easily 512
gig you know instances on Amazon so like

198
00:39:40,920 --> 00:39:46,200
if you had a CSV that was bigger than
512 gig you know that would be
impressive if that did happen I guess

199
00:39:48,510 --> 00:40:03,519
you would have to instead
save that as a be calls array and create
a slightly different version that reads
from a because array just streaming in
or maybe from a desk data frame which
also so it would be easy to do I don't

200
00:40:03,619 --> 00:40:15,549
think I've seen real-world situations
where you have 512 gigabyte
collaborative filtering matrices but
yeah we can do it okay now this is PI

201
00:40:15,649 --> 00:40:32,640
torch specific this next bit is that
when you define like the actual work to
be done which is here return user times
movie dot some you have to put it in a
special method called forward okay and

202
00:40:32,640 --> 00:40:45,549
this is this idea that like it's very
likely you're on that right in a neural
net the thing where you calculate the
next set of activations is called the
the forward pass and so that's doing a

203
00:40:45,649 --> 00:40:54,369
forward calculation the gradients is
called the backward calculation we don't
have to do that because PI torch
calculates that automatically so we just

204
00:40:54,469 --> 00:41:01,529
have to define forward so we create a
new class we define forward and here we
write in our definition of dot product

205
00:41:01,529 --> 00:41:13,979
ok so that's it so now that we've
created this class definition we can
instantiate our model right and we can
call our model and get back the numbers
be expected okay so that's it that's how

206
00:41:16,529 --> 00:41:29,811
we create a custom PI torch layer and if
you compare that to like any other
library around pretty much this is way
easier basically I guess because we're
leveraging what's already in person so

207
00:41:29,911 --> 00:41:34,499
let's go ahead and now create a more
complex module and we're going to

208
00:41:38,039 --> 00:41:49,670
basically do the same thing we've got to
have a forward again we're going to have
our users x movies dot sum but we're
going to do one more thing before hand
which is we're going to create two

209
00:41:49,670 --> 00:42:00,254
embedding matrices and then we're going
to look up our users and our movies in
those inventing matrices so let's go
through and and do that so the first

210
00:42:00,354 --> 00:42:07,054
thing to realize is
that the uses the user IDs and the movie
IDs may not be contiguous you know like

211
00:42:07,154 --> 00:42:16,864
they're maybe they start at a million
and go to a million in 1000 so right so

212
00:42:16,964 --> 00:42:25,952
if we just used those IDs directly to
look up into an embedding matrix we
would have to create an embedding matrix
of size 1 million 1000 right which we

213
00:42:26,052 --> 00:42:38,210
don't want to do so the first thing I do
is to get a list of the unique user IDs
and then I create a mapping from every
user ID to a contiguous integer this

214
00:42:38,310 --> 00:43:03,640
thing I've done here where I've created
a dictionary which maps from every
unique thing to a unique index is well
worth studying
during the week because like it's is
super super handy it's something you
very very often have to do in all kinds
of machine learning all right and so I
won't go through it here it's easy
enough to figure out if you can't figure
it out just ask on the forum anyway so

215
00:43:06,729 --> 00:43:18,439
once we've got the mapping from user to
a contiguous index we then can say let's
now replace the user ID column with that

216
00:43:18,539 --> 00:43:31,639
contiguous index right so pandas dot
apply applies an arbitrary function and
python lambda is how you create an
anonymous function on the fly and this
anonymous function simply returns the NS

217
00:43:31,739 --> 00:43:36,039
through the same thing for movies and so

218
00:43:36,039 --> 00:43:41,319
after that we now have the same ratings
table we had before but our IDs have
been mapped to contiguous integers

219
00:43:43,690 --> 00:43:49,239
therefore they're things that we can
look up into an embedding matrix so

220
00:43:49,239 --> 00:43:54,819
let's get the count of our users in our
movies and let's now go ahead and try

221
00:43:54,819 --> 00:44:00,609
and create our Python version of this
okay

222
00:44:00,609 --> 00:44:21,760
so earlier on when we created our
simplest possible PI torch module
there was no like state we didn't need a
constructor because we weren't like
saying how many users are there or how
many movies are there or how many
factors do we want or whatever right

223
00:44:21,760 --> 00:44:37,850
anytime we want to do something like
this where we're passing in and saying
we want to construct our module with
this number of users and this number of
movies then we need a constructor for

224
00:44:37,850 --> 00:44:50,550
our class and you create a constructor
in Python by defining a dunder init
underscore underscore init underscore
underscore yet special name
so this just creates a constructor and

225
00:44:50,650 --> 00:45:01,930
if you haven't done over before you
wanted to do some study during the week
but it's pretty simple idea this is just
the thing that when we create this
object this is what gets wrong okay

226
00:45:02,030 --> 00:45:16,920
again special python thing when you
create your own constructor you have to
call the parent class constructor and if
you want to have all of the cool
behavior of a PI porch module you get
that by inheriting from an end module

227
00:45:17,020 --> 00:45:27,685
neural net module okay so basically by
inheriting here and calling the
superclass constructor we now have a
fully functioning PI torch layer okay so

228
00:45:27,785 --> 00:45:41,870
now we have to give it some behavior and
so we give us some behavior by storing
some things in it all right so here
we're going to create something called
self dot you users and that is going to

229
00:45:41,870 --> 00:45:55,280
be an embedding layer a number of rows
is an user's number of columns is in
factors so that is exactly this right
the number of rows is M users number of

230
00:45:55,280 --> 00:46:01,880
columns is inventors and then we'll have
to do the same thing for movies okay so

231
00:46:01,880 --> 00:46:09,910
that's going to go ahead and create
these two randomly initialized arrays
however when you randomly initialize

232
00:46:12,920 --> 00:46:33,665
over an array it's important to randomly
initialize it to a reasonable set of
numbers like a reasonable scale right if
we randomly initialize them from like
naught to a million then we would start
out and you know these things would
start out being like you know billions
and billions of
writing and that's going to be very hard
to do gradient descent on so I just kind

233
00:46:33,765 --> 00:46:51,020
of manually figured here like okay about
what size numbers are going to give me
about the right readiness and so we
don't we know we did ratings between
about normal five so if we start out
with stuff between about naught and 0.05

234
00:46:51,020 --> 00:47:13,960
then we're going to get ratings of about
the right level you can easily enough
like that calculate that in in neural
nets there are standard algorithms for
basically doing doing that calculation
and the basic the key algorithm is
something called initialization from

235
00:47:14,060 --> 00:47:34,765
climbing her and the basic idea is that
you take the yeah you basically set the
weights equal to a normal distribution
with a standard deviation which is

236
00:47:34,865 --> 00:47:48,950
basically inversely proportional to the
number of things in the previous layer
and so in our previous layer so in this

237
00:47:48,950 --> 00:48:04,940
case we basically if you basically take
that nor to 0.05 and multiply it by the
fact that you've got 40 things with a 40
or 50 things coming out of it
50 50 things coming out of it and then
you're going to get something about the

238
00:48:04,940 --> 00:48:28,430
right size pi torch has already has like
her initialization class they're like we
don't in normally in real life have to
think about this we can just call the
existing initialization functions but
we're trying to do this all like from
scratch here okay without any special
stuff going on so there's quite a bit of

239
00:48:28,430 --> 00:48:53,774
pi torch notation here so self dot u
we've already set to an instance of the
embedding class it has a dot weight
attribute which contains the actual the
actual embed images
so that contains this the actual
embedding matrix is not a tensor it's a

240
00:48:53,874 --> 00:49:12,284
variable a variable is exactly the same
as a tensor in other words it supports
the exact same operations as a tensor
but it also does automatic
differentiation that's all a variable is
basically to pull the tensor out of a
variable you get its data attribute okay

241
00:49:12,384 --> 00:49:22,484
so this is so this is now the tensor of
the weight matrix of the self dot you're

242
00:49:22,584 --> 00:49:33,419
inventing and then something that's
really handy to know is that all of the
tensor functions in pi torch you can
stick an underscore at the end and that

243
00:49:33,519 --> 00:49:48,344
means do it in place all right so this
is say create a random uniform random
number of an appropriate size for this
tensor and don't return it but actually
fill in that matrix unless okay so

244
00:49:48,444 --> 00:50:12,869
that's a super handy thing to know about
I mean it wouldn't be rocket science
otherwise we would have to have gone
[Music]
okay here's the non in-place version
that's what saves us some typing saves
us some screen noise that's all okay so

245
00:50:12,969 --> 00:50:21,449
now we've got our randomly initialized
embedding weight matrices and so now the

246
00:50:21,549 --> 00:50:31,069
forward I'm actually going to use the
same columnar model data that we used
for Russman and so it's actually going

247
00:50:31,069 --> 00:50:38,579
to be passed both categorical variables
and continuous variables and in this
case there are no continuous variables

248
00:50:38,679 --> 00:50:48,329
so I'm just going to grab the 0th column
out of the categorical variables and
call it users and the first column and
call it movies okay so I'm just kind of

249
00:50:48,429 --> 00:50:58,264
too lazy to create my own I've lots to
do about too lazy out that we do have a
special class with this but I'm trying
to avoid creating a special class so
just going to leverage this columnar

250
00:50:58,364 --> 00:51:21,220
model data plus okay so we can basically
grab our user and movies mini-batches
right and remember this is not a single
user in a single movie this is going to
be a whole mini batch of them we can now
look up that mini batch of users in our
embedding matrix U and the movies in are
embedding matrix okay so this is like

251
00:51:21,320 --> 00:51:31,359
exactly the same is just doing an array
lookup to grab the user ID numbered
value but we're doing that a whole mini
batch at a time right and so it's

252
00:51:31,459 --> 00:51:43,449
because PI torch can do a whole mini
batch at a time with pretty much
everything that we can get really easy
speed up we don't have to write any
loops on the whole to do everything
through our mini batch and in fact if

253
00:51:43,549 --> 00:51:52,509
you do ever loop through your mini batch
manually you don't get GPU acceleration
that's really important to know right so
you never want to loop have a for loop

254
00:51:52,609 --> 00:51:59,339
going through your mini batch you always
want to do things in this kind of like
whole mini batch at a time but pretty

255
00:51:59,339 --> 00:52:05,604
much everything imply torch does things
are holding events at a time so you
shouldn't have to worry about it

256
00:52:05,704 --> 00:52:13,239
and then here's our product just like
before right so having to find that I'm

257
00:52:13,339 --> 00:52:27,684
now going to go ahead and say alright my
X values is everything except the rating
and the timestamp in my writings table
my Y is my rating and then I can just

258
00:52:27,784 --> 00:52:39,546
say okay let's grab a model data from a
data frame using that X and that Y and
here is our list of categorical
variables okay and then so let's now

259
00:52:39,646 --> 00:52:47,519
instantiate that PI torch object alright
so we've now created that from scratch

260
00:52:49,549 --> 00:52:55,469
and then the next thing we need to do is
to create an optimizer so this is part

261
00:52:55,469 --> 00:53:02,640
of pi torch the only fast AI thing here
is this line right because that's like I

262
00:53:02,640 --> 00:53:15,630
don't think showing you how to build
data sets and data load is interesting
enough really we might do that in part
two of the course and
it's actually so straightforward like a
lot of you are already doing it on the
forums so I'm not going to show you that

263
00:53:17,220 --> 00:53:33,610
in this part but if you're interested
feel free to talk on the forums about it
but I'm just going to basically take the
thing that feeds us data is a given
particularly cuz these things are so
flexible right you know if you've got
stuff enough data frame you can just use
this you don't have to rewrite it so

264
00:53:33,710 --> 00:53:39,130
that's the only fast AI thing we're
using so this is a PI torch thing and so

265
00:53:39,230 --> 00:53:51,579
optiom is the thing and pi torch that
gives us an optimizer will be learning
about that very shortly so it's actually
the thing that's going to update our

266
00:53:51,679 --> 00:53:57,939
weights pi torch calls them the
parameters of the model so earlier on we

267
00:53:58,039 --> 00:54:07,859
set model equals embedding dot blah blah
right and because embedding dot derives
from NN module we get all of the pi
torch module behavior and one of the

268
00:54:10,230 --> 00:54:29,319
things we got for free is the ability to
say got parameters so that's pretty
that's pretty any right that's the thing
that basically is going to automatically
give us a list of all of the weights in
our model that have to be updated and so
that's what gets passed to the optimizer

269
00:54:29,419 --> 00:54:38,721
we also passed the optimized at the
learning rate the weight decay which
we'll talk about later and momentum that
we'll talk about later okay one other

270
00:54:38,821 --> 00:54:43,950
thing that I'm not going to do right now
but we will do later is to write a

271
00:54:45,809 --> 00:54:58,230
training loop so the training loop is a
thing that lives for each mini batch and
updates the weight to subtract the
gradient times the moment there's a

272
00:54:58,230 --> 00:55:02,935
function in fast AI which is the
training loop and it's it's pretty

273
00:55:03,035 --> 00:55:25,099
simple here it is right for a POC in
epochs this is just the thing that shows
a progress bar so ignore this for X
comma Y in my training data loader
calculate the loss

274
00:55:25,490 --> 00:55:39,960
print out the lots you know in a
progress bar call any callbacks you have
and at the end call the call the metrics
on the validation alright so there's

275
00:55:39,960 --> 00:55:55,120
there's just eh
Apoc go through each mini batch and do
one step of optimizer step is basically
going to take advantage of this
optimizer but we'll be writing that from

276
00:55:55,220 --> 00:56:06,745
scratch shortly so this is notice we're
not using a learner okay we're just
using a hi book module so this this fit
thing although it's passed to a part of

277
00:56:06,845 --> 00:56:14,700
fast AI it's like lower down the layers
of abstraction now this is the thing
that takes a regular high torch model so

278
00:56:14,700 --> 00:56:33,355
if you ever want to like skip as much
faster eye stuff as possible like you've
got some high torch model you've got
some code on the internet you basically
want to run it that you don't want to
write your own training loop then this
is this is what you want to do you want
to call fast a high speed version and so

279
00:56:33,455 --> 00:56:41,250
what you'll find is like the library is
designed so that you can kind of dig in
at any layer abstraction you like right

280
00:56:41,250 --> 00:56:51,640
and so at this layer of abstraction
you're not going to get things like
stochastic gradient descent with
restarts you're not going to get like
differential learning rates like all

281
00:56:51,740 --> 00:57:02,100
that stuff that's in the learner like
you could do it but you'd have to write
it all about by hand yourself alright
and that's the downside of kind of going
down to this level of abstraction the

282
00:57:02,100 --> 00:57:13,375
upside is that as you saw the code for
this is very simple it's just a simple
training loop it takes a standard 5
torch model so this is like this is a
good thing for us to use here we can we
just call it and it looks exactly like

283
00:57:13,475 --> 00:57:24,230
what we used to see all right we got our
validation and training loss for the 3 e

284
00:57:24,330 --> 00:57:33,190
bus now you'll notice that we wanted
something around 0.76 so we're not there

285
00:57:33,290 --> 00:57:41,369
so in other words the the the default
fast AI collaborative dory
rhythm is doing something smarter than

286
00:57:41,369 --> 00:57:57,960
this so we're going to try and do that
one thing that we can do since we're
calling our you know this lower level
fifth function there's no learning rate
and kneeling we could do our own
learning rate annealing so you can hear
it see here there's a first day I
function called set learning rates you

287
00:57:57,960 --> 00:58:05,275
can pass in a standard height watch
optimizer and pass in your new learning
rate and then call fit again and so this

288
00:58:05,375 --> 00:58:17,185
is how we can let manually do a learning
rate schedule and so you can see we've
got a little bit better 1.13 where you
still got a long way to go okay so I

289
00:58:17,285 --> 00:58:33,669
think what we might do is we might have
a seven minute break and then we're
going to come back and try and improve
this core of it for those who are

290
00:58:36,869 --> 00:58:54,960
interested somebody was asking me the
break for a kind of a quick walkthrough
so this is totally optional but if you
go into the first day I library there's
a model py file and that's where fit is

291
00:58:54,960 --> 00:59:12,204
which we're just looking at which goes
through each epoch in epochs and then
goes through each x and y in the mini
batch and then it calls this step
function so the step function is here

292
00:59:12,304 --> 00:59:39,569
and you can see the key thing is it
calculates the output from the model the
models for M right and so if you
remember our dot product we didn't
actually call model dot forward we just
called model parentheses and that's
because the N n dot module automatically
you know when you call it as if it's a
function it passes it along to forward
okay so that's that's what that's doing

293
00:59:42,059 --> 00:59:51,772
there right and then the rest of this
world will learn about shortly which is
basically doing the the loss function
and
the backward pass okay so for those who

294
00:59:51,872 --> 01:00:00,130
are interested that's that's kind of
gets you a bit of a sense of how the
cone it's structured if you want to look

295
01:00:00,230 --> 01:00:09,780
at it and as I say like the the faster I
code is designed to both be world-class
performance but also pretty easy to read

296
01:00:09,780 --> 01:00:22,840
so like feel free like take a look at it
and if you want to know what's going on
just ask on the forums and if you you
know if you think is anything that could
be clearer let us know because yeah the

297
01:00:22,940 --> 01:00:29,995
code is definitely now we're going to be
digging into the code or in law okay so

298
01:00:30,095 --> 01:00:36,790
let's try and improve this a little bit
and let's start off by improving it in

299
01:00:36,890 --> 01:00:47,830
Excel so you might have noticed here
that we've kind of got the idea that use
a 72
you know like sci-fi modern movies with

300
01:00:47,930 --> 01:00:57,150
special effects you know whatever and
movie number 27 is sci-fi and that
special effects so much dialogue but

301
01:00:57,150 --> 01:01:17,040
we're missing an important case which is
like user 72 is pretty enthusiastic on
the hall and on average rates things
higher and Highland you know and movie
27 you know it's just a popular movie
you know it's just on average its higher

302
01:01:17,040 --> 01:01:29,640
so what would really like is to add a
constant for the user and a constant for
the movie and remember in neural network
terms we call that a bias that's what we
want to add a bias so we could easily do

303
01:01:32,280 --> 01:01:48,050
that and if we go into the bias tab here
we've got the same data as before and
we've got the same latent factors as
before and I've just got one extra row
here and one extra column here and you

304
01:01:48,150 --> 01:02:01,929
won't be surprised here that we now take
these same matrix multiplication as
before and we add in that and we add in
that okay so that's

305
01:02:02,029 --> 01:02:27,630
bias so other than that we've got
exactly the same loss function over here
and so just like before we can now go
ahead and solve that and now our
changing variables include the bias and
we can say solve and if we leave that
for a little while it will come to a
better result than we had before

306
01:02:27,630 --> 01:02:37,019
okay so that's the first thing we're
going to do to improve our model and
there's really very little show just to

307
01:02:37,119 --> 01:02:48,635
make the code a bit shorter I have to
find a function called get embedding
which takes a number of inputs and a
number of factors so the number of rows
and the embedding matrix Nomos they're

308
01:02:48,735 --> 01:02:55,685
both medications creates the embedding
and then randomly initializes it I don't

309
01:02:55,785 --> 01:03:00,575
know why I'm doing negative to positive
here and it zeroed last time honestly it

310
01:03:00,675 --> 01:03:05,765
doesn't matter much as long as it's in
the right ballpark and then we return

311
01:03:05,865 --> 01:03:20,866
that initialized emitting so now we need
not just our users by factors which are
Chuck into you our movies by factors
which I've shocked into M but we also
need users by one which will put into UV
user bias and movies by one which will

312
01:03:20,966 --> 01:03:27,634
put into the movie bias okay so this is
just doing a list comprehension going
through each of the tuples create an

313
01:03:27,734 --> 01:03:34,630
embedding for each of them and putting
them into these things okay so now our

314
01:03:34,630 --> 01:04:10,655
forward is exactly the same as before u
times M sub I mean this is actually a
little confusing because we're doing it
into two steps maybe they make it a bit
easier let's pull this out
put it up here put this in parentheses
okay so maybe that looks a little bit
more familiar all right you times n dot
some that's the same dot product and
then here it is going to add in our user
pious and

315
01:04:10,755 --> 01:04:15,962
our movie bus dot squeeze is the PI
torch thing that adds an additional unit

316
01:04:16,062 --> 01:04:29,840
axis that's not going to make any sense
if you haven't done broadcasting before
I'm not going to do a broadcasting in
this course because we've already done
it
and we're doing it in the machine

317
01:04:29,940 --> 01:04:40,700
learning course but basically in in
short broadcasting is what happens when
you do something like this where um is a
matrix you be self-taught you the users
is a is a vector how do you add a vector

318
01:04:40,800 --> 01:05:00,600
to a matrix and basically what it does
is it duplicates the vector so that it
makes it the same size as the matrix and
the particular way whether it duplicates
it across columns or down rows or how it
does it is called broadcasting the

319
01:05:00,700 --> 01:05:05,280
broadcasting rules are the same as numpy
Pytor didn't actually used to support

320
01:05:05,380 --> 01:05:11,920
broadcasting so I was actually the guy
who first added broadcasting to PI torch
using an ugly hack and then the pipe or

321
01:05:11,920 --> 01:05:17,110
authors did an awesome job of supporting
it actually inside the language so now

322
01:05:17,110 --> 01:05:22,180
you can use the same broadcasting
operations in five torches non-player if
you haven't dealt with this before it's

323
01:05:24,640 --> 01:05:37,390
really important to learn it because
like it's it's kind of the most
important fundamental way to do
computations quickly in the high-end
paid warship it's the thing that lets
you not have to do loops

324
01:05:37,390 --> 01:05:46,870
how could you imagine here if I had to
look through every row of this matrix
and add each did you know this vector to
every row it would be slow the you know
a lot more code and the idea of

325
01:05:50,590 --> 01:05:59,770
broadcasting it actually goes all the
way back to APL which was a language
designed in the 50s by an extraordinary
guy called Ken Iverson yeah APL was

326
01:05:59,770 --> 01:06:10,330
originally designed or written out as a
new type of mathematical notation he has
this great essay called notation as a
tool for thought and the idea was that

327
01:06:10,330 --> 01:06:15,220
like really good notation could actually
make you think of better things and part

328
01:06:15,220 --> 01:06:22,820
of that notation is this idea of
broadcasting I'm incredibly enthusiastic
about it

329
01:06:22,920 --> 01:06:34,832
and we're gonna use it plenty so either
watch the machine learning lesson or you
know google numpy broadcasting for
information anyway

330
01:06:34,932 --> 01:06:45,195
so basically it works reasonably
intuitively we can add on we can add the
vectors to the matrix all right

331
01:06:45,295 --> 01:07:04,445
having done that we're now going to do
one more trick which is I think it was
your net asked earlier about could we
squish the ratings to be between one and
five and the answer is we could right

332
01:07:04,545 --> 01:07:19,455
and specifically what we could do is we
could put it through a sigmoid function
all right
so remind you a sigmoid function looks

333
01:07:19,555 --> 01:07:24,900
like that right and this is that's one

334
01:07:24,900 --> 01:07:51,610
okay we could put it through a secret
function so we could take like four
point nine six and put it through a
sigmoid function and like that you know
that's kind of high so it kind of be
over here somewhere right
and then we could multiply that sigmoid
like the result of that by five for
example all right
and in this case we want it to be
between one and five right so maybe we
would multiply it by four and add one

335
01:07:51,610 --> 01:08:08,710
instance that's the basic idea and so
here is that trick we take the result so
the result is basically the the thing
that comes straight out of the dot
product plus the addition of the biases
and put it through a sigmoid function

336
01:08:08,710 --> 01:08:33,645
now in pi torch basically all of the
functions you can do to tensors are
available inside this thing called
capital F and this is like totally
standard in pi torch it's actually
called torch and or functional but
everybody including all of the pipe
torch Doc's import torch start and end
are functional as capital F all right so

337
01:08:33,745 --> 01:08:39,849
capital F dot sigmoid means a function
called sigmoid that is coming from
tortures functional module right and so

338
01:08:39,949 --> 01:08:48,388
that's going to apply a sigmoid function
for the result so I squish them all
between zero and one using that nice

339
01:08:48,389 --> 01:09:00,084
little shape and then I can multiply
that by five minus one plus four right
and then add on one and that's gonna
give me plumbing between one and five

340
01:09:00,184 --> 01:09:24,569
okay so like there's no need to do this
I could comment it out and it'll still
work right but now it has to come up
with a set of calculations that are
always between one and five right where
else if I leave this in then it's like
makes it really easy it's basically like
oh if you think this is a really good
movie just calculate a really high
number it's a really crappy movies low
number and I'll make sure it's in the

341
01:09:24,569 --> 01:09:37,959
right regions so even though this is a
neural network it's still a good example
of this kind of like if you're doing any
kind of parameter fitting try and make
it so that the thing that you want your
function to return it's like it's easy

342
01:09:38,059 --> 01:09:44,304
for it to return that okay so that's why
we do that that function squishing so we

343
01:09:44,404 --> 01:09:50,589
call this embedding dot bias so we can
create that in the same way as before

344
01:09:50,689 --> 01:10:01,350
you'll see here I'm calling dr. to put
it on the GPU because we're not using
any learner stuff normally it'll all
happen for you but we have to manually
say put it on the GPU this is the same

345
01:10:01,350 --> 01:10:05,580
as before create our optimizer fit
exactly the same as before and these

346
01:10:05,580 --> 01:10:23,770
numbers are looking good all right and
again we'll do a little change to our
learning rate learning rate schedule and
we're down to 0.8 so we're actually
pretty close pretty close so that's the

347
01:10:23,870 --> 01:10:34,885
key steps and this is how this is how
most collaborative filtering is done and

348
01:10:34,985 --> 01:10:47,664
unit reminded me of an important point
which is that this is not strictly
speaking a matrix factorization because
strictly

349
01:10:47,764 --> 01:10:56,170
a matrix factorization would take that
matrix by that matrix to create this

350
01:10:56,270 --> 01:11:14,860
matrix and remembering anywhere that
this is empty like here or here we're
putting in a zero right we're saying if
the original was empty put in a zero

351
01:11:14,960 --> 01:11:22,290
right now normally you can't do that
with normal matrix factorization normal
matrix factorization it creates the

352
01:11:22,290 --> 01:11:29,650
whole matrix and so it was a real
problem actually when people used to try
and use traditional linear algebra for

353
01:11:29,750 --> 01:11:36,780
this because when you have these sparse
matrices like in practice
this matrix is not doesn't have many

354
01:11:36,780 --> 01:11:43,650
gaps because we picked the users that
watch the most movies and the movies
that are the most watched but if you
look at the whole matrix it's it's

355
01:11:43,650 --> 01:11:49,495
mainly empty and so traditional
techniques treated empty is zero and so
like you basically have to predict a

356
01:11:49,595 --> 01:11:55,213
zero as if the fact that I haven't
watched a movie means I don't like the

357
01:11:55,313 --> 01:12:11,220
movie that's gives terrible answers so
this probabilistic matrix factorization
approach takes advantage of the fact
that our data structure actually looks
like this rather than that cross tab

358
01:12:11,220 --> 01:12:16,110
right and so it's only calculating the
loss for the user ID movie ID
combinations that actually appear that's

359
01:12:18,240 --> 01:12:25,030
its if you like use red a1 movie I think
102 9 should be 3 it's actually three
and a half sauce is 0.5 like there's

360
01:12:25,130 --> 01:12:35,440
nothing here that's ever going to
calculate a prediction or a loss for a
user movie combination that doesn't
appear in this table by definition the

361
01:12:35,540 --> 01:12:42,325
only stuff that we can appear in a mini
batch is what's in this table okay and

362
01:12:43,825 --> 01:12:53,500
like a lot of this happened
interestingly enough actually in the
Netflix price so before the Netflix
prize came along there's probabilistic

363
01:12:53,600 --> 01:13:02,540
matrix factorization it had actually
already been invented but nobody noticed
all right and then in the
first year of the Netflix price someone

364
01:13:02,640 --> 01:13:10,625
wrote this like really really famous
blog post where they basically said like
hey check this out
incredibly simple technique works

365
01:13:10,725 --> 01:13:15,860
incredibly well when suddenly all the
net fix leaderboard entries and so

366
01:13:15,960 --> 01:13:21,500
that's quite a few years ago now and
this is like now every collaborative

367
01:13:21,600 --> 01:13:40,975
filtering approach does this not every
collaborative filtering approach adds
this sigmoid thing by the way it's not
like rocket science this is this is not
like the NLP thing we saw last week
which is like hey this is a new
state-of-the-art like this is you know
not particularly uncommon but there are
still people that don't do this it
definitely helps a lot I have to have

368
01:13:41,075 --> 01:14:20,050
this and so actually you know what we
could do is maybe now's a good time to
have a look at the definition of this
right so the column data module contains
all these definitions and we can now
compare this to the thing we originally
used which was whatever came out of
collaborative data set all right so
let's go to collab filter data set here
it is and we called get learner all

369
01:14:20,050 --> 01:14:29,985
right so we can go down to get Elena and
that created a collab filter learner
passing in the model from get model is

370
01:14:30,085 --> 01:14:48,785
get model so created an embedding bias
and so here is embedding drop bias and
you can see here here it is like it's
the same thing there's the embedding for
each of the things here's our forward
that does the you times I dot some plus

371
01:14:48,885 --> 01:15:00,340
plus sigmoid so in fact we have just
actually rebuilt what's in the past our
library literally okay it's a little

372
01:15:00,340 --> 01:15:11,410
shorter and easier because we're taking
advantage of the fact that there's a
special collaborative filtering data set
so we can actually we're getting past in

373
01:15:11,410 --> 01:15:16,104
the users and the items and we don't
have to pull them out of cat
Kant's but other than that this is

374
01:15:16,204 --> 01:15:32,604
exactly the same so hopefully you can
see like the faster you have ivory is
not some inscrutable code containing
concepts you can never understand we've
actually just built up this entire thing
from scratch ourselves and so why did we

375
01:15:32,704 --> 01:15:51,690
get 0.76 rather than 0.8 you know I I
think it's simply because we used
stochastic gradient descent with
restarts or the cycle multiplier and an
atom optimizer you know like a few
little training chase some I'm looking

376
01:15:51,690 --> 01:16:08,219
at this and thinking that is we could
totally improve this small but maybe
looking at the date and doing some
tricks with the date because this this
is kind of a just a regular kind of
smaller no way yeah you can add more

377
01:16:08,219 --> 01:16:18,179
features yeah exactly exactly so like
now that you've seen this you could now
you know even if you didn't have
embedding dot bias in a notebook that

378
01:16:18,179 --> 01:16:29,850
you've written yourself through some
other model that's in fast AI you could
look at it in faster and be like oh that
does most of the things that I'd want to
do but it doesn't deal with time and so
you could just go oh okay let's grab it

379
01:16:29,850 --> 01:16:46,830
copy it you know pop it into my notebook
and let's create you know the better
version all right and then you can start
playing that and you can now create your
own model class from the open source

380
01:16:46,830 --> 01:17:01,889
code here and so yeah your that's
mentioning a couple things we could do
we could try and incorporate in time
stamp so we could assume that maybe well
maybe there's just like some for a
particular user over time users tend to
get more or less positive about movies

381
01:17:01,989 --> 01:17:06,690
also remember there was the list of
genres for each movie maybe we could

382
01:17:08,940 --> 01:17:27,110
incorporate that so one problem is it's
a little bit difficult to incorporate
that stuff into this embedding bias
model because it's kind of it's pretty
custom right so what we're going to do
next is we're going to try to create a
neural net version

383
01:17:27,110 --> 01:17:49,435
of this hey so the basic idea here is
we're going to take exactly the same
thing as we had before here's our list
of users right and here is Erin Bates
alright and here's our list of movies
and here is our embedded right and so as

384
01:17:49,535 --> 01:17:58,880
you can see I've just kind of transposed
the movie ones so that so that they're
all in the same orientation and here is

385
01:17:58,880 --> 01:18:09,370
our user movie rating but D cross tab
okay so in the original format so each
row is a user movie rating okay so the

386
01:18:09,470 --> 01:18:45,170
first thing I do is I need to replace
user 14 with that users contiguous in
this right and so I can do that in Excel
using this match that basically says
what you know how far down this list do
you have to go and it said user 14 was
the first thing in that list okay
user 29 was the second thing in that
list so forth okay so this is the same
as that thing that we did in our Python
code where we basically created a

387
01:18:45,170 --> 01:18:58,640
dictionary to master so now we can for
this particular user movie rating
combination we can look up the
appropriate embedding right and so you

388
01:18:58,640 --> 01:19:22,580
can see here what it's doing is it's
saying
all right let's basically offset from
the start of this list and the number of
rows we're going to go down is equal to
the user index and the number of columns
we're going to go across is one two
three four or five okay and so you can
see what it does is it creates point one
nine point six three point three one
here it is point one nine point okay so

389
01:19:22,580 --> 01:19:51,800
so this is literally modern embedding
this but remember this is exactly the
same as doing a one hot encoding right
because if instead this was a vector
containing one zero zero zero zero
right and we multiplied that by this
matrix then the only row it's going to
return would be the first one okay so so

390
01:19:51,800 --> 01:19:59,190
it's really useful to remember that
embedding actually just is a matrix
product the only reason it exists the

391
01:19:59,290 --> 01:20:12,840
only reason it exists is because this is
an optimization you know this let's pipe
or to know like okay this is just a
matrix multiply but I guarantee you that
you know this thing is one hard encoded

392
01:20:12,940 --> 01:20:19,280
therefore you don't have to actually do
the matrix multiply you can just do a
directory of that okay so that's

393
01:20:19,280 --> 01:20:28,830
literally all an embedding is is it is a
computational performance thing for a
particular kind of matrix multiplier all

394
01:20:28,930 --> 01:20:47,900
right so that looks up that uses user
and then we can look up that users movie
all right so here is movie ID movie ID
four one seven which apparently is
indexed number fourteen here it is here
so it should have been point seven five
point four seven yes it is point seven
five point plus it okay

395
01:20:49,460 --> 01:21:06,370
so we've now got the user embedding and
the movie embedding and rather than
doing a dot product of those two okay
which is what we do normally instead

396
01:21:06,370 --> 01:21:17,562
what if we concatenate the two together
into a single vector of length 10 and
then feed that into a neural net okay

397
01:21:17,662 --> 01:21:33,840
and so anytime we've got you know a
tensor of import activations or in this
case a tensor of actually this is a
tensor of output activations this is
coming out of an embedding layer we can

398
01:21:33,940 --> 01:21:39,525
chuck it in a neural net because neural
Nets we now know can calculate anything

399
01:21:39,625 --> 01:21:47,344
okay including hopefully collaborative
filtering so let's try that so here is

400
01:21:47,444 --> 01:21:52,699
our embedding net so

401
01:21:52,799 --> 01:22:10,050
this time I have not bothered to create
a separate bias because instead the
linear layer in pi torch already has a
bias in it all right so when we go NN

402
01:22:10,050 --> 01:22:39,210
Linea right that's kind of draw this out
so we've got our U matrix right and this
is the number of users and this is the
number of factors right and we've got
our M matrix that so here's our number
of movies and here's our again number of

403
01:22:39,210 --> 01:22:50,099
factors okay and so remember we look up
a single user we look up a single movie

404
01:22:50,099 --> 01:22:57,239
and let's grab them and concatenate them
together okay so here's like the user
part here's the movie part and then

405
01:23:00,480 --> 01:23:27,610
let's put that through a matrix product
right so that number of rows here is
going to have to be the number of users
plus the number of movies right because
that's how long that is and then the
number of columns can be anything we
want because we're going to take that so
in this case we're going to pick 10
apparently so it's pick 10 and then

406
01:23:27,710 --> 01:23:49,329
we're going to stick that through a rail
you and then stick that through another
matrix which obviously needs to be of
size 10 here and then the number of
columns is a size 1 because we want to
predict a single rating okay and so

407
01:23:49,429 --> 01:23:57,879
that's our kind of flow chart of what's
going on right it is a standard I'm

408
01:23:57,979 --> 01:24:05,190
called a one hidden layer neural net it
depends how you think of it like there's
kind of an embedding layer but because

409
01:24:05,190 --> 01:24:21,270
is linear and this is linear the two
together is really one linear layer
right this just a computational
convenience so it's really got one
hidden layer because it's got one layer
before this nonlinear activation so in

410
01:24:21,270 --> 01:24:27,690
order to create a linear layer with some
number of rows and some number of
columns you just go in and on in the

411
01:24:31,290 --> 01:24:46,020
machine learning class this week we
learnt how to create a linear layer from
scratch by creating our own weight
matrix and our own biases so if you want
to check that out you couldn't do so
there right but it's the same basic

412
01:24:46,020 --> 01:24:57,750
technique we've already seen so we
create our embeddings we create our two
linear layers that's all the stuff that
we need to start with you know really if

413
01:24:57,750 --> 01:25:06,415
I wanted to make this more general I
would have had another parameter here
called like num hidden you know equals
equals 10 and then this would be a

414
01:25:06,515 --> 01:25:32,820
parameter and then you could like more
easily play around with different
numbers of activations so when we say
like okay in this layer I'm going to
create a layer with this many
activations all I mean assuming it's a
fully connected layer is my linear layer
has how many columns in its weight
matrix that's how many activations it

415
01:25:32,820 --> 01:25:52,320
creates all right so we grab our users
and movies we put them through our
embedding matrix and then we concatenate
them together
okay so torch cat concatenate them
together on the first dimension so in
other words we concatenate the columns
together to create longer rows okay so
that's concatenating on dimension one

416
01:25:56,300 --> 01:26:01,580
drop out we'll come back to her in a
moment we've got that briefly

417
01:26:01,580 --> 01:26:11,020
so then having done that we'll put it
through that linear layer we had we'll
do our value and you'll notice that

418
01:26:11,120 --> 01:26:16,199
value is again inside our capital F and
end up optional right it's just a
function

419
01:26:16,199 --> 01:26:21,855
so remember activation function
are basically things that take one
activation in and spit one activation

420
01:26:21,955 --> 01:26:31,676
out in this case taking something that
can have negatives or positives and
truncate the negatives to zero that's

421
01:26:31,776 --> 01:26:38,740
what well you does and then here's a
sigmoid so that's that that is now a

422
01:26:38,840 --> 01:26:45,690
genuine neural network I don't know if I
get to call it deep it's only got one
hidden layer but it's definitely a
neural network all right and so we can

423
01:26:48,030 --> 01:26:54,007
now construct it we can put it on the
GPU you can create an optimizer for it
and we can fit it now you'll notice

424
01:26:54,107 --> 01:27:02,995
there's one other thing I've been
passing to fit which is what loss
function are we trying to minimize okay
this is the mean squared error loss and

425
01:27:03,095 --> 01:27:11,320
again it's inside F okay pretty much all
the functions are inside it okay so one

426
01:27:11,420 --> 01:27:20,340
of the things that you have to pass fit
is something saying like how do you
score it's what counts as good or bad so

427
01:27:20,340 --> 01:27:27,560
it should I mean now that we have a real
neural net do we have to use the same
number of embeddings for users and

428
01:27:27,840 --> 01:27:38,100
that's a great question you don't know
absolutely right
you don't and so like we've got a lot of
benefits here right because if we you

429
01:27:38,100 --> 01:27:54,810
know think about you know we're grabbing
a user embedding or concatenating it
with a movie embedding which maybe is
like some different size but then also

430
01:27:54,810 --> 01:28:05,110
perhaps we looked up the genre of the
movie and like you know there's actually
a embedding matrix of like number of
genres by I don't know three or

431
01:28:05,210 --> 01:28:13,770
something and so like we could then
concatenate like a genre embedding and
then maybe the timestamp is in here as a
continuous number right and so then that

432
01:28:13,870 --> 01:28:18,570
whole thing we can then feed into you
know and you're on it all right and then

433
01:28:24,420 --> 01:28:43,639
at the end remember a final
non-linearity was a sigmoid right so we
can now recognize that
thing we did where we did sigmoid x max
reading vote - min reading + blah blah
blah is actually just another nonlinear
activation function alright remember in

434
01:28:43,739 --> 01:28:48,879
our last layer we use generally
different kinds of activation functions

435
01:28:48,979 --> 01:29:06,580
so as we said we don't need any
activation function at all right we
could just do that right but by not
having any nonlinear activation function
we're just making it harder so that's
why we put the sigmoid in there as well

436
01:29:06,680 --> 01:29:19,509
okay so we can then fit it in the usual
way and there we go
you know interestingly we actually got a
better score than we did with our this

437
01:29:19,609 --> 01:29:50,789
model so I'll be interesting to try
training this with stochastic gradient
descent with restarts and see if it's
actually better you know maybe you can
play around with the number of hidden
layers and the drop out and whatever
else and see if you can come up with you
know get a better answer than point
seven six ish okay so so general so this

438
01:29:50,789 --> 01:29:57,389
is like if you were going deep into
collaborative filtering at your
workplace
whatever this wouldn't be a bad way to

439
01:29:57,389 --> 01:30:15,420
go I could like I'd start out with like
oh okay here's like a flat footed
dataset 30 in first day I get learner
there's you know not much I can send it
basically number of factors is about the
only thing that I pass in I can learn
for a while maybe try a few different
approaches and then you're like okay

440
01:30:15,420 --> 01:30:30,869
there's like that's how I go if I use
the defaults okay how do I make it
better and then I'd be like dig into the
code and seeing like okay well what if
Jeremy actually do here this is actually
what I want you know and so one of the

441
01:30:30,869 --> 01:30:39,699
nice things about the neural net
approach is that you know as unit
mentioned we can have different numbers

442
01:30:39,799 --> 01:30:51,000
of embeddings we can choose how many
hidden and we can also choose
drop now right so so what we're actually

443
01:30:51,000 --> 01:31:18,510
doing is we haven't just got real you
that we're also going like okay let's
let's delete a few things at random
alright let's drop out so in this case
we were deleting after the first linear
layer 75% of them all right and then
after the second one in like 75% of them

444
01:31:18,510 --> 01:31:28,320
so we can add a whole lot of
regularization yes so you know this it
kind of feels like the this this
embedding net you know you could you

445
01:31:28,320 --> 01:31:51,025
could change this again we could like
have it so that we can pass into the
constructor well if you're gonna make it
look as much as possible like what we
had before we could surpass him peace
peace equals 0.75 oh I'm not sure this
is the best API but it's not terrible

446
01:31:51,125 --> 01:32:08,790
probably what since we've only got
exactly two layers we could say p1
equals 0.75 v p2 v and so then this will
be P 1 this will be Peter you know where

447
01:32:18,630 --> 01:32:28,044
we go and like if you wanted to go
further you could make it look more like
our structured data learner you could

448
01:32:28,144 --> 01:32:44,845
actually have a thing this number of
hidden you know maybe you could make a
list and so then rather than creating
exactly one hidden layer and one output
layer this could be a little loop that
creates and hidden miners each one of

449
01:32:44,945 --> 01:32:49,570
the size you want so like this is all
stuff you can play with during the
hearing the week if you want to and I

450
01:32:49,670 --> 01:33:05,555
feel like if you've got like a much
smaller collaborative children data set
you know maybe you need like more
regularization or whatever
it's a much bigger one maybe more layers
would help I don't know you know III

451
01:33:05,655 --> 01:33:21,390
haven't seen much discussion of this
kind of neural network approach to
collaborative filtering but I'm not a
collaborative filtering expert so maybe
it's maybe it's around but that'd be
interesting thing to try so the next

452
01:33:21,490 --> 01:33:34,870
thing I wanted to do was to talk about
the training loop so what's actually
happening inside the training loop so at

453
01:33:34,870 --> 01:33:58,640
the moment we're basically passing off
the actual updating of the weights to PI
torches optimizer but what I'm going to
do is like understand what that
optimizer is is actually good and we're
also I also want to understand what this
Momentum's him he's doing so you'll find

454
01:33:58,740 --> 01:34:08,110
we have a spreadsheet called grab disk
gradient descent and it's kind of
designed to be read left to right sorry
right to left worksheet was so the

455
01:34:11,200 --> 01:34:16,030
rightmost worksheet is some data right
and we're going to implement gradient

456
01:34:16,030 --> 01:34:21,815
descent in Excel because obviously
everybody wants to do deep learning in
it Selman we've done collaborative

457
01:34:22,115 --> 01:34:30,040
filtering in Excel we've done
convolutions in Excel so now we need SJD
in Excel so we can replace - once and
for all

458
01:34:30,430 --> 01:34:41,250
okay so let's start by creating some
data right and so here's you know here's
some independent you know I've got one
column of X's you know and one column of

459
01:34:41,350 --> 01:35:01,760
wise and these are actually directly
linearly related so this is this is
random right and this one here is equal
to x times 2 plus 30 ok so let's try and

460
01:35:01,860 --> 01:35:13,675
use Excel to take that data and try and
learn those parameters

461
01:35:13,775 --> 01:35:20,910
okay that's going to be able so let's
start with the most basic version of SGD

462
01:35:21,010 --> 01:35:35,190
and so the first thing I'm going to do
is I'm going to run a macro so you can
see what this looks like so I'll hit run
and it does five eight bucks under
another five eight bucks
- another five eight bucks okay so the

463
01:35:35,290 --> 01:35:41,925
first one was pretty terrible it's hard
to see so I'll just delete that first
one get better scaling alright so you

464
01:35:42,025 --> 01:35:53,060
can see it actually it's pretty
constantly improving the loss all right
this is the loss per pot all right so

465
01:35:53,060 --> 01:36:01,045
how do we do that so let's reset it so
here is my X's and my y's and what I do
is I start out by assuming some

466
01:36:01,145 --> 01:36:31,105
intercept and some slope right so this
is my randomly initialized weights so I
have randomly initialized them both to
one you could pick a different random
number if you like but I promise that I
randomly picked the number one twice
there you go
it was a random number between one and

467
01:36:31,205 --> 01:36:44,265
one so here is my intercept and slope
I'm just going to copy them over here
right so you can literally see this is
just equal see one here is equals c2
okay so I'm gonna start with my very
first row of data x equals 14 y equals

468
01:36:44,365 --> 01:36:58,855
58 and my goal is to come up after I
look at this piece of data I want to
come up with a slightly better intercept
and a slightly better slope okay so to

469
01:36:58,955 --> 01:37:10,460
do that I need to first of all basically
figure out which direction is is down in
other words if I make my intercept a

470
01:37:10,460 --> 01:37:17,640
little bit higher or a little bit lower
would it make my error a little bit
better or a little bit worse so let's

471
01:37:17,740 --> 01:37:22,320
start out by calculating the error so to
calculate the error the first thing we

472
01:37:22,420 --> 01:37:28,664
need is a prediction so the prediction
is equal to the interest

473
01:37:28,764 --> 01:37:36,524
at plus x times so that is our zero
hidden layer neural network okay

474
01:37:36,624 --> 01:37:42,899
and so here is our era it's equal to our
prediction - our actual squared so we

475
01:37:42,999 --> 01:37:57,049
could like play around with this I don't
want my error to be 18-49 I'd like it to
be lower so what if we set the
intercepts to one point one 18-49 goes
to 1840 okay so a higher intercept would

476
01:37:57,049 --> 01:38:13,999
be better okay what about the slope to
increase that it goes from 1849 to 1730
okay a higher slope would be better as
well not surprising because we know
actually that there should be 30 into so

477
01:38:14,099 --> 01:38:23,059
one way to figure that out
you know encode and this protein is to
do literally what I just did is to add a

478
01:38:23,059 --> 01:38:26,419
little bit to the intercept and the
slope and see what happens and that's

479
01:38:26,419 --> 01:38:30,769
called finding the derivative through
finite differencing right and so let's

480
01:38:30,769 --> 01:38:40,969
go ahead and do that so here is the
value of my error if I add 0.01 to my

481
01:38:40,969 --> 01:38:48,674
intercept all right so it's c4 plus 0.01
and then I just put that into my Lydian
function and then I subtract my actual

482
01:38:48,774 --> 01:39:05,324
all squared all right and so that causes
my arrow to go down a bit
that's our increasing my is that
increasing will see for increasing the
intercept a little bit has caused my
arrow to go down so what's the

483
01:39:05,424 --> 01:39:11,749
derivative well the derivative is equal
to how much the dependent variable
changed by divided by how much the
independent variable changed by all

484
01:39:13,999 --> 01:39:21,511
right and so there it is right our
dependent variable changed by that -
that right and our independent variable

485
01:39:21,611 --> 01:39:27,239
we changed by 0.01 so there is the
estimated value of the error dB so

486
01:39:27,339 --> 01:39:39,570
remember when people talking about
derivatives right this is this is all
they're doing is they're saying what's
this value but as we make this number
smaller and smaller and smaller
and smaller as it as limits to zero

487
01:39:43,530 --> 01:39:56,430
I'm not mad enough to think in terms of
like derivatives and integrals and stuff
like that so whatever I think about this
I always think about you know an actual
like plus 0.01 and divided by 0.01
because like I just find that easier

488
01:39:56,430 --> 01:40:06,025
just like I'd ever think about
probability density functions I always
think about actual probabilities of that
toss a coin
something happens three times so I

489
01:40:06,125 --> 01:40:16,255
always think like remember it's it's
totally fair to do this because a
computer is discrete it's not continuous
like a computer can't do anything

490
01:40:16,355 --> 01:40:23,460
infinitely small anyway right so it's
actually got to be calculating things at
some level of precision right and our

491
01:40:23,460 --> 01:40:29,490
brains kind of need that as well so this
is like my version of Jeffery Clinton's

492
01:40:29,490 --> 01:40:33,660
like to visualize things in more than
two dimensions you just like say 12

493
01:40:33,660 --> 01:40:36,780
dimensions really quickly well
visualizing in two dimensions this is my

494
01:40:36,780 --> 01:40:41,970
equivalent you know to to think about
derivatives just think about division

495
01:40:41,970 --> 01:40:47,640
and like although all the mathematicians
say no you can't do that you actually

496
01:40:47,640 --> 01:40:51,810
can like if you think of DX dy is being
literally you know change in X over

497
01:40:51,810 --> 01:40:58,080
changing Y like the division actually
like the calculations do work like all
the time so okay so let's do the same

498
01:41:01,710 --> 01:41:07,680
thing now with changing my slope by a
little bit and so here's the same thing

499
01:41:07,680 --> 01:41:13,620
right and so you can see both of these
are negative okay so that's saying if I

500
01:41:13,620 --> 01:41:20,010
increase my intercept my loss goes down
if I increase my slope my loss goes down

501
01:41:20,010 --> 01:41:29,010
right and so my derivative of my error
with respect to my slope is is actually

502
01:41:29,010 --> 01:41:36,240
pretty high and that's not surprising
because it's actually you know the

503
01:41:36,240 --> 01:41:42,020
constant term is just being added where
else as slope is being multiplied by 40

504
01:41:42,890 --> 01:41:49,950
okay now find that differencing is all
very well and good but there's a big
problem with finite difference seeing in

505
01:41:52,040 --> 01:41:56,050
Hyden
no spaces and the problem is this right

506
01:41:56,050 --> 01:42:02,770
and this is like you don't need to learn
how to calculate derivatives or

507
01:42:02,770 --> 01:42:06,969
integrals but you need to learn how to
think about them spatially right and so

508
01:42:06,969 --> 01:42:13,630
remember we have some vector very high
dimensional vector it's got like a

509
01:42:13,630 --> 01:42:22,090
million items in it right and it's going
through some weight matrix right of size

510
01:42:22,090 --> 01:42:26,890
like 1 million by size a hundred
thousand or whatever and it's spitting

511
01:42:26,890 --> 01:42:33,580
out something of size I hundred thousand
and so you need to realize like there

512
01:42:33,580 --> 01:42:38,380
isn't like a gradient yeah but it's like
for every one of these things in this

513
01:42:38,380 --> 01:42:45,850
vector right there's a gradient in every
direction you know in every part of the

514
01:42:45,850 --> 01:42:52,719
output right so it actually has not a
single gradient number not even a

515
01:42:52,719 --> 01:43:03,100
gradient vector but a gradient matrix
right and so this this is a lot to

516
01:43:03,100 --> 01:43:07,989
calculate right I would literally have
to like add a little bit to this and see
what happens to all of these add a

517
01:43:09,790 --> 01:43:14,739
little bit to this see what happens to
all of these right to fill in one column

518
01:43:14,739 --> 01:43:21,400
of this at a time so that's going to be
horrendously slow like that so that's

519
01:43:21,400 --> 01:43:24,250
why like if you're ever thinking like
how we can just do this with finite

520
01:43:24,250 --> 01:43:28,480
differencing just remember like okay we
we're dealing in the with these very

521
01:43:28,480 --> 01:43:37,870
high dimensional vectors where you know
this this kind of matrix calculus like

522
01:43:37,870 --> 01:43:42,880
all the concepts are identical but when
you actually draw it out like this you

523
01:43:42,880 --> 01:43:47,199
suddenly realize like okay for each
number I could change there's a whole
bunch of numbers that impacts and I have

524
01:43:49,060 --> 01:43:54,310
this whole matrix of things to compute
right and so your gradient calculations

525
01:43:54,310 --> 01:43:59,260
can take up a lot of memory and they can
take up a lot of time so we want to find

526
01:43:59,260 --> 01:44:07,000
some way to do this more quickly okay
and it's definitely well worth like

527
01:44:07,000 --> 01:44:14,200
spending time kind of studying these
ideas of like you know the idea of like

528
01:44:14,200 --> 01:44:24,640
the gradients like look up things like
Jacobian and Hessian they're the things

529
01:44:24,640 --> 01:44:29,020
that you want to search for just that
unfortunately people normally write

530
01:44:29,020 --> 01:44:35,860
about them with you know lots of great
letters and bla bla bla right but there
are some there are some nice you know

531
01:44:39,760 --> 01:44:43,060
intuitive explanations out there and
hopefully you can share them on the

532
01:44:43,060 --> 01:44:47,830
forum if you find them because this is
stuff you really need to really need to
understand in here you know because

533
01:44:51,960 --> 01:44:56,380
you're trying to train something and
it's not working properly and like later

534
01:44:56,380 --> 01:45:00,640
on we'll learn how to like look inside
hi torch to like actually get the values

535
01:45:00,640 --> 01:45:04,240
of the gradients and you need to know
like okay well how would I like what the
gradients you know what would I consider

536
01:45:06,430 --> 01:45:10,840
unusual like you know these are the
things that turn you into a really

537
01:45:10,840 --> 01:45:15,580
awesome deep learning practitioner is
when you can like debug your problems by

538
01:45:15,580 --> 01:45:19,720
like grabbing the gradients and doing
histograms of them and like knowing you

539
01:45:19,720 --> 01:45:23,530
know that you could like plot that all
each layer my average gradients getting
worse or you know bigger okay so the

540
01:45:28,270 --> 01:45:34,270
trick to doing this more quickly is to
do it analytically rather than through

541
01:45:34,270 --> 01:45:40,150
finite differencing and so analytically
is basically there is a list you
probably all learned it at high school

542
01:45:41,500 --> 01:45:46,600
there is a literally a list of rules
that for every mathematical function

543
01:45:46,600 --> 01:45:53,050
there's a like this is the derivative of
that function so you probably remember a

544
01:45:53,050 --> 01:46:02,920
few of them for example x squared - it's
alright and so we actually have here an

545
01:46:02,920 --> 01:46:10,720
x squared so here is our two x right now
the one that I actually want you to know

546
01:46:10,720 --> 01:46:17,550
is not any of the individual rules but I
want you to know the chain rule right

547
01:46:17,550 --> 01:46:22,450
which
you've got some function of some

548
01:46:22,450 --> 01:46:26,380
function of something why is this
important

549
01:46:26,380 --> 01:46:33,670
I don't know that's a linear layer
that's a rally right and then we can

550
01:46:33,670 --> 01:46:42,160
kind of keep going backwards map etc
right a neural net is just a function of

551
01:46:42,160 --> 01:46:45,100
a function of a function of a function
where the innermost is you know it's

552
01:46:45,100 --> 01:47:00,940
basically linear rally your linear rally
your dot linear sigmoid or soft mass all
right and so it's a function of a

553
01:47:02,170 --> 01:47:06,940
function of a function and so therefore
to calculate the derivative of the
weights in your model the loss of your

554
01:47:10,960 --> 01:47:13,330
model with respect to the weights of
your model you're going to need to use

555
01:47:13,330 --> 01:47:17,860
the chain rule and specifically whatever
layer it is that you're up to like I

556
01:47:17,860 --> 01:47:23,020
want to calculate the derivative here
and got a need to use all of these all

557
01:47:23,020 --> 01:47:25,690
of these ones because that's all that's
that's the function that's being applied
right and that's why they call this back

558
01:47:28,150 --> 01:47:36,310
propagation because the value of the
derivative of that is equal to that

559
01:47:36,310 --> 01:47:43,810
derivative now basically you can do it
like this you can say let's call you is

560
01:47:43,810 --> 01:47:47,020
this right
let's call that you all right then it's

561
01:47:47,020 --> 01:47:56,440
simply equal to the derivative of that
times derivative of that right
you just multiply them together and so

562
01:47:59,170 --> 01:48:03,910
that's what back propagation is like
it's not that back propagation is a new

563
01:48:03,910 --> 01:48:10,240
thing for you to learn it's not a new
algorithm it is literally take the

564
01:48:10,240 --> 01:48:16,270
derivative of every one of your layers
and multiply them all together so like

565
01:48:16,270 --> 01:48:23,050
it doesn't deserve a new name right
apply the chain rule to my layers does

566
01:48:23,050 --> 01:48:27,790
not deserve a new name but it gets one
because us neural networks folk really

567
01:48:27,790 --> 01:48:31,750
need to seem as clever as possible it's
really important that everybody else

568
01:48:31,750 --> 01:48:34,929
thinks
we are way outside of their capabilities

569
01:48:34,929 --> 01:48:39,429
so the fact that you're here means that
we've failed because you guys somehow

570
01:48:39,429 --> 01:48:43,540
think that you're capable right so
remember it's really important when you

571
01:48:43,540 --> 01:48:48,010
talk to other people that you say
backpropagation and rectified linear

572
01:48:48,010 --> 01:48:54,400
unit rather than like multiply the
layers gradients or replace negatives
with zeros okay so so here we go

573
01:48:57,520 --> 01:49:03,159
so here is so I've just gone ahead and
grabbed the derivative unfortunately
there is no automatic differentiation in

574
01:49:05,079 --> 01:49:09,610
Excel yet so I did the alternative which
is to paste the formula into Wolfram

575
01:49:09,610 --> 01:49:14,020
Alpha and got back the derivative so
there's the first derivative and there's

576
01:49:14,020 --> 01:49:19,510
the second derivative analytically we
only have one layer in this infinite

577
01:49:19,510 --> 01:49:23,860
finally small neural network so we don't
have to worry about the chain rule and
we should see that this analytical

578
01:49:25,480 --> 01:49:28,599
derivative is pretty close to our
estimated derivative from the find out

579
01:49:28,599 --> 01:49:33,880
differencing and indeed it is right and
we should see that these ones are pretty

580
01:49:33,880 --> 01:49:38,790
similar as well and indeed they are
right and if you're you know back when I

581
01:49:38,790 --> 01:49:45,130
implemented my own neural Nets 20 years
ago I you know had to actually calculate

582
01:49:45,130 --> 01:49:48,550
the derivatives and so I always would
write like had something that would

583
01:49:48,550 --> 01:49:52,090
check the derivatives using finite
difference in and so for those poor

584
01:49:52,090 --> 01:49:55,420
people that they'd have to write these
things by hand you'll still see that

585
01:49:55,420 --> 01:49:59,710
they have like a finite differencing
checkout so if you ever do have to

586
01:49:59,710 --> 01:50:05,469
implement a derivative by hand please
make sure that you have a finite

587
01:50:05,469 --> 01:50:10,719
differencing checker so that you can
test it alright so there's no

588
01:50:10,719 --> 01:50:17,139
derivatives so we know that if we
increase B then we're going to get a

589
01:50:17,139 --> 01:50:22,239
slightly better loss so let's increase B
by a bit how much should we increase it
by well we'll increase it by some more

590
01:50:24,670 --> 01:50:28,060
for this so the motor-pod we're going to
choose is called a learning rate and so

591
01:50:28,060 --> 01:50:37,300
here's our learning rate so here's one
enoch 4 ok so our new value is equal to

592
01:50:37,300 --> 01:50:45,219
whatever it was before
- our derivative times our learning rate

593
01:50:45,219 --> 01:50:52,239
okay so we've gone from one to one point
or one and then a we've done the same
thing so it's gone from one to one point

594
01:50:56,349 --> 01:51:02,530
one two so this is a special kind of
mini batch it's a mini batch of size one

595
01:51:02,530 --> 01:51:09,099
okay so we call this online grading does
it just means mini batch of size one so

596
01:51:09,099 --> 01:51:11,800
then we can go into the next one next is
86

597
01:51:11,800 --> 01:51:17,590
why is 202 right this is my intercept
and slope copied across from the last

598
01:51:17,590 --> 01:51:26,170
row okay so here's my new wire
prediction here's my new era here are my

599
01:51:26,170 --> 01:51:32,559
derivatives here are my new a and B okay
so we keep doing that for every mini

600
01:51:32,559 --> 01:51:41,139
batch of one until eventually we run out
at the end of the new pocket okay and so

601
01:51:41,139 --> 01:51:48,849
then at the end of an epoch we would
grab our intercept and slope and paste

602
01:51:48,849 --> 01:51:56,440
them back over here as our new values
there we are and we can now continue
again all right so we're now starting

603
01:51:59,050 --> 01:52:06,070
with pops today's either in the wrong
spot it should be pasted special
transpose values all right okay

604
01:52:10,420 --> 01:52:13,809
so there's a new intercept there's any
slope possibly I got that the wrong way

605
01:52:13,809 --> 01:52:19,570
around but anyway you get the idea and
then we continue okay so I recorded the
world's tiniest macro which literally

606
01:52:22,989 --> 01:52:32,070
just copies the final slope and puts it
into the new slope copies the final

607
01:52:32,070 --> 01:52:40,989
intercept put the new intercept and does
that five times and after each time it

608
01:52:40,989 --> 01:52:45,550
grabs the root mean squared error and
pastes it into the next spare area and

609
01:52:45,550 --> 01:52:49,690
that is attached to this Run button and
so that's going to go ahead and do that
five times okay so that's stochastic

610
01:52:53,769 --> 01:52:58,990
gradient descent and if so so to turn
this into a CNN

611
01:52:58,990 --> 01:53:05,170
all right you would just replace this
error function right and therefore this

612
01:53:05,170 --> 01:53:12,220
prediction with the output of that
convolutional example spreadsheet okay

613
01:53:12,220 --> 01:53:19,750
and that then would be in CNN being
trained with with SGD okay

614
01:53:19,750 --> 01:53:30,610
now the problem is that you'll see when
I run this it's kind of going very

615
01:53:30,610 --> 01:53:35,410
slowly right we know that we need to get
to a slope of two and an intercept of

616
01:53:35,410 --> 01:53:40,180
thirty and you can kind of see it this
rate it's going to take a very long time

617
01:53:40,180 --> 01:53:51,850
right and specifically it's like it
keeps going the same direction so it's

618
01:53:51,850 --> 01:53:56,830
like come on take a hint that's a good
direction so they come on take a hint

619
01:53:56,830 --> 01:54:01,260
that's a good direction please keep
doing that but more is called momentum

620
01:54:01,260 --> 01:54:11,350
right so on our next spreadsheet we're
going to implement momentum okay so what

621
01:54:11,350 --> 01:54:16,510
momentum does is the same thing and what
to simplify this spreadsheet I've

622
01:54:16,510 --> 01:54:21,430
removed the finite difference cause okay
other than that this is just the same

623
01:54:21,430 --> 01:54:28,810
right so it's true what our X is our
wise A's and B's predictions our error

624
01:54:28,810 --> 01:54:39,340
is now over here okay and here's our
derivatives okay our new calculation for

625
01:54:39,340 --> 01:54:49,660
this particular row our new calculation
here for our new a term just like before

626
01:54:49,660 --> 01:54:59,110
is is equal to whatever a was before -
okay now this time I'm not taking the

627
01:54:59,110 --> 01:55:03,850
derivative but I'm - income other number
times the loan rate so what's this other

628
01:55:03,850 --> 01:55:13,020
number okay so this other number is
equal to the derivative

629
01:55:13,020 --> 01:55:26,800
times what's this k 1.02 plus 0.98 times
the thing just above it okay so this is

630
01:55:26,800 --> 01:55:32,650
a linear interpolation between this rows
derivative for this mini-batches
derivative and whatever direction we

631
01:55:35,830 --> 01:55:41,740
went last time right so in other words
keep going the same direction as you

632
01:55:41,740 --> 01:55:48,940
were before right then update it a
little bit right and so in our rich in

633
01:55:48,940 --> 01:55:55,600
our Python just before we had a momentum
of 0.9 okay so you can see what tends to

634
01:55:55,600 --> 01:56:03,280
happen is that our negative kind of gets
more and more negative right all the way
up to like 2,000 where else with our

635
01:56:08,080 --> 01:56:14,200
standard SGD approach a derivatives are
kind of all over the place right

636
01:56:14,200 --> 01:56:18,790
sometimes there's 700 something negative
7 positive 100 you know so this is

637
01:56:18,790 --> 01:56:24,100
basically saying like yeah if you've
been going down for quite a while keep

638
01:56:24,100 --> 01:56:28,300
doing that until finally here it's like
okay that's that seems to be far enough

639
01:56:28,300 --> 01:56:32,140
so that's being less and less and less
negative all right mister we start being

640
01:56:32,140 --> 01:56:35,680
positive again so you can kind of see
why it's called momentum it's like once

641
01:56:35,680 --> 01:56:40,240
you start traveling in a particular
direction for a particular weight you're
kind of the wheel start spinning and

642
01:56:41,950 --> 01:56:46,810
then once the gradient turns around the
other way it's like Oh slow down we've

643
01:56:46,810 --> 01:56:51,700
got this kind of event um and then
finally turn back around right so when
we do it this way all right we can do

644
01:56:57,340 --> 01:57:04,300
exactly the same thing right and after
five iterations we're at 89 where else

645
01:57:04,300 --> 01:57:11,950
before after five iterations we're at
104 right and after a few more let's go

646
01:57:11,950 --> 01:57:22,620
maybe 15 okay so get this 102 for us
here

647
01:57:26,160 --> 01:57:31,660
it's going right so it's it's it's a bit
better it's not hips better you can

648
01:57:31,660 --> 01:57:38,710
still see like these numbers they're not
zipping along right but it's definitely

649
01:57:38,710 --> 01:57:41,680
an improvement and it also gives us
something else to tune which is nice

650
01:57:41,680 --> 01:57:45,910
like so if this is kind of a
well-behaved error surface right in

651
01:57:45,910 --> 01:57:50,800
other words like although it might be
bumpy along the way there's kind of some

652
01:57:50,800 --> 01:57:55,300
overall direction like imagine you're
going down a hill right and there's like

653
01:57:55,300 --> 01:57:59,170
bumps oh alright so the mobile more
momentum you got going to skipping over

654
01:57:59,170 --> 01:58:03,330
the tops right so we could say like okay
let's increase our beater up to 0.98

655
01:58:03,330 --> 01:58:08,140
right and see if that like allows us to
train a little faster and whoa look at

656
01:58:08,140 --> 01:58:12,160
that suddenly what's going to okay so
one nice thing about things like
momentum is it's like another parameter

657
01:58:13,750 --> 01:58:21,670
that you can choose to try and make your
model train better in practice basically

658
01:58:21,670 --> 01:58:26,230
everybody does this every like you look
at any like image net winner or whatever
they all use momentum okay and so back

659
01:58:37,930 --> 01:58:44,740
over here when we said here's SGD that
basically means use the basic tab of our
Excel spreadsheet but then momentum

660
01:58:46,960 --> 01:58:58,000
equals 0.9 means add in put a point nine
over here okay and so that that's kind

661
01:58:58,000 --> 01:59:03,240
of your like default starting point so

662
01:59:03,720 --> 01:59:18,220
let's keep going and talk about Adam so
Adam is something which I actually was

663
01:59:18,220 --> 01:59:22,720
not right earlier on in this course I
said we've been using Adam by default we

664
01:59:22,720 --> 01:59:25,690
actually haven't we've actually been I
noticed our we've actually been using
SGD with momentum by default and the

665
01:59:29,020 --> 01:59:36,690
reason is that Adam has had
much faster as you'll see it's much much
faster to learn with but there's been

666
01:59:38,489 --> 01:59:41,700
some problems which is people who
haven't been getting quite as good like

667
01:59:41,700 --> 01:59:46,440
final answers with Adam as they have
with std with momentum and that's why

668
01:59:46,440 --> 01:59:50,670
you'll see like all the you know image
net winning solutions and so forth and

669
01:59:50,670 --> 01:59:56,880
all the academic papers always use SGD
with momentum and I'll Adam seems to be

670
01:59:56,880 --> 02:00:00,330
a particular problem in NLP people
really haven't got Adam working at all

671
02:00:00,330 --> 02:00:08,640
well the good news is this was I built
it looks like this was solved two weeks

672
02:00:08,640 --> 02:00:14,730
ago it basically it turned out that the
way people were dealing with a
combination of weight decay in Adam had

673
02:00:17,310 --> 02:00:22,200
a nasty kind of bargainer basically and
that's that's kind of carried through to
every single library and one of our

674
02:00:24,750 --> 02:00:31,170
students and then Sahara has actually
just completed a prototype of adding is

675
02:00:31,170 --> 02:00:36,390
this new version of Adam has called Adam
W into fast AI and he's confirmed that

676
02:00:36,390 --> 02:00:43,710
he's getting much faster both the faster
performance and also the better accuracy

677
02:00:43,710 --> 02:00:49,560
so hopefully we'll have this Adam W in
faster ideally before next week we'll

678
02:00:49,560 --> 02:00:53,660
see how we go very very soon
so so it is worth telling you about
about Adam so let's talk about it it's

679
02:00:58,830 --> 02:01:04,560
actually incredibly simple but again you
know make sure you make it sound really

680
02:01:04,560 --> 02:01:09,690
complicated when you tell people so that
you can so here's the same spreadsheet
again right and here's our randomly

681
02:01:13,920 --> 02:01:19,020
selected a and B again somehow it's
still one here's a prediction here's our

682
02:01:19,020 --> 02:01:25,230
derivatives okay so now how we count
letting on you hey you could immediately
see it's looking pretty hopeful because

683
02:01:28,290 --> 02:01:34,110
even by like row ten we're like we're
seeing the numbers move a lot more right

684
02:01:34,110 --> 02:01:42,960
so this is looking pretty encouraging so
how are we calculating this it's equal

685
02:01:42,960 --> 02:01:49,989
to our previous value with B minus j h
we're gonna have to find out what that
is times our learning rate divided by

686
02:01:56,739 --> 02:02:00,400
the square root of LH okay so I'm gonna
have to dig it and see what's going on

687
02:02:00,400 --> 02:02:05,650
one thing to notice here is that my
learning rate is way higher than it used

688
02:02:05,650 --> 02:02:12,730
to be but then we're dividing it by this
big number okay so let's start out by

689
02:02:12,730 --> 02:02:19,300
looking and seeing what this day-out
thing is okay

690
02:02:19,300 --> 02:02:25,690
j8 is identical to what we had before j8
is equal to the linear interpolation of
the derivative and the previous

691
02:02:28,750 --> 02:02:36,550
direction okay so that was easy so one
part of atom is to use momentum in the

692
02:02:36,550 --> 02:02:41,680
way we just defined it
okay the second piece was to divide by

693
02:02:41,680 --> 02:02:48,610
square root L 8 what is that square root
L 8 okay is another linear interpolation
of something and something else and

694
02:02:52,920 --> 02:03:00,610
specifically it's a linear interpolation
of F 8 squared okay it's a linear

695
02:03:00,610 --> 02:03:07,210
interpolation of the derivative squared
along with the derivative squared last

696
02:03:07,210 --> 02:03:13,210
time okay so in other words we've got
two pieces of momentum going on here

697
02:03:13,210 --> 02:03:22,420
one is calculating the momentum version
of the gradient the other is calculating

698
02:03:22,420 --> 02:03:28,780
the momentum version of the gradient
squared and we often refer to this idea

699
02:03:28,780 --> 02:03:34,090
as a exponentially weighted moving
average in other words it's basically

700
02:03:34,090 --> 02:03:37,630
equal to the average of this one and the
last one in the last one in the last one

701
02:03:37,630 --> 02:03:42,130
that we're like multiplicatively
decreasing the previous ones right

702
02:03:42,130 --> 02:03:47,830
because we're multiplying it by 0.9
times what 999 and so you actually see

703
02:03:47,830 --> 02:03:53,370
that for instance in the faster I code

704
02:04:00,180 --> 02:04:10,380
if you look at fish we don't just
calculate the average loss right because

705
02:04:10,380 --> 02:04:14,980
what I actually want we certainly don't
just report the loss for every mini
match because that just bounces around

706
02:04:16,210 --> 02:04:22,360
so much so instead I say average loss is
equal to whatever the average loss was

707
02:04:22,360 --> 02:04:31,900
last time times 0.98 plus the loss this
time times 0.02 right so in other words

708
02:04:31,900 --> 02:04:36,489
the faster you library the thing that
it's actually when you do like the

709
02:04:36,489 --> 02:04:40,930
learning rate finder or plot loss it's
actually showing you the exponentially

710
02:04:40,930 --> 02:04:47,290
weighted moving average of the loss okay
so it's like a really handy concept it

711
02:04:47,290 --> 02:04:52,390
appears quite a lot right the other in
handy concept know about is this idea of

712
02:04:52,390 --> 02:04:58,120
like you've got two numbers one of them
is multiplied by some value the other is

713
02:04:58,120 --> 02:05:02,770
multiplied by one minus that value so
this is a linear interpolation of two

714
02:05:02,770 --> 02:05:09,190
values you'll see it all the time and
for some reason deep learning people
nearly always use the value alpha when

715
02:05:11,949 --> 02:05:15,280
they do this so like keep an eye out if
you're reading a paper or something and

716
02:05:15,280 --> 02:05:22,840
you see like alpha times bla bla bla bla
bla plus one minus alpha times some

717
02:05:22,840 --> 02:05:28,180
other bla bla bla bla right immediately
like when people read papers none of us

718
02:05:28,180 --> 02:05:33,730
like read every thing in the equation we
look at it we go oh linear interpolation
right and I said something I was just

719
02:05:35,890 --> 02:05:39,670
talking to Rachel about yesterday is
like whether we could start trying to

720
02:05:39,670 --> 02:05:44,080
find like a a new way of writing papers
where we literally refactor them right

721
02:05:44,080 --> 02:05:50,620
like it'd be so much better to have
written like linear interpolate bla bla

722
02:05:50,620 --> 02:05:55,090
bla bla bla right because then you don't
have to have that pattern recognition

723
02:05:55,090 --> 02:05:59,440
right but until we convince the world to
change how they write papers this is
what you have to do is you have to look

724
02:06:00,820 --> 02:06:06,850
you know know what to look for right and
once you do suddenly the huge page with

725
02:06:06,850 --> 02:06:11,020
formulas
that at all like you often notice like

726
02:06:11,020 --> 02:06:15,580
for example the two things in here like
they might be totally identical but this
might be a time T and this might be at

727
02:06:17,260 --> 02:06:21,730
like time t minus y or something right
like it's very often these big ugly
formulas turn out to be really really

728
02:06:24,400 --> 02:06:30,340
simple if only they had ripped out them
okay so what are we doing with this

729
02:06:30,340 --> 02:06:37,540
gradient squared so what we were doing
with the gradient squared is we were

730
02:06:37,540 --> 02:06:42,370
taking the square root and then we were
adjusting the learning rate by dividing

731
02:06:42,370 --> 02:06:49,320
the learning rate by that okay so
gradient squared is always positive

732
02:06:49,320 --> 02:06:55,300
right and we're taking the exponentially
waiting move moving average of a bunch

733
02:06:55,300 --> 02:06:58,750
of things that are always positive and
then we're taking the square root of

734
02:06:58,750 --> 02:07:04,060
that right so when is this number going
to be high it's going to be particularly

735
02:07:04,060 --> 02:07:09,550
high if there's like one big you know if
the gradients got a lot of variation

736
02:07:09,550 --> 02:07:14,980
that's oh there's a high variance of
gradient then this G squared thing is
going to be a really high number for us

737
02:07:16,840 --> 02:07:22,630
if it's like a constant amount right
it's going to be smaller that cuz when

738
02:07:22,630 --> 02:07:26,980
you add things that are squared the
squared slight jump out much bigger for
us if there wasn't if there wasn't much

739
02:07:28,510 --> 02:07:34,870
change it's not going to be as big so
basically this number at the bottom here

740
02:07:34,870 --> 02:07:41,320
is going to be high if our Brady --nt is
changing a lot now what do you want to

741
02:07:41,320 --> 02:07:46,210
do if you've got something which is like
first negative and then positive and

742
02:07:46,210 --> 02:07:53,260
then small and then high right well you
probably want to be more careful right

743
02:07:53,260 --> 02:07:56,650
you probably don't want to take a big
step because you can't really trust it
right so when the when the variance of

744
02:07:59,290 --> 02:08:03,480
the gradient is high we're going to
divide our learning rate by a big number

745
02:08:03,480 --> 02:08:09,760
we also found learning rate is very
similar kind of size all the time then

746
02:08:09,760 --> 02:08:13,120
we probably feel pretty good about the
step so we're dividing it by a small

747
02:08:13,120 --> 02:08:18,100
amount yeah and so this is called an
adaptive learning rate yeah and like a

748
02:08:18,100 --> 02:08:22,240
lot of people have this confusion about
atom I've seen it on the forum actually
like there's some kind of adaptive

749
02:08:24,700 --> 02:08:29,650
learning rate where somehow you like
setting different learning rates for

750
02:08:29,650 --> 02:08:33,010
different layers or something it's like
no not really

751
02:08:33,010 --> 02:08:37,720
right all we're doing is we're just
saying like this keep track of the

752
02:08:37,720 --> 02:08:43,060
average of the squares of the gradients
and use that to adjust the learning rate

753
02:08:43,060 --> 02:08:48,550
so there's still one learning rate okay
in this case it's one okay but

754
02:08:48,550 --> 02:08:55,030
effectively every parameter at every
epoch is being kind of like getting a

755
02:08:55,030 --> 02:08:58,840
bigger jump if the learning rate if the
gradients been pretty constant for that

756
02:08:58,840 --> 02:09:04,660
wait and a smaller jump otherwise okay
and that's Adam that's the entirety of

757
02:09:04,660 --> 02:09:09,940
Adam in in Excel right so there's now no
reason at all why you can't train

758
02:09:09,940 --> 02:09:13,540
imagenet in Excel because you've got
you've got access to all of the pieces

759
02:09:13,540 --> 02:09:18,660
you need and so let's try this out run

760
02:09:19,830 --> 02:09:25,000
okay that's not bad right five and we
straight up to twenty nine and two right

761
02:09:25,000 --> 02:09:31,480
so the difference between like you know
standard SGD and this is is huge and

762
02:09:31,480 --> 02:09:35,140
basically that you know the key
difference was that it figured out that

763
02:09:35,140 --> 02:09:42,100
we need to be you know moving this
number much faster okay and so and so it

764
02:09:42,100 --> 02:09:48,370
do and so you can see we've now got like
two different parameters one is kind of
momentum for the gradient piece the

765
02:09:50,500 --> 02:09:56,500
other is the momentum for the gradient
squared piece and there I think they're

766
02:09:56,500 --> 02:10:00,670
called like I think there's just a
couple of the beta I think when you when

767
02:10:00,670 --> 02:10:03,790
you want to change it in PI tortes is I
think what beta which is just a couple

768
02:10:03,790 --> 02:10:15,430
of two numbers you can change Jeremy so
so you set the yeah I think I understand

769
02:10:15,430 --> 02:10:21,310
this concept of you know one day when a
gradient is it goes up and down then

770
02:10:21,310 --> 02:10:26,770
you're not really sure which direction
should should go so you should kind of

771
02:10:26,770 --> 02:10:29,590
slow things down
therefore you subtract that gradient

772
02:10:29,590 --> 02:10:35,500
from the learning rate so but how do you
implement how far do you go I guess

773
02:10:35,500 --> 02:10:38,610
maybe I miss something
early on you do you set a number

774
02:10:38,610 --> 02:10:45,930
somewhere we divide yeah we divide the
learning rate divided by the square root

775
02:10:45,930 --> 02:10:50,880
of the moving average gradient squared
so that's where we use it

776
02:10:50,880 --> 02:10:57,720
oh I'm sorry can you be a little more
sure so d2 is the learning rate which is

777
02:10:57,720 --> 02:11:02,850
one yeah m27
is our moving average of the squared

778
02:11:02,850 --> 02:11:12,750
gradients so we just go D 2 divided by
square root and preserve that's it okay

779
02:11:12,750 --> 02:11:19,860
thanks I have one question yeah
so the new method that you just

780
02:11:19,860 --> 02:11:27,480
mentioned which is in the process of
getting implemented in yes how different

781
02:11:27,480 --> 02:11:34,739
is it from here okay let's do that so to
understand Adam W we have to understand

782
02:11:34,739 --> 02:11:39,510
wait okay and maybe we'll learn more
about that later let's see how we go now

783
02:11:39,510 --> 02:11:46,440
with great okay
so the idea is that when you have lots
and lots of parameters like we do with

784
02:11:48,570 --> 02:11:54,330
you know most of the neural Nets we
train you very often have like more

785
02:11:54,330 --> 02:11:57,989
parameters and data points or you know
like regularization becomes important

786
02:11:57,989 --> 02:12:03,360
and we've learnt how to avoid
overfitting by using dropout right which

787
02:12:03,360 --> 02:12:08,910
randomly deletes some activations in the
hope that's going to learn some kind of

788
02:12:08,910 --> 02:12:14,100
more resilient set of weights there's
another kind of Ritter ization we can

789
02:12:14,100 --> 02:12:19,140
use called weight decay or l2
regularization and it's actually comes

790
02:12:19,140 --> 02:12:23,400
kind of as a kind of classic statistical
technique and the idea is that we take
our loss function right so we take out

791
02:12:26,130 --> 02:12:32,880
like arrow squared loss function and we
add an additional piece to it let's add

792
02:12:32,880 --> 02:12:39,270
weight decay right now the additional
piece we add is to basically add the

793
02:12:39,270 --> 02:12:49,619
square of the weights so we'd say plus B
squared plus a squared
okay that is now wait 2 K or L tree

794
02:12:55,960 --> 02:13:04,840
regularization and so the idea is that
now the the loss function wants to keep

795
02:13:04,840 --> 02:13:10,750
the weight small because increasing the
weights makes the loss worse and so it's

796
02:13:10,750 --> 02:13:15,909
only going to increase the weights if
the loss improves by more than the

797
02:13:15,909 --> 02:13:19,630
amount of that penalty and in fact to
make this weight to get to proper weight

798
02:13:19,630 --> 02:13:24,219
decay
we then need some multiplier yeah right

799
02:13:24,219 --> 02:13:31,719
so if you remember back in our here we
said weight decay equals W d5e neg 4

800
02:13:31,719 --> 02:13:39,159
okay so to actually use the same way to
K I would have to multiply by 0.005 all

801
02:13:39,159 --> 02:13:47,679
right so that's actually now the same
weight okay so if you have a really high

802
02:13:47,679 --> 02:13:52,150
weight decay that it's going to set all
the parameters to zero so it'll never

803
02:13:52,150 --> 02:13:57,550
over fit right because it can't set any
parameter to anything and so as you

804
02:13:57,550 --> 02:14:02,770
gradually decrease the weight decay a
few more weights can actually be used

805
02:14:02,770 --> 02:14:07,210
right but the ones that don't help much
it's still going to leave at zero or

806
02:14:07,210 --> 02:14:14,469
close to zero right so that's what
that's what weight decay is is is

807
02:14:14,469 --> 02:14:21,780
literally to change the loss function to
a D in this sum of squares of weights

808
02:14:21,780 --> 02:14:28,809
times some parameter some hyper
parameter I should say the problem is

809
02:14:28,809 --> 02:14:35,829
that if you put that into the loss
function as I have here then it ends up
in the moving average of gradients and

810
02:14:38,139 --> 02:14:42,940
the moving average of Squared's of
gradients for atom right and so
basically we end up when there's a lot

811
02:14:46,929 --> 02:14:53,559
of variation we end up decreasing the
amount of weight decay and if there's

812
02:14:53,559 --> 02:14:56,920
very little variation we end up
increasing the amount of weight decay so

813
02:14:56,920 --> 02:15:02,739
we end up basically saying penalize
parameters you know weights that are
really

814
02:15:03,070 --> 02:15:09,580
hi unless their gradient varies a lot
which is never what we intended right

815
02:15:09,580 --> 02:15:15,460
that's just not not the plan at all so
the trick with Adam W is we basically

816
02:15:15,460 --> 02:15:22,000
remove weight decay from here so it's
not in the last function it's not in the

817
02:15:22,000 --> 02:15:28,480
G not in the G squared and we move it so
that instead it's it's it's added

818
02:15:28,480 --> 02:15:32,110
directly to the when we update with the
learning rate

819
02:15:32,110 --> 02:15:36,400
it's out of there instead so in other
words it would be we would put the

820
02:15:36,400 --> 02:15:40,300
weight decay or I should a gradient of
the weight decay in here when we

821
02:15:40,300 --> 02:15:47,410
calculate the new a mu V so it never
ends up in our G M G squared so that was

822
02:15:47,410 --> 02:15:51,970
like a super fast description which will
probably only make sense if you listen

823
02:15:51,970 --> 02:15:57,340
to a three or four times on the video
and then talk about it on the forum yeah

824
02:15:57,340 --> 02:16:01,960
but if you're interested let me know and
we can also look at Ann Ann's code

825
02:16:01,960 --> 02:16:11,440
that's implemented yes and you know the
the idea of using weight decay is it's a

826
02:16:11,440 --> 02:16:17,410
really helpful regularizer because it's
basically this way that we can kind of

827
02:16:17,410 --> 02:16:26,950
stay like you know please don't increase
any of the weight values unless the you

828
02:16:26,950 --> 02:16:34,510
know improvement in the loss is worth it
and so generally speaking pretty much

829
02:16:34,510 --> 02:16:40,150
all state of the art models have both
dropout and weight decay and I don't

830
02:16:40,150 --> 02:16:46,540
claim to know like how to set each one
and how much of H to use to say like you

831
02:16:46,540 --> 02:16:53,200
it's worth trying both to go back to the
idea of embeddings is there any way to

832
02:16:53,200 --> 02:16:57,460
interpret the final to reduce it
embeddings like absolutely we're gonna
look at that next week I've it's super

833
02:16:59,170 --> 02:17:03,130
fun it turns out that you know we'll
learn what some of the worst movies of

834
02:17:03,129 --> 02:17:05,550
all time

835
02:17:06,620 --> 02:17:11,460
it's Letham it's that John Travolta
Scientology once my battleship earth or
something I think that was like the

836
02:17:12,660 --> 02:17:21,420
worst movie of all time according to our
beds to many recommendations for scaling

837
02:17:21,420 --> 02:17:27,030
the l2 penalty or is that kind of based
on how how wide the notes are how many

838
02:17:27,030 --> 02:17:32,640
notes about III have no suggestion at
all like I I kind of look for like
papers or cackle competitions or

839
02:17:35,280 --> 02:17:40,350
whatever similar and try to set up
frankly the same it seems like in a

840
02:17:40,350 --> 02:17:46,080
particular area like computer vision
object recognition it's like somewhere

841
02:17:46,080 --> 02:17:50,340
between one in neck four or one in egg
five seems to work you know

842
02:17:50,340 --> 02:17:57,120
actually in the Adam W paper the authors
point out that with this new approach it

843
02:17:57,120 --> 02:18:00,480
actually becomes like it seems to be
much more stable as to what the right
way to K amounts are so hopefully now

844
02:18:02,459 --> 02:18:06,149
when we start playing with it
we'll be able to have some definitive

845
02:18:06,150 --> 02:18:10,680
recommendations by the time we get to
part two all right well that's nine

846
02:18:10,680 --> 02:18:17,280
o'clock so this week you know practice
the thing that you're least familiar

847
02:18:17,280 --> 02:18:21,060
with so if it's like jacobians and
Hessians read about those if it's
broadcasting read about those if it's

848
02:18:23,309 --> 02:18:26,969
understanding python ooo read about that
you know try to implement your own

849
02:18:26,969 --> 02:18:32,219
custom layers read the faster higher
layers you know and and talk on the

850
02:18:32,219 --> 02:18:39,439
forum about anything that you find weird
or confusing alright see you next week

